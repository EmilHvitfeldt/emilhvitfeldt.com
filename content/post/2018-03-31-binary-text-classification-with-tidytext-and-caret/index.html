---
title: Binary text classification with Tidytext and caret
date: '2018-03-31'
categories: [tidytext, caret]
image:
  preview_only: true
---



<p>the scope of this blog post is to show how to do binary text classification using standard tools such as <code>tidytext</code> and <code>caret</code> packages. One of if not the most common binary text classification task is the spam detection (spam vs non-spam) that happens in most email services but has many other application such as language identification (English vs non-English).</p>
<p>In this post I’ll showcase 5 different classification methods to see how they compare with this data. The methods all land on the less complex side of the spectrum and thus does not include creating complex deep neural networks.</p>
<p>An expansion of this subject is multiclass text classification which I might write about in the future.</p>
<div id="packages" class="section level2">
<h2>Packages</h2>
<p>We load the packages we need for this project. <code>tidyverse</code> for general data science work, <code>tidytext</code> for text manipulation and <code>caret</code> for modeling.</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(caret)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>The data we will be using for this demonstration will be some English<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <a href="https://data.world/crowdflower/disasters-on-social-media">social media disaster tweets</a> discussed in <a href="https://arxiv.org/pdf/1705.02009.pdf">this article</a>.
It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.</p>
<pre class="r"><code>data &lt;- read_csv(&quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv&quot;)</code></pre>
<p>And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren’t classified, marked <code>"Can't Decide"</code> so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don’t actually have that much information in them as they are limited in characters and some only contain a couple of words.</p>
<p>We will at this stage remove what appears to be urls using some regex and <code>str_replace_all</code>, and we will select the columns <code>id</code>, <code>disaster</code> and <code>text</code>.</p>
<pre class="r"><code>data_clean &lt;- data %&gt;%
  filter(choose_one != &quot;Can&#39;t Decide&quot;) %&gt;%
  mutate(id = `_unit_id`,
         disaster = choose_one == &quot;Relevant&quot;,
         text = str_replace_all(text, &quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+&quot;, &quot;&quot;)) %&gt;%
  select(id, disaster, text)</code></pre>
<p>First we take a quick look at the distribution of classes and we see if the classes are balanced</p>
<pre class="r"><code>data_clean %&gt;%
  ggplot(aes(disaster)) +
  geom_bar()</code></pre>
<p>And we see that is fairly balanced so we don’t have to worry about sampling this time.</p>
<p>The representation we will be using in this post will be the <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words</a> representation in which we just count how many times each word appears in each tweet disregarding grammar and even word order (mostly).</p>
<p>We will construct a tf-idf vector model in which each unique word is represented as a column and each document (tweet in our case) is a row of the tf-idf values. This will create a very large matrix/data.frame (a column of each unique word in the total data set) which will overload a lot of the different models we can implement, furthermore will a lot of the words (or features in ML slang) not add considerably information. We have a trade off between information and computational speed.</p>
<p>First we will remove all the stop words, this will insure that common words that usually don’t carry meaning doesn’t take up space (and time) in our model. Next will we only look at words that appear in 10 different tweets. Lastly we will be looking at both <a href="https://en.wikipedia.org/wiki/N-gram">unigrams and bigrams</a> to hopefully get a better information extraction.</p>
<pre class="r"><code>data_counts &lt;- map_df(1:2,
                      ~ unnest_tokens(data_clean, word, text, 
                                      token = &quot;ngrams&quot;, n = .x)) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;) %&gt;%
  count(id, word, sort = TRUE)</code></pre>
<p>We will only look at words at appear in at least 10 different tweets.</p>
<pre class="r"><code>words_10 &lt;- data_counts %&gt;%
  group_by(word) %&gt;%
  summarise(n = n()) %&gt;% 
  filter(n &gt;= 10) %&gt;%
  select(word)</code></pre>
<p>we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix.</p>
<pre class="r"><code>data_dtm &lt;- data_counts %&gt;%
  right_join(words_10, by = &quot;word&quot;) %&gt;%
  bind_tf_idf(word, id, n) %&gt;%
  cast_dtm(id, word, tf_idf)</code></pre>
<p>This leaves us with 2993 features. We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction.</p>
<pre class="r"><code>meta &lt;- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %&gt;%
  left_join(data_clean[!duplicated(data_clean$id), ], by = &quot;id&quot;)</code></pre>
<p>We also create the index (based on the <code>meta</code> data.frame) to separate the data into a training and test set.</p>
<pre class="r"><code>set.seed(1234)
trainIndex &lt;- createDataPartition(meta$disaster, p = 0.8, list = FALSE, times = 1)</code></pre>
<p>since a lot of the methods take data.frames as inputs we will take the time and create these here:</p>
<pre class="r"><code>data_df_train &lt;- data_dtm[trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame()
data_df_test &lt;- data_dtm[-trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame()

response_train &lt;- meta$disaster[trainIndex]</code></pre>
<p>Now each row in the data.frame is a document/tweet (yay tidy principles!!).</p>
</div>
<div id="missing-tweets" class="section level2">
<h2>Missing tweets</h2>
<p>In the feature selection earlier we decided to turn our focus towards certain words and word-pairs, with that we also turned our focus AWAY from certain words. Since the tweets are fairly short in length it wouldn’t be surprising if a handful of the tweets completely skipped out focus as we noted earlier. Lets take a look at those tweets here.</p>
<pre class="r"><code>data_clean %&gt;%
  anti_join(meta, by = &quot;id&quot;) %&gt;%
  head(25) %&gt;%
  pull(text)</code></pre>
<p>We see that a lot of them appears to be part of urls that our regex didn’t detect, furthermore it appears that in those tweet the sole text was the url which wouldn’t have helped us in this case anyways.</p>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>Now that we have the data all clean and tidy we will turn our heads towards modeling. We will be using the wonderful <code>caret</code> package which we will use to employ the following models</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support vector machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/LogitBoost">LogitBoost</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_forest">Random forest</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">feed-forward neural networks</a></li>
</ul>
<p>These where chosen because of their frequent use ( <a href="http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf">why SVM are good at text classification</a> ) or because they are common in the classification field. They were also chosen because they where able to work with data with this number of variables in a reasonable time.</p>
<p>First time around we will not use a resampling method.</p>
<pre class="r"><code>trctrl &lt;- trainControl(method = &quot;none&quot;)</code></pre>
</div>
<div id="svm" class="section level2">
<h2>SVM</h2>
<p>The first model will be the <code>svmLinearWeights2</code> model from the <a href="https://cran.r-project.org/web/packages/LiblineaR/index.html">LiblineaR</a> package. Where we specify default parameters.</p>
<pre class="r"><code>svm_mod &lt;- train(x = data_df_train,
                 y = as.factor(response_train),
                 method = &quot;svmLinearWeights2&quot;,
                 trControl = trctrl,
                 tuneGrid = data.frame(cost = 1, 
                                       Loss = 0, 
                                       weight = 1))</code></pre>
<p>We predict on the test data set based on the fitted model.</p>
<pre class="r"><code>svm_pred &lt;- predict(svm_mod,
                    newdata = data_df_test)</code></pre>
<p>lastly we calculate the confusion matrix using the <code>confusionMatrix</code> function in the <code>caret</code> package.</p>
<pre class="r"><code>svm_cm &lt;- confusionMatrix(svm_pred, meta[-trainIndex, ]$disaster)
svm_cm</code></pre>
<p>and we get an accuracy of 0.7461646.</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>Naive-Bayes</h2>
<p>The second model will be the <code>naive_bayes</code> model from the <a href="https://cran.r-project.org/web/packages/naivebayes/index.html">naivebayes</a> package. Where we specify default parameters.</p>
<pre class="r"><code>nb_mod &lt;- train(x = data_df_train,
                y = as.factor(response_train),
                method = &quot;naive_bayes&quot;,
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                      usekernel = FALSE,
                                      adjust = FALSE))</code></pre>
<p>We predict on the test data set based on the fitted model.</p>
<pre class="r"><code>nb_pred &lt;- predict(nb_mod,
                   newdata = data_df_test)</code></pre>
<p>calculate the confusion matrix</p>
<pre class="r"><code>nb_cm &lt;- confusionMatrix(nb_pred, meta[-trainIndex, ]$disaster)
nb_cm</code></pre>
<p>and we get an accuracy of 0.5564854.</p>
</div>
<div id="logitboost" class="section level2">
<h2>LogitBoost</h2>
<p>The third model will be the <code>LogitBoost</code> model from the <a href="https://cran.r-project.org/web/packages/caTools/index.html">caTools</a> package. We don’t have to specify any parameters.</p>
<pre class="r"><code>logitboost_mod &lt;- train(x = data_df_train,
                        y = as.factor(response_train),
                        method = &quot;LogitBoost&quot;,
                        trControl = trctrl)</code></pre>
<p>We predict on the test data set based on the fitted model.</p>
<pre class="r"><code>logitboost_pred &lt;- predict(logitboost_mod,
                           newdata = data_df_test)</code></pre>
<p>calculate the confusion matrix</p>
<pre class="r"><code>logitboost_cm &lt;- confusionMatrix(logitboost_pred, meta[-trainIndex, ]$disaster)
logitboost_cm</code></pre>
<p>and we get an accuracy of 0.632729.</p>
</div>
<div id="random-forest" class="section level2">
<h2>Random forest</h2>
<p>The fourth model will be the <code>ranger</code> model from the <a href="https://cran.r-project.org/web/packages/ranger/index.html">caTools</a> package. Where we specify default parameters.</p>
<pre class="r"><code>rf_mod &lt;- train(x = data_df_train, 
                y = as.factor(response_train), 
                method = &quot;ranger&quot;,
                trControl = trctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])),
                                      splitrule = &quot;gini&quot;,
                                      min.node.size = 1))</code></pre>
<p>We predict on the test data set based on the fitted model.</p>
<pre class="r"><code>rf_pred &lt;- predict(rf_mod,
                   newdata = data_df_test)</code></pre>
<p>calculate the confusion matrix</p>
<pre class="r"><code>rf_cm &lt;- confusionMatrix(rf_pred, meta[-trainIndex, ]$disaster)
rf_cm</code></pre>
<p>and we get an accuracy of 0.7777778.</p>
</div>
<div id="nnet" class="section level2">
<h2>nnet</h2>
<p>The fifth and final model will be the <code>nnet</code> model from the <a href="https://cran.r-project.org/web/packages/nnet/index.html">caTools</a> package. Where we specify default parameters. We will also specify <code>MaxNWts = 5000</code> such that it will work. It will need to be more then the number of columns multiplied the size.</p>
<pre class="r"><code>nnet_mod &lt;- train(x = data_df_train,
                    y = as.factor(response_train),
                    method = &quot;nnet&quot;,
                    trControl = trctrl,
                    tuneGrid = data.frame(size = 1,
                                          decay = 5e-4),
                    MaxNWts = 5000)</code></pre>
<p>We predict on the test data set based on the fitted model.</p>
<pre class="r"><code>nnet_pred &lt;- predict(nnet_mod,
                     newdata = data_df_test)</code></pre>
<p>calculate the confusion matrix</p>
<pre class="r"><code>nnet_cm &lt;- confusionMatrix(nnet_pred, meta[-trainIndex, ]$disaster)
nnet_cm</code></pre>
<p>and we get an accuracy of 0.7173408.</p>
</div>
<div id="comparing-models" class="section level2">
<h2>Comparing models</h2>
<p>To see how the different models stack out we combine the metrics together in a <code>data.frame</code>.</p>
<pre class="r"><code>mod_results &lt;- rbind(
  svm_cm$overall, 
  nb_cm$overall,
  logitboost_cm$overall,
  rf_cm$overall,
  nnet_cm$overall
  ) %&gt;%
  as.data.frame() %&gt;%
  mutate(model = c(&quot;SVM&quot;, &quot;Naive-Bayes&quot;, &quot;LogitBoost&quot;, &quot;Random forest&quot;, &quot;Neural network&quot;))</code></pre>
<p>visualizing the accuracy for the different models with the red line being the “No Information Rate” that is, having a model that just picks the model common class.</p>
<pre class="r"><code>mod_results %&gt;%
  ggplot(aes(model, Accuracy)) +
  geom_point() +
  ylim(0, 1) +
  geom_hline(yintercept = mod_results$AccuracyNull[1],
             color = &quot;red&quot;)</code></pre>
<p>As you can see all but one approach does better then the “No Information Rate” on its first try before tuning the hyperparameters.</p>
</div>
<div id="tuning-hyperparameters" class="section level2">
<h2>Tuning hyperparameters</h2>
<p>After trying out the different models we saw quite a spread in performance. But it important to remember that the results might be because of good/bad default hyperparameters. There are a few different ways to handle this problem. I’ll show on of them here, grid search, on the SVM model so you get the idea.</p>
<p>We will be using 10-fold <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a> and 3 repeats, which will slow down the procedure, but will try to limit and reduce overfitting. We will be using <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search">grid search</a> approach to find optimal hyperparameters. For the sake of time have to fixed 2 of the hyperparameters and only let one vary. Remember that the time it takes to search though all combinations take a long time when then number of hyperparameters increase.</p>
<pre class="r"><code>fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,
                           number = 3,
                           repeats = 3,
                           search = &quot;grid&quot;)</code></pre>
<p>We have decided to limit the search around the <code>weight</code> parameter’s default value 1.</p>
<pre class="r"><code>svm_mod &lt;- train(x = data_df_train,
                 y = as.factor(response_train),
                 method = &quot;svmLinearWeights2&quot;,
                 trControl = fitControl,
                 tuneGrid = data.frame(cost = 0.01, 
                                       Loss = 0, 
                                       weight = seq(0.5, 1.5, 0.1)))</code></pre>
<p>and once it have finished running we can plot the train object to see which value is highest.</p>
<pre class="r"><code>plot(svm_mod)</code></pre>
<p>And we see that it appear to be just around 1. It is important to search multiple parameters at the SAME TIME as it can not be assumed that the parameters are independent of each others. Only reason I didn’t do that here was to same the time.</p>
<p>I will leave to you the reader to find out which of the models have the highest accuracy after doing parameter tuning.</p>
<p>I hope you have enjoyed this overview of binary text classification.</p>
<details closed>
<p><summary> <span title="Click to Expand"> session information </span> </summary></p>
<pre class="r"><code>
─ Session info ───────────────────────────────────────────────────────────────
 setting  value                       
 version  R version 3.6.0 (2019-04-26)
 os       macOS Mojave 10.14.6        
 system   x86_64, darwin15.6.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 ctype    en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2020-04-23                  

─ Packages ───────────────────────────────────────────────────────────────────
 ! package     * version date       lib source        
 P assertthat    0.2.1   2019-03-21 [?] CRAN (R 3.6.0)
 P backports     1.1.6   2020-04-05 [?] CRAN (R 3.6.0)
 P blogdown      0.18    2020-03-04 [?] CRAN (R 3.6.0)
 P bookdown      0.18    2020-03-05 [?] CRAN (R 3.6.0)
 P cli           2.0.2   2020-02-28 [?] CRAN (R 3.6.0)
 P clipr         0.7.0   2019-07-23 [?] CRAN (R 3.6.0)
 P crayon        1.3.4   2017-09-16 [?] CRAN (R 3.6.0)
 P desc          1.2.0   2018-05-01 [?] CRAN (R 3.6.0)
 P details     * 0.2.1   2020-01-12 [?] CRAN (R 3.6.0)
 P digest        0.6.25  2020-02-23 [?] CRAN (R 3.6.0)
 P evaluate      0.14    2019-05-28 [?] CRAN (R 3.6.0)
 P fansi         0.4.1   2020-01-08 [?] CRAN (R 3.6.0)
 P glue          1.4.0   2020-04-03 [?] CRAN (R 3.6.0)
 P htmltools     0.4.0   2019-10-04 [?] CRAN (R 3.6.0)
 P httr          1.4.1   2019-08-05 [?] CRAN (R 3.6.0)
 P knitr       * 1.28    2020-02-06 [?] CRAN (R 3.6.0)
 P magrittr      1.5     2014-11-22 [?] CRAN (R 3.6.0)
 P png           0.1-7   2013-12-03 [?] CRAN (R 3.6.0)
 P R6            2.4.1   2019-11-12 [?] CRAN (R 3.6.0)
 P Rcpp          1.0.4.6 2020-04-09 [?] CRAN (R 3.6.0)
   renv          0.9.3   2020-02-10 [1] CRAN (R 3.6.0)
 P rlang         0.4.5   2020-03-01 [?] CRAN (R 3.6.0)
 P rmarkdown     2.1     2020-01-20 [?] CRAN (R 3.6.0)
 P rprojroot     1.3-2   2018-01-03 [?] CRAN (R 3.6.0)
 P sessioninfo   1.1.1   2018-11-05 [?] CRAN (R 3.6.0)
 P stringi       1.4.6   2020-02-17 [?] CRAN (R 3.6.0)
 P stringr       1.4.0   2019-02-10 [?] CRAN (R 3.6.0)
 P withr         2.1.2   2018-03-15 [?] CRAN (R 3.6.0)
 P xfun          0.13    2020-04-13 [?] CRAN (R 3.6.2)
 P xml2          1.3.0   2020-04-01 [?] CRAN (R 3.6.2)
 P yaml          2.2.1   2020-02-01 [?] CRAN (R 3.6.0)

[1] /Users/emilhvitfeldthansen/Desktop/blogv4/renv/library/R-3.6/x86_64-apple-darwin15.6.0
[2] /private/var/folders/m0/zmxymdmd7ps0q_tbhx0d_26w0000gn/T/RtmplZt2GU/renv-system-library

 P ── Loaded and on-disk path mismatch.
</code></pre>
</details>
<p><br></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">#benderrule</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
