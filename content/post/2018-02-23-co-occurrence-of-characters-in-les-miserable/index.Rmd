---
title: Co Occurrence of Characters in Les Miserable
date: '2018-02-23'
categories: [spacyr, ggplot2]
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE, 
  cache = TRUE,
  collapse = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
knit_hooks$set(optipng = hook_optipng)
opts_chunk$set("optipng" = "-o5")
```

*This code have been lightly revised to make sure it works as of 2020-04-21.*  

## What are we doing?

The inspiration for this post is [this beutiful vizualization](https://bost.ocks.org/mike/miserables/) from [Mike Bostock](https://bost.ocks.org/mike/). It nicely visualizes the co-occurrence of characters (when two characters appear in the same chapter) in the novel [Les Misérables](https://en.wikipedia.org/wiki/Les_Mis%C3%A9rables) by [Victor Hugo](https://en.wikipedia.org/wiki/Victor_Hugo) using the data collected by [Jacques Bertin](https://en.wikipedia.org/wiki/Jacques_Bertin) (and his assistants).  

The way this post will differentiate itself from this is that we are going to collect the data ourselves using [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition). Named entity recognition is the discipline of location and classifying named entities in text. Furthermore will we also try to cluster the characters according to their appearance in the novel. 

disclaimer! I have of the time of writing this analysis not read of familiarized myself with Les Misérables in a attempt to show how a blind text analysis would run.

## Loading package and backend

for this we will need `tidyverse` for general data science tasks, `spacyr` for the named entity recognition and `igraph` for some graph related transformation.

```{r, message=FALSE}
library(tidyverse)
library(spacyr)
library(igraph)
```

We will be using the [spacy](https://spacy.io/) NLP back-end as the parser for this analysis since it provides named entity recognition as one of its functionalities.

## Data

Les Miserable is quite a long novel, in the terms of words and pages, however due to its age is it in the public domain and is easily available on [Project Gutenberg](https://www.gutenberg.org/).

```{r, message=FALSE}
lesmis_raw <- gutenbergr::gutenberg_download(135)
```

Looking thought the beginning of the text we notice how a large part of the beginning of the document is table of content and other information that isn't of interest in this analysis. Manually checking leads to be discard the first 650 lines of the data. We will also add a `chapter` column using a regex.

```{r}
lesmis_line <- lesmis_raw %>%
  slice(-(1:650)) %>%
  mutate(chapter = cumsum(str_detect(text, "CHAPTER ")))
```

For the use in `cnlp_annotate()` we need a data.frame where each row is a full chapter, with the 2 necessary columns `id` and `text`. This is accomplished using a simple `map`.

```{r}
lesmis <- map_df(seq_len(max(lesmis_line$chapter)),
                 ~ tibble(id = .x,
                          text = lesmis_line %>% 
                                   filter(chapter == .x) %>% 
                                   pull(text) %>% 
                                   paste(collapse = " ")))
```

Now we are all ready to run the spacy parser which will only take a couple of minutes.

```{r}
lesmis_obj <- spacy_parse(lesmis$text)
```

the output we are given nothing more then a simple tibble

```{r, eval=FALSE}
lesmis_obj
```

```{r, echo=FALSE}
head(lesmis_obj)
```

the entity information can be extracted using `entity_extract()`

```{r, eval=FALSE}
entity_extract(lesmis_obj)
```

```{r, echo=FALSE}
head(entity_extract(lesmis_obj))
```

We see quite a few different `entity_type`s, in fact lets take a quick look at the different types that are in this text

```{r}
entity_extract(lesmis_obj) %>%
  pull(entity_type) %>%
  unique()
```

This labeling is explained [here](https://spacy.io/api/annotation#named-entities). After a bit of investigating I have decided that we only will look at "PERSON" and "ORG" (which is due in part to Napoleon being classified as a organisation.) Furthermore I will limit further analysis to about the 50 most mentioned characters. The rational behind this is that it hopefully would capture most of the important characters, with the weight that characters that are mentioned sparingly but consistently is more important then characters with high density in a few chapter. We will include a few more characters in case we have to exclude some of them after looking.

```{r}
top_person_df <- entity_extract(lesmis_obj) %>%
  filter(entity_type %in% c("ORG", "PERSON")) %>%
  count(entity, sort = TRUE) %>%
  slice(seq_len(60))

top_person_vec <- top_person_df %>% pull(entity)
top_person_vec
```

After looking we see a few things we would like to fix before moving on. Firstly is "CHAPTER IV" and "CHAPTER VI" wrongly both classified as "ORG"s. " ", "-" and "exclaimed:--" and "Monsieur" have also been misclassified. "Jean Valjean's" have been classified differently then "Jean Valjean" which is also the case with "Fauchelevent" and "M. Fauchelevent", "M. Madeleine" and "Madeleine", "M. Gillenormand", "Gillenormand" and "Mademoiselle Gillenormand". We will remove the miss-classifications here, and create a list of all the characters with all of their names. The list is named with the character's main name for later subsetting. 

```{r}
top_person_vec_clean <- top_person_vec[-c(9, 13, 29, 34, 42, 56)] 

complications <- list(c("Jean Valjean", "Jean Valjean's"),
                      c("Fauchelevent", "M. Fauchelevent"),
                      c("Madeleine", "M. Madeleine"),
                      c("Gillenormand", "M. Gillenormand", "Mademoiselle Gillenormand"))

characters <- setdiff(top_person_vec_clean, unlist(complications)) %>%
  as.list() %>%
  c(complications)

names(characters) <- map_chr(characters, ~ .x[1])
```

We expand the grid of all possible co occurrences and count how many times they both occur within a chapter.

```{r}
co_occurrence <- expand.grid(map_chr(characters, ~ .x[1]), 
                             map_chr(characters, ~ .x[1])) %>%
  set_names(c("person1", "person2")) %>%
  mutate(cooc = map2_dbl(person1, person2,
                         ~ sum(str_detect(lesmis$text, str_c(.x, collapse = "|")) & 
                               str_detect(lesmis$text, str_c(.y, collapse = "|")))))
```

## Visualize

now that we have the co occurrence data we can make some visualizations!! (I will take care of labels etc in the end. Hang on!)

```{r}
co_occurrence %>%
  ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()
```

So at a first glance is it hard to see anything due to the default color-scale and the fact that a couple of people, Jean Valjean and Marius, appear in a much higher number of chapters (perhaps they are main characters?). To get a more manageable scale we disregard co occurrence if they have been in less then 5 chapters together(remember that there are a total of 365 chapters in novel).

```{r}
co_occurrence_1 <- co_occurrence %>%
  mutate(cooc = ifelse(cooc > 5, log(cooc), NA))

co_occurrence_1 %>%
    ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()
```

Now we finally see some of the fruit of our earlier work. It is definitely clear that there are groups of people that might form communities but it is unclear which and how many from this heat-map by itself. We would like to reorder the axis's in the hope that it would create more clarity.  

This data here can be seen as a [Adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix) here the row numbers are vertices and the tiles-values are the edges connecting them. So in a sense we would like to do some cluster analysis on this graph. This can be done by doing some Spectral Graph Partitioning in which we calculate the eigenvectors and sort the vertices by the second smallest eigenvector.

```{r}
eigen <- co_occurrence_1 %>%
#  mutate(cooc = !is.na(cooc)) %>%
  igraph::graph_from_data_frame() %>%
  igraph::as_adj() %>%
  eigen()

eigenvec2_sort <- data.frame(eigen = eigen$vectors[, length(eigen$values) - 1]) %>%
  mutate(row = row_number(),
         names = names(characters)) %>%
  arrange(eigen)

eigen_names <- eigenvec2_sort %>% pull(names)
```

We use sorted names to re-level the factors in the co occurrence data and see if it reveals more structure.

```{r}
co_occurrence_1 %>%
  mutate(person1 = factor(person1, levels = eigen_names),
         person2 = factor(person2, levels = eigen_names)) %>%
    ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()
```

it isn't much but it appears to have moved the data slight closer to the diagonal. We will still need to locate some communities in this data. this can be done using the plotted eigenvector.

```{r}
eigenvec2_sort %>% pull(eigen) %>% plot(type = "o")
```

And what we are looking at is not their position but at the jumps. There can more easily be seen when we look at the diffs

```{r}
eigenvec2_sort %>% pull(eigen) %>% diff() %>% plot()
abline(h = 0.02)
```

And after playing around a little it seems that `0.02` is a appropriate cutoff. 

```{r}
cummunity_df <- eigenvec2_sort %>%
  mutate(community = c(0, diff(eigen) > 0.02) %>% cumsum()) %>%
  select(names, community)
```

We will color-code the final visualization according to this clustering. So with a couple of joins

```{r}
co_occurrence_comm <- co_occurrence_1 %>%
  filter(!is.na(cooc)) %>%
  mutate(person1_chr = as.character(person1),
         person2_chr = as.character(person2),
         person1 = factor(person1, levels = eigen_names),
         person2 = factor(person2, levels = eigen_names)) %>%
  left_join(cummunity_df, by = c("person1_chr" = "names")) %>%
  left_join(cummunity_df, by = c("person2_chr" = "names")) %>%
  mutate(community = ifelse(community.x == community.y, community.x, NA),
         community = ifelse(!is.na(cooc), community, NA))
```

With a couple of final touch-ups and we arrive at the final result:

```{r, fig.width=7, fig.height=7, fig.asp = 1}
co_occurrence_comm %>%
  ggplot(aes(person1, person2, alpha = cooc, fill = factor(community))) +
  geom_tile(color = "grey50") +
  scale_alpha(range = c(0.5, 1)) +
  scale_fill_brewer(palette = "Set1", na.value = "grey50") +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill = "none", alpha = "none") +
  coord_fixed() +
  labs(x = NULL, y = NULL, 
       title = "Les Misérables Co-occurrence", 
       subtitle = "with color-coded communities")
```

## Conclusion

While I wasn't able to find as full clusters as Jacques Bertin I still managed to get quite a lot of information out of the text regardless. I had fun in the progress and there are many more things I see myself doing with this new data set and `spacyr`.  
And while I couldn't find a good way to include it in the main body of text. I almost finished the main analysis before realizing that [Monsieur](https://en.wikipedia.org/wiki/Monsieur) means. Mention your mistakes in your posts so others can learn from them!

```{r details, echo=FALSE}
library(details) 

sessioninfo::session_info() %>%
  details::details(summary = 'session information')
```
