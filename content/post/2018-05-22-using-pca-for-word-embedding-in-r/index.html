---
title: Using PCA for word embedding in R
date: '2018-05-22'
categories: [tidytext]
---



<p>In my earlier post on <a href="https://www.hvitfeldt.me/2018/03/binary-text-classification-with-tidytext-and-caret/">binary text classification</a> was one of the problems that occurred was the sheer size of the data when trying to fit a model. The bag of words method of having each column describe the occurrence of a specific word in each document (row) is appealing from a mathematical perspective, but gives rise for large sparse matrices which aren’t handled well by some models in R. This leads to slow running code at best and crashing at worst.</p>
<p>We will try to combat this problem by using something called <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> which is a general term for the process of mapping textural information to a lower dimensional space. This is a special case of dimensionality reduction, and we will use the simple well known method <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a> for our word embedding today. We are essentially trying to squeeze as much information into as little space as possible such that our models can run in a reasonable time.</p>
<p>We will use the same data as in the earlier post, and the PCA procedure is very inspired by Julia Silge recent post <a href="https://juliasilge.com/blog/stack-overflow-pca/">Understanding PCA using Stack Overflow data</a> which you should read if you haven’t already!!</p>
<div id="data-prepossessing" class="section level2">
<h2>Data prepossessing</h2>
<p>We will use standard <code>tidyverse</code> tool set for this post. We will use <code>randomForest</code> model as this approach should be much faster.</p>
<pre class="r"><code>library(tidyverse)
## Warning: package &#39;tibble&#39; was built under R version 3.6.2
library(tidytext)
library(broom)
library(randomForest)</code></pre>
<p>The data we will be using for this demonstration will be some <a href="https://data.world/crowdflower/disasters-on-social-media">social media disaster tweets</a> discussed in <a href="https://arxiv.org/pdf/1705.02009.pdf">this article</a>.
It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.</p>
<pre class="r"><code>data &lt;- read_csv(&quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv&quot;)</code></pre>
<p>And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren’t classified, marked <code>"Can't Decide"</code> so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don’t actually have that much information in them as they are limited in characters and some only contain a couple of words.</p>
<p>We will at this stage remove what appears to be urls using some regex and <code>str_replace_all</code>, and we will select the columns <code>id</code>, <code>disaster</code> and <code>text</code>.</p>
<pre class="r"><code>data_clean &lt;- data %&gt;%
  filter(choose_one != &quot;Can&#39;t Decide&quot;) %&gt;%
  mutate(id = `_unit_id`,
         disaster = choose_one == &quot;Relevant&quot;,
         text = str_replace_all(text, &quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+&quot;, &quot;&quot;)) %&gt;%
  select(id, disaster, text)</code></pre>
<p>We then extract all unigrams, bigram and remove stopwords.</p>
<pre class="r"><code>data_counts &lt;- map_df(1:2,
                      ~ unnest_tokens(data_clean, word, text, 
                                      token = &quot;ngrams&quot;, n = .x)) %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)</code></pre>
<p>We will only focus on the top 10000 most used words for the remainder of the analysis.</p>
<pre class="r"><code>top10000 &lt;- data_counts %&gt;%
  count(word, sort = TRUE) %&gt;%
  slice(1:10000) %&gt;%
  select(word)</code></pre>
<p>we will then count the words again, but this time we will count the word occurrence within each document and remove the underused words.</p>
<pre class="r"><code>unnested_words &lt;- data_counts %&gt;%
  count(id, word, sort = TRUE) %&gt;%
  inner_join(top10000, by = &quot;word&quot;)</code></pre>
<p>We then cast the data.frame to a sparse matrix.</p>
<pre class="r"><code>sparse_word_matrix &lt;- unnested_words %&gt;%
  cast_sparse(id, word, n)</code></pre>
<p>In the last post we used this matrix for the modeling, but the size was quite obstacle.</p>
<pre class="r"><code>dim(sparse_word_matrix)
## [1] 10829 10000</code></pre>
<p>We have a row for each document and a row for each of the top 10000 words, but most of the elements are empty so each of the variables don’t contain much information. We will do word embedding by applying PCA to the sparse word count matrix. Like Julia Silge we will use the wonderful <a href="https://bwlewis.github.io/irlba/">irlba</a> package that facilities PCA on sparse matrices. First we scale the matrix and then we apply PCA where we request 64 columns.</p>
<p>This stage will take some time, but that is the trade-off we will be making when using word embedding. We take some computation time up front in exchange for quick computation later down the line.</p>
<pre class="r"><code>word_scaled &lt;- scale(sparse_word_matrix)
word_pca &lt;- irlba::prcomp_irlba(word_scaled, n = 64)</code></pre>
<p>Then we will create a meta data.frame to take care of tweets that disappeared when we cleaned them earlier.</p>
<pre class="r"><code>meta &lt;- tibble(id = as.numeric(dimnames(sparse_word_matrix)[[1]])) %&gt;%
  left_join(data_clean[!duplicated(data_clean$id), ], by = &quot;id&quot;)</code></pre>
<p>Now we combine the PCA matrix with proper response variable (disaster/no-disaster) with the addition ot a training/testing split variable.</p>
<pre class="r"><code>class_df &lt;- data.frame(word_pca$x) %&gt;%
  mutate(response = factor(meta$disaster),
         split = sample(0:1, NROW(meta), replace = TRUE, prob = c(0.2, 0.8)))</code></pre>
<p>We now have a data frame with 64 explanatory variables instead of the 10000 we started with. This a huge reduction which hopefully should pay off. For this demonstration will we try using two kinds of models. Standard logistic regression and a random forest model. Logistic regression is a good baseline which should be blazing fast now since the reduction have taken place and the random forest model which generally was quite slow should be more manageable this time around.</p>
<pre class="r"><code>model &lt;- glm(response ~ ., 
             data = filter(class_df, split == 1), 
             family = binomial)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

y_pred &lt;- predict(model, 
                  type = &quot;response&quot;,
                  newdata = filter(class_df, split == 0) %&gt;% select(-response))
## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading

y_pred_logical &lt;- if_else(y_pred &gt; 0.5, 1, 0)
(con &lt;- table(y_pred_logical, filter(class_df, split == 0) %&gt;% pull(response)))
##               
## y_pred_logical FALSE TRUE
##              0  1180  631
##              1    82  330
sum(diag(con)) / sum(con)
## [1] 0.6792623</code></pre>
<p>it work fairly quickly and we get a decent accuracy of 68%. Remember this method isn’t meant to improve the accuracy but rather to improve the computational time.</p>
<pre class="r"><code>model &lt;- randomForest(response ~ ., 
                      data = filter(class_df, split == 1))

y_pred &lt;- predict(model, 
                  type = &quot;class&quot;,
                  newdata = filter(class_df, split == 0) %&gt;% select(-response))

(con &lt;- table(y_pred, filter(class_df, split == 0) %&gt;% pull(response)))
##        
## y_pred  FALSE TRUE
##   FALSE  1106  379
##   TRUE    156  582
sum(diag(con)) / sum(con)
## [1] 0.7593342</code></pre>
<p>This one takes slightly longer to run due to the number of trees, but it does give us the nifty 76% accuracy which is pretty good considering we only look at tweets.</p>
<p>And this is all that there is to it! The dimensionality reduction method was able to reduce the number of variables while retaining most of the information within those variables such that we can run our procedures at a faster phase without much loss. There is still a lot of individual improvements to be done if this was to be used further, both in terms of hyper-parameter selection in the modeling choices but also the number of PCA variables that should be used in the final modelling. Remember that this is just one of the more simpler methods, with more advanced word representation methods being <a href="https://nlp.stanford.edu/projects/glove/">glove</a> and <a href="https://www.tensorflow.org/tutorials/word2vec">word2vec</a>.</p>
</div>
<div id="data-viz" class="section level2">
<h2>Data viz</h2>
<p>Since Julia did most of the legwork for the visualizations so we will take a look at how each of the words contribute to the first four components.</p>
<pre class="r"><code>tidied_pca &lt;- bind_cols(Tag = colnames(word_scaled),
                        tidy(word_pca$rotation)) %&gt;%
    gather(PC, Contribution, PC1:PC4)
## Warning: &#39;tidy.matrix&#39; is deprecated.
## See help(&quot;Deprecated&quot;)

tidied_pca %&gt;% 
    filter(PC %in% paste0(&quot;PC&quot;, 1:4)) %&gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE) +
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank()) + 
    labs(x = &quot;Words&quot;,
         y = &quot;Relative importance in each principal component&quot;) +
    facet_wrap(~ PC, ncol = 2)</code></pre>
<p><img src="/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-11-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>What we see is quite different then what Julia found in her study. We have just a few words doing most of the contributions in each of component. Lets zoom in to take a look at the words with the most influence on the different components:</p>
<pre class="r"><code>map_df(c(-1, 1) * 20,
    ~ tidied_pca %&gt;%
        filter(PC == &quot;PC1&quot;) %&gt;% 
        top_n(.x, Contribution)) %&gt;%
    mutate(Tag = reorder(Tag, Contribution)) %&gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
          axis.ticks.x = element_blank()) +
    labs(x = &quot;Words&quot;,
         y = &quot;Relative importance in principle component&quot;,
         title = &quot;PC1&quot;)</code></pre>
<p><img src="/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-12-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>We would like to see some sensible separation between the positive words and the negative words (with regard to contribution). However I haven’t been able to come up with a meaning full grouping for the first 3 components. The fourth on the other hand have all the positive influencing words containing numbers in one way or another.</p>
<pre class="r"><code>map_df(c(-1, 1) * 20,
    ~ tidied_pca %&gt;%
        filter(PC == &quot;PC4&quot;) %&gt;% 
        top_n(.x, Contribution)) %&gt;%
    mutate(Tag = reorder(Tag, Contribution)) %&gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
          axis.ticks.x = element_blank()) +
    labs(x = &quot;Words&quot;,
         y = &quot;Relative importance in principle component&quot;,
         title = &quot;PC4&quot;)</code></pre>
<p><img src="/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-13-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>This is all I have for this time. Hope you enjoyed it!</p>
</div>
