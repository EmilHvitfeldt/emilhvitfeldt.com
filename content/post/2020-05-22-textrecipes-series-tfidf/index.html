---
title: 'Textrecipes series: TF-IDF'
date: '2020-05-22'
slug: textrecipes-series-tfidf
categories: [tidymodels, textrecipes, textrecipes series, tidytuesday]
image:
  caption: 'Photo by [Kym Ellis](https://unsplash.com/@kymellis) on [Unsplash](https://unsplash.com/)'
  preview_only: false
---



<p>This is the third blog post in the <a href="https://github.com/tidymodels/textrecipes">textrecipes</a> series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be showcasing how to perform <a href="http://www.tfidf.com/">term frequency-inverse document frequency</a> (Tf-IDF for short).</p>
<div id="packages" class="section level2">
<h2>Packages üì¶</h2>
<p>The packages used in the post shouldn‚Äôt come to any surprise if you have been following the series. <a href="https://www.tidymodels.org/">tidymodels</a> for modeling, tidyverse for EDA, <a href="https://textrecipes.tidymodels.org/">textrecipes</a> for text preprocessing, vip for visualizing variable importance, and doParallel to parallelize the hyperparameter tuning.</p>
<pre class="r"><code>library(tidymodels)
library(tidyverse)
library(textrecipes)
library(vip)
library(doParallel)
theme_set(theme_minimal())</code></pre>
</div>
<div id="exploring-the-data" class="section level2">
<h2>Exploring the data ‚õè</h2>
<p>We will be using a #tidytuesday dataset from almost a year ago, it contains a lot of <a href="https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-28">wine reviews</a>. <a href="https://twitter.com/drob">David Robinson</a> did a very nice screen about this dataset</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/AQzZNIyjyWM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>David goes into a lot of detail explaining what he is doing and I highly recommend to watch this one if you are interested in using text in regression.
Fortunately he didn‚Äôt use tidymodels so this post will bring a little something new.
Our goal for this post is to build a model that predicts the score (denotes <code>points</code>) a particular wine has.</p>
<pre class="r"><code>wine_ratings &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv&quot;)
## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<p>We load in the data with <code>read_csv()</code> and immediately use <code>glimpse()</code> to get an idea of the data we have to work with</p>
<pre class="r"><code>glimpse(wine_ratings)
## Rows: 129,971
## Columns: 14
## $ X1                    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1‚Ä¶
## $ country               &lt;chr&gt; &quot;Italy&quot;, &quot;Portugal&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;Spain&quot;,‚Ä¶
## $ description           &lt;chr&gt; &quot;Aromas include tropical fruit, broom, brimston‚Ä¶
## $ designation           &lt;chr&gt; &quot;Vulk√† Bianco&quot;, &quot;Avidagos&quot;, NA, &quot;Reserve Late H‚Ä¶
## $ points                &lt;dbl&gt; 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87,‚Ä¶
## $ price                 &lt;dbl&gt; NA, 15, 14, 13, 65, 15, 16, 24, 12, 27, 19, 30,‚Ä¶
## $ province              &lt;chr&gt; &quot;Sicily &amp; Sardinia&quot;, &quot;Douro&quot;, &quot;Oregon&quot;, &quot;Michig‚Ä¶
## $ region_1              &lt;chr&gt; &quot;Etna&quot;, NA, &quot;Willamette Valley&quot;, &quot;Lake Michigan‚Ä¶
## $ region_2              &lt;chr&gt; NA, NA, &quot;Willamette Valley&quot;, NA, &quot;Willamette Va‚Ä¶
## $ taster_name           &lt;chr&gt; &quot;Kerin O‚ÄôKeefe&quot;, &quot;Roger Voss&quot;, &quot;Paul Gregutt&quot;, ‚Ä¶
## $ taster_twitter_handle &lt;chr&gt; &quot;@kerinokeefe&quot;, &quot;@vossroger&quot;, &quot;@paulgwine¬†&quot;, NA‚Ä¶
## $ title                 &lt;chr&gt; &quot;Nicosia 2013 Vulk√† Bianco  (Etna)&quot;, &quot;Quinta do‚Ä¶
## $ variety               &lt;chr&gt; &quot;White Blend&quot;, &quot;Portuguese Red&quot;, &quot;Pinot Gris&quot;, ‚Ä¶
## $ winery                &lt;chr&gt; &quot;Nicosia&quot;, &quot;Quinta dos Avidagos&quot;, &quot;Rainstorm&quot;, ‚Ä¶</code></pre>
<p>This dataset barely contains any numeric variables.
The only numeric is the price. As with many prices in data, it is a good idea to log transform them since they are highly skewed</p>
<pre class="r"><code>wine_ratings %&gt;%
  mutate(price_log = log(price)) %&gt;%
  pivot_longer(c(price, price_log)) %&gt;%
  ggplot(aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~name, scales = &quot;free_x&quot;) 
## Warning: Removed 17992 rows containing non-finite values (stat_bin).</code></pre>
<p><img src="/post/2020-05-22-textrecipes-series-tfidf/index_files/figure-html/unnamed-chunk-4-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>Since most of the data most likely will be factors let us take a look at the cardinality of each variable</p>
<pre class="r"><code>map_int(wine_ratings, n_distinct)
##                    X1               country           description 
##                129971                    44                119955 
##           designation                points                 price 
##                 37980                    21                   391 
##              province              region_1              region_2 
##                   426                  1230                    18 
##           taster_name taster_twitter_handle                 title 
##                    20                    16                118840 
##               variety                winery 
##                   708                 16757</code></pre>
<p>But wait! the number of unique descriptions is not the same as the number of rows.
This seems very odd since they will be multiple sentences long and the likelihood of two different people writing the exact same description is very low.</p>
<p>let us take a selection of duplicated descriptions and see if anything stands out.</p>
<pre class="r"><code>wine_ratings %&gt;%
  filter(duplicated(description)) %&gt;%
  slice(1:3) %&gt;%
  pull(description)
## [1] &quot;This is weighty, creamy and medium to full in body. It has plenty of lime and pear flavors, plus slight brown sugar and vanilla notes.&quot;                                                             
## [2] &quot;There&#39;s a touch of toasted almond at the start, but then this Grillo revs up in the glass to deliver notes of citrus, stone fruit, crushed stone and lemon tart. The mouthfeel is crisp and simple.&quot;
## [3] &quot;Lightly herbal strawberry and raspberry aromas are authentic and fresh. On the palate, this is light and juicy, with snappy, lean flavors of red fruit and dry spice. The finish is dry and oaky.&quot;</code></pre>
<p>as we feared these are pretty specific and would be unlikely to be duplications at random.
We will assume that this problem is a scraping error and remove the duplicate entries.
Additionally, some of the points values are missing, since this is our target variable will I remove those data points as well.</p>
<div class="warning">
<p>If you are working on a real project you shouldn‚Äôt simply delete observations like that. Both of these errors smell a little bit like bad scraping so your first course of action should be testing your data pipeline for errors.</p>
</div>
<div class="note">
<p>I found out about the issue with duplicate descriptions when I was browsing through other people‚Äôs analyses of the dataset.</p>
</div>
<p>Before we do any more analysis, let us remove the troublesome observations.</p>
<pre class="r"><code>wine_ratings &lt;- wine_ratings %&gt;%
  filter(!duplicated(description), !is.na(price))

wine_ratings
## # A tibble: 111,567 x 14
##       X1 country description designation points price province region_1 region_2
##    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   
##  1     1 Portug‚Ä¶ This is ri‚Ä¶ Avidagos        87    15 Douro    &lt;NA&gt;     &lt;NA&gt;    
##  2     2 US      Tart and s‚Ä¶ &lt;NA&gt;            87    14 Oregon   Willame‚Ä¶ Willame‚Ä¶
##  3     3 US      Pineapple ‚Ä¶ Reserve La‚Ä¶     87    13 Michigan Lake Mi‚Ä¶ &lt;NA&gt;    
##  4     4 US      Much like ‚Ä¶ Vintner&#39;s ‚Ä¶     87    65 Oregon   Willame‚Ä¶ Willame‚Ä¶
##  5     5 Spain   Blackberry‚Ä¶ Ars In Vit‚Ä¶     87    15 Norther‚Ä¶ Navarra  &lt;NA&gt;    
##  6     6 Italy   Here&#39;s a b‚Ä¶ Belsito         87    16 Sicily ‚Ä¶ Vittoria &lt;NA&gt;    
##  7     7 France  This dry a‚Ä¶ &lt;NA&gt;            87    24 Alsace   Alsace   &lt;NA&gt;    
##  8     8 Germany Savory dri‚Ä¶ Shine           87    12 Rheinhe‚Ä¶ &lt;NA&gt;     &lt;NA&gt;    
##  9     9 France  This has g‚Ä¶ Les Natures     87    27 Alsace   Alsace   &lt;NA&gt;    
## 10    10 US      Soft, supp‚Ä¶ Mountain C‚Ä¶     87    19 Califor‚Ä¶ Napa Va‚Ä¶ Napa    
## # ‚Ä¶ with 111,557 more rows, and 5 more variables: taster_name &lt;chr&gt;,
## #   taster_twitter_handle &lt;chr&gt;, title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;</code></pre>
<p>Countries look like it would be important to include, doing a little bar chart reveals a high imbalance in where the wines are coming from.
We will definitely need to weed out some of the low count countries</p>
<pre class="r"><code>wine_ratings %&gt;% 
  count(country, sort = TRUE) %&gt;%
  ggplot(aes(n, fct_reorder(country, n), label = country)) +
  geom_col() +
  geom_text(hjust = 0, nudge_x = 1000)
## Warning: Removed 1 rows containing missing values (geom_text).</code></pre>
<p><img src="/post/2020-05-22-textrecipes-series-tfidf/index_files/figure-html/unnamed-chunk-8-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>This dataset is restricted to review of wine that scored 80 points or more,</p>
<pre class="r"><code>wine_ratings %&gt;% 
  ggplot(aes(points)) +
  geom_bar()</code></pre>
<p><img src="/post/2020-05-22-textrecipes-series-tfidf/index_files/figure-html/unnamed-chunk-9-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>It looks like the 80 wasn‚Äôt as hard cutoff, and the points even look bell-shaped.</p>
<p>I‚Äôll be using <code>tester_name</code> and <code>variety</code> as well in the final analysis.</p>
</div>
<div id="modeling-Ô∏è" class="section level2">
<h2>Modeling ‚öôÔ∏è</h2>
<p>We start by doing a simple training test split of the data using the <a href="https://yardstick.tidymodels.org/">yardstick</a> package.</p>
<pre class="r"><code>set.seed(1234)
wine_split &lt;- initial_split(wine_ratings)

wine_training &lt;- training(wine_split)
wine_testing &lt;- training(wine_split)</code></pre>
<p>Next will we use <a href="https://recipes.tidymodels.org/">recipes</a> and textrecipes to specify the preprocessing of the data.
We
- Use <code>step_log()</code> to take the logarithm of <code>price</code>
- Use <code>step_uknowm()</code> to turn missing values in factors into levels with name ‚Äúunknown‚Äù
- Use <code>step_other()</code> to lump together factor levels that don‚Äôt take up more the 1% of the counts.
- Use <code>step_dummy()</code> to dummify the factor variables
- Use <code>step_tokenize()</code> to turn the descriptions into tokens
- Use <code>step_stopwords()</code> to remove stop words from the tokens (ALWAYS manually verify your stop word list)
- Use <code>step_tokenfilter()</code> to limit the number of tokens we will use when calculating tf-idf. We will only keep tokens if they appear more then 100 times and of those will be at most take the 2000 most frequent tokens.
- Use <code>step_tfidf()</code> to calculate the term frequency-inverse document frequency of the tokens.
- Use <code>step_normalize()</code> to normalize all the predictors to have a standard deviation of one and a mean of zero. We need to do this because it‚Äôs important for lasso regularization.</p>
<pre class="r"><code>rec_spec &lt;- recipe(points ~ description + price + country + variety + taster_name, 
                   data = wine_training) %&gt;%
  step_log(price) %&gt;%
  step_unknown(country, variety, taster_name) %&gt;%
  step_other(country, variety, threshold = 0.01) %&gt;%
  step_dummy(country, variety, taster_name) %&gt;%
  step_tokenize(description) %&gt;%
  step_stopwords(description) %&gt;%
  step_tokenfilter(description, min_times = 100, max_tokens = 2000) %&gt;%
  step_tfidf(description) %&gt;%
  step_normalize(all_predictors())</code></pre>
<p>We will use lasso regression and we will use the ‚Äúglmnet‚Äù engine.</p>
<pre class="r"><code>lasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%
  set_engine(&quot;glmnet&quot;)</code></pre>
<p>I have specified <code>penalty = tune()</code> because I want to use <a href="https://tune.tidymodels.org/">tune</a> to find the best value of the penalty by doing hyperparameter tuning.</p>
<p>We set up a parameter grid using <code>grid_regular()</code></p>
<pre class="r"><code>param_grid &lt;- grid_regular(penalty(), levels = 50)</code></pre>
<div class="note">
<p>searching over 50 levels might seem like a lot. But remember that glmnet is able to calculate them all at once.
This is because it relies on its warms starts for speed and it is often faster to fit a whole path than compute a single fit.</p>
</div>
<p>We will also set up some bootstraps of the data so we can evaluate the performance multiple times for each level.</p>
<pre class="r"><code>wine_boot &lt;- bootstraps(wine_training, times = 10)</code></pre>
<p>the last thing we need to use is to create a workflow object to combine the preprocessing step with the model.
This is important because we want the preprocessing steps to happen in the bootstraps.</p>
<pre class="r"><code>lasso_wf &lt;- workflow() %&gt;%
  add_recipe(rec_spec) %&gt;%
  add_model(lasso_spec)</code></pre>
<p>now we are ready to perform the parameter tuning. We will be using <code>doParallel</code> to speed up the calculations by using multiple cores.</p>
<pre class="r"><code>doParallel::registerDoParallel()
set.seed(42)
lasso_grid &lt;- tune_grid(
  lasso_wf,
  resamples = wine_boot,
  grid = param_grid
) 

lasso_grid
## # Bootstrap sampling 
## # A tibble: 10 x 4
##    splits                id          .metrics           .notes          
##    &lt;list&gt;                &lt;chr&gt;       &lt;list&gt;             &lt;list&gt;          
##  1 &lt;split [83.7K/30.8K]&gt; Bootstrap01 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  2 &lt;split [83.7K/30.8K]&gt; Bootstrap02 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  3 &lt;split [83.7K/30.8K]&gt; Bootstrap03 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  4 &lt;split [83.7K/30.6K]&gt; Bootstrap04 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  5 &lt;split [83.7K/30.8K]&gt; Bootstrap05 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  6 &lt;split [83.7K/30.7K]&gt; Bootstrap06 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  7 &lt;split [83.7K/30.8K]&gt; Bootstrap07 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  8 &lt;split [83.7K/30.8K]&gt; Bootstrap08 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
##  9 &lt;split [83.7K/30.9K]&gt; Bootstrap09 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;
## 10 &lt;split [83.7K/30.8K]&gt; Bootstrap10 &lt;tibble [100 √ó 4]&gt; &lt;tibble [1 √ó 1]&gt;</code></pre>
<p>Now that the grid search has finished we can look at the best performing models with <code>show_best()</code>.</p>
<pre class="r"><code>show_best(lasso_grid, metric =  &quot;rmse&quot;)
## # A tibble: 5 x 6
##    penalty .metric .estimator  mean     n std_err
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1 0.00356  rmse    standard    1.64    10 0.00222
## 2 0.00222  rmse    standard    1.64    10 0.00220
## 3 0.00139  rmse    standard    1.64    10 0.00219
## 4 0.00569  rmse    standard    1.64    10 0.00227
## 5 0.000869 rmse    standard    1.64    10 0.00218</code></pre>
<p>We are quite satisfied with these results!
Use <code>select_best()</code> to extract the best performing one</p>
<pre class="r"><code>best_penalty &lt;- select_best(lasso_grid, metric =  &quot;rmse&quot;)</code></pre>
<p>and we will use that value of penalty in our final workflow object</p>
<pre class="r"><code>final_wf &lt;- finalize_workflow(
  lasso_wf,
  best_penalty
)</code></pre>
<p>Now all there is to do is to fit the workflow on the real training dataset.</p>
<pre class="r"><code>final_lasso &lt;- final_wf %&gt;%
  fit(data = wine_training)</code></pre>
<p>And then, finally, let‚Äôs return to our test data. The tune package has a function last_fit() which is nice for situations when you have tuned and finalized a model or workflow and want to fit it one last time on your training data and evaluate it on your testing data. You only have to pass this function your finalized model/workflow and your split.</p>
<p>Finally can we return to our testing dataset. We can use the <code>last_fit()</code> function to apply our finalized workflow to the testing dataset and see what performance we are getting.</p>
<pre class="r"><code>last_fit(final_lasso, wine_split) %&gt;%
  collect_metrics()
## ! Resample1: model (predictions): There are new levels in a factor: NA
## # A tibble: 2 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       1.65 
## 2 rsq     standard       0.721</code></pre>
</div>
<div id="look-at-best-and-worst-performing-reviews" class="section level2">
<h2>Look at best and worst-performing reviews</h2>
<p>A good way of looking at how well your model is performing is to look at the observations it got right and which it got wrong.</p>
<pre class="r"><code>wine_eval &lt;- wine_training %&gt;%
  bind_cols(
    predict(final_lasso, new_data = wine_training)
  )</code></pre>
<p>First let us plot the observed vs the predicted values, we have added a little bit of horizontal noise to prevent overplotting too much.
A red line what been added at the middle</p>
<pre class="r"><code>wine_eval %&gt;% 
  ggplot(aes(points, .pred)) +
  geom_jitter(height = 0, width = 0.2, alpha = 0.1) +
  geom_abline(color = &quot;firebrick&quot;, slope = 1, intercept = 0)</code></pre>
<p><img src="/post/2020-05-22-textrecipes-series-tfidf/index_files/figure-html/unnamed-chunk-23-1.png" width="700px" style="display: block; margin: auto;" /></p>
<p>The main takeaway from this chart is that the model is overestimating the points given to low reviews and underestimates the points given to high reviews.</p>
<p>Below is the the the reviews that the model underestimates for the lowest-rated reviews</p>
<div class="note">
<p>remember that the model didn‚Äôt only use text input to determine its predictions.</p>
</div>
<pre class="r"><code>wine_eval %&gt;%
  filter(points == 83) %&gt;%
  arrange(.pred) %&gt;%
  slice(1:5) %&gt;%
  pull(description)
## [1] &quot;Thin, awkward and excessively herbal, this sour wine seems barely ripe.&quot;                                              
## [2] &quot;Soft, sugary sweet and simple, with cherry jam and chocolate flavors.&quot;                                                
## [3] &quot;A bit harsh and bitter, with some unripe, green flavors alongside blackberries. Finishes dry and astringent.&quot;         
## [4] &quot;A very simple Chardonnay with sour apple flavor.&quot;                                                                     
## [5] &quot;Weedy and vegetal despite some decent raspberry, cherry and spice flavors. Okay for a big party where nobody&#39;s fussy.&quot;</code></pre>
<p>All of these reviews are fairly short, and might not give as much weight as the other variables.</p>
<p>overestimated bad reviews</p>
<pre class="r"><code>wine_eval %&gt;%
  filter(points == 83) %&gt;%
  arrange(desc(.pred)) %&gt;%
  slice(1:5) %&gt;%
  pull(description)
## [1] &quot;Cherry and saut√©ed morels provide an intriguing introduction to this deeply hued wine, but there are also hints of something funkier, like wet newspaper. On the palate, the pleasing black cherry returns, as does anise, some bitter elements and chewy tannins.&quot;                                                                     
## [2] &quot;With refreshingly tart blackberry fruit, this Zweigelt has some concentration and displays a good balance between light body, refreshing acidity and clean, lively fruit notes. A great party wine.&quot;                                                                                                                                    
## [3] &quot;Pipe tobacco, raspberry reduction and a sweet sagebrush character promise an interesting experience. Once sipped, the overwhelmingly sweet cherry, cough-syrup flavor pushes this wine beyond a dry, and the typical Cabernet Franc herbs that emerge in the midpalate don&#39;t overwhelm its cloying nature. Good for dessert, by itself.&quot;
## [4] &quot;Lifted floral and stone fruit aromatics add intensity to peachy, grape flavors on the palate of this wine. It&#39;s rich and luscious, with a bright burst of tangerine on the finish.&quot;                                                                                                                                                     
## [5] &quot;A confected wine that shows polished wood and mint flavors but little fruit. It puts on a great show, but without depth.&quot;</code></pre>
<p>underestimated good reviews</p>
<pre class="r"><code>wine_eval %&gt;%
  filter(points == 95) %&gt;%
  arrange(.pred) %&gt;%
  slice(1:5) %&gt;%
  pull(description)
## [1] &quot;Dark and racy, with a Northern Rh√¥ne-style aroma of black pepper, cedar and black currants, but there&#39;s nothing Rh√¥ne-like about the flavor. It&#39;s distinctly California, with soft, sweetly ripe tannins bursting with blackberry jam, black currants, anise, vanilla and sweet oak flavors. The wine is fully dry. Absolutely first-rate Syrah.&quot;                                                                              
## [2] &quot;Rued Ranch consistently is the winery&#39;s top bottling. The 2011, like its predecessors, is modest in alcohol but vast in citrus, tropical fruit, honeysuckle and mineral flavors, and enriched with smoky oak and buttery notes. It has a delicate mouthfeel that makes it a joy to savor.&quot;                                                                                                                                     
## [3] &quot;Apple and passion fruit are the aromatic top notes, hovering above a nose of honey and musk. Notes of honey, caramel and maple syrup seem to unite on the palate, guided and framed by the sharpest, most luminous acidity. Extremes are at work here, revealing both utter sweetness and utter acidity. Dazzling all of the senses, this wine leaves a rich aftertaste.&quot;                                                      
## [4] &quot;This sweet, luscious, dessert-style wine keeps enough acidity to contrast and compensate for the sugar. There&#39;s nothing flabby here, but how sweet it is! Caramel apples, molasses, maple syrup and honey coalesce around concentrated fruits, ranging from citrus through tropical. Orange and peach, pineapple and papaya are all to be found. A half bottle is rich enough to satisfy four tasters.&quot;                        
## [5] &quot;Hailing from the town of Suvereto in the southern Maremma, this sophomore Cabernet-Merlot blend from the same owner as Bellavista and Contadi Castaldi in Franciacorta is flat-out awesome. One whiff of the bouquet says it all: earth, currant, blackberry and coffee. The palate is equally sensational‚Äîa magic carpet ride of plum fruit, pure oak and solid but forgiving tannins. Drink and enjoy any time through 2006.&quot;</code></pre>
<p>overrated good reviews</p>
<pre class="r"><code>wine_eval %&gt;%
  filter(points == 95) %&gt;%
  arrange(desc(.pred)) %&gt;%
  slice(1:5) %&gt;%
  pull(description)
## [1] &quot;This is a deeply rich, very impressive wine from a great white-wine vintage. It&#39;s concentrated with the essence of pure Chardonnay. It has tension along with a full texture. Age it for at least 10 years.&quot;                                                             
## [2] &quot;Produced from vines mainly planted in the 1970s, this is a concentrated dark wine. Rich and ripe, it has powerful tannins mingled with beautifully perfumed fruits. The wine is dense and packed with floral fruitiness as well as fine, sweet tannins. Drink from 2025.&quot;
## [3] &quot;Powerful and complex, this dark wine comes from vines at the heart of appellation, rich and dense. It offers the potential of great fruit, rich acidity and considerable structure, and will develop giving richness and long aging. Drink from 2023.&quot;                   
## [4] &quot;The grandest of all white Burgundy vineyards, Montrachet has produced a powerful, great wine, with impressive ripe, dense fruit. There is still elegance, a sense of proportion, but the opulence of the Chardonnay here is impossible to resist.&quot;                       
## [5] &quot;A solidly structured wine, it is both elegant and intensely powerful. It is disclosing its fruitiness slowly, at the moment dense and concentrated. A restrained edge provides complexity and balance. It will need aging, so don&#39;t drink before 2022.&quot;</code></pre>
</div>
