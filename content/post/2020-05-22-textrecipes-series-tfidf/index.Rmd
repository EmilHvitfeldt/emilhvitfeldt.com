---
title: 'Textrecipes series: TF-IDF'
date: '2020-05-22'
slug: textrecipes-series-tfidf
categories: [tidymodels, textrecipes, textrecipes series, tidytuesday]
image:
  caption: 'Photo by [Kym Ellis](https://unsplash.com/@kymellis) on [Unsplash](https://unsplash.com/)'
  preview_only: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  collapse = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
knit_hooks$set(optipng = hook_optipng)
opts_chunk$set("optipng" = "-o5")
```

This is the third blog post in the [textrecipes](https://github.com/tidymodels/textrecipes) series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be showcasing how to perform [term frequency-inverse document frequency](http://www.tfidf.com/) (Tf-IDF for short). 

## Packages `r emo::ji("package")`

The packages used in the post shouldn't come to any surprise if you have been following the series. [tidymodels](https://www.tidymodels.org/) for modeling, tidyverse for EDA, [textrecipes](https://textrecipes.tidymodels.org/) for text preprocessing, vip for visualizing variable importance, and doParallel to parallelize the hyperparameter tuning.

```{r, message=FALSE}
library(tidymodels)
library(tidyverse)
library(textrecipes)
library(vip)
library(doParallel)
theme_set(theme_minimal())
```

## Exploring the data `r emo::ji("pick")`

We will be using a #tidytuesday dataset from almost a year ago, it contains a lot of [wine reviews](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-28). [David Robinson](https://twitter.com/drob) did a very nice screen about this dataset

<iframe width="560" height="315" src="https://www.youtube.com/embed/AQzZNIyjyWM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

David goes into a lot of detail explaining what he is doing and I highly recommend to watch this one if you are interested in using text in regression. 
Fortunately he didn't use tidymodels so this post will bring a little something new.
Our goal for this post is to build a model that predicts the score (denotes `points`) a particular wine has.

```{r, message=FALSE}
wine_ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv")
```

We load in the data with `read_csv()` and immediately use `glimpse()` to get an idea of the data we have to work with

```{r}
glimpse(wine_ratings)
```

This dataset barely contains any numeric variables.
The only numeric is the price. As with many prices in data, it is a good idea to log transform them since they are highly skewed

```{r}
wine_ratings %>%
  mutate(price_log = log(price)) %>%
  pivot_longer(c(price, price_log)) %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~name, scales = "free_x") 
```

Since most of the data most likely will be factors let us take a look at the cardinality of each variable

```{r}
map_int(wine_ratings, n_distinct)
```

But wait! the number of unique descriptions is not the same as the number of rows.
This seems very odd since they will be multiple sentences long and the likelihood of two different people writing the exact same description is very low.

let us take a selection of duplicated descriptions and see if anything stands out.

```{r}
wine_ratings %>%
  filter(duplicated(description)) %>%
  slice(1:3) %>%
  pull(description)
```

as we feared these are pretty specific and would be unlikely to be duplications at random.
We will assume that this problem is a scraping error and remove the duplicate entries. 
Additionally, some of the points values are missing, since this is our target variable will I remove those data points as well.

:::warning
If you are working on a real project you shouldn't simply delete observations like that. Both of these errors smell a little bit like bad scraping so your first course of action should be testing your data pipeline for errors.
:::

:::note
I found out about the issue with duplicate descriptions when I was browsing through other people's analyses of the dataset.
:::

Before we do any more analysis, let us remove the troublesome observations.

```{r}
wine_ratings <- wine_ratings %>%
  filter(!duplicated(description), !is.na(price))

wine_ratings
```

Countries look like it would be important to include, doing a little bar chart reveals a high imbalance in where the wines are coming from. 
We will definitely need to weed out some of the low count countries

```{r}
wine_ratings %>% 
  count(country, sort = TRUE) %>%
  ggplot(aes(n, fct_reorder(country, n), label = country)) +
  geom_col() +
  geom_text(hjust = 0, nudge_x = 1000)
```

This dataset is restricted to review of wine that scored 80 points or more, 

```{r}
wine_ratings %>% 
  ggplot(aes(points)) +
  geom_bar()
```

It looks like the 80 wasn't as hard cutoff, and the points even look bell-shaped.

I'll be using `tester_name` and `variety` as well in the final analysis.

## Modeling `r emo::ji("gear")`

We start by doing a simple training test split of the data using the [yardstick](https://yardstick.tidymodels.org/) package.

```{r}
set.seed(1234)
wine_split <- initial_split(wine_ratings)

wine_training <- training(wine_split)
wine_testing <- training(wine_split)
```

Next will we use [recipes](https://recipes.tidymodels.org/) and textrecipes to specify the preprocessing of the data.
We 
- Use `step_log()` to take the logarithm of `price`
- Use `step_uknowm()` to turn missing values in factors into levels with name "unknown"
- Use `step_other()` to lump together factor levels that don't take up more the 1% of the counts.
- Use `step_dummy()` to dummify the factor variables
- Use `step_tokenize()` to turn the descriptions into tokens
- Use `step_stopwords()` to remove stop words from the tokens (ALWAYS manually verify your stop word list)
- Use `step_tokenfilter()` to limit the number of tokens we will use when calculating tf-idf. We will only keep tokens if they appear more then 100 times and of those will be at most take the 2000 most frequent tokens.
- Use `step_tfidf()` to calculate the term frequency-inverse document frequency of the tokens.
- Use `step_normalize()` to normalize all the predictors to have a standard deviation of one and a mean of zero. We need to do this because it’s important for lasso regularization.

```{r}
rec_spec <- recipe(points ~ description + price + country + variety + taster_name, 
                   data = wine_training) %>%
  step_log(price) %>%
  step_unknown(country, variety, taster_name) %>%
  step_other(country, variety, threshold = 0.01) %>%
  step_dummy(country, variety, taster_name) %>%
  step_tokenize(description) %>%
  step_stopwords(description) %>%
  step_tokenfilter(description, min_times = 100, max_tokens = 2000) %>%
  step_tfidf(description) %>%
  step_normalize(all_predictors())
```

We will use lasso regression and we will use the "glmnet" engine.

```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
```

I have specified `penalty = tune()` because I want to use [tune](https://tune.tidymodels.org/) to find the best value of the penalty by doing hyperparameter tuning.

We set up a parameter grid using `grid_regular()`

```{r}
param_grid <- grid_regular(penalty(), levels = 50)
```

:::note
searching over 50 levels might seem like a lot. But remember that glmnet is able to calculate them all at once.
This is because it relies on its warms starts for speed and it is often faster to fit a whole path than compute a single fit.
:::

We will also set up some bootstraps of the data so we can evaluate the performance multiple times for each level.

```{r}
wine_boot <- bootstraps(wine_training, times = 10)
```

the last thing we need to use is to create a workflow object to combine the preprocessing step with the model.
This is important because we want the preprocessing steps to happen in the bootstraps.

```{r}
lasso_wf <- workflow() %>%
  add_recipe(rec_spec) %>%
  add_model(lasso_spec)
```

now we are ready to perform the parameter tuning. We will be using `doParallel` to speed up the calculations by using multiple cores.

```{r}
doParallel::registerDoParallel()
set.seed(42)
lasso_grid <- tune_grid(
  lasso_wf,
  resamples = wine_boot,
  grid = param_grid
) 

lasso_grid
```

Now that the grid search has finished we can look at the best performing models with `show_best()`.

```{r}
show_best(lasso_grid, metric =  "rmse")
```

We are quite satisfied with these results!
Use `select_best()` to extract the best performing one

```{r}
best_penalty <- select_best(lasso_grid, metric =  "rmse")
```

and we will use that value of penalty in our final workflow object

```{r}
final_wf <- finalize_workflow(
  lasso_wf,
  best_penalty
)
```

Now all there is to do is to fit the workflow on the real training dataset.

```{r}
final_lasso <- final_wf %>%
  fit(data = wine_training)
```

And then, finally, let’s return to our test data. The tune package has a function last_fit() which is nice for situations when you have tuned and finalized a model or workflow and want to fit it one last time on your training data and evaluate it on your testing data. You only have to pass this function your finalized model/workflow and your split.

Finally can we return to our testing dataset. We can use the `last_fit()` function to apply our finalized workflow to the testing dataset and see what performance we are getting.

```{r}
last_fit(final_lasso, wine_split) %>%
  collect_metrics()
```

## Look at best and worst-performing reviews

A good way of looking at how well your model is performing is to look at the observations it got right and which it got wrong.

```{r}
wine_eval <- wine_training %>%
  bind_cols(
    predict(final_lasso, new_data = wine_training)
  )
```

First let us plot the observed vs the predicted values, we have added a little bit of horizontal noise to prevent overplotting too much.
A red line what been added at the middle

```{r}
wine_eval %>% 
  ggplot(aes(points, .pred)) +
  geom_jitter(height = 0, width = 0.2, alpha = 0.1) +
  geom_abline(color = "firebrick", slope = 1, intercept = 0)
```

The main takeaway from this chart is that the model is overestimating the points given to low reviews and underestimates the points given to high reviews.

Below is the the the reviews that the model underestimates for the lowest-rated reviews

:::note
remember that the model didn't only use text input to determine its predictions.
:::

```{r}
wine_eval %>%
  filter(points == 83) %>%
  arrange(.pred) %>%
  slice(1:5) %>%
  pull(description)
```

All of these reviews are fairly short, and might not give as much weight as the other variables.

overestimated bad reviews

```{r}
wine_eval %>%
  filter(points == 83) %>%
  arrange(desc(.pred)) %>%
  slice(1:5) %>%
  pull(description)
```

underestimated good reviews

```{r}
wine_eval %>%
  filter(points == 95) %>%
  arrange(.pred) %>%
  slice(1:5) %>%
  pull(description)
```

overrated good reviews

```{r}
wine_eval %>%
  filter(points == 95) %>%
  arrange(desc(.pred)) %>%
  slice(1:5) %>%
  pull(description)
```
