---
title: 'Textrecipes Version 0.4.0'
date: '2020-11-13'
slug: textrecipes-version-0-4-0
categories:
  - tidymodels
  - textrecipes
tags:
  - tidymodels
  - textecipes
summary: Announcement post for version 0.4.0 of textrecipes
image:
  preview_only: true

---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  collapse = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
knit_hooks$set(optipng = hook_optipng)
opts_chunk$set("optipng" = "-o5")
```

I'm happy to announce that version 0.4.0 of [textrecipes](https://textrecipes.tidymodels.org/) got on CRAN a couple of days ago.
This will be a brief post going over the major additions and changes.

## Breaking changes `r emo::ji("bomb")`

I put this change at the top of this post to make sure it gets enough coverage.
The `step_lda()` function will no longer accepts character variables and instead takes [tokenlist](https://textrecipes.tidymodels.org/reference/tokenlist.html) variables. 
I don't expect this to affect too many people since it appears that the use of this step is fairly limited.

For a recipe where `step_lda()` is used on a variable `text_var`

```{r, eval=FALSE}
recipe(~ text_var, data = data) %>%
  step_lda(text_var)
```

can be made to work the same as before by including this `step_tokenize()` step before it.
It includes a custom tokenizer which was used inside the old version of `step_lda()`

```{r, eval=FALSE}

recipe(~ text_var, data = data) %>%
  step_tokenize(text_var, 
                custom_token = function(x) text2vec::word_tokenizer(tolower(x))) %>%
  step_lda(text_var)
```

This change was long overdue since it didn't follow the rest of the steps since it was doing tokenization internally.
This change provides more flexability when using `step_lda()` in its current state and allows me to consider [adding more engine to `step_lda()`](https://github.com/tidymodels/textrecipes/issues/112).

## Cleaning `r emo::ji("soap")`

If your data has weird characters and spaces in them messing up your model then the following steps will make you very happy.
`step_clean_levels()` and `step_clean_names()` works much like [janitor](https://garthtarr.github.io/meatR/janitor.html)'s `clean_names()` function.
Character variables and column names are changes such that they only contain alphanumeric characters and underscores.

Consider the `Smithsonian` data.frame.
The `name` variable contains entries with many character, cases, spaces, and punctuations.

```{r, message=FALSE}
library(recipes)
library(textrecipes)
library(modeldata)

data(Smithsonian)
Smithsonian
```

When using `step_clean_levels()`

```{r}
recipe(~ name, data = Smithsonian) %>%
  step_clean_levels(name) %>%
  prep() %>%
  bake(new_data = NULL)
```

We see that everything has been cleaned to avoid potential confusion and errors.

the almost more important step is `step_clean_names()` as it allows you to clean the variables names that could trip up various modeling packages

```{r}
ugly_names <- tibble(
  `        Some      spaces     ` = 1,
  `BIGG and small case` = 2,
  `.period` = 3
)

recipe(~ ., data = ugly_names) %>%
  step_clean_names(all_predictors()) %>%
  prep() %>%
  bake(new_data = NULL)
```

## New tokenizers

There is two new `engine`s available in `step_tokenize()`.
the [tokenizers.bpe](https://github.com/bnosac/tokenizers.bpe) engine lets you perform [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) on you text as a mean of tokenization.

```{r}
data("okc_text")

recipe(~ essay6, data = okc_text) %>%
  step_tokenize(essay6, engine = "tokenizers.bpe") %>%
  step_tokenfilter(essay6, max_times = 100) %>%
  step_tf(essay6) %>%
  prep() %>%
  bake(new_data = NULL)
```

additional arguments can be passed to `tokenizers.bpe::bpe()` via the `training_options` argument.

```{r}
recipe(~ essay6, data = okc_text) %>%
  step_tokenize(essay6, 
                engine = "tokenizers.bpe",
                training_options = list(vocab_size = 100)) %>%
  step_tf(essay6) %>%
  prep() %>%
  bake(new_data = NULL)
```

The second engine is access to [udpipe](https://github.com/bnosac/udpipe).
To use this engine you must first download a udpipe model

```{r, message=FALSE}
library(udpipe)
udmodel <- udpipe_download_model(language = "english")
udmodel
```

And then you need to pass it into `training_options` under the name `model`.
This will then use the tokenizer

```{r}
recipe(~ essay6, data = okc_text) %>%
  step_tokenize(essay6, engine = "udpipe", 
                training_options = list(model = udmodel)) %>%
  step_tf(essay6) %>%
  prep() %>%
  bake(new_data = NULL)
```

But where it gets really interesting is that we are able to extract the lemmas

```{r}
recipe(~ essay6, data = okc_text) %>%
  step_tokenize(essay6, engine = "udpipe", 
                training_options = list(model = udmodel)) %>%
  step_lemma(essay6) %>%
  step_tf(essay6) %>%
  prep() %>%
  bake(new_data = NULL)
```

or use the [part of speech tags](https://en.wikipedia.org/wiki/Part_of_speech) in later steps, such as below where we are filtering to only keep nouns.

```{r}
recipe(~ essay6, data = okc_text) %>%
  step_tokenize(essay6, engine = "udpipe", 
                training_options = list(model = udmodel)) %>%
  step_pos_filter(essay6, keep_tags = "NOUN") %>%
  step_tf(essay6) %>%
  prep() %>%
  bake(new_data = NULL)
```

This is all for this release. I hope you found some of it useful. I would love to hear what you are using `textrecipes` with!
