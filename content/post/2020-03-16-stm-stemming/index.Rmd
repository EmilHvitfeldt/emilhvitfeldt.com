---
title: Using stm to Investigate if Stemming is Appropriate
date: '2020-03-16'
categories: [tidytext]
image:
  caption: 'Image credit: [**Jess Bailey**](https://unsplash.com/photos/aQm-rXgZ4n8)'
  preview_only: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  cache = TRUE,
  collapse = TRUE,
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  out.width = "700px")
knit_hooks$set(optipng = hook_optipng)
opts_chunk$set("optipng" = "-o5")
```

It is known that topic modeling does not benefit from stemming [ref](https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf).
I propose a workflow to investigate if stemming is appropriate as a method for data reduction. 

1. Take all the tokens and apply the stemming algorithm you would like to test
1. Construct a list of words that should be equal under stemming
1. Apply a topic model to your original data
1. Predict the topic for each word created in 2.

If grouped words are predicted to the same topic then we assume that stemming would not make much of a difference. 
If the words are predicted to be indifferent topics then we have a suspicion that the stemmed and unstemmed words have different uses and stemming would be ill-advised.

First, we load the packages we will be using.

```{r, message=FALSE}
library(tidytext)
library(tidyverse)
library(stm)
library(hcandersenr)
library(SnowballC)
```

As a first test, we pick 3 English^[[#benderrule](https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/)] fairy tales by H.C. Andersens using the [hcandersenr](https://github.com/EmilHvitfeldt/hcandersenr) package. 
To create multiple "documents" for each fairy tale we start by tokenizing to sentences.
Then we give each sentence a unique identifier.

```{r}
fairy_tales <- hcandersen_en %>%
  filter(book %in% c("The fir tree", "The tinder-box", "Thumbelina")) %>%
  unnest_tokens(token, text, token = "sentences") %>%
  group_by(book) %>%
  mutate(sentence = row_number()) %>%
  ungroup() %>%
  unite(document, book, sentence)

fairy_tales
```

Now we unnest the tokens to words and create a new variable of the stemmed words

```{r}
fairy_tales_tokens <- fairy_tales %>%
  unnest_tokens(token, token) %>%
  mutate(token_stem = wordStem(token))

fairy_tales_tokens
```

We can take a look at all the times where stemming we can look at all the times stemming yields a different token.

```{r}
different <- fairy_tales_tokens %>%
  select(token, token_stem) %>%
  filter(token != token_stem) %>%
  unique()

different
```

In this example, we have `r nrow(different)` different tokens. 
But since stemming can collapse multiple different tokens into one.

```{r}
different %>%
  count(token_stem, sort = TRUE)
```

We can use the `different` data.frame and construct a list of words that would land in the same bucket after stemming.

```{r}
stem_buckets <- split(different$token, different$token_stem) %>%
  imap(~ c(.x, .y))

stem_buckets[21:25]
```

Here we see that "anxiou" and "anxious" would look the same after stemming, likewise will "apples", "apple" and "appl".
The main point of this exercise is to see if the words in these groups of words end up in the topic when during topic modeling.

```{r}
stm_model <- fairy_tales_tokens %>%
  count(document, token) %>%
  cast_sparse(document, token, n) %>%
  stm(K = 3, verbose = FALSE)

stm_model
```

In this case, I fit the model to 3 topics because I knew that would be the right number since I picked the data.
When doing this on your data you should run multiple models with a varying number of topics to find the best one. 
For more information please read [Training, Evaluating, and Interpreting Topic Models](https://juliasilge.com/blog/evaluating-stm/) by [Julia Silge](https://twitter.com/juliasilge).

Now that we have a `stm` model and a list of words, 
We can inspect the model object to check if multiple words are put in the same topic.
Below is a function that will take a vector of characters and a `stm` model and return `TRUE` if all the words appear in the same topic and `FALSE` if they don't.

```{r}
stm_match <- function(x, model) {
  topics <- tidy(model) %>%
  filter(term %in% x) %>%
  group_by(term) %>%
  top_n(1, beta) %>%
  ungroup() %>%
  select(topic) %>%
  n_distinct()
  
  topics == 1
}
```

As an example, if we pass the words "believed" and "believ"

```{r}
stm_match(c("believed", "believ"), stm_model)
```

We see that they did end up in the same bucket.
If we instead pass in "dog" and "happy" they land in different topics.

```{r}
stm_match(c("dog", "happy"), stm_model)
```

All of this is not perfect, there is still some uncertainty but it is a good first step to evaluate if stemming is appropriate for your application.

```{r}
tested <- tibble(terms = stem_buckets,
                 stem = names(stem_buckets)) %>%
  mutate(match = map_lgl(terms, stm_match, stm_model))

tested
```

First, we'll look at the distribution of `TRUE`s and `FALSE`s.

```{r, eval=FALSE}
tested %>%  
  ggplot(aes(match)) +
  geom_bar()
```

```{r, echo=FALSE}
tested %>%  
  ggplot(aes(match)) +
  geom_bar() +
  theme_minimal()
```

So it looks like most of the word groups were put into the same topic during modeling.
This is a good sign. 
Please note that this category includes a lot of false positives.
This is happening because `stm_match()` also returns true for a case where one of the words appears in the model and all other words don't.
So for the case of "accompanied" and "accompani", the word "accompanied" was present in one of the topics, but the word "accompani" was not present in the original data and hence did not appear in any of the topics.
In this case, the `TRUE` value we are getting is saying that the data doesn't provide enough evidence to indicate that stemming would be bad.
By looking at a sample of `TRUE` cases we see that a lot of them are happening because the stemmed word isn't being used, like the words "aliv", "alon" and "alwai".
On the other side, we have that the words "allowed" and "allow" are both real words AND they appeared in the same topic.

```{r}
tested %>%
  filter(match) %>%
  slice(10:15) %>%
  pull(terms)
```

Turning our head to the `FALSE` cases.
These cases will not have any false positives as both words would have to appear in the original corpus for them to put into different topics.
These cases are still not going to be perfect, but will again be an indication.

```{r}
tested %>%
  filter(!match) %>%
  pull(terms) %>%
  head()
```

This is the list I would advise you to look over carefully.
Check to make sure that you are okay with the number and count of misgroupings you would get by applying stemming.

```{r details, echo=FALSE}
library(details) 

sessioninfo::session_info() %>%
  details::details(summary = 'session information')
```
