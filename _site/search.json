[
  {
    "objectID": "talk/2021-06-15-nyhackr-text-preprocessing/index.html",
    "href": "talk/2021-06-15-nyhackr-text-preprocessing/index.html",
    "title": "Text Preprocessing in R",
    "section": "",
    "text": "Slides Video\n\n\nInvited talk at New York Open Statistical Programming Meetup\nText constitutes an ever-growing part of the data available to us today. However, it is a non-trivial task to transform text, represented as long strings of characters, into numbers that we can use in our statistical and machine learning models. This talk will focus on the {textrecipes} package and its recent advancements in the realm of text preprocessing."
  },
  {
    "objectID": "talk/2018-09-25-color-palette-similarity/index.html",
    "href": "talk/2018-09-25-color-palette-similarity/index.html",
    "title": "Similarity Measure in the Space of Color Palettes",
    "section": "",
    "text": "Slides Video Github\n\n\nRelated to my project of creating a catalog of all available color palettes in R https://github.com/EmilHvitfeldt/r-color-palettes and its associated R package https://CRAN.R-project.org/package=paletteer I wanted to expand the project to support a higher degree of explorability. There is already quite a bit theory of color similarity and image similarity that will provide useful but unfortunately insufficient. For standard color similarity using some kind of space measure in a perceptual color space will likely give you what you need, but this approach will start to struggle when you need to compare groups of colors since ordering starts making a difference. Image similarity can likewise be done by comparing color histograms, this approach does still not capture the number of colors or other qualities such as classifying according to type (Sequential, Diverging or Qualitative). My goal is to use expert knowledge to calculate features that can be used to calculates similarities."
  },
  {
    "objectID": "talk/2020-01-23-happyscientist-git-and-github/index.html",
    "href": "talk/2020-01-23-happyscientist-git-and-github/index.html",
    "title": "Git & Github",
    "section": "",
    "text": "Slides Github\n\n\nWorking alone or with other people becomes increasing difficult with the increase of files and people. This seminar goes into detail why and how to use git in collaborative research. Material in this talk is heavely inspired by Excuse me, do you have a moment to talk about version control? by Jenny Bryan."
  },
  {
    "objectID": "talk/2019-01-22-happyscientist-rstudio-and-r/index.html",
    "href": "talk/2019-01-22-happyscientist-rstudio-and-r/index.html",
    "title": "RStudio and R Resources",
    "section": "",
    "text": "Slides Github\n\n\nThis seminar contains various links and resources for use in R and Rstudio."
  },
  {
    "objectID": "talk/2021-11-23-barcelona/index.html",
    "href": "talk/2021-11-23-barcelona/index.html",
    "title": "Work Smarter, Not Harder RStudio",
    "section": "",
    "text": "Slides Github\n\n\nInvited talk at Barcelona RUG\nThis talk will take you around the myriad of customization and automation available to all RStudio users. This talk will visual customization to custom keybindings to make your workflow smooth and efficient so you can spend all your time focusing on R!"
  },
  {
    "objectID": "talk/2022-06-09-nyr-adventofcode/index.html",
    "href": "talk/2022-06-09-nyr-adventofcode/index.html",
    "title": "I Did Advent of Code and This is What I Learned",
    "section": "",
    "text": "Slides Video Github\n\n\nInvited talk at NYR conference\nMany of us like puzzles. Wrestling with simple-to-understand questions, with trickly-to-find-solutions gives you a special type of bliss when you finally crack it. “Advent of Code” has been going for 7 years, challenging over 200,000 people with 25 days of action-packed programming puzzles. Participating in these events has taught me a lot about myself, my motivations, and my tool of choice, R.\nThis talk will pull back the curtain on a collective journey of learning. A series of elf-inspired tasks provides a challenge for all ages, skill levels, and languages. Tweeting, Slacking, and Discording with the extended R community, bringing people together to share, learn and send memes. A unique opportunity to learn something new; a different programming language, a new set of packages, or simply going as fast as possible to the horror of all style guides. I hope to introduce you to a unique learning experience that will bring you as much enjoyment as it has done for me."
  },
  {
    "objectID": "talk/2019-03-28-happyscientist-building-r-packages/index.html",
    "href": "talk/2019-03-28-happyscientist-building-r-packages/index.html",
    "title": "Building R Packages",
    "section": "",
    "text": "Slides Github\n\n\nBuilding a R package can seem daunting with its many files and structure. This seminar will go through the different use cases for a R package, dos and don’ts and best practices. Finally a live demonstration starting with the creation of a R package ending with release on CRAN."
  },
  {
    "objectID": "talk/2020-07-09-themis/index.html",
    "href": "talk/2020-07-09-themis/index.html",
    "title": "themis: dealing with imbalanced data by using synthetic oversampling",
    "section": "",
    "text": "Slides Video Github\n\n\nMany classification tasks come with an unbalanced dataset. Examples range from disease prediction to fraud detection. Naively applying your model will lead to an ineffective predictor that only predicts the majority class.\nThe themis package implements various established algorithms that adjust this imbalance in the data by either removing cases from the majority classes or by synthetically adds cases to the minority classes until the desired ratio is met.\nA walkthrough of the heart of the synthetic oversampling algorithms will be given in code and visualization along with talk about performance.\nthemis was created because of a lack of unity and speed in existing R packages."
  },
  {
    "objectID": "talk/2020-09-26-palette2vec/index.html",
    "href": "talk/2020-09-26-palette2vec/index.html",
    "title": "palette2vec: A new way to explore color palettes",
    "section": "",
    "text": "Slides Video\n\n\nThere are many palettes available in various R packages. Having a way to explore all of these palettes are already found within the https://github.com/EmilHvitfeldt/r-color-palettes repository and the {paletteer} package.\nThis talk shows what happens when we take one step further into explorability. Using handcrafted color features, dimensionality reduction, and interactive tools will we create and explore a color palette embedding. In this embedded space will we interactively be able to cluster palettes, find neighboring palettes, and even generate new palettes in a whole new way. The Package in question is: https://github.com/EmilHvitfeldt/palette2vec"
  },
  {
    "objectID": "talk/2018-10-29-laerug-text-mining/index.html",
    "href": "talk/2018-10-29-laerug-text-mining/index.html",
    "title": "Text Analysis in R",
    "section": "",
    "text": "Slides Github\n\n\nAn ever-increasing amount of textual data is available us. I’ll talk you through a structured way to do exploratory data analysis(also called text mining) using tidytext to gain insight into the plain unstructured text. This will be followed by a demonstration of how modeling strategies can facilitate decision making when you have text as part of your data."
  },
  {
    "objectID": "talk/2020-12-01-slcrug-stopwords/index.html",
    "href": "talk/2020-12-01-slcrug-stopwords/index.html",
    "title": "Looking at stop words: why you shouldn’t blindly trust model defaults",
    "section": "",
    "text": "Slides Video\n\n\nInvited talk at Salt Lake City R Users Group\nRemoving stop words is a fairly common step in natural language processing, and NLP packages often supply a default list. However, most documentation and tutorials don’t explore the nuances of selecting an appropriate list. Defaults for machine learning and modeling can be helpful but may be misleading or wrong. This talk will focus on the importance of checking assumptions and defaults in the software you use."
  },
  {
    "objectID": "talk/2022-06-23-user-textrecipes/index.html",
    "href": "talk/2022-06-23-user-textrecipes/index.html",
    "title": "Improvements in text preprocessing using textrecipes",
    "section": "",
    "text": "Slides Video Github\n\n\nAccepted talk at useR2022!\nText constitutes an ever-growing part of the data available to us today. However, it is a non-trivial task to transform text, represented as long strings of characters, into numbers that we can use in our statistical and machine learning models. textrecipes has been around for a couple of years to aid the practitioner in transforming text data into a format that is suitable for machine learning models. This talk gives a brief overview of the basic functionality of the package and a look at exciting recent additions."
  },
  {
    "objectID": "talk/2022-06-20-user-tidymodels/index.html",
    "href": "talk/2022-06-20-user-tidymodels/index.html",
    "title": "Machine learning with {tidymodels}",
    "section": "",
    "text": "Slides Github\nAccepted workshop at useR2022!\nThis workshop will provide a gentle introduction to machine learning with R using the modern suite of predictive modeling packages called tidymodels. We will build, evaluate, compare, and tune predictive models. Along the way, we’ll learn about key concepts in machine learning including overfitting, the holdout method, the bias-variance trade-off, ensembling, cross-validation, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, and tune."
  },
  {
    "objectID": "talk/2020-10-09-predictive-modeling-with-text/index.html",
    "href": "talk/2020-10-09-predictive-modeling-with-text/index.html",
    "title": "Predictive modeling with text using tidy data principles",
    "section": "",
    "text": "Slides\nInvited workshop for R/Pharma Conference\nHave you ever encountered text data and suspected there was useful insight latent within it but felt frustrated about how to find that insight? Are you familiar with dplyr and ggplot2, and ready to learn how unstructured text data can be used for prediction within the tidyverse and tidymodels ecosystems?\nDo you need a flexible framework for handling text data that allows you to engage in tasks from exploratory data analysis to supervised predictive modeling? This tutorial is geared toward an R user with intermediate familiarity with R, RStudio, the basics of regression and classification modeling, and tidyverse packages such as dplyr and ggplot2.\nThis person is comfortable with the main functions from dplyr and ggplot2 and is now ready to learn how to analyze and model text using tidy data principles. This R user has some experience with statistical modeling (such as using lm() and glm()) for prediction and classification and wants to learn how to build models with text."
  },
  {
    "objectID": "talk/2020-04-27-happyscientist-recipes/index.html",
    "href": "talk/2020-04-27-happyscientist-recipes/index.html",
    "title": "Reproducible preprocessing with recipes",
    "section": "",
    "text": "Slides Github\n\n\nTalk about using the recipes package to do preprocessing."
  },
  {
    "objectID": "project/genderify/index.html",
    "href": "project/genderify/index.html",
    "title": "genderify",
    "section": "",
    "text": "Website \n\n\nTime and time will certain developers try and fail to develop a service where they are trying to classify the gender of a name/user/customer. This will ALWAYS fails, and this website will tell you why.\nhttps://emilhvitfeldt.github.io/genderify/\nMentioned in:\n\nhttps://syncedreview.com/2020/07/30/ai-powered-genderify-platform-shut-down-after-bias-based-backlash/\nhttps://tampere.ai/en/kite-blog-anticipation-impact-and-accountability-in-ai"
  },
  {
    "objectID": "project/smltar/index.html",
    "href": "project/smltar/index.html",
    "title": "Supervised Machine Learning for Text Analysis in R",
    "section": "",
    "text": "Website    github \n\n\nFull online version is available at https://smltar.com/\nAt online booksellers:\n\nbookshop.org\nAmazon\nroutledge\nBarnes & Noble"
  },
  {
    "objectID": "project/themis/index.html",
    "href": "project/themis/index.html",
    "title": "themis",
    "section": "",
    "text": "github    CRAN \n\n\nA dataset with an uneven number of cases in each class is said to be unbalanced. Many models produce a subpar performance on unbalanced datasets. A dataset can be balanced by increasing the number of minority cases using SMOTE, BorderlineSMOTE and ADASYN. Or by decreasing the number of majority cases using NearMiss or Tomek link removal."
  },
  {
    "objectID": "project/textdata/index.html",
    "href": "project/textdata/index.html",
    "title": "textdata",
    "section": "",
    "text": "github    CRAN \n\n\nThe goal of textdata is to provide access to text-related data sets for easy access without bundling them inside a package. Some text datasets are too large to store within an R package or are licensed in such a way that prevents them from being included in an OSS-licensed package. Instead, this package provides a framework to download, parse, and store the datasets on the disk and load them when needed."
  },
  {
    "objectID": "project/textrecipes/index.html",
    "href": "project/textrecipes/index.html",
    "title": "textrecipes",
    "section": "",
    "text": "github    CRAN \n\n\nConverting text to numerical features requires specifically created procedures, which are implemented as steps according to the ‘recipes’ package. These steps allows for tokenization, filtering, counting (tf and tfidf) and feature hashing."
  },
  {
    "objectID": "project/r-color-palettes/index.html",
    "href": "project/r-color-palettes/index.html",
    "title": "R Color Palettes",
    "section": "",
    "text": "github    Website \n\n\nThe r-color-palettes repository tries to give a convenient overview of all the color palettes you can use in R."
  },
  {
    "objectID": "project/prismatic/index.html",
    "href": "project/prismatic/index.html",
    "title": "prismatic",
    "section": "",
    "text": "github    CRAN \n\n\nThe goal of prismatic is to provide color manipulation tools in R, in a intuitive, low-dependency and functional way.\n\nintuitive All the working functions are prefixed with clr_ (color) allowing for easy autocompletion.\nlow-dependency Only depends on farver.\nfunctional All functions have consistant inputs and outputs and are thus fully pipeable.\n\nAnnouncement Blogpost"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hello, my name is Emil Hvitfeldt. Thanks for stopping by!",
    "section": "",
    "text": "I’m a Software Engineer at Posit PBC working on the tidymodels team. My educational material started right here on this blog but has grown over the years to include talks, university courses, books, and workshops at the conferences like satRday, R in Pharma, useR, rstudio::conf and upcoming posit::conf.\nMy non-work interests include: Jigsaw Puzzles, True Crime Podcasts, Reading, Nintendo Switch.\n\n\nHow to say my name, the h and d are silent"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "ISLR tidymodels labs\n\n\n\nwebsite\n\n\n\nProvides labs to ‘An Introduction to Statistical Learning’ using tidymodels\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Machine Learning for Text Analysis in R\n\n\n\nbook\n\n\n\nBook written with Julia Silge\n\n\n\n\n\n\nAug 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngenderify\n\n\n\nwebsite\n\n\n\nsatirical Gender Classification Service\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthemis\n\n\n\nR package\n\n\n\nExtra Recipes Steps for imbalanced data\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprismatic\n\n\n\nR package\n\n\n\nManipulate and visualize colors in a intuitive, low-dependency and functional way\n\n\n\n\n\n\nOct 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextdata\n\n\n\nR package\n\n\n\nProvides a framework to download, parse, and store text datasets\n\n\n\n\n\n\nJun 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscotus\n\n\n\nR package\n\n\n\nDataset of Supreme Court of the United States opinions\n\n\n\n\n\n\nMar 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidygutenbergr\n\n\n\nR package\n\n\n\nCleaned gutenbergr text\n\n\n\n\n\n\nMar 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpage\n\n\n\nR package\n\n\n\nCreates Page Layout Visualizations\n\n\n\n\n\n\nJan 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhcandersenr\n\n\n\nR package\n\n\n\nAn R Package for H.C. Andersens fairy tales\n\n\n\n\n\n\nJan 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextrecipes\n\n\n\nR package\n\n\n\nExtra Recipes Steps for Text Processing\n\n\n\n\n\n\nDec 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Text Data\n\n\n\nwebsite\n\n\n\nList of textual data sources to be used for text mining in R\n\n\n\n\n\n\nAug 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npaletteer\n\n\n\nR package\n\n\n\nA comprehensive collection of color palettes in R using a common interface\n\n\n\n\n\n\nJul 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Color Palettes\n\n\n\nwebsite\n\n\n\nComprehensive list of color palettes available in R\n\n\n\n\n\n\nJun 16, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "\nSlidecraft - the art of creating pretty slides\n",
    "section": "",
    "text": "2023\n\n\n\n\n  \n\n\n\n\nSlidecraft - the art of creating pretty slides\n\n\n\nDelightful talk covering recipes from beginning to end.\n\n\n\nJul 13, 2023\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nFlexible feature engineering using {recipes}\n\n\n\nDelightful talk covering recipes from beginning to end.\n\n\n\nJun 7, 2023\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\ntime aware recipes\n\n\n\nTalk covering experimental work on time aware recipes.\n\n\n\nMay 5, 2023\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nTidy Text Tutorial\n\n\n\n2 hour tutorial, 1 part text mining, 1 part modeling with text.\n\n\n\nApr 22, 2023\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nHaving a purrr-tastic time\n\n\n\nThis was a small talk about purrr in general and some of the new things that was included in the 1.0.0 release.\n\n\n\nFeb 21, 2023\n\n\nSlides Video Github\n\n\n\n\n\n\n2022\n\n\n\n\n  \n\n\n\n\ntidyclust - expanding tidymodels to clustering\n\n\n\nThis talk marks the grand introduction of tidyclust, a new package that provides a tidy unified interface to clustering model within the tidymodels framework.\n\n\n\nJul 27, 2022\n\n\nSlides Video Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nImprovements in text preprocessing using textrecipes\n\n\n\nThis talk gives a brief overview of the basic functionality of the textrecipes package and a look at exciting recent additions.\n\n\n\nJun 23, 2022\n\n\nSlides Video Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nMachine learning with {tidymodels}\n\n\n\nThis workshop will provide a gentle introduction to machine learning with R using the modern suite of predictive modeling packages called tidymodels.\n\n\n\nJun 20, 2022\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nI Did Advent of Code and This is What I Learned\n\n\n\nThis talk will pull back the curtain on a collective journey of learning, doing Advent of Code using R.\n\n\n\nJun 9, 2022\n\n\nSlides Video Github\n\n\n\n\n\n\n2021\n\n\n\n\n  \n\n\n\n\nWork Smarter, Not Harder RStudio\n\n\n\nThis talk will take you around the myriad of customization and automation available to all RStudio users.\n\n\n\nNov 23, 2021\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nCreating and Styling PPTX Slides with {rmarkdown}\n\n\n\nThis talk shows how we can use the pptxtemplates package to automatically style powerpoints.\n\n\n\nAug 27, 2021\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nText Preprocessing in R\n\n\n\nThis talk will focus on the {textrecipes} package and its recent advancements in the realm of text preprocessing.\n\n\n\nJun 15, 2021\n\n\nSlides Video\n\n\n\n\n\n\n\n\n  \n\n\n\n\nTidymodels - An Overview\n\n\n\nThis talk will walk through the landscape of packages and their individual place, helping you get a bird's eye view.\n\n\n\nJun 3, 2021\n\n\nSlides Github\n\n\n\n\n\n\n2020\n\n\n\n\n  \n\n\n\n\nPredictive modeling with text using tidy data principles\n\n\n\nThis tutorial is geared toward an R user with intermediate familiarity with R, RStudio, the basics of regression and classification modeling, and tidyverse packages such as dplyr and ggplot2.\n\n\n\nOct 9, 2020\n\n\nSlides\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLooking at stop words: why you shouldn’t blindly trust model defaults\n\n\n\nThis talk will focus on the importance of checking assumptions and defaults in the software you use.\n\n\n\nSep 26, 2020\n\n\nSlides Video\n\n\n\n\n\n\n\n\n  \n\n\n\n\npalette2vec: A new way to explore color palettes\n\n\n\nThis talk shows what happens when we take one step further into explorability. Using handcrafted color features, dimensionality reduction, and interactive tools will we create and explore a color palette embedding.\n\n\n\nSep 26, 2020\n\n\nSlides Video\n\n\n\n\n\n\n\n\n  \n\n\n\n\nthemis: dealing with imbalanced data by using synthetic oversampling\n\n\n\nA walkthrough of the heart of the synthetic oversampling algorithms will be given in code and visualization along with talk about performance.\n\n\n\nJul 9, 2020\n\n\nSlides Video Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPredictive modeling with text using tidy data principles\n\n\n\nThis tutorial is geared toward an R user with intermediate familiarity with R, RStudio, the basics of regression and classification modeling, and tidyverse packages such as dplyr and ggplot2.\n\n\n\nJul 7, 2020\n\n\nSlides Video Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nReproducible preprocessing with recipes\n\n\n\nTalk about using the recipes package to do preprocessing.\n\n\n\nJan 23, 2020\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGit & Github\n\n\n\nThis seminar goes into detail why and how to use git in collaborative research.\n\n\n\nJan 23, 2020\n\n\nSlides Github\n\n\n\n\n\n\n2019\n\n\n\n\n  \n\n\n\n\nBuilding a package that fits into an evolving ecosystem\n\n\n\nThis talk tells the tale of the package textrecipes; starting with the Github issue that sparked the idea for the package, go over the trials and challenges associated with building a package that heavily integrates with other packages all the way to the release on CRAN.\n\n\n\nNov 19, 2019\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nData visualization with ggplot2\n\n\n\nHastly put together slides for Data-viz prep for Hackathon.\n\n\n\nNov 9, 2019\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBuilding R Packages\n\n\n\nThis seminar will go through the different use cases for a R package, dos and don’ts and best practices.\n\n\n\nMar 28, 2019\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDebugging and Profiling in R\n\n\n\nThis seminar will cover strategies and techniques for performing debugging and code profiling in R.\n\n\n\nFeb 19, 2019\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWorking with tidymodels\n\n\n\nThis talk will go through how to use tidymodels to do modeling in a tidy fashion.\n\n\n\nJan 29, 2019\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nRStudio and R Resources\n\n\n\nThis seminar contains various links and resources for use in R and Rstudio.\n\n\n\nJan 22, 2019\n\n\nSlides Github\n\n\n\n\n\n\n2018\n\n\n\n\n  \n\n\n\n\nText Analysis in R\n\n\n\nI’ll talk you through a structured way to do exploratory data analysis(also called text mining) using tidytext to gain insight into the plain unstructured text.\n\n\n\nOct 29, 2018\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBest Practices in R\n\n\n\nThis presentation will let you though a lot of different aspects of what you can do in R to make yourself happy, make sure that future you is happy, and avoid getting mad at past you.\n\n\n\nOct 29, 2018\n\n\nSlides Github\n\n\n\n\n\n\n\n\n  \n\n\n\n\nSimilarity Measure in the Space of Color Palettes\n\n\n\nTalk about designing a similarity measure to look at color palettes.\n\n\n\nSep 25, 2018\n\n\nSlides Video Github\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16.\nI will In this post explore the ethnic diversity of the student population in schools in California. We will utilize the data provided by California Department of Education that has Enrollment by School which includes “school-level enrollment by racial/ethnic designation, gender, and grade”.\nWe will combine this data with two other datasets that contain:\nHopefully, we will be able to draw some cool inferences using these datasets while working through the complications you get when you combine datasets from the wild."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#loading-packages",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#loading-packages",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Loading packages",
    "text": "Loading packages\nThis will be a fairly standard data science exercise so we will stick to the tidyverse, rvest for scraping, patchwork for plot stitching, and add a little fanciness with ggmap for local maps.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(patchwork)\nlibrary(ggmap)"
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#enrollment-data",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#enrollment-data",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Enrollment data",
    "text": "Enrollment data\nWe start by downloading the dataset to our local disk and read it from there this is mainly to be nice, and for the fact, the download speed on the files we work with are a little slow. This can be done using readr’s read_tsv().\ndata &lt;- readr::read_tsv(\"filesenr.asp.txt\")\nTo get an idea of the data let’s have a quick glimpse()\nglimpse(data)\n## Rows: 129,813\n## Columns: 23\n## $ CDS_CODE  &lt;chr&gt; \"33672490000001\", \"33672490000001\", \"33672490000001\", \"33672…\n## $ COUNTY    &lt;chr&gt; \"Riverside\", \"Riverside\", \"Riverside\", \"Riverside\", \"Riversi…\n## $ DISTRICT  &lt;chr&gt; \"San Jacinto Unified\", \"San Jacinto Unified\", \"San Jacinto U…\n## $ SCHOOL    &lt;chr&gt; \"Nonpublic, Nonsectarian Schools\", \"Nonpublic, Nonsectarian …\n## $ ETHNIC    &lt;dbl&gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5, …\n## $ GENDER    &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"F\", \"F\", \"M\", \"F\", …\n## $ KDGN      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4,…\n## $ GR_1      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4,…\n## $ GR_2      &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3,…\n## $ GR_3      &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6,…\n## $ GR_4      &lt;dbl&gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4,…\n## $ GR_5      &lt;dbl&gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6,…\n## $ GR_6      &lt;dbl&gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_7      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_8      &lt;dbl&gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ UNGR_ELM  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_9      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_10     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_11     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_12     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ UNGR_SEC  &lt;dbl&gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ ENR_TOTAL &lt;dbl&gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1,…\n## $ ADULT     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\nand we notice that CDS_CODE is being read as a chr which is favorable in this case since we don’t want it as an integer but rather as an ID variable. If this column had been converted to integers we would have lost leading zeros which could lead to trouble."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#longitude-and-latitude-data",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#longitude-and-latitude-data",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Longitude and latitude data",
    "text": "Longitude and latitude data\nThe longitude and latitude of the schools are also available on the California Department of Education’s website here. It includes quite a lot of information, a lot of it uninteresting for this project so will take a subset of it for further analysis. read_tsv complaints a little bit when reading it in, but after some crosschecking with the .xls file also provided, does it seem to be correct so we will ignore the error (don’t just ignore error all the time! most of the time they are telling you something important! in this case, the problems stem from missing values and tab-separated values don’t mix that nicely).\nlonglat_raw &lt;- read_tsv(\"pubschls.txt\")\nlonglat &lt;- longlat_raw %&gt;%\n  select(CDSCode, Latitude, Longitude, School, City)\nLet us have a peek at the data:\nglimpse(longlat)\n## Rows: 10,014\n## Columns: 5\n## $ CDSCode   &lt;chr&gt; \"01100170000000\", \"01100170109835\", \"01100170112607\", \"01100…\n## $ Latitude  &lt;dbl&gt; 37.65821, 37.52144, 37.80452, 37.86899, 37.78465, 37.84737, …\n## $ Longitude &lt;dbl&gt; -122.0971, -121.9939, -122.2682, -122.2784, -122.2386, -122.…\n## $ School    &lt;chr&gt; NA, \"FAME Public Charter\", \"Envision Academy for Arts & Tech…\n## $ City      &lt;chr&gt; \"Hayward\", \"Newark\", \"Oakland\", \"Berkeley\", \"Oakland\", \"Oakl…\nwe see that the columns have been read in appropriate ways. We recognize the CDS_CODE from before as CDSCode in this dataset which we will use to combine the datasets later."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#income-data",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#income-data",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Income data",
    "text": "Income data\nLastly, we will get some income data, I found some fairly good data on Wikipedia. We use simple rvest tools to extract the table from the website and give it column names.\nWhile the income data is quite lovely, would it be outside the scope of this post to identify which census-designated place (CDP) each of the schools belongs it. The second best option is to use the income data on a county-by-county basis. This will mean that we trade a bit of granularity for time. But hopefully, it will still lead to some meaningful findings.\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_California_locations_by_income\"\nincome_data &lt;- read_html(url) %&gt;%\n  html_nodes(\"table\") %&gt;%\n  .[2] %&gt;%\n  html_table() %&gt;%\n  .[[1]] %&gt;%\n  set_names(c(\"county\", \"population\", \"population_density\", \n              \"per_capita_income\", \"median_household_income\",\n              \"median_family_income\"))\nlets take a look at the table:\nglimpse(income_data)\n## Rows: 58\n## Columns: 6\n## $ county                  &lt;chr&gt; \"Alameda\", \"Alpine\", \"Amador\", \"Butte\", \"Calav…\n## $ population              &lt;chr&gt; \"1,559,308\", \"1,202\", \"37,159\", \"221,578\", \"44…\n## $ population_density      &lt;chr&gt; \"2,109.8\", \"1.6\", \"62.5\", \"135.4\", \"44.0\", \"18…\n## $ per_capita_income       &lt;chr&gt; \"$36,439\", \"$24,375\", \"$27,373\", \"$24,430\", \"$…\n## $ median_household_income &lt;chr&gt; \"$73,775\", \"$61,343\", \"$52,964\", \"$43,165\", \"$…\n## $ median_family_income    &lt;chr&gt; \"$90,822\", \"$71,932\", \"$68,765\", \"$56,934\", \"$…\nhere we see a couple of things that look weirds. Every column character is valued, which it shouldn’t be since 5 out of the 6 variables should be numerical.\nWe will use the wonderful parse_number function from the readr package to convert the character values to numeric.\nincome_data_clean &lt;- income_data %&gt;%\n  mutate_at(vars(population:median_family_income), parse_number)\na quick glimpse to see everything went well\nglimpse(income_data_clean)\n## Rows: 58\n## Columns: 6\n## $ county                  &lt;chr&gt; \"Alameda\", \"Alpine\", \"Amador\", \"Butte\", \"Calav…\n## $ population              &lt;dbl&gt; 1559308, 1202, 37159, 221578, 44921, 21424, 10…\n## $ population_density      &lt;dbl&gt; 2109.8, 1.6, 62.5, 135.4, 44.0, 18.6, 1496.0, …\n## $ per_capita_income       &lt;dbl&gt; 36439, 24375, 27373, 24430, 29296, 22211, 3877…\n## $ median_household_income &lt;dbl&gt; 73775, 61343, 52964, 43165, 54936, 50503, 7979…\n## $ median_family_income    &lt;dbl&gt; 90822, 71932, 68765, 56934, 67100, 56472, 9508…\nAnd we are good to go!"
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#ethnic-diversity",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#ethnic-diversity",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Ethnic diversity",
    "text": "Ethnic diversity\nTo be able to compare the ethnic diversity between schools we need a measure that describes diversity, to begin with. I’ll use the diversity index developed in 1991 by a researcher at the University of North Carolina at Chapel Hill. The index simply asks: “What is the probability that two people in a population picked at random will be from different ethnicity”. At first, this can be seen as a daunting task, but if we look at it geometrically we notice that the calculations are quite straightforward.\nIf we imagine a population of 10 people split into two ethnicities, with 5 in each. Then we can draw the following outcome space:\ndiversity_plot(makeup = rep(5, 2))\n\n(the code for diversity_plot is displayed at the end of the article for those interested.)\nWhere the colored squares represent random picks with the same ethnicity, light grey squares pick with different ethnicities. Dark grey indicates impossible picks (the same person picked twice) and should be ignored. Now the diversity index can be calculated by dividing the number of light grey squares by the sum of light grey and colored squares.\nBefore we go on, let us look at how different populations have different diversity indexes. If we only have two groups, will the diversity score converge to 0.5 for large populations, however with small populations will the index be quite large since each person contributes such a big percentage of the group, making it harder to pick another one in the same group.\ndiversity_plot(rep(1, 2)) + \n  diversity_plot(rep(2, 2)) +\n  diversity_plot(rep(4, 2)) +\n  diversity_plot(rep(8, 2))\n\nEffectively giving that adding a single person from a new group will maximize the contribution to the index.\ndiversity_plot(c(rep(1, 2), 1, 1, 1)) + \n  diversity_plot(c(rep(2, 2), 1, 1, 1)) +\n  diversity_plot(c(rep(4, 2), 1, 1, 1)) +\n  diversity_plot(c(rep(8, 2), 1, 1, 1))\n\ndiversity_plot(c(2, 3, 4, 5, 6) * 1) + \n  diversity_plot(c(2, 3, 4, 5, 6) * 2) +\n  diversity_plot(c(2, 3, 4, 5, 6) * 3) +\n  diversity_plot(c(2, 3, 4, 5, 6) * 4)\n\nNow that we have seen the diversity index in use let’s apply it to the data we have collected.\nWe would like to have the data in a different kind of tidy format, namely we want each row to represent each school. Taking another look at the data\nglimpse(data)\n## Rows: 129,813\n## Columns: 23\n## $ CDS_CODE  &lt;chr&gt; \"33672490000001\", \"33672490000001\", \"33672490000001\", \"33672…\n## $ COUNTY    &lt;chr&gt; \"Riverside\", \"Riverside\", \"Riverside\", \"Riverside\", \"Riversi…\n## $ DISTRICT  &lt;chr&gt; \"San Jacinto Unified\", \"San Jacinto Unified\", \"San Jacinto U…\n## $ SCHOOL    &lt;chr&gt; \"Nonpublic, Nonsectarian Schools\", \"Nonpublic, Nonsectarian …\n## $ ETHNIC    &lt;dbl&gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5, …\n## $ GENDER    &lt;chr&gt; \"M\", \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"F\", \"F\", \"M\", \"F\", …\n## $ KDGN      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4,…\n## $ GR_1      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4,…\n## $ GR_2      &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3,…\n## $ GR_3      &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6,…\n## $ GR_4      &lt;dbl&gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4,…\n## $ GR_5      &lt;dbl&gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6,…\n## $ GR_6      &lt;dbl&gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_7      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_8      &lt;dbl&gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ UNGR_ELM  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_9      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_10     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_11     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ GR_12     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ UNGR_SEC  &lt;dbl&gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ ENR_TOTAL &lt;dbl&gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1,…\n## $ ADULT     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\nand we would like to across each ETHNIC category within each school, ignoring GENDER (including gender could be an interesting question for another time). Luckily the total enrollment is already calculated for us ENR_TOTAL so we don’t have to do that manually.\nsmall_data &lt;- data %&gt;%\n  select(CDS_CODE, ETHNIC, ENR_TOTAL) %&gt;%\n  group_by(CDS_CODE, ETHNIC) %&gt;%\n  summarize(ENR_TOTAL = sum(ENR_TOTAL)) %&gt;%\n  spread(ETHNIC, ENR_TOTAL, fill = 0) %&gt;%\n  ungroup()\n## `summarise()` has grouped output by 'CDS_CODE'. You can override using the `.groups` argument.\nglimpse(small_data)\n## Rows: 10,483\n## Columns: 10\n## $ CDS_CODE &lt;chr&gt; \"01100170112607\", \"01100170123968\", \"01100170124172\", \"011001…\n## $ `0`      &lt;dbl&gt; 0, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 23, 0, 4, 2, 7, 0, …\n## $ `1`      &lt;dbl&gt; 5, 6, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 3, 6, 0, 0, 4, 1, 1, 0…\n## $ `2`      &lt;dbl&gt; 8, 24, 161, 22, 0, 11, 0, 15, 0, 3, 30, 3, 98, 109, 49, 91, 4…\n## $ `3`      &lt;dbl&gt; 5, 2, 1, 0, 0, 2, 0, 23, 0, 2, 2, 0, 4, 16, 0, 1, 1, 9, 0, 0,…\n## $ `4`      &lt;dbl&gt; 1, 2, 7, 5, 1, 1, 2, 2, 1, 0, 18, 0, 21, 35, 16, 34, 21, 104,…\n## $ `5`      &lt;dbl&gt; 212, 125, 18, 96, 21, 109, 123, 452, 425, 21, 190, 8, 17, 97,…\n## $ `6`      &lt;dbl&gt; 150, 23, 14, 75, 58, 95, 21, 84, 23, 1, 33, 6, 7, 81, 98, 63,…\n## $ `7`      &lt;dbl&gt; 18, 16, 35, 115, 2, 13, 1, 1, 8, 10, 85, 12, 30, 98, 177, 164…\n## $ `9`      &lt;dbl&gt; 4, 7, 121, 57, 5, 11, 0, 10, 4, 2, 21, 2, 6, 41, 64, 62, 34, …\nBefore we move on, let us reference the data documentation to see what each of the ETHNIC numbers means. File Structure which states the following:\n\nCode 0 = Not reported\nCode 1 = American Indian or Alaska Native, Not Hispanic\nCode 2 = Asian, Not Hispanic\nCode 3 = Pacific Islander, Not Hispanic\nCode 4 = Filipino, Not Hispanic\nCode 5 = Hispanic or Latino\nCode 6 = African American, not Hispanic\nCode 7 = White, not Hispanic\nCode 9 = Two or More Races, Not Hispanic\n\nSo now we have two decisions we need to deal with before moving on. How to take care of the “Not reported” cases, and “Two or More Races, Not Hispanic”. The second point has been discussed before Updating the USA TODAY Diversity Index.\nLet’s start with the case of code 0. We notice that the category generally is quite small compared to the other groups, so we need to reason about what would happen if we drop them.\ndiversity_plot(c(10, 10, 2)) +\n  diversity_plot(c(10, 10)) +\n  diversity_plot(c(11, 11)) +\n  diversity_plot(c(10, 10, 10, 3)) +\n  diversity_plot(c(10, 10, 10)) +\n  diversity_plot(c(11, 11, 11)) +\n  plot_layout(nrow = 2)\n\nas we see that having them be a separate group (first column), gives a higher diversity score than ignoring them (second column) or adding them evenly to the remaining groups (third column). However, ignoring them and spreading them gives mostly the same diversity index (when the group is small compared to the whole). Thus we will drop the “Not reported” column as it would otherwise inflate the diversity index.\nCoping with multi-ethnicity is quite hard, and we will pick between four options.\n\nAssume everyone in “Two or More Races, Not Hispanic” is the same ethnicity\nAssume everyone in “Two or More Races, Not Hispanic” is all different ethnicities\nIgnore the group “Two or More Races, Not Hispanic”\nDistribute the group “Two or More Races, Not Hispanic” evenly into the other groups\n\nLet us evaluate the different choices with some visualizations.\nAssume we have 3 main groups. with 5 in each, and an additional 3 people who have picked “Two or More Races, Not Hispanic”.\ndiversity_plot(c(5, 5, 5, 3)) +\n  diversity_plot(c(5, 5, 5, rep(1, 3))) +\n  diversity_plot(c(5, 5, 5)) +\n  diversity_plot(c(6, 6, 6))\n\nOption 3 and 4 both yield low diversity index considering that the choice of “Two or More Races, Not Hispanic” must indicate that they are different enough not to be put into one of the more precisely defined groups. The first option while appalling from a computational standpoint would treat a black-white mixture and an Asian-American Indian as members of the same group even though they had no racial identity in common. Thus We will work with the second option which in any case might overestimate the diversity of any given population as they oath to be some people “Two or More Races, Not Hispanic” with identical mixes (siblings would be a prime example).\nAfter deciding we can create a dplyr friendly function to calculate the diversity index based on a collection of columns. Here we denote y as the column denoting “Two or More Races, Not Hispanic”.\ndiversity &lt;- function(..., y) {\n x &lt;- sapply(list(...), cbind)\n total &lt;- cbind(x, y)\n 1 - (rowSums(x ^ 2) - rowSums(x)) / (rowSums(total) ^ 2 - rowSums(total))\n}"
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#bring-back-the-data",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#bring-back-the-data",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Bring back the data!",
    "text": "Bring back the data!\nWe are finally able to calculate some diversity indexes!!! Using our recently made function within mutate gives us what we need.\ndata_diversity &lt;- small_data %&gt;%\n  mutate(diversity = diversity(`1`, `2`, `3`, `4`, `5`, `6`, `7`, y = `9`))\nLet us run a summary to see what ranges we get.\nsummary(data_diversity$diversity)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##  0.0000  0.3113  0.5216  0.4686  0.6351  1.0000      88\nInteresting. All the numbers are between 0 and 1 which is good! that means that the diversity worked as intended. However there are 88 schools have NA valued diversity, lets look at those schools:\nfilter(data_diversity, is.na(diversity)) %&gt;% head()\n## # A tibble: 6 x 11\n##   CDS_CODE         `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity\n##   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n## 1 01612590121210     0     0     0     0     0     0     1     0     0       NaN\n## 2 04755070000001     2     0     0     0     0     0     0     1     0       NaN\n## 3 06615980000001     0     0     0     0     0     0     0     1     0       NaN\n## 4 07618040000000     0     0     0     0     0     0     0     1     0       NaN\n## 5 09619600000001     0     0     0     0     0     0     0     1     0       NaN\n## 6 10622400114587     0     0     0     0     0     1     0     0     0       NaN\nSo it seems like some of the schools have such a low number of kids that the calculations break down. We will exclude these schools for now.\ndata_diversity &lt;- data_diversity %&gt;%\n  filter(!is.na(diversity))\nNext let us look at the distribution of diversity indexes.\nggplot(data_diversity, aes(diversity)) +\n  geom_histogram(binwidth = 0.01)\n\nWe notice that most of the schools follow a nice shape, except a spike at 0 and 1. Lets look at the 0’s first:\ndata_diversity %&gt;%\n  filter(diversity == 0) %&gt;%\n  head(20)\n## # A tibble: 20 x 11\n##    CDS_CODE        `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity\n##    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n##  1 016116800000…     0     0     0     0     0     0     2     0     0         0\n##  2 021002502300…     0     2     0     0     0     0     0     0     0         0\n##  3 076171300000…     0     0     0     0     0     0     0     6     0         0\n##  4 096195200000…     0     0     0     0     0     0     0     3     0         0\n##  5 096197800000…     0     0     0     0     0     0     0     4     0         0\n##  6 106212510304…     0     0     0     0     0    26     0     0     0         0\n##  7 106212510308…     0     0     0     0     0     2     0     0     0         0\n##  8 106215801089…     1     0     0     0     0    13     0     0     0         0\n##  9 106236401268…     0     0     0     0     0    47     0     0     0         0\n## 10 107380910301…     0     0     0     0     0    12     0     0     0         0\n## 11 107399900000…     0     0     0     0     0     3     0     0     0         0\n## 12 107512710302…     0     0     0     0     0    18     0     0     0         0\n## 13 107512710307…     0     0     0     0     0     5     0     0     0         0\n## 14 107523410303…     0     0     0     0     0     8     0     0     0         0\n## 15 107523460058…     0     0     0     0     0   174     0     0     0         0\n## 16 117656211300…     2     0     0     0     0     9     0     0     0         0\n## 17 126281000000…     0     0     0     0     0     0     0     2     0         0\n## 18 127538212300…     0     0     0     0     0     0     0    11     0         0\n## 19 127538261078…     0     0     0     0     0     0     0     7     0         0\n## 20 131013213301…     0     0     0     0     0     8     0     0     0         0\nThese all appear to be schools with just one race in them, most appear to be of category 5 which is “Hispanic or Latino”. Next lets take a look at the 1’s:\ndata_diversity %&gt;%\n  filter(diversity == 1) %&gt;%\n  head(20)\n## # A tibble: 20 x 11\n##    CDS_CODE        `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity\n##    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n##  1 016112700000…     0     0     1     0     0     1     0     1     0         1\n##  2 046152300000…     0     0     0     0     0     0     0     1     1         1\n##  3 056155600000…     0     0     0     0     0     1     0     1     0         1\n##  4 076176200000…     0     0     0     0     0     0     1     1     0         1\n##  5 076177000000…     1     0     1     0     0     1     0     1     0         1\n##  6 097378300000…     1     1     0     0     0     1     0     1     0         1\n##  7 106215801202…     0     0     0     0     0     0     1     1     0         1\n##  8 117548111301…     0     0     0     0     0     1     0     1     0         1\n##  9 141014014300…     0     1     0     0     0     1     0     1     0         1\n## 10 156336200000…     0     0     0     0     0     1     1     0     0         1\n## 11 176402200000…     0     0     0     0     0     1     0     1     0         1\n## 12 196456819961…     0     0     0     0     0     1     0     1     0         1\n## 13 197343761144…     0     0     0     0     0     1     1     0     1         1\n## 14 207641420300…     0     0     0     0     0     1     0     1     0         1\n## 15 246578900000…     0     0     0     0     0     0     1     1     0         1\n## 16 257358525301…     0     0     0     0     0     0     0     1     1         1\n## 17 261026401286…     0     0     0     0     0     1     0     1     1         1\n## 18 286624100000…     0     0     0     0     0     1     0     1     0         1\n## 19 316684500000…     0     0     0     0     0     1     0     1     0         1\n## 20 336697701341…     0     0     0     0     0     1     1     1     0         1\nAs we have seen before, a diversity index of 1 can only happen if the maximal number of students in each category is 1. These schools seem to be rather small. Taking a look at the first school here\ndata %&gt;% filter(CDS_CODE == \"17640220000001\") %&gt;% pull(SCHOOL)\n## [1] \"Nonpublic, Nonsectarian Schools\" \"Nonpublic, Nonsectarian Schools\"\nand we get that it is a “Nonpublic, Nonsectarian Schools” which by further investigation shows that there are quite a few of those. Lets take a look at the total enrollment in each of the schools, we have seen a couple of instances with low enrollment and we wouldn’t want those to distort our view.\ntotal_students &lt;- data %&gt;% \n  select(CDS_CODE:SCHOOL, ENR_TOTAL) %&gt;% \n  group_by(CDS_CODE) %&gt;%\n  summarise(ENR_TOTAL = sum(ENR_TOTAL)) %&gt;% \n  ungroup() \nWe also create a meta data.frame which stores the CDS_CODE along with county, district, and school name.\nmeta &lt;- data %&gt;%\n  select(CDS_CODE:SCHOOL) %&gt;%\n  distinct() \nLooking at the distribution of total enrollment\ntotal_students %&gt;% \n  ggplot(aes(ENR_TOTAL)) +\n  geom_histogram(bins = 1000)\n\nWe see a big hump around the 600-700 mark, but also a big spike towards 0, lets take a closer look at the small values\ntotal_students %&gt;% \n  filter(ENR_TOTAL &lt; 250) %&gt;%\n  ggplot(aes(ENR_TOTAL)) +\n  geom_histogram(bins = 250)\n\nHere we make another choice that will ultimately change the outcome of the analysis. But I will restrict the investigation to schools with a total enrollment of 50 or more.\ndata_big &lt;- total_students %&gt;%\n  left_join(meta, by = \"CDS_CODE\") %&gt;%\n  filter(ENR_TOTAL &gt;= 50)\nThen we join that back to our enrollment data such that we have meta and diversity information in the same data.frame.\ndata_enrollment &lt;- data_diversity %&gt;%\n  right_join(data_big, by = \"CDS_CODE\")\nWe start by looking to see if there is some correlation between total enrollment (ENR_TOTAL) and the diversity index (diversity). We fit with a simple linear model, to begin with.\ndata_enrollment %&gt;%\n  ggplot(aes(ENR_TOTAL, diversity)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\") +\n  theme_minimal() +\n  ylim(0, 1) +\n  labs(x = \"Enrollment\", \n       y = \"Diversity Index\",\n       title = \"Larger Californian schools tend to have a higher diversity index\")\n## `geom_smooth()` using formula 'y ~ x'\n\nIt is quite a small slope but still about a 5% increase in diversity per 2000 students. Let us verify the correlation by checking the null hypothesis.\ndata_enrollment %&gt;%\n  mutate(ENR_TOTAL = ENR_TOTAL / 1000) %&gt;%\n  lm(diversity ~ ENR_TOTAL, data = .) %&gt;%\n  summary()\n## \n## Call:\n## lm(formula = diversity ~ ENR_TOTAL, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5053 -0.1584  0.0533  0.1658  0.3926 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 0.447968   0.003489 128.399  &lt; 2e-16 ***\n## ENR_TOTAL   0.023239   0.004132   5.624 1.92e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2111 on 9407 degrees of freedom\n## Multiple R-squared:  0.003351,   Adjusted R-squared:  0.003245 \n## F-statistic: 31.63 on 1 and 9407 DF,  p-value: 1.923e-08\nWe can’t reject the null hypothesis of no correlation and we get a better estimate of the slope to 2.3%. Of cause, we can’t extrapolate the results outside the range of the existing data since the response variable is bounded."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#where-are-we",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#where-are-we",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Where are we?",
    "text": "Where are we?\nLet’s try to join the enrollment data to the geographical data so we can bring out some maps! The longlat dataset has quite an unfortunate number of missing values.\nlonglat$Latitude %&gt;% is.na() %&gt;% mean()\n## [1] 0.2784102\nBut we will see if we can work with it anyways.\nmiss_lat_div &lt;- data_enrollment %&gt;%\n  left_join(longlat, by = c(\"CDS_CODE\" = \"CDSCode\")) %&gt;% \n  filter(is.na(Latitude)) %&gt;%\n  pull(diversity)\n\nhave_lat_div &lt;- data_enrollment %&gt;%\n  left_join(longlat, by = c(\"CDS_CODE\" = \"CDSCode\")) %&gt;% \n  filter(!is.na(Latitude)) %&gt;%\n  pull(diversity)\nSadly it turns out that the longitude and latitude are NOT missing at random as the distribution of the diversity index is not the same for missing and non-missing data.\ndensity(miss_lat_div) %&gt;% plot()\n\ndensity(have_lat_div) %&gt;% plot()\n\nWe will still venture on with the remaining data, but with a higher caution than before. Hopefully, we will still be able to see some clustering in the remaining data.\nmap &lt;- map_data(\"state\") %&gt;%\n  filter(region == \"california\")\n\nmap_point &lt;- data_enrollment %&gt;%\n  left_join(longlat, by = c(\"CDS_CODE\" = \"CDSCode\")) %&gt;%\n  filter(!is.na(Latitude)) \n\nmap %&gt;%\n  ggplot(aes(long, lat, group = group)) +\n  geom_polygon(fill = \"grey80\") +\n  coord_map() +\n  theme_minimal() +\n  geom_point(aes(Longitude, Latitude, color = diversity),\n             data = map_point,\n             inherit.aes = FALSE, alpha = 0.1) +\n  scale_color_viridis_c(option = \"A\") +\n  labs(title = \"Diversity of schools across California\")\n\nWe don’t notice any major geographical trends. However, there are still some gold nuggets here. Dark grey points are overlapping low-diversity index points since the alpha is set to 0.1 therefore we see a couple of instances in mid-California where there are a couple of low-diversity index schools, with each point landing on a certain city.\nThe two major cities (Los Angeles and San Francisco) both have quite a few schools with high diversity, however, Los Angeles has some quite dark spots. Let us zoom in to the city level. Since we are getting more local, we will be using the ggmap package to quarry the Google Maps so we get a nice underlying map.\nLA_map &lt;- get_map(location = 'Los Angeles', zoom = 9)\nggmap(LA_map) +\n  geom_point(data = map_point %&gt;%\n                      filter(Longitude &gt; -119, Longitude &lt; -117,\n                             Latitude &lt; 34.5, Latitude &gt; 33.5), \n             aes(Longitude, Latitude, color = diversity),\n             alpha = 0.2) +\n  scale_color_viridis_c(option = \"A\") +\n  labs(title = \"Low diversity areas tends towards city centers \\nin Greater Los Angeles Area\")\n\nThis map shows much more interesting trends!! It appears that low diversity schools cluster together near city centers."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#where-is-the-money",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#where-is-the-money",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Where is the money?",
    "text": "Where is the money?\nWe will end this post by taking a look at if the median household income in the county in which the school is located correlates with the diversity index we have calculated for each school.\nThis is simply done by joining the two data.frames together and piping them into ggplot.\ndata_enrollment %&gt;% \n  left_join(income_data_clean, by = c(\"COUNTY\" = \"county\")) %&gt;%\n  ggplot(aes(median_household_income, diversity)) +\n  geom_jitter(alpha = 0.1, width = 500) +\n  geom_smooth(method = \"lm\") +\n  ylim(0, 1) +\n  theme_minimal() +\n  labs(x = \"Median household income\", y = \"Diversity Index\",\n       title = \"Higher diversity index in CA counties with high Median household income\")\n\nand we do indeed see a positive correlation between median household income and ethnic diversity in schools in California.\nThis is the end of this analysis, I hope you enjoyed it! See you again next time."
  },
  {
    "objectID": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#diversity-plotting-function",
    "href": "post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index.html#diversity-plotting-function",
    "title": "Analysing ethnic diversity in Californian school",
    "section": "Diversity plotting function",
    "text": "Diversity plotting function\ndiversity_plot &lt;- function(makeup, fix = 0.45) {\n  df &lt;- tibble(id = seq_len(sum(makeup)),\n               race = imap(makeup, ~ rep(.y, .x)) %&gt;% unlist())\n  \n  cols &lt;- structure(rainbow(length(makeup)), \n            names = as.character(seq_len(length(makeup)))) %&gt;%\n    c(\"diag\" = \"grey30\",\n      \"other\" = \"grey70\")\n  \n  df1 &lt;- crossing(df, df) %&gt;%\n    mutate(fill = case_when(id == id1 ~ \"diag\",\n                            race == race1 ~ as.character(race),\n                            TRUE ~ \"other\")) \n  \n  df1 %&gt;%\n    ggplot(aes(xmin = id - fix, xmax = id + fix,\n               ymin = id1 - fix, ymax = id1 + fix)) +\n    geom_rect(aes(fill = fill)) +\n    theme(panel.background = element_blank(), \n          panel.border = element_blank()) +\n    scale_y_continuous(breaks = df[[\"id\"]], labels = df[[\"race\"]]) +\n    scale_x_continuous(breaks = df[[\"id\"]], labels = df[[\"race\"]]) +\n    coord_fixed() +\n    scale_fill_manual(values = cols) +\n    guides(fill = \"none\") +\n    labs(title = paste0(\"The diversity score is \", \n                        signif(mean(df1$fill %in% c(\"diag\", \"other\")), digits = 3)))\n}\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-13                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n bitops        1.0-7   2021-04-24 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggmap       * 3.0.0   2019-02-05 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jpeg          0.1-8.1 2019-10-24 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lattice       0.20-44 2021-05-02 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n mapproj       1.2.7   2020-02-03 [1] CRAN (R 4.1.0)                   \n maps          3.3.0   2018-04-03 [1] CRAN (R 4.1.0)                   \n Matrix        1.3-3   2021-05-04 [1] CRAN (R 4.1.0)                   \n mgcv          1.8-35  2021-04-18 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n nlme          3.1-152 2021-02-04 [1] CRAN (R 4.1.0)                   \n patchwork   * 1.1.1   2020-12-17 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n plyr          1.8.6   2020-03-03 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n RgoogleMaps   1.4.5.3 2020-02-12 [1] CRAN (R 4.1.0)                   \n rjson         0.2.20  2018-06-08 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n sp            1.4-5   2021-01-10 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n viridisLite   0.4.0   2021-04-13 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/changing-glyph-in-ggplot2/index.html",
    "href": "post/changing-glyph-in-ggplot2/index.html",
    "title": "Changing Glyph in legend in ggplot2",
    "section": "",
    "text": "Introduction\nThe newest version of ggplot2 3.2.0 gave us the ability to change the glyph in the legend like so\nlibrary(ggplot2)\n\nggplot(economics_long, aes(date, value01, colour = variable)) +\n  geom_line(key_glyph = \"timeseries\")\n\nAnd they can likewise be specified with the draw_key_* functions as well\nggplot(economics_long, aes(date, value01, colour = variable)) +\n  geom_line(key_glyph = draw_key_timeseries)\n\n\n\nShowcase\nThe following are all the available draw_key_* functions in ggplot2. Notice that the dark gray color in dot-plot and polygon is a result of an unspecified fill aesthetic. Code to generate these figures can be found at the end of this post.\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n               \n\n\nCustom glyph key\nSince the draw_key_* function just returns a grob, you can use spend some time and create your own custom glyphs! Taking inspiration from draw_key_boxplot\ndraw_key_boxplot\n## function (data, params, size) \n## {\n##     grobTree(linesGrob(0.5, c(0.1, 0.25)), linesGrob(0.5, c(0.75, \n##         0.9)), rectGrob(height = 0.5, width = 0.75), linesGrob(c(0.125, \n##         0.875), 0.5), gp = gpar(col = data$colour %||% \"grey20\", \n##         fill = alpha(data$fill %||% \"white\", data$alpha), lwd = (data$size %||% \n##             0.5) * .pt, lty = data$linetype %||% 1))\n## }\n## &lt;bytecode: 0x7fcdd88de808&gt;\n## &lt;environment: namespace:ggplot2&gt;\nwill I try to make a glyph by myself using both points and lines.\nlibrary(grid)\nlibrary(rlang)\ndraw_key_smile &lt;- function(data, params, size) {\n  grobTree(\n    pointsGrob(0.25, 0.75, size = unit(.25, \"npc\"), pch = 16),\n    pointsGrob(0.75, 0.75, size = unit(.25, \"npc\"), pch = 16),\n    linesGrob(c(0.9, 0.87, 0.78, 0.65, 0.5, 0.35, 0.22, 0.13, 0.1), \n              c(0.5, 0.35, 0.22, 0.13, 0.1, 0.13, 0.22, 0.35, 0.5)),\n    gp = gpar(\n      col = data$colour %||% \"grey20\",\n      fill = alpha(data$fill %||% \"white\", data$alpha),\n      lwd = (data$size %||% 0.5) * .pt,\n      lty = data$linetype %||% 1\n    )\n  )\n}\n\nggplot(economics_long, aes(date, value01, colour = variable)) +\n  geom_line(key_glyph = draw_key_smile)\n\nAnd it looks so happy!\n\n\nAppendix\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(grid)\n\ndraws &lt;- ls(getNamespace(\"ggplot2\"), pattern = \"^draw_key_\")\n\nlegend_fun &lt;- function(x) {\n  ggg &lt;- economics_long %&gt;%\n    mutate(variable = factor(variable, labels = paste(\"Option\", LETTERS[1:5]))) %&gt;%\n    ggplot(aes(date, value01, colour = variable)) +\n  geom_line(key_glyph = get(x)) +\n    labs(color = x) \n  \n  legend &lt;- cowplot::get_legend(ggg)\n  \n  grid.newpage()\n  grid.draw(legend)\n}\n\npurrr::walk(draws[1:12], legend_fun)\np &lt;- ggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) + \n  geom_text(aes(colour = factor(ceiling(seq_len(nrow(mtcars)) %% 5), labels = paste(\"Option\", LETTERS[1:5])))) +\n  labs(color = \"draw_key_text\")\nlegend &lt;- cowplot::get_legend(p)\n\ngrid.newpage()\ngrid.draw(legend)\npurrr::walk(draws[14:16], legend_fun)\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n cowplot       1.1.1   2020-12-30 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n magrittr    * 2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr         0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n rlang       * 0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble        3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2019-05-21-center-continuous-palettes-in-ggplot2/index.html",
    "href": "post/2019-05-21-center-continuous-palettes-in-ggplot2/index.html",
    "title": "Center continuous palettes in ggplot2",
    "section": "",
    "text": "Using a divergent color palette can be beneficial when you want to draw attention to some values compared to a fixed point. Like temperature around freezing, monetary values around zero, and so on. However, it can be hard to align 0 to the middle of a continuous color scale. This post will explain how to do this correctly for scale_colour_distiller and scale_fill_distiller, and this will also work for extension packages such as scico.\n\nPackages and data\nlibrary(ggplot2)\nlibrary(scico)\n\ntheme_set(theme_minimal())\n\nexample_data &lt;- data.frame(name = letters[1:10],\n                           value = -2:7 + 0.5)\n\n\nThe problem\nFirst, let’s construct a simple chart, we have a bar chart where some of the bars go up, and some of the bars go down.\nggplot(example_data, aes(name, value)) +\n  geom_col()\n\nNext, let’s add some color by assigning the value to the fill aesthetic.\nggplot(example_data, aes(name, value, fill = value)) +\n  geom_col()\n\nUsing a sequential palette for a chart like this doesn’t give us much insight. Lets add a divergent scale with scale_fill_gradient2(). While it is doing its job, you still have to define the colors yourself.\nggplot(example_data, aes(name, value, fill = value)) +\n  geom_col() +\n  scale_fill_gradient2()\n\nLets instead use the scale_fill_distiller() function to access the continuous versions of the brewer scales.\nggplot(example_data, aes(name, value, fill = value)) +\n  geom_col() +\n  scale_fill_distiller(type = \"div\")\n\nBut look! some of the upwards-facing bars are colored green instead of orange.\n\n\nThe solution\nThe solution is to manually specify the limits of the color palette such that the center of the palette appears in the middle of the range. This is simply done by finding the absolute maximum of the range of the variable to are mapping to the color. We then set the limits to go from negative max to positive max, thus making zero appear in the middle.\nlimit &lt;- max(abs(example_data$value)) * c(-1, 1)\n\nggplot(example_data, aes(name, value, fill = value)) +\n  geom_col() +\n  scale_fill_distiller(type = \"div\", limit = limit)\n\nThis approach also works with the scico package.\nlimit &lt;- max(abs(example_data$value)) * c(-1, 1)\n\nggplot(example_data, aes(name, value, fill = value)) +\n  geom_col() +\n  scale_fill_scico(palette = \"cork\", limit = limit) \n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date       lib source                           \n assertthat     0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n blogdown       1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown       0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib          0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli            3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr          0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools      0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace     2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon         1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI            1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n desc           1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details      * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest         0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr          1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate       0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi          0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver         2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n generics       0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2      * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue           1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable         0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n highr          0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools      0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr           1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib      0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite       1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr        * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling       0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lifecycle      1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n magrittr       2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar         1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png            0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr          0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6             2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n RColorBrewer   1.1-2   2014-12-07 [1] CRAN (R 4.1.0)                   \n rlang          0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown      2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot      2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass           0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales         1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n scico        * 1.2.0   2020-06-08 [1] CRAN (R 4.1.0)                   \n sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi        1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr        1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble         3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyselect     1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n utf8           1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs          0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr          2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun           0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2           1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml           2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/textrecipes-version-0-4-0/index.html",
    "href": "post/textrecipes-version-0-4-0/index.html",
    "title": "Textrecipes Version 0.4.0",
    "section": "",
    "text": "I’m happy to announce that version 0.4.0 of textrecipes got on CRAN a couple of days ago. This will be a brief post going over the major additions and changes."
  },
  {
    "objectID": "post/textrecipes-version-0-4-0/index.html#breaking-changes",
    "href": "post/textrecipes-version-0-4-0/index.html#breaking-changes",
    "title": "Textrecipes Version 0.4.0",
    "section": "Breaking changes 💣",
    "text": "Breaking changes 💣\nI put this change at the top of this post to make sure it gets enough coverage. The step_lda() function will no longer accepts character variables and instead takes tokenlist variables. I don’t expect this to affect too many people since it appears that the use of this step is fairly limited.\nFor a recipe where step_lda() is used on a variable text_var\nrecipe(~ text_var, data = data) %&gt;%\n  step_lda(text_var)\ncan be made to work the same as before by including this step_tokenize() step before it. It includes a custom tokenizer which was used inside the old version of step_lda()\n\nrecipe(~ text_var, data = data) %&gt;%\n  step_tokenize(text_var, \n                custom_token = function(x) text2vec::word_tokenizer(tolower(x))) %&gt;%\n  step_lda(text_var)\nThis change was long overdue since it didn’t follow the rest of the steps since it was doing tokenization internally. This change provides more flexability when using step_lda() in its current state and allows me to consider adding more engine to step_lda()."
  },
  {
    "objectID": "post/textrecipes-version-0-4-0/index.html#cleaning",
    "href": "post/textrecipes-version-0-4-0/index.html#cleaning",
    "title": "Textrecipes Version 0.4.0",
    "section": "Cleaning 🧼",
    "text": "Cleaning 🧼\nIf your data has weird characters and spaces in them messing up your model then the following steps will make you very happy. step_clean_levels() and step_clean_names() works much like janitor’s clean_names() function. Character variables and column names are changes such that they only contain alphanumeric characters and underscores.\nConsider the Smithsonian data.frame. The name variable contains entries with many character, cases, spaces, and punctuations.\nlibrary(recipes)\nlibrary(textrecipes)\nlibrary(modeldata)\n\ndata(Smithsonian)\nSmithsonian\n## # A tibble: 20 x 3\n##    name                                                    latitude longitude\n##    &lt;chr&gt;                                                      &lt;dbl&gt;     &lt;dbl&gt;\n##  1 Anacostia Community Museum                                  38.9     -77.0\n##  2 Arthur M. Sackler Gallery                                   38.9     -77.0\n##  3 Arts and Industries Building                                38.9     -77.0\n##  4 Cooper Hewitt, Smithsonian Design Museum                    40.8     -74.0\n##  5 Freer Gallery of Art                                        38.9     -77.0\n##  6 Hirshhorn Museum and Sculpture Garden                       38.9     -77.0\n##  7 National Air and Space Museum                               38.9     -77.0\n##  8 Steven F. Udvar-Hazy Center                                 38.9     -77.4\n##  9 National Museum of African American History and Culture     38.9     -77.0\n## 10 National Museum of African Art                              38.9     -77.0\n## 11 National Museum of American History                         38.9     -77.0\n## 12 National Museum of the American Indian                      38.9     -77.0\n## 13 George Gustav Heye Center                                   40.7     -74.0\n## 14 National Museum of Natural History                          38.9     -77.0\n## 15 National Portrait Gallery                                   38.9     -77.0\n## 16 National Postal Museum                                      38.9     -77.0\n## 17 Renwick Gallery                                             38.9     -77.0\n## 18 Smithsonian American Art Museum                             38.9     -77.0\n## 19 Smithsonian Institution Building                            38.9     -77.0\n## 20 National Zoological Park                                    38.9     -77.1\nWhen using step_clean_levels()\nrecipe(~ name, data = Smithsonian) %&gt;%\n  step_clean_levels(name) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 20 x 1\n##    name                                                   \n##    &lt;fct&gt;                                                  \n##  1 anacostia_community_museum                             \n##  2 arthur_m_sackler_gallery                               \n##  3 arts_and_industries_building                           \n##  4 cooper_hewitt_smithsonian_design_museum                \n##  5 freer_gallery_of_art                                   \n##  6 hirshhorn_museum_and_sculpture_garden                  \n##  7 national_air_and_space_museum                          \n##  8 steven_f_udvar_hazy_center                             \n##  9 national_museum_of_african_american_history_and_culture\n## 10 national_museum_of_african_art                         \n## 11 national_museum_of_american_history                    \n## 12 national_museum_of_the_american_indian                 \n## 13 george_gustav_heye_center                              \n## 14 national_museum_of_natural_history                     \n## 15 national_portrait_gallery                              \n## 16 national_postal_museum                                 \n## 17 renwick_gallery                                        \n## 18 smithsonian_american_art_museum                        \n## 19 smithsonian_institution_building                       \n## 20 national_zoological_park\nWe see that everything has been cleaned to avoid potential confusion and errors.\nthe almost more important step is step_clean_names() as it allows you to clean the variables names that could trip up various modeling packages\nugly_names &lt;- tibble(\n  `        Some      spaces     ` = 1,\n  `BIGG and small case` = 2,\n  `.period` = 3\n)\n\nrecipe(~ ., data = ugly_names) %&gt;%\n  step_clean_names(all_predictors()) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 1 x 3\n##   some_spaces bigg_and_small_case period\n##         &lt;dbl&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n## 1           1                   2      3"
  },
  {
    "objectID": "post/textrecipes-version-0-4-0/index.html#new-tokenizers",
    "href": "post/textrecipes-version-0-4-0/index.html#new-tokenizers",
    "title": "Textrecipes Version 0.4.0",
    "section": "New tokenizers",
    "text": "New tokenizers\nThere is two new engines available in step_tokenize(). the tokenizers.bpe engine lets you perform Byte Pair Encoding on you text as a mean of tokenization.\ndata(\"okc_text\")\n\nrecipe(~ essay6, data = okc_text) %&gt;%\n  step_tokenize(essay6, engine = \"tokenizers.bpe\") %&gt;%\n  step_tokenfilter(essay6, max_times = 100) %&gt;%\n  step_tf(essay6) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 750 x 100\n##    `tf_essay6_:` `tf_essay6_!` `tf_essay6_?` `tf_essay6_?&lt;br` tf_essay6_...\n##            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n##  1             1             0             0                0             0\n##  2             0             0             0                0             0\n##  3             0             0             0                0             0\n##  4             0             0             0                0             0\n##  5             0             0             0                0             0\n##  6             0             0             0                0             0\n##  7             0             0             0                1             0\n##  8             0             0             0                0             0\n##  9             0             0             0                0             0\n## 10             0             0             0                0             0\n## # … with 740 more rows, and 95 more variables: `tf_essay6_'s` &lt;dbl&gt;,\n## #   `tf_essay6_\"` &lt;dbl&gt;, `tf_essay6_\"&gt;` &lt;dbl&gt;, `tf_essay6_)` &lt;dbl&gt;,\n## #   `tf_essay6_▁-` &lt;dbl&gt;, `tf_essay6_▁(` &lt;dbl&gt;, `tf_essay6_▁&lt;a` &lt;dbl&gt;,\n## #   `tf_essay6_▁all` &lt;dbl&gt;, `tf_essay6_▁also` &lt;dbl&gt;, `tf_essay6_▁always` &lt;dbl&gt;,\n## #   `tf_essay6_▁am` &lt;dbl&gt;, `tf_essay6_▁an` &lt;dbl&gt;, `tf_essay6_▁as` &lt;dbl&gt;,\n## #   `tf_essay6_▁at` &lt;dbl&gt;, `tf_essay6_▁being` &lt;dbl&gt;, `tf_essay6_▁better` &lt;dbl&gt;,\n## #   `tf_essay6_▁but` &lt;dbl&gt;, `tf_essay6_▁class=\"ilink\"` &lt;dbl&gt;,\n## #   `tf_essay6_▁d` &lt;dbl&gt;, `tf_essay6_▁doing` &lt;dbl&gt;, `tf_essay6_▁friends` &lt;dbl&gt;,\n## #   `tf_essay6_▁from` &lt;dbl&gt;, `tf_essay6_▁future` &lt;dbl&gt;, `tf_essay6_▁get` &lt;dbl&gt;,\n## #   `tf_essay6_▁go` &lt;dbl&gt;, `tf_essay6_▁going` &lt;dbl&gt;, `tf_essay6_▁good` &lt;dbl&gt;,\n## #   `tf_essay6_▁have` &lt;dbl&gt;, `tf_essay6_▁href=` &lt;dbl&gt;, `tf_essay6_▁i've` &lt;dbl&gt;,\n## #   `tf_essay6_▁if` &lt;dbl&gt;, `tf_essay6_▁into` &lt;dbl&gt;, `tf_essay6_▁it's` &lt;dbl&gt;,\n## #   `tf_essay6_▁just` &lt;dbl&gt;, `tf_essay6_▁know` &lt;dbl&gt;, `tf_essay6_▁life` &lt;dbl&gt;,\n## #   `tf_essay6_▁life,` &lt;dbl&gt;, `tf_essay6_▁life.` &lt;dbl&gt;, `tf_essay6_▁lot` &lt;dbl&gt;,\n## #   `tf_essay6_▁love` &lt;dbl&gt;, `tf_essay6_▁m` &lt;dbl&gt;, `tf_essay6_▁make` &lt;dbl&gt;,\n## #   `tf_essay6_▁me` &lt;dbl&gt;, `tf_essay6_▁more` &lt;dbl&gt;, `tf_essay6_▁much` &lt;dbl&gt;,\n## #   `tf_essay6_▁myself` &lt;dbl&gt;, `tf_essay6_▁new` &lt;dbl&gt;, `tf_essay6_▁not` &lt;dbl&gt;,\n## #   `tf_essay6_▁one` &lt;dbl&gt;, `tf_essay6_▁other` &lt;dbl&gt;, `tf_essay6_▁our` &lt;dbl&gt;,\n## #   `tf_essay6_▁out` &lt;dbl&gt;, `tf_essay6_▁p` &lt;dbl&gt;, `tf_essay6_▁people` &lt;dbl&gt;,\n## #   `tf_essay6_▁really` &lt;dbl&gt;, `tf_essay6_▁right` &lt;dbl&gt;,\n## #   `tf_essay6_▁should` &lt;dbl&gt;, `tf_essay6_▁so` &lt;dbl&gt;, `tf_essay6_▁some` &lt;dbl&gt;,\n## #   `tf_essay6_▁spend` &lt;dbl&gt;, `tf_essay6_▁take` &lt;dbl&gt;, `tf_essay6_▁than` &lt;dbl&gt;,\n## #   `tf_essay6_▁there` &lt;dbl&gt;, `tf_essay6_▁they` &lt;dbl&gt;,\n## #   `tf_essay6_▁things` &lt;dbl&gt;, `tf_essay6_▁thinking` &lt;dbl&gt;,\n## #   `tf_essay6_▁this` &lt;dbl&gt;, `tf_essay6_▁time` &lt;dbl&gt;,\n## #   `tf_essay6_▁travel` &lt;dbl&gt;, `tf_essay6_▁up` &lt;dbl&gt;, `tf_essay6_▁want` &lt;dbl&gt;,\n## #   `tf_essay6_▁way` &lt;dbl&gt;, `tf_essay6_▁we` &lt;dbl&gt;, `tf_essay6_▁what's` &lt;dbl&gt;,\n## #   `tf_essay6_▁when` &lt;dbl&gt;, `tf_essay6_▁where` &lt;dbl&gt;,\n## #   `tf_essay6_▁whether` &lt;dbl&gt;, `tf_essay6_▁who` &lt;dbl&gt;, `tf_essay6_▁why` &lt;dbl&gt;,\n## #   `tf_essay6_▁will` &lt;dbl&gt;, `tf_essay6_▁work` &lt;dbl&gt;, `tf_essay6_▁world` &lt;dbl&gt;,\n## #   `tf_essay6_▁would` &lt;dbl&gt;, `tf_essay6_▁you` &lt;dbl&gt;, tf_essay6_a &lt;dbl&gt;,\n## #   tf_essay6_al &lt;dbl&gt;, tf_essay6_ed &lt;dbl&gt;, tf_essay6_er &lt;dbl&gt;,\n## #   tf_essay6_es &lt;dbl&gt;, tf_essay6_ing &lt;dbl&gt;, `tf_essay6_ing,` &lt;dbl&gt;,\n## #   tf_essay6_ly &lt;dbl&gt;, `tf_essay6_s,` &lt;dbl&gt;, tf_essay6_s. &lt;dbl&gt;,\n## #   tf_essay6_y &lt;dbl&gt;\nadditional arguments can be passed to tokenizers.bpe::bpe() via the training_options argument.\nrecipe(~ essay6, data = okc_text) %&gt;%\n  step_tokenize(essay6, \n                engine = \"tokenizers.bpe\",\n                training_options = list(vocab_size = 100)) %&gt;%\n  step_tf(essay6) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 750 x 100\n##    `tf_essay6_-` `tf_essay6_,` `tf_essay6_;` `tf_essay6_:` `tf_essay6_!`\n##            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n##  1             0             1             1             2             0\n##  2             0            13             1             0             0\n##  3             0             1             0             0             1\n##  4             0             0             0             0             0\n##  5             0             0             0             0             1\n##  6             0             1             0             0             0\n##  7             0             4             0             1             0\n##  8             2             0             0             0             0\n##  9             0             0             0             0             0\n## 10             0            10             0             0             0\n## # … with 740 more rows, and 95 more variables: `tf_essay6_?` &lt;dbl&gt;,\n## #   tf_essay6_. &lt;dbl&gt;, `tf_essay6_'` &lt;dbl&gt;, `tf_essay6_\"` &lt;dbl&gt;,\n## #   `tf_essay6_(` &lt;dbl&gt;, `tf_essay6_)` &lt;dbl&gt;, `tf_essay6_[` &lt;dbl&gt;,\n## #   `tf_essay6_]` &lt;dbl&gt;, `tf_essay6_*` &lt;dbl&gt;, `tf_essay6_/` &lt;dbl&gt;,\n## #   `tf_essay6_&` &lt;dbl&gt;, `tf_essay6_+` &lt;dbl&gt;, `tf_essay6_&lt;` &lt;dbl&gt;,\n## #   `tf_essay6_&lt;BOS&gt;` &lt;dbl&gt;, `tf_essay6_&lt;br` &lt;dbl&gt;, `tf_essay6_&lt;EOS&gt;` &lt;dbl&gt;,\n## #   `tf_essay6_&lt;PAD&gt;` &lt;dbl&gt;, `tf_essay6_&lt;UNK&gt;` &lt;dbl&gt;, `tf_essay6_=` &lt;dbl&gt;,\n## #   `tf_essay6_&gt;` &lt;dbl&gt;, `tf_essay6_~` &lt;dbl&gt;, `tf_essay6_▁` &lt;dbl&gt;,\n## #   `tf_essay6_▁/` &lt;dbl&gt;, `tf_essay6_▁/&gt;` &lt;dbl&gt;, `tf_essay6_▁a` &lt;dbl&gt;,\n## #   `tf_essay6_▁and` &lt;dbl&gt;, `tf_essay6_▁b` &lt;dbl&gt;, `tf_essay6_▁c` &lt;dbl&gt;,\n## #   `tf_essay6_▁d` &lt;dbl&gt;, `tf_essay6_▁f` &lt;dbl&gt;, `tf_essay6_▁g` &lt;dbl&gt;,\n## #   `tf_essay6_▁h` &lt;dbl&gt;, `tf_essay6_▁i` &lt;dbl&gt;, `tf_essay6_▁l` &lt;dbl&gt;,\n## #   `tf_essay6_▁m` &lt;dbl&gt;, `tf_essay6_▁o` &lt;dbl&gt;, `tf_essay6_▁p` &lt;dbl&gt;,\n## #   `tf_essay6_▁s` &lt;dbl&gt;, `tf_essay6_▁t` &lt;dbl&gt;, `tf_essay6_▁th` &lt;dbl&gt;,\n## #   `tf_essay6_▁the` &lt;dbl&gt;, `tf_essay6_▁to` &lt;dbl&gt;, `tf_essay6_▁w` &lt;dbl&gt;,\n## #   `tf_essay6_▁wh` &lt;dbl&gt;, tf_essay6_0 &lt;dbl&gt;, tf_essay6_1 &lt;dbl&gt;,\n## #   tf_essay6_2 &lt;dbl&gt;, tf_essay6_3 &lt;dbl&gt;, tf_essay6_4 &lt;dbl&gt;, tf_essay6_5 &lt;dbl&gt;,\n## #   tf_essay6_6 &lt;dbl&gt;, tf_essay6_8 &lt;dbl&gt;, tf_essay6_9 &lt;dbl&gt;, tf_essay6_a &lt;dbl&gt;,\n## #   tf_essay6_al &lt;dbl&gt;, tf_essay6_an &lt;dbl&gt;, tf_essay6_at &lt;dbl&gt;,\n## #   tf_essay6_b &lt;dbl&gt;, tf_essay6_br &lt;dbl&gt;, tf_essay6_c &lt;dbl&gt;,\n## #   tf_essay6_d &lt;dbl&gt;, tf_essay6_e &lt;dbl&gt;, tf_essay6_en &lt;dbl&gt;,\n## #   tf_essay6_er &lt;dbl&gt;, tf_essay6_es &lt;dbl&gt;, tf_essay6_f &lt;dbl&gt;,\n## #   tf_essay6_g &lt;dbl&gt;, tf_essay6_h &lt;dbl&gt;, tf_essay6_i &lt;dbl&gt;,\n## #   tf_essay6_in &lt;dbl&gt;, tf_essay6_ing &lt;dbl&gt;, tf_essay6_it &lt;dbl&gt;,\n## #   tf_essay6_j &lt;dbl&gt;, tf_essay6_k &lt;dbl&gt;, tf_essay6_l &lt;dbl&gt;, tf_essay6_m &lt;dbl&gt;,\n## #   tf_essay6_n &lt;dbl&gt;, tf_essay6_nd &lt;dbl&gt;, tf_essay6_o &lt;dbl&gt;,\n## #   tf_essay6_on &lt;dbl&gt;, tf_essay6_or &lt;dbl&gt;, tf_essay6_ou &lt;dbl&gt;,\n## #   tf_essay6_ow &lt;dbl&gt;, tf_essay6_p &lt;dbl&gt;, tf_essay6_q &lt;dbl&gt;,\n## #   tf_essay6_r &lt;dbl&gt;, tf_essay6_re &lt;dbl&gt;, tf_essay6_s &lt;dbl&gt;,\n## #   tf_essay6_t &lt;dbl&gt;, tf_essay6_u &lt;dbl&gt;, tf_essay6_v &lt;dbl&gt;, tf_essay6_w &lt;dbl&gt;,\n## #   tf_essay6_x &lt;dbl&gt;, tf_essay6_y &lt;dbl&gt;, tf_essay6_z &lt;dbl&gt;\nThe second engine is access to udpipe. To use this engine you must first download a udpipe model\nlibrary(udpipe)\nudmodel &lt;- udpipe_download_model(language = \"english\")\nudmodel\n##      language\n## 1 english-ewt\n##                                                                                                                          file_model\n## 1 /Users/emilhvitfeldthansen/Github/hvitfeldt.me/content/post/2020-11-13-textrecipes-0.4.0-release/english-ewt-ud-2.5-191206.udpipe\n##                                                                                                                                  url\n## 1 https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe\n##   download_failed download_message\n## 1           FALSE               OK\nAnd then you need to pass it into training_options under the name model. This will then use the tokenizer\nrecipe(~ essay6, data = okc_text) %&gt;%\n  step_tokenize(essay6, engine = \"udpipe\", \n                training_options = list(model = udmodel)) %&gt;%\n  step_tf(essay6) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 750 x 4,044\n##    `tf_essay6_-` `tf_essay6_--` `tf_essay6_---` `tf_essay6_---&lt;` `tf_essay6_--&`\n##            &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n##  1             0              0               0                0               0\n##  2             0              0               0                0               0\n##  3             0              0               0                0               0\n##  4             0              0               0                0               0\n##  5             0              0               0                0               0\n##  6             0              0               0                0               0\n##  7             0              0               0                0               0\n##  8             2              0               0                0               0\n##  9             0              0               0                0               0\n## 10             0              0               0                0               0\n## # … with 740 more rows, and 4,039 more variables: `tf_essay6_--ernesto` &lt;dbl&gt;,\n## #   `tf_essay6_-apocalypse.&lt;` &lt;dbl&gt;, `tf_essay6_-dominated` &lt;dbl&gt;,\n## #   `tf_essay6_-friendly` &lt;dbl&gt;, `tf_essay6_-insane` &lt;dbl&gt;,\n## #   `tf_essay6_-languages` &lt;dbl&gt;, `tf_essay6_-linear` &lt;dbl&gt;,\n## #   `tf_essay6_-my` &lt;dbl&gt;, `tf_essay6_-numbingly` &lt;dbl&gt;,\n## #   `tf_essay6_-voyeurism` &lt;dbl&gt;, `tf_essay6_,` &lt;dbl&gt;, `tf_essay6_,&lt;` &lt;dbl&gt;,\n## #   `tf_essay6_;` &lt;dbl&gt;, `tf_essay6_;-)` &lt;dbl&gt;, `tf_essay6_;)` &lt;dbl&gt;,\n## #   `tf_essay6_:` &lt;dbl&gt;, `tf_essay6_:-)` &lt;dbl&gt;, `tf_essay6_:-d` &lt;dbl&gt;,\n## #   `tf_essay6_:)` &lt;dbl&gt;, `tf_essay6_:&lt;` &lt;dbl&gt;, `tf_essay6_:d` &lt;dbl&gt;,\n## #   `tf_essay6_:p` &lt;dbl&gt;, `tf_essay6_!` &lt;dbl&gt;, `tf_essay6_!!` &lt;dbl&gt;,\n## #   `tf_essay6_!!!` &lt;dbl&gt;, `tf_essay6_!)` &lt;dbl&gt;, `tf_essay6_!&lt;` &lt;dbl&gt;,\n## #   `tf_essay6_?` &lt;dbl&gt;, `tf_essay6_?!` &lt;dbl&gt;, `tf_essay6_?!?!` &lt;dbl&gt;,\n## #   `tf_essay6_?!&lt;` &lt;dbl&gt;, `tf_essay6_??` &lt;dbl&gt;, `tf_essay6_????` &lt;dbl&gt;,\n## #   `tf_essay6_??&lt;` &lt;dbl&gt;, `tf_essay6_?\"` &lt;dbl&gt;, `tf_essay6_?&lt;` &lt;dbl&gt;,\n## #   tf_essay6_. &lt;dbl&gt;, tf_essay6_.. &lt;dbl&gt;, tf_essay6_... &lt;dbl&gt;,\n## #   tf_essay6_.... &lt;dbl&gt;, `tf_essay6_....?` &lt;dbl&gt;, tf_essay6_..... &lt;dbl&gt;,\n## #   tf_essay6_...... &lt;dbl&gt;, tf_essay6_....... &lt;dbl&gt;, tf_essay6_........ &lt;dbl&gt;,\n## #   tf_essay6_.......... &lt;dbl&gt;, tf_essay6_........... &lt;dbl&gt;,\n## #   tf_essay6_....fishing &lt;dbl&gt;, tf_essay6_...jk &lt;dbl&gt;,\n## #   tf_essay6_...zombies &lt;dbl&gt;, `tf_essay6_.)` &lt;dbl&gt;, `tf_essay6_.&lt;` &lt;dbl&gt;,\n## #   tf_essay6_.erykah &lt;dbl&gt;, tf_essay6_.sex &lt;dbl&gt;, `tf_essay6_'` &lt;dbl&gt;,\n## #   `tf_essay6_'.` &lt;dbl&gt;, `tf_essay6_'&lt;` &lt;dbl&gt;, `tf_essay6_'d` &lt;dbl&gt;,\n## #   `tf_essay6_'em` &lt;dbl&gt;, `tf_essay6_'ll` &lt;dbl&gt;, `tf_essay6_'m` &lt;dbl&gt;,\n## #   `tf_essay6_'re` &lt;dbl&gt;, `tf_essay6_'s` &lt;dbl&gt;, `tf_essay6_'ve` &lt;dbl&gt;,\n## #   `tf_essay6_\"` &lt;dbl&gt;, `tf_essay6_\"&gt;` &lt;dbl&gt;, `tf_essay6_\"&gt;modest` &lt;dbl&gt;,\n## #   `tf_essay6_(` &lt;dbl&gt;, `tf_essay6_(:` &lt;dbl&gt;, `tf_essay6_)` &lt;dbl&gt;,\n## #   `tf_essay6_[` &lt;dbl&gt;, `tf_essay6_]` &lt;dbl&gt;, `tf_essay6_*` &lt;dbl&gt;,\n## #   `tf_essay6_/` &lt;dbl&gt;, `tf_essay6_/&gt;` &lt;dbl&gt;, `tf_essay6_/a` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=actuary` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=anything+frivolous` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=art` &lt;dbl&gt;, `tf_essay6_/interests?i=bdsm` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=bigender\"&gt;` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=brunch` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=comfortable` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=communication` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=community` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=documentary` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=entp` &lt;dbl&gt;, `tf_essay6_/interests?i=field` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=film` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=filmmaking` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=gender-identity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=gender\"&gt;` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=honey%0abees` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=integrity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=legos` &lt;dbl&gt;, `tf_essay6_/interests?i=life` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=love` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=masturbatory` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=modest+running+shorts+in+neutral+tones` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=muzak` &lt;dbl&gt;, …\nBut where it gets really interesting is that we are able to extract the lemmas\nrecipe(~ essay6, data = okc_text) %&gt;%\n  step_tokenize(essay6, engine = \"udpipe\", \n                training_options = list(model = udmodel)) %&gt;%\n  step_lemma(essay6) %&gt;%\n  step_tf(essay6) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 750 x 3,546\n##    `tf_essay6_-` `tf_essay6_--` `tf_essay6_---` `tf_essay6_---&lt;` `tf_essay6_--&`\n##            &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n##  1             0              0               0                0               0\n##  2             0              0               0                0               0\n##  3             0              0               0                0               0\n##  4             0              0               0                0               0\n##  5             0              0               0                0               0\n##  6             0              0               0                0               0\n##  7             0              0               0                0               0\n##  8             2              0               0                0               0\n##  9             0              0               0                0               0\n## 10             0              0               0                0               0\n## # … with 740 more rows, and 3,541 more variables: `tf_essay6_--ernesto` &lt;dbl&gt;,\n## #   `tf_essay6_-apocalypse.&lt;` &lt;dbl&gt;, `tf_essay6_-dominated` &lt;dbl&gt;,\n## #   `tf_essay6_-friendly` &lt;dbl&gt;, `tf_essay6_-insane` &lt;dbl&gt;,\n## #   `tf_essay6_-language` &lt;dbl&gt;, `tf_essay6_-linear` &lt;dbl&gt;,\n## #   `tf_essay6_-my` &lt;dbl&gt;, `tf_essay6_-numbingly` &lt;dbl&gt;,\n## #   `tf_essay6_-voyeurism` &lt;dbl&gt;, `tf_essay6_,` &lt;dbl&gt;, `tf_essay6_,&lt;` &lt;dbl&gt;,\n## #   `tf_essay6_;` &lt;dbl&gt;, `tf_essay6_;-)` &lt;dbl&gt;, `tf_essay6_;)` &lt;dbl&gt;,\n## #   `tf_essay6_:` &lt;dbl&gt;, `tf_essay6_:-)` &lt;dbl&gt;, `tf_essay6_:-d` &lt;dbl&gt;,\n## #   `tf_essay6_:)` &lt;dbl&gt;, `tf_essay6_:&lt;` &lt;dbl&gt;, `tf_essay6_:d` &lt;dbl&gt;,\n## #   `tf_essay6_:p` &lt;dbl&gt;, `tf_essay6_!` &lt;dbl&gt;, `tf_essay6_!!` &lt;dbl&gt;,\n## #   `tf_essay6_!!!` &lt;dbl&gt;, `tf_essay6_!)` &lt;dbl&gt;, `tf_essay6_!&lt;` &lt;dbl&gt;,\n## #   `tf_essay6_?` &lt;dbl&gt;, `tf_essay6_?!` &lt;dbl&gt;, `tf_essay6_?!?!` &lt;dbl&gt;,\n## #   `tf_essay6_?!&lt;` &lt;dbl&gt;, `tf_essay6_??` &lt;dbl&gt;, `tf_essay6_????` &lt;dbl&gt;,\n## #   `tf_essay6_??&lt;` &lt;dbl&gt;, `tf_essay6_?\"` &lt;dbl&gt;, `tf_essay6_?&lt;` &lt;dbl&gt;,\n## #   tf_essay6_. &lt;dbl&gt;, tf_essay6_.. &lt;dbl&gt;, tf_essay6_... &lt;dbl&gt;,\n## #   tf_essay6_.... &lt;dbl&gt;, `tf_essay6_....?` &lt;dbl&gt;, tf_essay6_..... &lt;dbl&gt;,\n## #   tf_essay6_...... &lt;dbl&gt;, tf_essay6_....... &lt;dbl&gt;, tf_essay6_........ &lt;dbl&gt;,\n## #   tf_essay6_.......... &lt;dbl&gt;, tf_essay6_........... &lt;dbl&gt;,\n## #   tf_essay6_....fish &lt;dbl&gt;, tf_essay6_...jk &lt;dbl&gt;, tf_essay6_...zomby &lt;dbl&gt;,\n## #   `tf_essay6_.)` &lt;dbl&gt;, `tf_essay6_.&lt;` &lt;dbl&gt;, tf_essay6_.erykah &lt;dbl&gt;,\n## #   tf_essay6_.sex &lt;dbl&gt;, `tf_essay6_'` &lt;dbl&gt;, `tf_essay6_'.` &lt;dbl&gt;,\n## #   `tf_essay6_'&lt;` &lt;dbl&gt;, `tf_essay6_'s` &lt;dbl&gt;, `tf_essay6_\"` &lt;dbl&gt;,\n## #   `tf_essay6_\"&gt;` &lt;dbl&gt;, `tf_essay6_\"&gt;modest` &lt;dbl&gt;, `tf_essay6_(` &lt;dbl&gt;,\n## #   `tf_essay6_(:` &lt;dbl&gt;, `tf_essay6_)` &lt;dbl&gt;, `tf_essay6_[` &lt;dbl&gt;,\n## #   `tf_essay6_]` &lt;dbl&gt;, `tf_essay6_*` &lt;dbl&gt;, `tf_essay6_/` &lt;dbl&gt;,\n## #   `tf_essay6_/&gt;` &lt;dbl&gt;, `tf_essay6_/a` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=actuary` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=anything+frivolous` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=art` &lt;dbl&gt;, `tf_essay6_/interests?i=bdsm` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=bigender\"&gt;` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=brunch` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=comfortable` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=communication` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=community` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=documentary` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=entp` &lt;dbl&gt;, `tf_essay6_/interests?i=field` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=film` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=filmmaking` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=gender-identity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=gender\"&gt;` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=honey%0abee` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=integrity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=lego` &lt;dbl&gt;, `tf_essay6_/interests?i=life` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=love` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=masturbatory` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=modest+running+shorts+in+neutral+tone` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=muzak` &lt;dbl&gt;, `tf_essay6_/interests?i=my` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=non-profit\"&gt;non-profit&lt;/a` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=nvc\"&gt;nvc&lt;/a` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=organize` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=politic` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=polyamory` &lt;dbl&gt;, …\nor use the part of speech tags in later steps, such as below where we are filtering to only keep nouns.\nrecipe(~ essay6, data = okc_text) %&gt;%\n  step_tokenize(essay6, engine = \"udpipe\", \n                training_options = list(model = udmodel)) %&gt;%\n  step_pos_filter(essay6, keep_tags = \"NOUN\") %&gt;%\n  step_tf(essay6) %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n## # A tibble: 750 x 1,970\n##    `tf_essay6_--er… `tf_essay6_-lan… `tf_essay6_-voy… `tf_essay6_:d`\n##               &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n##  1                0                0                0              0\n##  2                0                0                0              0\n##  3                0                0                0              0\n##  4                0                0                0              0\n##  5                0                0                0              0\n##  6                0                0                0              0\n##  7                0                0                0              0\n##  8                0                0                0              0\n##  9                0                0                0              0\n## 10                0                0                0              0\n## # … with 740 more rows, and 1,966 more variables: `tf_essay6_:p` &lt;dbl&gt;,\n## #   tf_essay6_...jk &lt;dbl&gt;, tf_essay6_...zombies &lt;dbl&gt;, `tf_essay6_'` &lt;dbl&gt;,\n## #   `tf_essay6_/a` &lt;dbl&gt;, `tf_essay6_/interests?i=anything+frivolous` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=art` &lt;dbl&gt;, `tf_essay6_/interests?i=bdsm` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=brunch` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=communication` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=community` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=documentary` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=entp` &lt;dbl&gt;, `tf_essay6_/interests?i=film` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=filmmaking` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=gender-identity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=honey%0abees` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=integrity` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=legos` &lt;dbl&gt;, `tf_essay6_/interests?i=life` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=love` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=masturbatory` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=modest+running+shorts+in+neutral+tones` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=muzak` &lt;dbl&gt;, `tf_essay6_/interests?i=my` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=politics` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=polyamory` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=production` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=science\"` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=synesthesia` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=technology` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=tennis` &lt;dbl&gt;,\n## #   `tf_essay6_/interests?i=truisms` &lt;dbl&gt;, `tf_essay6_+theory` &lt;dbl&gt;,\n## #   `tf_essay6_&lt;a` &lt;dbl&gt;, `tf_essay6_=p` &lt;dbl&gt;, `tf_essay6_&gt;.` &lt;dbl&gt;,\n## #   `tf_essay6_&gt;communication` &lt;dbl&gt;, `tf_essay6_&gt;my` &lt;dbl&gt;,\n## #   `tf_essay6_&gt;science&lt;/a` &lt;dbl&gt;, `tf_essay6_&gt;truisms` &lt;dbl&gt;,\n## #   `tf_essay6_&gt;urban` &lt;dbl&gt;, tf_essay6_1st &lt;dbl&gt;, tf_essay6_a &lt;dbl&gt;,\n## #   tf_essay6_abba &lt;dbl&gt;, tf_essay6_ability &lt;dbl&gt;, tf_essay6_absence &lt;dbl&gt;,\n## #   tf_essay6_abstract &lt;dbl&gt;, tf_essay6_abundance &lt;dbl&gt;,\n## #   tf_essay6_accents &lt;dbl&gt;, tf_essay6_acceptance &lt;dbl&gt;,\n## #   tf_essay6_accident &lt;dbl&gt;, tf_essay6_action &lt;dbl&gt;, tf_essay6_actions &lt;dbl&gt;,\n## #   tf_essay6_activities &lt;dbl&gt;, tf_essay6_activity &lt;dbl&gt;,\n## #   tf_essay6_actors &lt;dbl&gt;, tf_essay6_acts &lt;dbl&gt;,\n## #   tf_essay6_actualization &lt;dbl&gt;, tf_essay6_addition &lt;dbl&gt;,\n## #   tf_essay6_adult &lt;dbl&gt;, tf_essay6_adventure &lt;dbl&gt;,\n## #   tf_essay6_adventures &lt;dbl&gt;, tf_essay6_adversity &lt;dbl&gt;,\n## #   tf_essay6_advocate &lt;dbl&gt;, tf_essay6_aeropress &lt;dbl&gt;,\n## #   tf_essay6_affairs &lt;dbl&gt;, tf_essay6_afterlife &lt;dbl&gt;,\n## #   tf_essay6_afternoon &lt;dbl&gt;, tf_essay6_age &lt;dbl&gt;, tf_essay6_agenda &lt;dbl&gt;,\n## #   tf_essay6_agent &lt;dbl&gt;, tf_essay6_ages &lt;dbl&gt;, tf_essay6_aggregate &lt;dbl&gt;,\n## #   tf_essay6_agriculture &lt;dbl&gt;, tf_essay6_ai &lt;dbl&gt;, tf_essay6_air &lt;dbl&gt;,\n## #   tf_essay6_aka &lt;dbl&gt;, tf_essay6_alarm &lt;dbl&gt;, tf_essay6_alert &lt;dbl&gt;,\n## #   tf_essay6_algorithms &lt;dbl&gt;, tf_essay6_alibi &lt;dbl&gt;, tf_essay6_aliens &lt;dbl&gt;,\n## #   tf_essay6_aloha &lt;dbl&gt;, tf_essay6_alps &lt;dbl&gt;, tf_essay6_am &lt;dbl&gt;,\n## #   tf_essay6_amnesia &lt;dbl&gt;, tf_essay6_amount &lt;dbl&gt;, tf_essay6_amp &lt;dbl&gt;,\n## #   tf_essay6_anagrams &lt;dbl&gt;, tf_essay6_analyzing &lt;dbl&gt;,\n## #   tf_essay6_anarchism &lt;dbl&gt;, tf_essay6_anarchists &lt;dbl&gt;,\n## #   tf_essay6_anaximander &lt;dbl&gt;, tf_essay6_android &lt;dbl&gt;,\n## #   tf_essay6_animal &lt;dbl&gt;, tf_essay6_animals &lt;dbl&gt;, tf_essay6_answer &lt;dbl&gt;,\n## #   tf_essay6_answers &lt;dbl&gt;, tf_essay6_anxiety &lt;dbl&gt;, …\nThis is all for this release. I hope you found some of it useful. I would love to hear what you are using textrecipes with!"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html",
    "title": "Real Emojis in ggplot2",
    "section": "",
    "text": "Note\n\n\n\nEmojis are now fully supported in {ggplot2} thanks to the {ragg} package. Read more about it here: Modern Text Features in R.\nI have been trying to use Emojis for a long time. It was part of my very first post on this blog. Others have made progress such as with emojifont, but it is not using the classical Apple Color Emoji font which is the most commonly recognized. I made a breakthrough when I was writing the packagecalander entry on ggtext. While the method is the best I have found it does have some cons.\nPros:\nCons:\nAll in all, it is a fair trade for my needs."
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#packages",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#packages",
    "title": "Real Emojis in ggplot2",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe load the essential packages to wrangle, collect data (we will use tweets), scrape websites, and handle emojis.\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(rvest)\n# devtools::install_github(\"clauswilke/ggtext\")\nlibrary(ggtext)\nlibrary(emo)"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#getting-the-tweets",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#getting-the-tweets",
    "title": "Real Emojis in ggplot2",
    "section": "Getting the tweets 🐦",
    "text": "Getting the tweets 🐦\nFor a simple dataset where we find emojis I’m going to get some tweets with the word “happy”.\nhappy &lt;- search_tweets(\"happy\", include_rts = FALSE, n = 1000)\nwe can use the ji_extract_all() function from the emo package. This will give us a list of emojis so we can use the unnest() function to get back to a tidy format. I’m going to do a simple count() of the emojis for the following visualizations.\nhappy_emojis &lt;- happy %&gt;%\n  mutate(emoji = emo::ji_extract_all(text)) %&gt;%\n  unnest(cols = c(emoji)) %&gt;%\n  count(emoji, sort = TRUE)\nNext is where the magic happens. We don’t have a way to displays emojis in ggplot2, but we can use ggtext to embed images into the text using HTML. Now we just need to get an image of each emoji. The following function will accept an emoji as a string and return the URL to a .png of that emoji.\nemoji_to_link &lt;- function(x) {\n  paste0(\"https://emojipedia.org/emoji/\",x) %&gt;%\n    read_html() %&gt;%\n    html_nodes(\"tr td a\") %&gt;%\n    .[1] %&gt;%\n    html_attr(\"href\") %&gt;%\n    paste0(\"https://emojipedia.org/\", .) %&gt;%\n    read_html() %&gt;%\n    html_node('div[class=\"vendor-image\"] img') %&gt;%\n    html_attr(\"src\")\n}\nThen this function will take that URL and construct the necessary HTML code to show the emoji PNGs.\nlink_to_img &lt;- function(x, size = 25) {\n  paste0(\"&lt;img src='\", x, \"' width='\", size, \"'/&gt;\")\n}\nTo be courteous we are only going to scrape the emojis we are going to use. So we will slice() the 10 most frequent emojis. We will also be adding a 5 second delay using slowly() and rate_delay() from purrr.\ntop_happy &lt;- happy_emojis %&gt;%\n  slice(1:10) %&gt;%\n  mutate(url = map_chr(emoji, slowly(~emoji_to_link(.x), rate_delay(1))),\n         label = link_to_img(url))"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#emoji-scatter-plot",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#emoji-scatter-plot",
    "title": "Real Emojis in ggplot2",
    "section": "emoji-scatter plot 📈",
    "text": "emoji-scatter plot 📈\nNow we can use the geom_richtext() function from ggtext to create a emoji scatter chart.\ntop_happy %&gt;%\n  ggplot(aes(emoji, n, label = label)) +\n  geom_richtext(aes(y = n), fill = NA, label.color = NA, # remove background and outline\n                label.padding = grid::unit(rep(0, 4), \"pt\") # remove padding\n  ) +\n  theme_minimal()\n\nThis is a little off, so let’s other these by counts and put them over a bar chart. I’m also going to the x-axis ticks and text.\noffset &lt;- max(top_happy$n) / 20\n\ntop_happy %&gt;%\n  ggplot(aes(fct_reorder(emoji, n, .desc = TRUE), n, label = label)) +\n  geom_col() +\n  geom_richtext(aes(y = n + offset), fill = NA, label.color = NA,\n                label.padding = grid::unit(rep(0, 4), \"pt\")\n  ) +\n  theme(axis.ticks.x = element_blank(),\n        axis.text.x = element_blank()) +\n  labs(x = NULL) +\n  theme_minimal()"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#emojis-in-labels-and-text",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#emojis-in-labels-and-text",
    "title": "Real Emojis in ggplot2",
    "section": "Emojis in labels and text 📊",
    "text": "Emojis in labels and text 📊\nWe are not only limited to using emojis in the geoms. We can set the text element using emojis to element_markdown(). Below we have the same bar chart as above but with the emoji as labels below instead of on top.\ntop_happy %&gt;%\n  ggplot(aes(fct_reorder(label, n, .desc = TRUE), n)) +\n  geom_col() +\n  theme_minimal() +\n  theme(axis.text.x = element_markdown()) +\n  labs(x = NULL)"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#adding-a-splash-of-color",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#adding-a-splash-of-color",
    "title": "Real Emojis in ggplot2",
    "section": "Adding a splash of color 🌈",
    "text": "Adding a splash of color 🌈\nWe can employ a little more scraping and color calculations to had colors to the bars according to the colors of the emoji. The following function takes a URL to a .png file and returns the most common color that isn’t purely black or pure white.\nmean_emoji_color &lt;- function(x) {\n  data &lt;- png::readPNG(RCurl::getURLContent(x))\n  color_freq &lt;- names(sort(table(rgb(data[,,1], data[,,2], data[,,3])), \n                           decreasing = TRUE))\n  setdiff(color_freq, c(\"#FFFFFF\", \"#000000\"))[1]\n}\nWe apply this to all the emoji URLs and color the bars accordingly.\nplot_data &lt;- top_happy %&gt;%\n  mutate(color = map_chr(url, slowly(~mean_emoji_color(.x), rate_delay(1))))\n\nplot_data %&gt;%\n  ggplot(aes(fct_reorder(label, n, .desc = TRUE), \n             color = color, \n             fill = unclass(prismatic::clr_lighten(color, 0.4)), n)) +\n  geom_col() +\n  scale_fill_identity() +\n  scale_color_identity() +\n  theme_minimal() +\n  theme(axis.text.x = element_markdown()) +\n  labs(x = NULL, y = \"Count\",\n       title = \"Emojis used in (small sample) of 'happy' tweets\",\n       subtitle = \"Displayed in ggplot2!!!\",\n       caption = \"@Emil_Hvitfeldt\")"
  },
  {
    "objectID": "post/2020-01-02-real-emojis-in-ggplot2/index.html#final-note",
    "href": "post/2020-01-02-real-emojis-in-ggplot2/index.html#final-note",
    "title": "Real Emojis in ggplot2",
    "section": "Final note 🗒",
    "text": "Final note 🗒\nIf you want to use emojis in the text you need to call theme_*() before theme() such that element_markdown() isn’t being overwritten.\n\n\n current session info \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                           \n askpass       1.1        2019-01-13 [1] CRAN (R 4.1.0)                   \n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1      2020-12-09 [1] CRAN (R 4.1.0)                   \n bitops        1.0-7      2021-04-24 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2      2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22       2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8      2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0      2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0      2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1      2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2      2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1      2021-02-08 [1] CRAN (R 4.1.0)                   \n curl          4.3.2      2021-06-23 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1      2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1      2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0      2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27     2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7      2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   \n emo         * 0.0.0.9000 2021-07-17 [1] Github (hadley/emo@3f03b11)      \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0      2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0      2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1      2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0      2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0      2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                   \n ggtext      * 0.1.1      2020-12-17 [1] CRAN (R 4.1.0)                   \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.1.0)                   \n gridtext      0.1.4      2020-12-10 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1      2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9        2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0      2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1    2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2      2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33       2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0      2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10     2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   \n markdown      1.1        2019-08-07 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8      2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                   \n openssl       1.4.4      2021-04-30 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1      2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7      2013-12-03 [1] CRAN (R 4.1.0)                   \n prettyunits   1.1.1      2020-01-24 [1] CRAN (R 4.1.0)                   \n prismatic     1.0.0      2021-01-05 [1] CRAN (R 4.1.0)                   \n progress      1.2.2      2019-05-16 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0      2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7      2021-07-07 [1] CRAN (R 4.1.0)                   \n RCurl         1.98-1.3   2021-03-16 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0      2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1      2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0      2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11     2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9        2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13       2020-11-12 [1] CRAN (R 4.1.0)                   \n rtweet      * 0.7.0      2020-01-08 [1] CRAN (R 4.1.0)                   \n rvest       * 1.0.0      2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                   \n selectr       0.4-2      2019-11-20 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2      2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2      2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3      2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1      2021-04-30 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1      2021-04-15 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1      2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2      2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24       2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/slidecraft-theme-variants/index.html",
    "href": "post/slidecraft-theme-variants/index.html",
    "title": "Slidecraft 101: theme variants",
    "section": "",
    "text": "Hello and welcome back to my multi-part series about what I like to call slidecrafting; The art of putting together slides that are functional and aesthetically pleasing. I will be using quarto presentations as my medium, but the advice should be fairly tool-agnostic. This is the third post, you can find the previous post under the category slidecraft 101.\nToday we will be looking at how we can create a theme with multiple variants. This way you can have black/white variants, white/red/blue, or any number of different styles. If you are familiar with xaringan you have seen an inverse theme variant already."
  },
  {
    "objectID": "post/slidecraft-theme-variants/index.html#what-is-a-variant",
    "href": "post/slidecraft-theme-variants/index.html#what-is-a-variant",
    "title": "Slidecraft 101: theme variants",
    "section": "What is a variant?",
    "text": "What is a variant?\nWhen we are slidecrafting, and you start having many slides. It can be helpful to bucket them into fewer types of slides. This way you can reuse the same style many times with minimal copy-pasting.\nUsing colors to create multiple variants of the same theme allows us to quickly add similar looking, yet different styles. The inverse theme variant of {xaringan} was a dark grey background slide, that accompanied the white background themed default. I and many other people found this inverse theme helpful for creating a break in slides. Typically using it as a section break or a background on which to show a quote.\nYou can also imagine having a couple of more similar theme variants that are used to denote theory/practice, idea/execution, pros/cons. The opportunities are endless, and we are not limited to only 2. I have in the past used themes that slowly changes colors as the slides progressed through the topics."
  },
  {
    "objectID": "post/slidecraft-theme-variants/index.html#the-scss-basics",
    "href": "post/slidecraft-theme-variants/index.html#the-scss-basics",
    "title": "Slidecraft 101: theme variants",
    "section": "The SCSS basics",
    "text": "The SCSS basics\nFortunately, adding this behavior in quarto revealjs slides. We need 2 things:\n\nMark slides that have each theme variant\ninclude css/scss to style each theme\n\nIn our .qmd document we can denote each slide as a class with the following syntax\n## Slide\n\n## Slide {.variant-one}\n\n## Slide {.variant-two}\nthis gives the slides the corresponding css class which we can create styles for. Notice how the first slide doesn’t have a variant specified. Depending on your usage, it is easier to have a good base style, and only use {.class} to specify when you want a different class.\ncreate a .scss file and add it to the themes in the yaml section\nformat:\n  revealjs:\n    theme: [default, styles.scss]\nAnd in the .scss file, we add the boilerplate information.\n/*-- scss:defaults --*/\n\n/*-- scss:rules --*/\nUnder the /*-- scss:rules --*/ section we can now specify all the css rules we want. And we do this by prefixing .variant to each style. As an example, if we want to change the color of the text we use .variant-one {color: blue;}, or the link color .variant-one a {color: green;}.\nYou can quite quickly end up making many changes. And this is where I find it helpful to use scss nesting. Nesting allows us to rewrite\n\n.variant-one {\n  color: #d6d6d6;\n}\n\n.variant-one h1, h2, h3 {\n  color: #f3f3f3;\n}\n\n.variant-one a {\n  color: #00e0e0;\n}\n\n.variant-one p code {\n  color: #ffd700;\n}\nas\n.variant-one {\n  color: #d6d6d6;\n  h1, h2, h3 {\n    color: #f3f3f3;\n  }\n\n  a {\n    color: #00e0e0;\n  }\n\n  p code {\n    color: #ffd700;\n  }\n}\nI find it quite readable and I encourage you to follow the link and read more about it! Using this syntax, having multiple different variants is quite effortless, and many IDEs will help highlight and collapse this type of syntax.\n.variant-one {\n  color: #d6d6d6;\n  h1, h2, h3 {\n    color: #f3f3f3;\n  }\n\n  a {\n    color: #00e0e0;\n  }\n\n  p code {\n    color: #ffd700;\n  }\n}\n\n.variant-two {\n  color: #a6a6d6;\n  h1, h2, h3 {\n    color: #222222;\n  }\n\n  a {\n    color: #f22341;\n  }\n\n  p code {\n    color: #ff00ff;\n  }\n}\nAnd that is all that is needed! I have taken the liberty to create two themes to show case how this is done in practice:\nhttps://github.com/EmilHvitfeldt/quarto-revealjs-inverse\n\nhttps://github.com/EmilHvitfeldt/quarto-revealjs-seasons"
  },
  {
    "objectID": "post/slidecraft-theme-variants/index.html#roundup",
    "href": "post/slidecraft-theme-variants/index.html#roundup",
    "title": "Slidecraft 101: theme variants",
    "section": "Roundup",
    "text": "Roundup\nI hoped you learned a lot! I use this trick quite a lot, and I hope that you found it useful!"
  },
  {
    "objectID": "post/2019-05-08-circle-love-making-hearts-with-circles/index.html",
    "href": "post/2019-05-08-circle-love-making-hearts-with-circles/index.html",
    "title": "Circle Love - making hearts with circles",
    "section": "",
    "text": "Why are we here?\nSome days ago I saw this little cute pen and it sparked something inside me.\n\nSee the Pen  Heart is Home by Chris Gannon (chrisgannon) on CodePen.\n\n\nI throw together some lines of code and took my first splash into using simple Features. This post is not meant as an introduction to sf, a great introduction to the sf objects is made by Jesse Sadler.\n\n\nLoading packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(patchwork)\n\n\nFirst run\nFirst, we create the center shape. I have gone for the heart shape, for which I found a parametric expression, I have wrapped all of this in a little function such that I can specify the number of points the polygon has.\nheart_fun &lt;- function(n) {\n  t &lt;- c(seq(0, 2 * pi, length.out = n), 0)\n  \n  out &lt;- data.frame(\n    x = c(16 * sin(t) ^ 3),\n    y = 13 * cos(t) - 5 * cos(2 * t) - 2 * cos(3 * t) - cos(4 * t)\n  )\n  out &lt;- as.matrix(out)\n  out &lt;- list(out)\n  st_polygon(out)\n}\nLet us check that the function works\nheart_fun(100)\n## POLYGON ((0 5, 0.004082058 5.082247, 0.03245962 5.325084, 0.1084517 5.716992, 0.2534598 6.239393, 0.4860975 6.867539, 0.8214215 7.571701, 1.270293 8.31857, 1.838891 9.072817, 2.528404 9.798711, 3.334892 10.46172, 4.24935 11.03003, 5.25795 11.47583, 6.342465 11.77642, 7.480851 11.915, 8.647981 11.88112, 9.816481 11.67082, 10.95766 11.28641, 12.04251 10.736, 13.04268 10.03268, 13.93146 9.193568, 14.68474 8.238708, 15.28179 7.189845, 15.70606 6.069255, 15.94569 4.898625, 15.99396 3.698075, 15.8495 2.485356, 15.51639 1.275288, 15.00393 0.07943237, 14.32642 -1.093982, 13.50257 -2.239884, 12.5549 -3.355982, 11.50893 -4.442201, 10.3923 -5.5, 9.233833 -6.531618, 8.062492 -7.539309, 6.906432 -8.524629, 5.792014 -9.487815, 4.742924 -10.42731, 3.77938 -11.33948, 2.917472 -12.21848, 2.168659 -13.05638, 1.539432 -13.84345, 1.031163 -14.56857, 0.6401401 -15.21987, 0.3577924 -15.78537, 0.1710904 -16.25367, 0.06311066 -16.61466, 0.01374229 -16.86016, 0.0005110288 -16.9844, -0.0005110288 -16.9844, -0.01374229 -16.86016, -0.06311066 -16.61466, -0.1710904 -16.25367, -0.3577924 -15.78537, -0.6401401 -15.21987, -1.031163 -14.56857, -1.539432 -13.84345, -2.168659 -13.05638, -2.917472 -12.21848, -3.77938 -11.33948, -4.742924 -10.42731, -5.792014 -9.487815, -6.906432 -8.524629, -8.062492 -7.539309, -9.233833 -6.531618, -10.3923 -5.5, -11.50893 -4.442201, -12.5549 -3.355982, -13.50257 -2.239884, -14.32642 -1.093982, -15.00393 0.07943237, -15.51639 1.275288, -15.8495 2.485356, -15.99396 3.698075, -15.94569 4.898625, -15.70606 6.069255, -15.28179 7.189845, -14.68474 8.238708, -13.93146 9.193568, -13.04268 10.03268, -12.04251 10.736, -10.95766 11.28641, -9.816481 11.67082, -8.647981 11.88112, -7.480851 11.915, -6.342465 11.77642, -5.25795 11.47583, -4.24935 11.03003, -3.334892 10.46172, -2.528404 9.798711, -1.838891 9.072817, -1.270293 8.31857, -0.8214215 7.571701, -0.4860975 6.867539, -0.2534598 6.239393, -0.1084517 5.716992, -0.03245962 5.325084, -0.004082058 5.082247, -2.350945e-46 5, 0 5))\nand that it plots correctly.\nplot(heart_fun(100))\n\nWe also create a helper function to create a unit circle.\ncircle_fun &lt;- function(n) {\n  t &lt;- c(seq(0, 2 * pi, length.out = n), 0)\n  \n  out &lt;- data.frame(\n    x = sin(t),\n    y = cos(t)\n  )\n  out &lt;- as.matrix(out)\n  out &lt;- list(out)\n  st_polygon(out)\n}\n\nplot(circle_fun(100))\n\nSo we have a heart shape, lets check the boundaries of that shape.\nst_bbox(heart_fun(100))\n##      xmin      ymin      xmax      ymax \n## -15.99396 -16.98440  15.99396  11.91500\nLets generate a sf polygon of both the heart and circle polygon.\ncircle &lt;- circle_fun(100)\nheart &lt;- heart_fun(100)\nNext, we want to generate a list of candidate points where we try to place circles. for now we will just randomly sample between -25 and 25 on the x-axis and -20 and 20 on the y axis. then we will save them as an sf object.\npoints &lt;- data.frame(x = runif(250, -25, 25),\n                     y = runif(250, -20, 20)) %&gt;% \n  sf::st_as_sf(coords = c(1, 2))\n\nplot(points)\n\nNext, we will filter the points such that we only consider points that are outside the heart shape.\npoints &lt;- points[!lengths(st_intersects(points, heart)), ]\nplot(points)\n\nNext, we will loop through every single point and calculate the distance (using st_distance) from the point to the heart. then we will place a circle on that point and scale it such that it has a radius equal to the distance we calculated. That way the heart shape should show given enough points.\nall_polygons &lt;- map(points[[1]],\n    ~ (circle * st_distance(heart, .x, by_element = TRUE)) + .x) %&gt;%\n  st_sfc()\nplot(all_polygons)\n\nAnd we get something nice! however, some of the circles become quite big. Let’s bound the radius and give it some variation.\nbound &lt;- function(x, limit) {\n  ifelse(x &gt; limit, runif(1, limit / 4, limit), x)\n}\n\nall_polygons &lt;- map(points[[1]],\n    ~ (circle * bound(st_distance(heart, .x, by_element = TRUE), 4)) + .x) %&gt;%\n  st_sfc()\n\nplot(all_polygons)\n\nNow let’s turn this into a data.frame and extract the x and y coordinate so we can use them for coloring.\nplotting_data &lt;- data.frame(all_polygons) %&gt;%\n  mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]),\n         y = map_dbl(geometry, ~st_centroid(.x)[[2]])) \nNow that we have everything we need we will turn to ggplot2 to pretty everything up.\nplotting_data %&gt;%\n  ggplot() +\n  geom_sf(aes(color = y, geometry = geometry), alpha = 0.2, fill = NA) +\n  coord_sf(datum = NA) +\n  theme_void() + \n  guides(color = \"none\")\n\nAnd we are done! It looks nice and pretty, now there is a bunch of things we can change.\n\ncolor scales\ncoloring patterns\ncircle arrangement (rectangle, circle, buffer)\n\n\n\nOne function plotting\nEverything from before is not wrapped up nicely and tight in this function.\ncircle_heart &lt;- function(n, center_sf, outside_sf, outside_filter = \"None\", plotting_margin = 5, ...) {\n  \n  bound &lt;- function(x, limit) {\n    ifelse(x &gt; limit, runif(1, limit / 4, limit), x)\n  }\n  \n  range &lt;- st_bbox(center_sf)\n  points &lt;- data.frame(x = runif(n, range[[\"xmin\"]] - plotting_margin, \n                                    range[[\"xmax\"]] + plotting_margin),\n                       y = runif(n, range[[\"ymin\"]] - plotting_margin, \n                                    range[[\"ymax\"]] + plotting_margin)) %&gt;% \n    sf::st_as_sf(coords = c(1, 2))\n  \n  if (outside_filter == \"buffer\") {\n    points &lt;- st_intersection(points, st_buffer(center_sf, plotting_margin))\n  } \n  \n  points &lt;- points[!lengths(st_intersects(points, center_sf)), ]\n  \n  all_polygons &lt;- map(points[[1]],\n    ~ (outside_sf * bound(st_distance(center_sf, .x, by_element = TRUE), 4)) + .x) %&gt;%\n  st_sfc()\n  \n  plotting_data &lt;- data.frame(all_polygons) %&gt;%\n  mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]),\n         y = map_dbl(geometry, ~st_centroid(.x)[[2]])) \n  \n  plotting_data %&gt;%\n    ggplot() +\n    geom_sf(..., mapping = aes(geometry = geometry)) +\n    coord_sf(datum = NA) +\n    theme_void()\n}\nIt returns a simple ggplot2 object that we then can further modify to our visual liking.\ncircle_heart(300, heart_fun(100), circle_fun(100))\n\nA handful of examples\np1 &lt;- circle_heart(300, heart_fun(100), circle_fun(100), \n                   plotting_margin = 10, fill = NA) +\n  aes(color = sin(x / y)) +\n  scale_color_viridis_c() +\n  guides(color = \"none\")\n\np2 &lt;- circle_heart(300, heart_fun(100), circle_fun(100), \n                   outside_filter = \"buffer\", plotting_margin = 10, color = NA, alpha = 0.4) +\n  aes(fill = cos(x / y)) +\n  scale_fill_viridis_c(option = \"A\") +\n  guides(fill = \"none\")\n\np3 &lt;- circle_heart(300, heart_fun(100), circle_fun(5), \n                   outside_filter = \"buffer\", plotting_margin = 10, color = NA, alpha = 0.4) +\n  aes(fill = x + y) +\n  scale_fill_gradient(low = \"pink\", high = \"black\") +\n  guides(fill = \"none\")\n\np4 &lt;- circle_heart(500, heart_fun(100), circle_fun(4), \n                   outside_filter = \"buffer\", plotting_margin = 10, color = NA, alpha = 0.4) +\n  aes(fill = atan2(y, x)) +\n  scale_fill_gradientn(colours = rainbow(256)) +\n  guides(fill = \"none\")\n\np5 &lt;- circle_heart(300, heart_fun(100), circle_fun(10), \n                   outside_filter = \"buffer\", plotting_margin = 10, color = NA, alpha = 0.4) +\n  aes(fill = factor(floor(x * y) %% 8)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  guides(fill = \"none\")\n\np6 &lt;- circle_heart(500, heart_fun(100), heart_fun(100) / 20, \n                   outside_filter = \"buffer\", plotting_margin = 10, color = \"grey70\", alpha = 0.4) +\n  aes(fill = (y %% 4) * (x %% 1)) +\n  scale_fill_gradientn(colours = cm.colors(256)) +\n  guides(fill = \"none\")\n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)\n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date       lib source                           \n assertthat     0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports      1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown       1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown       0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom          0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib          0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger     1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n class          7.3-19  2021-05-03 [1] CRAN (R 4.1.0)                   \n classInt       0.4-3   2020-04-07 [1] CRAN (R 4.1.0)                   \n cli            3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr          0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools      0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace     2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon         1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI            1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr         2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc           1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details      * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest         0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr        * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n e1071          1.7-7   2021-05-23 [1] CRAN (R 4.1.0)                   \n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate       0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi          0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver         2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats      * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs             1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics       0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2      * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue           1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable         0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven          2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr          0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms            1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools      0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr           1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib      0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite       1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n KernSmooth     2.23-20 2021-05-03 [1] CRAN (R 4.1.0)                   \n knitr        * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n lifecycle      1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate      1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr       2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n modelr         0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n patchwork    * 1.1.1   2020-12-17 [1] CRAN (R 4.1.0)                   \n pillar         1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png            0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n proxy          0.4-26  2021-06-07 [1] CRAN (R 4.1.0)                   \n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6             2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n RColorBrewer   1.1-2   2014-12-07 [1] CRAN (R 4.1.0)                   \n Rcpp           1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr        * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl         1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex         2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang          0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown      2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot      2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi     0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rvest          1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass           0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales         1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n sf           * 1.0-0   2021-06-09 [1] CRAN (R 4.1.0)                   \n stringi        1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr      * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble       * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr        * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect     1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidyverse    * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n units          0.7-2   2021-06-08 [1] CRAN (R 4.1.0)                   \n utf8           1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs          0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n viridisLite    0.4.0   2021-04-13 [1] CRAN (R 4.1.0)                   \n withr          2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun           0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2           1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml           2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/smltar-announcement/index.html",
    "href": "post/smltar-announcement/index.html",
    "title": "Supervised Machine Learning for Text Analysis in R",
    "section": "",
    "text": "I have been waiting a long time to finally be able to craft this blog post. Last Friday Julia Silge and I led a userR! 2020 online tutorial on “predictive modeling with text using tidy data principles”. The tutorial was host by R-Ladies en Argentina and I could not be more grateful for all the work the organizers put into this making this event happen.\n\nMaterials for this tutorial are available on GitHub, with two main resources in the repo:\n\nSlides, which you can see rendered here and the source for here\n\n\n\n\nAn R Markdown file to work through\n\nIf you get stuck, you can post a question as an issue on this repo or post on RStudio Community\nDuring the tutorial, I was excited and proud to publicly announce the book Julia and I are working on! The book is called “Supervised Machine Learning for Text Analysis in R” to be published in the Chapman & Hall/CRC Data Science Series! An online version is and will continue to be available at smltar.com. This year long project have been an exciting time of my life and I have been learning about, not just about the subject matter at hand, but about publishing, polishing and reviewing.\nThe book has been divided into 3 main sections:\n\nNatural language features: This section covers the considerations and methods one can use to turn text into a numerical representation we can feed into our model. We are writing about but not limited to; tokenization, stemming and stop words (yes! you read that right! we have a whole chapter about stop words! And it was needed). This section is in really good shape.\nMachine learning methods: We investigate the power of some of the simpler and more lightweight models in our toolbox. We are doing full walkthroughs of classification and regression with commentary and considers We drew from these chapters in our useR tutorial.\nDeep learning methods: Given more time and resources, we see what is possible once we turn to neural networks. This section is still to come.\n\nI already have a lot of people to thank for making this possible!\n\nJulia for seeing the promising in this book idea and taking on the big task with me\nour Chapman & Hall editor John Kimmel\nThe helpful technical reviewers\nDesirée De Leon for the site beautiful design of the book’s website\nMax Kuhn and Davis Vaughan for the amazing work on tidymodels which we using in the second section of the book\nMy wife for her continued support and her faint attempts to feign interest when I talk for about the book ❤️\nAlberto Cairo for lending an ear and encouragements to this idea"
  },
  {
    "objectID": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html",
    "href": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html",
    "title": "Repetition in musicals with tidytext",
    "section": "",
    "text": "Warning\n\n\n\nThis code does no longer works as the genius website has changed its format and the scraper described in this post no longer works. If you interesting in scraping song lyric I recommend the genius package by JosiahParry.\nLately, I have been wondering how to quantify how repetitive a text is, specifically how repetitive are the lyrics to songs. I will specifically limit this post to English1 lyrics. I’m by no means the first one, Colin Morris did a great piece on language compression with his Are Pop Lyrics Getting More Repetitive? which I highly recommend you go read. Instead of looking at pop lyrics will we instead be focusing on some popular musicals to see if general patterns emerge within each show.\nI plan to use the magic of the tidyverse with the inclusion of tidytext to find the percentage of repeated consecutive sequences of words() also called n-grams) of varying length and then compare the results. We will use rvest to extract the lyrics from the web. However, for larger data needs official APIs are recommended."
  },
  {
    "objectID": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#extracting-song-lyrics",
    "href": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#extracting-song-lyrics",
    "title": "Repetition in musicals with tidytext",
    "section": "Extracting song lyrics",
    "text": "Extracting song lyrics\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rvest)\nWe will first take a peek at a specific song,\nthe_stars_look_down &lt;- \"https://genius.com/Elton-john-the-stars-look-down-lyrics\"\n\nthe_stars_look_down %&gt;%\n  read_lines() %&gt;%\n  head(20)\nAnd we find a whole of lines of no interest of us, which is to be expected. After some digging, I manage to find that the lyrics are packed between p tags we can do this with rvest’s html_nodes().\nthe_stars_look_down_lyrics &lt;- the_stars_look_down %&gt;%\n  read_html() %&gt;%\n  html_nodes(\"p\") %&gt;%\n  html_text() %&gt;%\n  str_split(\"\\n\") %&gt;%\n  unlist() %&gt;%\n  tibble(text = .)\n\nthe_stars_look_down_lyrics\nWe noticed some of the rows are indications of who is talking, these are quickly dealt with by a filter. Now we employ our tidytext arsenal with unnest_tokens and we find all the bi-grams (pairs of consecutive words). (notice how they overlap)\nthe_stars_look_down_lyrics %&gt;%\n  filter(!str_detect(text, \"\\\\[\")) %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\nnow from this we can summarize to find the number of unique bigrams and by extension the percentage of repeated bigrams.\nthe_stars_look_down_lyrics %&gt;%\n  filter(!str_detect(text, \"\\\\[\")) %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  summarise(length = length(bigram),\n            unique = length(unique(bigram))) %&gt;%\n  mutate(repetition = 1 - unique / length)\nSo we see that in this particular song around 38% of bigrams are present at least twice. We will expect this percentage to be strictly decreasing for \\(n\\) increasing, but what we are interested in both the rate it is decreasing but also the general level.\nNow we generalizing this procedure using some purrr to give us a nice data.frame out in the end. The range 1:5 was picked after some trial and error, and it seemed to me that most trends died out around the 5-6 mark rendering data points rather uninteresting.\nsongfun &lt;- function(hyperlink, repnum = 1:5) {\n  \n  lyrics &lt;- hyperlink %&gt;%\n  read_html() %&gt;%\n  html_nodes(\"p\") %&gt;%\n  html_text() %&gt;%\n  str_split(\"\\n\") %&gt;%\n  unlist() %&gt;%\n  tibble(text = .)\n  \n  map_dfr(repnum, \n          ~ lyrics %&gt;% \n            unnest_tokens(ngram, text, token = \"ngrams\", n = .x) %&gt;%\n            summarise(n = .x,\n                      length = length(ngram),\n                      unique = length(unique(ngram))) %&gt;%\n            mutate(repetition = 1 - unique / length,\n                   name = hyperlink))\n}\nNow to try this out, we plug in the link again, and pipe the result into ggplot to give us a nice visualization\nsongfun(\"https://genius.com/Elton-john-the-stars-look-down-lyrics\") %&gt;%\n  ggplot(aes(n, repetition)) +\n  geom_line() +\n  coord_cartesian(ylim = 0:1)\nfrom this plot alone we can see that roughly 3/4 of the words used in the song are used more than twice, while on the other end of the scale just shy of 25% of the 5-grams are used more than once. This plot by itself doesn’t provide too much meaningful information by itself. So next step is to gather information for more songs to compare.\nThis function takes a link to an album page, and uses similar techniques used earlier to detect the song in the album, find the lyrics with songfun, process it, and spits out a data.frame.\nalbumfun &lt;- function(hlink, ...) { \n  song_links &lt;- tibble(text = readLines(hlink)) %&gt;%\n    filter(str_detect(text, \"          &lt;a href=\\\"https://genius.com/\")) %&gt;%\n    mutate(text = str_replace(text, \"&lt;a href=\\\"\", \"\")) %&gt;%\n    mutate(text = str_replace(text, \"\\\" class=\\\"u-display_block\\\"&gt;\", \"\")) %&gt;%\n    mutate(text = str_replace(text, \" *\", \"\")) %&gt;%\n    mutate(song = str_replace(text, \"https://genius.com/\", \"\"))\n\n  nsongs &lt;- nrow(song_links)\n  out &lt;- tibble()\n  for (i in 1:nsongs) {\n    ting &lt;- songfun(song_links$text[i], ...)\n    out &lt;- rbind(out, ting)\n  }\n  out %&gt;%\n    mutate(album = hlink)\n}"
  },
  {
    "objectID": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#analysis",
    "href": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#analysis",
    "title": "Repetition in musicals with tidytext",
    "section": "Analysis",
    "text": "Analysis\nWe use our function to get the data for several different musicals.\nbillyelliot &lt;- albumfun(hlink = \"https://genius.com/albums/Elton-john/Billy-elliot-the-musical-original-london-cast-recording\")\nthebookofmormon &lt;- albumfun(hlink = \"https://genius.com/albums/Book-of-mormon/The-book-of-mormon-original-broadway-recording\")\nlionking &lt;- albumfun(hlink = \"https://genius.com/albums/The-lion-king/The-lion-king-original-broadway-cast-recording\")\navenueq &lt;- albumfun(hlink = \"https://genius.com/albums/Robert-lopez-and-jeff-marx/Avenue-q-original-broadway-cast-recording\")\noklahoma &lt;- albumfun(hlink = \"https://genius.com/albums/Richard-rodgers/Oklahoma-original-motion-picture-soundtrack\")\nsoundofmusic &lt;- albumfun(hlink = \"https://genius.com/albums/Richard-rodgers/The-sound-of-music-original-soundtrack-recording\")\nintheheights &lt;- albumfun(hlink = \"https://genius.com/albums/Lin-manuel-miranda/In-the-heights-original-broadway-cast-recording\")\nlemiserables &lt;- albumfun(hlink = \"https://genius.com/albums/Les-miserables-original-broadway-cast/Les-miserables-1987-original-broadway-cast\")\nphantomoftheopera &lt;- albumfun(hlink = \"https://genius.com/albums/Andrew-lloyd-webber/The-phantom-of-the-opera-original-london-cast-recording\")\nand a quick explorative plot tells us that it is working as intended, we see some variation in both slopes and offset, telling us that Billy Elliot has some range in its songs.\nbillyelliot %&gt;%\n  ggplot(aes(n, repetition)) +\n  geom_line(aes(group = name)) +\n  labs(title = \"Billy Elliot\") +\n  coord_cartesian(ylim = 0:1)\nto further compare we bind all the data.frames together for ease of handling\nmusical_names &lt;- c(\"The Phantom of the Opera\", \"The Book of Mormon\", \n                   \"Billy Elliot\", \"Les Miserables\", \"In the Heights\", \n                   \"Oklahoma\", \"The Sound of music\", \"Avenue Q\", \"Lion King\")\n\nrbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma,\n      soundofmusic, intheheights, lemiserables, phantomoftheopera) %&gt;%\n  mutate(album = factor(album, label = musical_names)) %&gt;%\n  ggplot(aes(n, repetition)) +\n  geom_line(aes(group = name), alpha = 0.5) +\n  facet_wrap(~ album)\nWow, here we see some differences in lyrical styles for the different musicals, from the evenness of the soundtrack to “In the Heights” to the range of “Lion King”. To try having them all in the same graph would be overwhelming. However, we could still plot the trend of each album in the same plot, fading out individual songs.\nrbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma,\n      soundofmusic, intheheights, lemiserables, phantomoftheopera) %&gt;%\n  ggplot(aes(n, repetition)) +\n  coord_cartesian(ylim = 0:1) +\n  geom_line(aes(group = name), alpha = 0.05) +\n  geom_smooth(aes(group = album, color = album), se = FALSE) +\n  theme_bw() +\n  labs(title = \"Repetition in musicals\") +\n  scale_colour_brewer(palette = \"Set1\",\n                      name = \"Musical\",\n                      labels = c(\"The Phantom of the Opera\", \"The Book of Mormon\", \n                                 \"Billy Elliot\", \"Les Miserables\",\n                                 \"In the Heights\", \"Oklahoma\", \n                                 \"The Sound of music\", \"Avenue Q\", \"Lion King\"))"
  },
  {
    "objectID": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#footnotes",
    "href": "post/2017-06-05-repetition-in-musicals-with-tidytext/index.html#footnotes",
    "title": "Repetition in musicals with tidytext",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html",
    "href": "post/slidecraft-colors-fonts/index.html",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "",
    "text": "Hello! This will be a multi-part series about what I like to call slidecrafting; The art of putting together slides that are functional and aesthetically pleasing. I will be using quarto presentations as my medium, but the advice should be fairly tool-agnostic.\nI think of slidecrafting as an art because of the inexact nature of many of the decisions you will be making. After all many slide decks can be distilled down into a series of still images, each of which should be carefully crafted.\nThis series will not be able to teach you everything you need, but it will cover the parts of the process that stay constant from deck to deck, so you can focus on the content and develop your brand/style."
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#theming",
    "href": "post/slidecraft-colors-fonts/index.html#theming",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Theming",
    "text": "Theming\nWhen I think of theming I think of all the different parts of the page I can change, this includes colors, fonts, sizes, shapes, etc etc. The two most impactful elements of a theme to modify are colors and fonts, let us see how we can find and apply these few changes.\nA little can go a long way! We like to joke that styling slides are a never-ending task. And while that is accurate, especially if you are a perfectionist or a beginner in the CSS/JS landscape, it shouldn’t be a reason why you don’t do any styling at all. If I’m strapped for time, I will spend 20-30 minutes up front creating and applying a minimal theme. After reading this post you should be able to as well.\nIn a quarto presentation, we can customize the theme beyond the basics by supplying a .scss file.\nWe can set this up by using the theme argument to the revealjs format like so:\n---\nformat:\n  revealjs: \n    theme: [default, custom.scss]\n---\nNext, we create the corresponding custom.scss file, which we will populate throughout this post. These comments are functional, we will go over what to put where when we get to it.\n/*-- scss:defaults --*/\n\n/*-- scss:rules --*/"
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#finding-colors",
    "href": "post/slidecraft-colors-fonts/index.html#finding-colors",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Finding Colors",
    "text": "Finding Colors\nWhen finding your colors you will mainly look for 3 different types of colors:\n\nbackground colors,\ntext colors,\ncontrast colors\n\nthis is a simplistic model and it will do us just fine. In general, I think having 3-6 colors is about the right amount of colors. This doesn’t mean that you couldn’t have 10 colors in your theme, but that you should be very deliberate in your choices.\nFor a small color theme, you would want a light and dark color for the background and text color. Dark text on a light background, or light text on dark background. Lastly, you pick a contrast color, something that looks good with both your background and text color.\nThis is what you get with the default themes that are provided in Quarto, and you can see them here in this gif:\n\nThis is also a perfect place to start looking for inspiration. using these themes with one or two modifications might get you all the way where you want to go. It is in general a good idea to look for inspiration in other people’s work.\nAnother personal favorite place of mine to go color theme hunting is on pinterest. I do a google search for “Pinterest color palettes” and go wild.\n\nIf you have any specific ideas in mind you can expand your search to include words like “sea”, “beach”, “Halloween”, or “pastel”.\nThe main thing you need to keep in mind, and the biggest difference from other types of colors you may have worked with, such as in data visualization, is that you need to have high contrast between your colors. This is by far the most important thing that separates a good theme from a bad theme. The goal for your slides is for other people to see them, if your contrast is low then people can’t.\nThere are many color contrast checking websites out there, I like the https://colourcontrast.cc/ and Color Contrast Checker by Coolors. If possible I try to have a contrast of at least 12, but something like 10 will be okay from time to time. Which is quite a high contrast without being impossible to hit.\n\nThis contrast requirement means that both your background and text color will be quite dark and light, as it is quite hard for most highly saturated colors to have high contrasts to anything else.\n\n\n\n\n\n\nWarning\n\n\n\nYou should try to avoid pure black and pure white. These colors can be a bit much and can be unpleasant to look at for long periods of time.\n\n\nThis contrast is related to font size, the smaller and finer the text is, the more important it is that you have good contrast.\nNext, we take a look at highlight colors! This is my term for the colors you use to add some pop and to direct the viewers’ eyes. These colors are used for anything from link colors, highlighting, buttons, and artistic elements.\nyou generally want 1 to 3 of these colors. Having at least 1 is perfectly sufficient and you can use it to great effect to direct the colors. 3 colors are where I’m still comfortable that they don’t get diluted. Using too many highlighting colors can confuse your viewers.\nThese colors should be different enough from the background and text color that they stand out. If you are using multiple highlighting colors you should make sure that they are colorblind-friendly with each other. I like to use the check_color_blindness() function from the prismatic package.\n\nAs we see above, the green and red colors don’t work well together because they are almost identical for people with Deuteranopia.\nTo recap:\n\nI think of or search for a palette I like\nI pull out 1-2 background colors, 1-2 text colors, and 1-3 highlight colors\nI use my color contrast checkers to validate and possibly modify my colors so that they are within range\nI check that the colors I have are colorblind-friendly\n…\nDone!\n\nAs long as I can keep the searching under 10 minutes the whole theme creation doesn’t take more than 15 minutes."
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#applying-colors",
    "href": "post/slidecraft-colors-fonts/index.html#applying-colors",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Applying Colors",
    "text": "Applying Colors\nLet us try all of that in practice. I found this nice blue and yellow color palette on Pinterest.\n\nusing a color picking tool, I love ColorSlurp I can extract the colors to be\n*Orient*\n02577B\n\n*Fountain Blue*\n5CB4C2\n\n*Morning Glory*\n99D9DD\n\n*Mystic*\nE1E8EB\n\n*Selective Yellow*\nF4BA02\nI’m thinking I want to use dark blue as my background colors, and the lightest color as my text color. Before I do any modification I get the following\n\nAnd by playing the sliders a little bit I have a contrast and some colors I’m happy with\n\nWe now open up our .scss file and fill in a couple of values. Many of the colorings are done by relations, so we can get a lot done by setting $body-bg, $body-color, and $link-color. This needs to be done inside scss:defaults.\n/*-- scss:defaults --*/\n$body-bg: #01364C;\n$body-color: #F7F8F9;\n$link-color: #99D9DD;\n\n/*-- scss:rules --*/\nWhile the above configurations are perfectly fine, I find that using sass variables to be clear, and it helps us tremendously if we start making more changes. So I create variables all with the prefix theme- and descriptive names so I know what is what.\n/*-- scss:defaults --*/\n$theme-darkblue: #01364C;\n$theme-blue: #99D9DD;\n$theme-white: #F7F8F9;\n$theme-yellow: #F4BA02;\n\n$body-bg: $theme-darkblue;\n$body-color: $theme-white;\n$link-color: $theme-blue;\n\n/*-- scss:rules --*/\nThis is more code, but now I can read at a glance what is happening. This gives us the following colors on our slides. All done with minimal code and effort. Using one of the highlight colors here to color the links, which also affects the hamburger menu and the progress bar at the bottom.\n\nThere are several sass variables that are used to control how our slides look. Notice how many of the values are defined as transformations of other values. So by setting $body-bg, $body-color, and $link-color we automatically gets things like $text-muted, $selection-bg, $border-color with values that works pretty well.\nLet us modify our theme just a bit more before moving on to fonts. We can use sass color functions to modify colors based on our theme.\nI want the headers to pop a little bit more, So I’m going to see if I can make them ever so slightly lighter blue. I see that the sass variable that controls the header color is $presentation-heading-color and that it defaults to $body-color. I use the lighten() function with $theme-blue, iterating a couple of times to find the perfect value.\n/*-- scss:defaults --*/\n$theme-darkblue: #01364C;\n$theme-blue: #99D9DD;\n$theme-white: #F7F8F9;\n$theme-yellow: #F4BA02;\n\n$body-bg: $theme-darkblue;\n$body-color: $theme-white;\n$link-color: $theme-blue;\n$presentation-heading-color: lighten($theme-blue, 15%);\n\n/*-- scss:rules --*/"
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#finding-fonts",
    "href": "post/slidecraft-colors-fonts/index.html#finding-fonts",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Finding Fonts",
    "text": "Finding Fonts\nWe find fonts the same way we find color; using our favorites of lots of googling. I always gravitate towards fonts.google.com. Generally, it is nice to use these online fonts because they are free and you don’t have to embed/ship them if you want to share your slides with others.\n\nOnce we are in here to can search around, looking for a font you like. For these slides, I’m going with Manrope for the text, and IBM Plex Serif for the headers. You must find a legible font, with a couple of styles and bold/italics support. This is going to make your life a lot easier once you get going.\nTo use “select” these fonts for use, you click on these links for each font type combination.\n\nThen you click this button to have a sidebar menu pop up.\n\nThis menu lets you select and deselect the fonts you have selected. When you are done, you can go to the “Use on the web” section, and click @import.\n\nAnd you want to copy the code inside the &lt;style&gt; tags. We are now ready to apply these fonts to our slides!"
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#applying-fonts",
    "href": "post/slidecraft-colors-fonts/index.html#applying-fonts",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Applying Fonts",
    "text": "Applying Fonts\nStart by adding the @import calls we found in the previous section. This should again go into the scss:defaults section of the .scss file. to modify the font we have 2 sass variables. First, we have $font-family-sans-serif to modify the general text, and $presentation-heading-font to modify the headers. Applying these changes gives us the following .scss file\n/*-- scss:defaults --*/\n$theme-darkblue: #01364C;\n$theme-blue: #99D9DD;\n$theme-white: #F7F8F9;\n$theme-yellow: #F4BA02;\n\n@import url('https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=Manrope:wght@200;300;400;500;600;700;800&display=swap');\n\n$body-bg: $theme-darkblue;\n$body-color: $theme-white;\n$link-color: $theme-blue;\n$presentation-heading-color: lighten($theme-blue, 15%);\n\n$font-family-sans-serif: 'Manrope', sans-serif;\n$presentation-heading-font: 'IBM Plex Serif', serif;\n\n/*-- scss:rules --*/\nWhich when renders results in the following slides.\n\nNotice how the fonts we used allows us to bold the words “finished presentation”.\nAnother thing you sometimes need to change depending on the font is the sizing of the text. The sass variable $presentation-font-size-root controls this, and defaults to 40px. Changing this one variable will affect everything on your slides.\nYou can also change the code font, this should ideally be a monospaced font. This is done using the $font-family-monospace sass variable."
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#using-css-classes",
    "href": "post/slidecraft-colors-fonts/index.html#using-css-classes",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Using CSS Classes",
    "text": "Using CSS Classes\nThe last tip for this blog post is the idea of using CSS classes, which is a quick and powerful way to add styling to your slides.\nRemember how we have 2 highlighting colors? We should have a way to apply these colors in our slides. For starters let us add a way to turn text these colors. Below are two CSS classes, named .blue and .yellow, that change the color and make the text bold with font-weight: bold;. Note that we need to put these classes into the scss:rules section.\n/*-- scss:rules --*/\n\n.blue {\n  color: $theme-blue;\n  font-weight: bold;\n}\n\n.yellow {\n  color: $theme-yellow;\n  font-weight: bold;\n}\nNow that we have our classes defined, you can either apply them using the visual editor in RStudio. Start by highlighting the text you want to apply the class to\n\nClick “Format” and select “Span…” This will only apply the changes to the highlighted words, instead of the whole paragraph.\n\nAnd then write in the class name in the “Classes” field. We see the word yellow by using the class “.yellow”.\n\nWe can also apply these changes in the source editor by using the [text]{.class} syntax. I added (slightly excessive) highlighting to a couple of words. See below\n[Quarto]{.blue} enables you to weave together [content]{.yellow} and [executable code]{.yellow} into a **finished presentation**. To learn more about [Quarto]{.blue} presentations see &lt;https://quarto.org/docs/presentations/&gt;.\nAnd we get the following slide\n\nCSS classes were a game changer for my slide-making. It is a little bit more manual, but if you can write the CSS you can apply it to your slides which IMO is a super powerful tool.\nThere are also changes where you just want to modify the CSS directly, these changes should also be applied in the scss:rules section. For example, in our example so far, we have used the blue color both to color the links, and as a highlighting color. This is very confusing, so let us make sure that all links are underlined. We can make this happen by adding.\nNote that CSS rules that target Reveal content generally need to use the .reveal .slide prefix to successfully override the theme’s default styles.\n.reveal .slide a {\n  text-decoration: underline;\n}\nAnd the changes have been applied."
  },
  {
    "objectID": "post/slidecraft-colors-fonts/index.html#roundup",
    "href": "post/slidecraft-colors-fonts/index.html#roundup",
    "title": "Slidecraft 101: Colors and Fonts",
    "section": "Roundup",
    "text": "Roundup\nI hoped you learned a lot! Creating and using themes might at first seem like a daunting task but I hope I have been able to convince you otherwise!\nHere are the .qmd file and the .scss file used for the examples in this post."
  },
  {
    "objectID": "post/slidecraft-code-output/index.html",
    "href": "post/slidecraft-code-output/index.html",
    "title": "Slidecraft 101: Code and Output",
    "section": "",
    "text": "Hello and welcome back to my multi-part series about what I like to call slidecrafting; The art of putting together slides that are functional and aesthetically pleasing. I will be using quarto presentations as my medium, but the advice should be fairly tool-agnostic. This is the second post, you can find the previous post under the category slidecraft 101.\nToday we will be looking at how we can style the code and the resulting output from our code chunks using the different options that quarto provides, as well as how we can use CSS to jazz it up even further."
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#style-code",
    "href": "post/slidecraft-code-output/index.html#style-code",
    "title": "Slidecraft 101: Code and Output",
    "section": "Style Code",
    "text": "Style Code\nto start with we need some code that produces some output, I’ll use the following dplyr example, but any code and language will work the same. Notice how we are setting echo: true to include the source code in the output.\n```r\n#| echo: true\nlibrary(dplyr)\n\nstarwars %&gt;% \n  mutate(name, bmi = mass / ((height / 100)  ^ 2)) %&gt;%\n  select(name:mass, bmi)\n```\nwithout any styling, this gives us the following slide\n\nNow we have a couple of my favorite quarto arguments and then some CSS.\nFirst, we have code-copy and code-link which both can be found in quarto revealjs reference. code-copy adds a little icon that when clicked copies the entire code block (super useful for teaching/reference slides) and code-link allows you to have downlit add clickable links to function in the code.\nLastly, I will point out code-line-numbers. This argument can either turn off or on the line numbers in the code block. This is nice and has a lot of benefits. Firstly you can turn off the numbering if you don’t find that it is a good use of slide real estate. Secondly, you can specify line numbers to be put into focus. So setting code-line-numbers: \"1,3\" in the above example yields us the following output\n\nWhich is a quick and easy way for you to direct the viewer’s attention.\nNext, let us talk about some styling. Before we do anything custom ourselves I should note that the 10 different built-in themes also affect the styling of the code and output, and these might give you what you need.\n\nIf you want to do a little bit more you need to get your hands dirty with some CSS and SCSS. Much like we did in the first blog post we are setting up a theme and applying it. I’m going with this new sandy background color I found.\n\nwhat I don’t like about this slide right now is that the code block doesn’t stand out that much. We have 3 major sass variables I want to introduce today; $code-block-bg, $code-block-border-color, and $code-block-font-size. $code-block-bg as the name suggests modifies the background of the code block, setting this color to white makes the code pop a little bit more\n\nWe can also change the border color with $code-block-border-color. This quantity defaults to being a lightened version of the main text color. We can instead set it to be a slightly darker version of the background color by setting $code-block-border-color: darken($body-bg, 20%); and getting this\n\nOr remove it entirely by setting it to be transparent with $code-block-border-color: #00000000;, for this look:\n\nLastly, we can change the font size with $code-block-font-size. While the size is pretty good right now, it is important that we know how to change it as it will depend on the fonts we use throughout the theme.\nBelow is an example where I turned up the size every so slightly"
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#style-output",
    "href": "post/slidecraft-code-output/index.html#style-output",
    "title": "Slidecraft 101: Code and Output",
    "section": "Style Output",
    "text": "Style Output\nThe next thing we want to look at is how to style the output. I found that adding styles to .reveal pre code did the trick in only changing the output, not the source.\nUsing the following code in our .scss file\n/*-- scss:rules --*/\n\n.reveal pre code {\n  background-color: #FFFFFF;\n}\nWill give us the following output\n\nWhich is already Quite a bit better. For this, I would do either one of two things.\n\nMake the source and output appear to be one large box\nSeparate the source and output more clearly\n\nLet us see how we can do both of these things.\nTo make the source and output appear as one, we need to modify the source box, since it has a little bit of a border that is making them unequal. Setting .reveal div.sourceCode {border: none} should remove the existing border, and we can remove the slight rounded border as well by also adding border-radius: 0px;. This gives a full .scss as follows\n/*-- scss:defaults --*/\n$theme-sand: #F1EAE5;\n$theme-white: #FFFFFF;\n\n$body-bg: $theme-sand;\n$code-block-bg: $theme-white;\n\n/*-- scss:rules --*/\n.reveal pre code {\n  background-color: $theme-white;\n}\n\n.reveal div.sourceCode {\n  border: none;\n  border-radius: 0px;\n}\nThis gives us the following slides\n\nIf we want some space, we can add margin-bottom: 10px !important; to the .reveal div.sourceCode specification, making it so there will be 10 pixels worth of space below the source, which gives some spacing between the two boxes. You need the !important to overwrite a more general style specification.\nMaking it so that part of the .scss file now looks like this\n.reveal div.sourceCode {\n  border: none;\n  border-radius: 0px;\n  margin-bottom: 10px !important;\n}\nAnd you now have a little bit of air between the boxes"
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#change-highlighting-theme",
    "href": "post/slidecraft-code-output/index.html#change-highlighting-theme",
    "title": "Slidecraft 101: Code and Output",
    "section": "Change highlighting theme",
    "text": "Change highlighting theme\nOne thing we haven’t looked at yet is how to change the highlighting theme. We will see how we can do that here. This is an often overlooked part of a presentation, that if you spend some time can tie your presentation together.\nThis is being controlled in the highlight-style argument. I would generally recommend that you set this option globally in your document by including it in your YAML file like so\n---\nformat:\n  revealjs: \n    theme: [default, custom.scss]\nhighlight-style: \"dracula\"\n---\nWhich would result in the following output\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the themes such as the \"dracula\" theme come with a built-in background color. If you set $code-block-bg in your .scss file it will be overwritten.\n\n\n{\n    \"text-color\": null,\n    \"background-color\": \"#f8f8f8\",\n    \"line-number-color\": \"#aaaaaa\",\n    \"line-number-background-color\": null,\n    \"text-styles\": {\n        \"Other\": {\n            \"text-color\": \"#8f5902\",\n            \"background-color\": null,\n            \"bold\": false,\n            \"italic\": false,\n            \"underline\": false\n        },\n        \"Attribute\": {\n            \"text-color\": \"#c4a000\",\n            \"background-color\": null,\n            \"bold\": false,\n            \"italic\": false,\n            \"underline\": false\n        },\nThere are many different highlighting themes that come shipped with Quarto. The ones with -dark and -light postfixes are themes that will adjust depending on whether your slides are dark or light.\nIf none of these are what you want, you can create your own. I would suggest taking one of the themes you like from the list linked above and modifying it to your liking. Copy that file into the same directory as your slides, and point to the file name instead. Like so:\n---\nformat:\n  revealjs: \n    theme: [default, custom.scss]\nhighlight-style: \"darkula.theme\"\n---\nIf your slides use both dark and light backgrounds you will likely need two sets of highlighting theme files, which you can set with:\n---\nformat:\n  revealjs: \n    theme: [default, custom.scss]\nhighlight-style:\n  light: light.theme\n  dark: dark.theme\n---\nI slowly modified the base theme, using the Working with Color Themes documentation to help me figure out what the different names mean.\nAnother thing that can aid, is opening the developer tools in your browser and hovering over the kind of elements you care about. Below I am doing just that, and I see that the pipe is of class sc\n\nFrom that, you can look at this list and see that it is a SpecialChar and you can modify your theme accordingly.\n\not: Other\nat: Attribute\nss: SpecialString\nan: Annotation\nfu: Function\nst: String\ncf: ControlFlow\nop: Operator\ner: Error\nbn: BaseN\nal: Alert\nva: Variable\npp: Preprocessor\nin: Information\nvs: VerbatimString\nwa: Warning\ndo: Documentation\nch: Char\ndt: DataType\nfl: Float\nco: Comment\ncv: CommentVar\ncn: Constant\nsc: SpecialChar\ndv: DecVal\nkw: Keyword\n\nWith these changes in place, I get to the following slides\n\nFor this theme, I also updated .reveal pre code to change the background color and the text color of the output to match the rest of the theme.\nYou generally don’t need to update all the items in the theme as you are unlikely to use all of them. I tend to update as I go, only updating the classes that I use."
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#use-fonts-with-ligatures",
    "href": "post/slidecraft-code-output/index.html#use-fonts-with-ligatures",
    "title": "Slidecraft 101: Code and Output",
    "section": "Use fonts with ligatures",
    "text": "Use fonts with ligatures\nI just added some new code to a slide\n\nand this code uses some multi-character symbols like != and |&gt;. These and many like them are common all over the different programming languages, so much so that people have created special fonts with ligatures to make them prettier.\nOne such font is FiraCode.\n\nAnd we can add these types of fonts to our slides as well! Start by downloading the font from the site, and copy over a .woff and .woff2 file to your slide directory. I selected FiraCode-Regular.woff and FiraCode-Regular.woff2.\nIn the /*-- scss:defaults --*/ part of our .scss file, we are going to add the following code. This is done to register the font family from the files we have included and to have it selected for use for monospace fonts.\n@font-face {\n    font-family: 'FiraCode';\n    src: url('../../../../../FiraCode-Regular.woff2') format('woff2'),\n         url('../../../../../FiraCode-Regular.woff') format('woff');\n}\n\n$font-family-monospace: 'FiraCode';\nYou might have noticed some ugliness with the ../../../../../s. To my knowledge, this is the best way of using a local font-face from a file.\nWith all of that, we now have beautiful ligatures"
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#step-up-your-style",
    "href": "post/slidecraft-code-output/index.html#step-up-your-style",
    "title": "Slidecraft 101: Code and Output",
    "section": "Step up your style",
    "text": "Step up your style\nEvery time I have shared some code with carbon.now.sh people go crazy because it always looks so good. And I agree, the defaults are really really nice.\nSo we are going to replicate this. This is going to be a little more involved, but if you don’t have that much code on your slides it will be a nice touch!\nWhat I do in this case is use reprex to generate the code and output as one, I paste that into my chunk and turn off evaluation with #| eval: false. I do this because then I don’t have to worry about styling the output to match.\nI switch over to using the \"nord\" highlight palette. It has this weird mistake because it thinks that the opening bracket is an error.\n\nI could copy over the .theme file and modify it in the right place, but I’m lazy and modify it directly in the .scss file instead by adding which will fix the color and remove the underline\ncode span.er {\n  color: #ebcb8b;\n  text-decoration: none;\n}\nThe line numbers are not too important this code chunk so I set #| code-line-numbers: false. Then I start working on styling the source div.\n.reveal div.sourceCode {\n  border: none;\n  border-radius: 5px;\n  margin-bottom: 10px !important;\n  box-shadow: 0 20px 47px rgb(0 0 0 / 55%);\n  width: fit-content;\n  margin: auto !important;\n}\nThe main thing that changes is that I brought back the border-radius. I added some box-shadows. These things alone make a huge difference in appearance.\nNext, I changed width: fit-content;, this makes it so the width of the div changes with the width of the code, the code I have right now isn’t that wide and it looked a little lopsided. Lastly, I set margin: auto !important;, which centers the div so it doesn’t cling to the left side.\nThe very final thing I changed is adding some padding to the code, this is the space between the code itself and the inside border of the div, I did this by adding\n.reveal div.sourceCode pre code {\n  padding: 25px;\n}\nAll of this results in the following slide:"
  },
  {
    "objectID": "post/slidecraft-code-output/index.html#roundup",
    "href": "post/slidecraft-code-output/index.html#roundup",
    "title": "Slidecraft 101: Code and Output",
    "section": "Roundup",
    "text": "Roundup\nI hoped you learned a lot! There are a lot of themes, colors, and options to play with. But with a little patience, you can create some nice code chunks for your slides.\nHere are the .qmd file and the .scss file used for the last example in this post."
  },
  {
    "objectID": "post/2017-07-20-analysing-user-2017-schedule-data/index.html",
    "href": "post/2017-07-20-analysing-user-2017-schedule-data/index.html",
    "title": "Analysing useR!2017 schedule data",
    "section": "",
    "text": "After attending useR!2017 for the first time, which great pleasure and new connections made. I decided to see if I could extract some of the information available in the public schedule. So as with my last post, I’ll do a bit of scraping followed by a few visualizations."
  },
  {
    "objectID": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#packages",
    "href": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#packages",
    "title": "Analysing useR!2017 schedule data",
    "section": "Packages",
    "text": "Packages\nlibrary(tidyverse)\nlibrary(utils)\nlibrary(plotly)\nlibrary(ltm)\nrequire(visNetwork)"
  },
  {
    "objectID": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#web-scraping",
    "href": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#web-scraping",
    "title": "Analysing useR!2017 schedule data",
    "section": "Web scraping",
    "text": "Web scraping\nI found this task easiest with the help of purrr:map(). First, we find the full schedules at the following links\nhttps://user2017.sched.com/2017-07-04/overview (Tuesday)\nhttps://user2017.sched.com/2017-07-05/overview (Wednesday)\nhttps://user2017.sched.com/2017-07-06/overview (Thursday)\nhttps://user2017.sched.com/2017-07-07/overview (Friday)\nthen we read the entire page into a tibble along with a day variable.\nday &lt;- c(\"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nlink &lt;- paste0(\"https://user2017.sched.com/2017-07-0\", 4:7, \"/overview\", sep = \"\")\n\nevent0 &lt;- map2_df(link, day,\n                  ~ tibble(text = readLines(.x),\n                           day = .y))\nthen with the help of stringr we extract the desired information from the document, following the idiom that “multiple simple regex is better than one complicated one.” I also filtered out most non-talk events.\nevents &lt;- event0 %&gt;% \n  filter(str_detect(text, \"&lt;span class='\") | str_detect(text, \"&lt;/h3&gt;\"),\n         !str_detect(text, \"REGISTRATION\"),\n         !str_detect(text, \"COFFEE BREAK\"),\n         !str_detect(text, \"LUNCH\"),\n         !str_detect(text, \"WELCOME\"),\n         !str_detect(text, \"Poster\"),\n         !str_detect(text, \"RIOT SESSION\"),\n         !str_detect(text, \"Buses\"),\n         !str_detect(text, \"Dinner\"),\n         !str_detect(text, \"CLOSING\")) %&gt;%\n  mutate(time = str_extract(text, \"&lt;h3&gt;.{1,7}\"), # time\n         time = str_replace(time, \"&lt;h3&gt; *\", \"\"),\n         id = str_extract(text, \"id='\\\\S{32}\"), # id\n         id = str_replace(id, \"id='\", \"\"),\n         name = str_extract(text, str_c(id, \".*\")), # name\n         name = str_replace(name, str_c(id, \"'&gt;\"), \"\"),\n         name = str_extract(name, \"^.*(?=( &lt;span))\"),\n         room = str_extract(text, 'vs\"&gt;(.*?)&lt;'),\n         room = str_replace(room, 'vs\"&gt;', \"\"),\n         room = str_replace(room, '&lt;',\"\")) %&gt;% # room\n  fill(time) %&gt;%\n  filter(!str_detect(text, \"&lt;h3&gt;\")) %&gt;%\n  dplyr::select(-text)\nlet’s take a look at what we have by now just to see that we have what we want.\nhead(events)\n## # A tibble: 6 x 5\n##   day     time   id                    name                               room  \n##   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;                 &lt;chr&gt;                              &lt;chr&gt; \n## 1 Tuesday 9:30am 893eab219225a0990770… Data Carpentry: Open and Reproduc… 2.02  \n## 2 Tuesday 9:30am 30c0eebdc887f3ad3aef… Dose-response analysis using R     4.02  \n## 3 Tuesday 9:30am 57ce234e5ce9082da3cc… Geospatial visualization using R   4.03  \n## 4 Tuesday 9:30am 95b110146486b0a5f802… Introduction to Bayesian inferenc… 2.01  \n## 5 Tuesday 9:30am 7294f7df20ab1a7c37df… Introduction to parallel computin… 3.01  \n## 6 Tuesday 9:30am f15703fe51e89294f2b5… Rcpp: From Simple Examples to Mac… PLENA…\nNow that we have all the information about the different events we can scrape every event page to find its attendees. This following chuck of code might seem a little hard at first, it helps to notice that there is a second tibble inside the big tibble.\npeople &lt;- map_df(events$id,\n       ~ tibble(attendee = tibble(text = readLines(\n         str_c(\"https://user2017.sched.com/event-goers/\", .x))) %&gt;%\n                filter(str_detect(text, \" +&lt;li&gt;&lt;a href=\")) %&gt;% \n                .$text %&gt;%\n                str_split(., \"li&gt;&lt;li\") %&gt;% \n                unlist(),\n       id = .x) %&gt;%\n  mutate(attendee = str_replace(attendee, \"(.*?)title=\\\"\", \"\"),\n         attendee = str_replace(attendee, \"\\\"&gt;&lt;(.*)\", \"\")) %&gt;%\n  filter(!str_detect(attendee, \"venue\"),\n         !str_detect(attendee, \"Private\")))\nLet’s again take a look at what we have by now just to see that we have what we want.\nhead(people)\n## # A tibble: 6 x 2\n##   attendee                                                id                    \n##   &lt;chr&gt;                                                   &lt;chr&gt;                 \n## 1 \"              &lt;li&gt;&lt;a href=\\\"\\\"&gt;Schedule&lt;/a&gt;&lt;/li&gt;\"      893eab219225a09907704…\n## 2 \"                                                     … 893eab219225a09907704…\n## 3 \"lvaudor\"                                               893eab219225a09907704…\n## 4 \"Alan Ponce\"                                            893eab219225a09907704…\n## 5 \"bpiccolo\"                                              893eab219225a09907704…\n## 6 \"Katharina Barzagar Nazari\"                             893eab219225a09907704…"
  },
  {
    "objectID": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#visualizations",
    "href": "post/2017-07-20-analysing-user-2017-schedule-data/index.html#visualizations",
    "title": "Analysing useR!2017 schedule data",
    "section": "visualizations",
    "text": "visualizations\nWith a data set with this many possibilities, the options are quite a few, so here I’ll just list a few of the ones I found handy. So first we just do a simple bubble plot, this will be done with left_join’s and count and piped straight into ggplot.\nleft_join(events, people, by = \"id\") %&gt;%\n  count(id) %&gt;%\n  left_join(events, by = \"id\") %&gt;%\n  filter(day == \"Friday\") %&gt;%\n  ggplot(aes(time, room, size = n)) +\n  geom_point() +\n  theme_bw() +\n  scale_size(range = c(5, 20)) +\n  labs(title = \"useR!2017 Friday schedule\",\n       x = \"\")\n\nSince both our room and time were simply character vectors, the ordering is not right. This can be fixed by setting the levels correctly. Here I have the ordered vectored for both room and time.\ntime_levels &lt;- c(\"9:15am\", \"9:30am\", \"11:00am\", \"11:18am\", \"11:30am\", \"11:36am\",\n                 \"11:54am\", \"12:12pm\", \"1:15pm\", \"1:30pm\", \"1:48pm\", \"2:00pm\", \n                 \"2:06pm\", \"2:24pm\", \"2:42pm\", \"3:30pm\", \"3:45pm\", \"4:00pm\", \n                 \"4:45pm\", \"4:55pm\", \"5:00pm\", \"5:05pm\", \"5:30pm\", \"5:35pm\", \n                 \"5:40pm\", \"5:45pm\", \"5:50pm\", \"5:55pm\", \"6:00pm\", \"6:05pm\", \n                 \"6:10pm\", \"6:15pm\", \"6:20pm\", \"7:00pm\")\nroom_levels &lt;- c(\"PLENARY\", \"2.01\", \"2.02\", \"3.01\", \"3.02\", \"4.01\", \"4.02\")\nand we deal with it with a single mutate like so\nleft_join(events, people, by = \"id\") %&gt;%\n  count(id) %&gt;%\n  left_join(events, by = \"id\") %&gt;%\n  mutate(time = factor(time, time_levels),\n         room = factor(room, room_levels)) %&gt;%\n  filter(day == \"Friday\") %&gt;%\n  ggplot(aes(time, room, size = n)) +\n  geom_point() +\n  theme_bw() +\n  scale_size(range = c(5, 20)) +\n  labs(title = \"useR!2017 Friday schedule\",\n       x = \"\")\n\nanother way to visualize it would be to use a stacked bar chart so\np &lt;- left_join(events, people, by = \"id\") %&gt;%\n  count(id) %&gt;%\n  left_join(events, by = \"id\") %&gt;%\n  filter(day == \"Thursday\") %&gt;%\n  mutate(time = factor(time, time_levels),\n         room = factor(room, rev(room_levels))) %&gt;%\n  ggplot(aes(time, fill = room, text = name)) +\n  geom_bar(aes(weight = n)) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"useR!2017 Thursday schedule\",\n       x = \"\")\np\n\nor with a bit of interactivity plotly::ggplotly can be used so that is possible to hover over each event to see name and size.\nggplotly(p, tooltip = c(\"n\", \"name\"), width = 700, height = 500)\n\n\n\n\n\n\nNetwork graph\nTo make our-self a simple network graph will I be using the visNetwork package, which has a lovely vignette. So here first of all to create a manageable graph I’ll subset all the Wednesday talks in room 4.02, which was the “Shiny I” and “Text Mining” block.\nsub_data &lt;- left_join(events, people, by = \"id\") %&gt;%\n  filter(day == \"Wednesday\", room == \"4.02\") %&gt;%\n  dplyr::select(name, attendee, time)\nI this graph I will let each node be an event and let the edges be to which degree they share attendees. So we start\nnode_size &lt;- sub_data %&gt;% \n  group_by(name, time) %&gt;%\n  summarize(n = n())\nto find how many attendees the events share we first find all the different pairs of events with utils::combn function and with purrr and inner_join finds how many they have in common. Since utils::combn gives its result in a matrix we have to fiddle just a bit to our way back to a tibble.\nconn &lt;- combn(node_size$name, 2) %&gt;%\n  as_tibble() %&gt;%\n  map_int(~ inner_join(sub_data %&gt;% filter(name == .x[1]), \n                       sub_data %&gt;% filter(name == .x[2]), by = \"attendee\")\n              %&gt;% nrow()) %&gt;%\n  rbind(combn(node_size$name, 2)) %&gt;% t() %&gt;% as.tibble()\n## Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n## Please use `as_tibble()` instead.\n## The signature and semantics have changed, see `?as_tibble`.\n## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\n## Using compatibility `.name_repair`.\nnames(conn) &lt;- c(\"n\", \"from\", \"to\")\nconn\n## # A tibble: 45 x 3\n##    n     from                               to                                  \n##    &lt;chr&gt; &lt;chr&gt;                              &lt;chr&gt;                               \n##  1 21    A Tidy Data Model for Natural Lan… bradio: Add data music widgets to y…\n##  2 57    A Tidy Data Model for Natural Lan… Developing and deploying large scal…\n##  3 82    A Tidy Data Model for Natural Lan… How we built a Shiny App for 700 us…\n##  4 84    A Tidy Data Model for Natural Lan… Interacting with databases from Shi…\n##  5 84    A Tidy Data Model for Natural Lan… manifestoR - a tool for data journa…\n##  6 99    A Tidy Data Model for Natural Lan… Neural Embeddings and NLP with R an…\n##  7 83    A Tidy Data Model for Natural Lan… ShinyProxy                          \n##  8 155   A Tidy Data Model for Natural Lan… Text Analysis and Text Mining Using…\n##  9 168   A Tidy Data Model for Natural Lan… Text mining, the tidy way           \n## 10 46    bradio: Add data music widgets to… Developing and deploying large scal…\n## # … with 35 more rows\nfor the node tibble we need to supply it with an id column, but I will also supply it with a label (name of the event), size (number of people in the event), and color (what book is this event in. green = Shiny I, blue = Text Mining).\nShiny_I &lt;- c(\"11:00am\", \"11:18am\", \"11:36am\", \"11:54am\", \"12:12pm\")\nText_Mining &lt;- c(\"1:30pm\", \"1:48pm\", \"2:06pm\", \"2:24pm\", \"2:42pm\")\nnodes &lt;- node_size %&gt;% \n  mutate(id = name,\n         label = str_wrap(name, width = 20),\n         size = n / 20,\n         color = case_when(\n           time %in% Shiny_I ~ \"lightgreen\",\n           time %in% Text_Mining ~ \"lightblue\"\n         ))\nfor the edge tibble it needs from and to columns that match with the id in the node tibble. I will also supply with a constant color column (because if omitted it would borrow from the node coloring) and a width column to indicate how many attendees they share. This is again done with a couple of left_joins and the connectivity is the average percentage of attendees they share. Lastly, we remove any edge with less than 0.5 connectivity to clear out the graph.\nedges &lt;- conn %&gt;% \n  left_join(node_size %&gt;% \n              dplyr::select(-time) %&gt;% \n              rename(n_from = n), \n                   by = c(\"from\" = \"name\")) %&gt;%\n  left_join(node_size %&gt;% \n              dplyr::select(-time) %&gt;% \n              rename(n_to = n), \n                   by = c(\"to\" = \"name\")) %&gt;%\n  mutate(n = as.numeric(n),\n         n_to = as.numeric(n_to),\n         n_from = as.numeric(n_from),\n         connectivity = (n / n_from + n / n_to) / 2,\n         width = connectivity * 10,\n         color = \"grey\") %&gt;%\n  filter(connectivity &gt; 0.5)\nThis yields us with a wonderful graph which shows a somehow clear divide between the two blocks.\nvisNetwork(nodes, edges, width = \"100%\")\n\n\n\n\nI hope you enjoyed this post and I would love you to see any visualization or analysis you might have regarding this data.\n\n\n session information \n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-04                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                            \n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.0.0)                    \n backports     1.2.1      2020-12-09 [1] CRAN (R 4.0.2)                    \n blogdown      1.3.2      2021-06-06 [1] Github (rstudio/blogdown@00a2090) \n bookdown      0.22       2021-04-22 [1] CRAN (R 4.0.2)                    \n broom         0.7.6      2021-04-05 [1] CRAN (R 4.0.2)                    \n bslib         0.2.4.9003 2021-05-05 [1] Github (rstudio/bslib@ba6a80d)    \n cellranger    1.1.0      2016-07-27 [1] CRAN (R 4.0.0)                    \n cli           3.0.0      2021-06-30 [1] CRAN (R 4.0.2)                    \n clipr         0.7.1      2020-10-08 [1] CRAN (R 4.0.2)                    \n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.0.5)                    \n colorspace    2.0-1      2021-05-04 [1] CRAN (R 4.0.2)                    \n crayon        1.4.1      2021-02-08 [1] CRAN (R 4.0.2)                    \n crosstalk     1.1.1      2021-01-12 [1] CRAN (R 4.0.2)                    \n data.table    1.14.0     2021-02-21 [1] CRAN (R 4.0.2)                    \n DBI           1.1.1      2021-01-15 [1] CRAN (R 4.0.2)                    \n dbplyr        2.1.1      2021-04-06 [1] CRAN (R 4.0.2)                    \n desc          1.3.0      2021-03-05 [1] CRAN (R 4.0.2)                    \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.0.0)                    \n digest        0.6.27     2020-10-24 [1] CRAN (R 4.0.2)                    \n dplyr       * 1.0.7      2021-06-18 [1] CRAN (R 4.0.2)                    \n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.0.2)                    \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.0.0)                    \n expm          0.999-6    2021-01-13 [1] CRAN (R 4.0.2)                    \n fansi         0.5.0      2021-05-25 [1] CRAN (R 4.0.2)                    \n farver        2.1.0      2021-02-28 [1] CRAN (R 4.0.2)                    \n forcats     * 0.5.1      2021-01-27 [1] CRAN (R 4.0.2)                    \n fs            1.5.0      2020-07-31 [1] CRAN (R 4.0.2)                    \n generics      0.1.0      2020-10-31 [1] CRAN (R 4.0.2)                    \n ggplot2     * 3.3.3      2020-12-30 [1] CRAN (R 4.0.2)                    \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.0.2)                    \n gtable        0.3.0      2019-03-25 [1] CRAN (R 4.0.0)                    \n haven         2.4.1      2021-04-23 [1] CRAN (R 4.0.2)                    \n highr         0.9        2021-04-16 [1] CRAN (R 4.0.2)                    \n hms           1.1.0      2021-05-17 [1] CRAN (R 4.0.2)                    \n htmltools     0.5.1.1    2021-01-22 [1] CRAN (R 4.0.2)                    \n htmlwidgets   1.5.3      2020-12-10 [1] CRAN (R 4.0.2)                    \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.0.2)                    \n jquerylib     0.1.4      2021-04-26 [1] CRAN (R 4.0.2)                    \n jsonlite      1.7.2      2020-12-09 [1] CRAN (R 4.0.2)                    \n knitr       * 1.33       2021-04-24 [1] CRAN (R 4.0.2)                    \n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.0.2)                    \n lattice       0.20-41    2020-04-02 [1] CRAN (R 4.0.5)                    \n lazyeval      0.2.2      2019-03-15 [1] CRAN (R 4.0.0)                    \n lifecycle     1.0.0      2021-02-15 [1] CRAN (R 4.0.2)                    \n ltm         * 1.1-1      2018-04-17 [1] CRAN (R 4.0.2)                    \n lubridate     1.7.10     2021-02-26 [1] CRAN (R 4.0.2)                    \n magrittr      2.0.1      2020-11-17 [1] CRAN (R 4.0.2)                    \n MASS        * 7.3-53.1   2021-02-12 [1] CRAN (R 4.0.5)                    \n Matrix        1.3-2      2021-01-06 [1] CRAN (R 4.0.5)                    \n modelr        0.1.8      2020-05-19 [1] CRAN (R 4.0.0)                    \n msm         * 1.6.8      2019-12-16 [1] CRAN (R 4.0.2)                    \n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.0.0)                    \n mvtnorm       1.1-1      2020-06-09 [1] CRAN (R 4.0.0)                    \n pillar        1.6.1      2021-05-16 [1] CRAN (R 4.0.2)                    \n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.0.0)                    \n plotly      * 4.9.2.9000 2020-12-29 [1] Github (ropensci/plotly@e741959)  \n png           0.1-7      2013-12-03 [1] CRAN (R 4.0.0)                    \n polycor     * 0.7-10     2019-08-05 [1] CRAN (R 4.0.2)                    \n purrr       * 0.3.4      2020-04-17 [1] CRAN (R 4.0.0)                    \n R6            2.5.0      2020-10-28 [1] CRAN (R 4.0.2)                    \n Rcpp          1.0.6      2021-01-15 [1] CRAN (R 4.0.2)                    \n readr       * 1.4.0      2020-10-05 [1] CRAN (R 4.0.2)                    \n readxl        1.3.1      2019-03-13 [1] CRAN (R 4.0.2)                    \n reprex        2.0.0      2021-04-02 [1] CRAN (R 4.0.2)                    \n rlang         0.4.11     2021-04-30 [1] CRAN (R 4.0.2)                    \n rmarkdown     2.8.6      2021-06-06 [1] Github (rstudio/rmarkdown@9dc5d97)\n rprojroot     2.0.2      2020-11-15 [1] CRAN (R 4.0.2)                    \n rstudioapi    0.13       2020-11-12 [1] CRAN (R 4.0.2)                    \n rvest         1.0.0      2021-03-09 [1] CRAN (R 4.0.2)                    \n sass          0.3.1.9003 2021-05-05 [1] Github (rstudio/sass@6166162)     \n scales        1.1.1      2020-05-11 [1] CRAN (R 4.0.0)                    \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.0.0)                    \n stringi       1.6.2      2021-05-17 [1] CRAN (R 4.0.2)                    \n stringr     * 1.4.0      2019-02-10 [1] CRAN (R 4.0.0)                    \n survival      3.2-10     2021-03-16 [1] CRAN (R 4.0.5)                    \n tibble      * 3.1.2      2021-05-16 [1] CRAN (R 4.0.2)                    \n tidyr       * 1.1.3      2021-03-03 [1] CRAN (R 4.0.2)                    \n tidyselect    1.1.1      2021-04-30 [1] CRAN (R 4.0.2)                    \n tidyverse   * 1.3.1      2021-04-15 [1] CRAN (R 4.0.2)                    \n utf8          1.2.1      2021-03-12 [1] CRAN (R 4.0.2)                    \n vctrs         0.3.8      2021-04-29 [1] CRAN (R 4.0.2)                    \n viridisLite   0.4.0      2021-04-13 [1] CRAN (R 4.0.2)                    \n visNetwork  * 2.0.9      2019-12-06 [1] CRAN (R 4.0.0)                    \n withr         2.4.2      2021-04-18 [1] CRAN (R 4.0.2)                    \n xfun          0.23       2021-05-15 [1] CRAN (R 4.0.2)                    \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.0.0)                    \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.0.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html",
    "title": "usethis workflow for package development",
    "section": "",
    "text": "Note\n\n\n\nThis code has been slightly revised to make sure it works as of 2018-12-16.\nThere has been a lot of progress in the aid of package development in R in recent times. The classic blogpost by Hilary Parker Writing an R Package From Scratch and its younger sister Writing an R package from scratch by Tomas Westlake are both great sources of information to create a package. For more general documentation on package development, you would be right to look at Hadley Wickham’s book R packages. The devtools package has always been instrumental for good package development, but some of these features and additional ones are now to be found in the usethis package. The usethis promises to\nIn this blog post, I’ll outline the basic workflow you can acquire using the tools in usethis. More specifically I’ll outline a workflow of an R package development. The course of any R package development can be broken down into these steps:\nBefore we start, I assume that you will be using Rstudio for this tutorial."
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-the-creation",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-the-creation",
    "title": "usethis workflow for package development",
    "section": "Before the creation",
    "text": "Before the creation\nBefore we get started we need to make sure we have the essential packages installed to create an R package development workflow\n#install.packages(c(\"devtools\", \"roxygen2\", \"usethis\"))\nlibrary(devtools)\nlibrary(roxygen2)\nlibrary(usethis)\nSide-note, if you are to create an R package, you need a name. It needs to be unique, especially if you plan on getting your package on CRAN. The available package can help you evaluate possible names to make sure they don’t clash with other names and that they don’t mean something rude. For this example I’m going to make a horrible name by shortening the phrases “usethis workflow”\nlibrary(available)\navailable(\"utwf\")\n\nthe only acronym it finds is “Umauma Triple Water Falls” so we are good to go. Next, we need to make sure that you have set up usethis, for this section I’ll refer to the original documentation usethis setup as it explains these steps better than I could."
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#creating-minimal-functional-package",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#creating-minimal-functional-package",
    "title": "usethis workflow for package development",
    "section": "Creating minimal functional package",
    "text": "Creating minimal functional package\nNow that you have followed the setup guide you are ready to create a minimal functional package.\nFor creation we will use the create_package() function to create an R package.\ncreate_package(\"~/Desktop/utwf\")\nuse_git()\nuse_github()\n\n\n\nAnd we are done! We now have a minimal R package, complete with a Github repository. With these files included:\n\nRight now it doesn’t have much, in fact, it doesn’t even have a single function in it. We can check that the package works by pressing “Install and Restart” in the “Build” panel. Alternatively, you can use the keyboard shortcut Cmd+Shift+B (Ctrl+Shift+B for Windows)."
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#one-time-modifications",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#one-time-modifications",
    "title": "usethis workflow for package development",
    "section": "One time modifications",
    "text": "One time modifications\nNow that we are up and running there is a bunch of things we should do before we start writing code. Firstly we will go over all the actions that only have to be done once and get those out of the way.\nFirstly we will go into the DESCRIPTION file and make sure that the Authors@R is populated correctly and modify the Title and Description fields.\nNext, we will license the package. This can be done using one of the following functions (we will use MIT for this example)\nuse_mit_license()\nuse_gpl3_license()\nuse_apl2_license()\nuse_cc0_license()\n\nChoice of which license you need is beyond the scope of this post. Please refer to the R Packages license section or https://choosealicense.com/ for further assistance.\nNow we add the readme files, this is done using the\nuse_readme_rmd()\n\nThis will create a readme.Rmd file that you can edit and knit as you normally would.\nNext we will setup some continuous integration. I’ll recommend trying to do all of the 3 following:\nuse_travis()\nuse_appveyor()\nuse_coverage(type = c(\"codecov\"))\n\nThese calls won’t do all the work for you, so you would have to follow the directions (following red circles) and turn on the services on the Travis and AppVeyor websites respectively, copy badges to the readme (typically placed right under the main title “# utwf”) and copy the code snippet to the .travis.yml file.\nYou will most likely also want to include unit testing, this can be achieved using the testthat package, to include the testing capacity of testthat in your package simply run the following\nuse_testthat()\n\nyou will need to add at least one test to avoid failed builds on Travis-ci and Appveyor. More information on how to do testing can be found at the Testing chapter in the R packages book.\nNext, we will add spell checking to our workflow, this is done with\nuse_spell_check()\n\nMake sure that the spelling package is installed before running.\nIf you are going to include data in your package, you would want to include a data-raw folder where the data is created/formatted.\nuse_data_raw()\n\nLastly, if you plan on doing a little larger project a NEWS file is very handy to keep track of what is happening in your package.\nuse_news_md()"
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#multiple-time-modifications",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#multiple-time-modifications",
    "title": "usethis workflow for package development",
    "section": "Multiple time modifications",
    "text": "Multiple time modifications\nNow that we have set up all the basics, the general development can begin.\nYour typical workflow will be repeating the following steps in the order that suits your flow\n\nWrite some code\nRestart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)\nBuild and Reload Cmd+Shift+B (Ctrl+Shift+B for Windows)\nTest Package Cmd+Shift+T (Ctrl+Shift+T for Windows)\nCheck Package Cmd+Shift+E (Ctrl+Shift+E for Windows)\nDocument Package Cmd+Shift+D (Ctrl+Shift+D for Windows)\n\nWriting code most likely includes writing functions, this is helped by the use_r() function by adding and opening a .R file that you write your function in\nuse_r(\"function_name\")\n\nThis function is very important and you will using it a lot, not only will it create the files you save your functions in, but it will also open the files if they are already created, this makes navigating your R files much easier. Once you have created your function it is time to add some tests! This is done using the use_test() function, and it works much the same way as the use_r().\nuse_test(\"function_name\")\n\nIn the creating of your functions, you might need to depend on another package, to add a function to the imports field in the DESCRIPTION file you can use the use_package() function\nuse_package(\"dplyr\") \n\nSpecial cases function includes use_rcpp(), use_pipe() and use_tibble().\nA vignette provides a nice piece of documentation once you have added a bunch of capabilities to your package.\nuse_vignette(\"How to do this cool analysis\")"
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-every-commit",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-every-commit",
    "title": "usethis workflow for package development",
    "section": "Before every commit",
    "text": "Before every commit\nBefore you commit, run the following commands one more time to make sure you didn’t break anything.\n\nRestart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)\nDocument Package Cmd+Shift+D (Ctrl+Shift+D for Windows)\nCheck Package Cmd+Shift+E (Ctrl+Shift+E for Windows)"
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-every-release",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#before-every-release",
    "title": "usethis workflow for package development",
    "section": "Before every release",
    "text": "Before every release\nYou have worked and have created something wonderful. You want to showcase the work. First, go knit the readme.Rmd file and then run these commands again to check that everything is working.\n\nRestart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)\nDocument Package Cmd+Shift+D (Ctrl+Shift+D for Windows)\nCheck Package Cmd+Shift+E (Ctrl+Shift+E for Windows)\n\nupdate the version number with the use of\nuse_version()\n\nAnd you are good to go!"
  },
  {
    "objectID": "post/2018-09-02-usethis-workflow-for-package-development/index.html#conclusion",
    "href": "post/2018-09-02-usethis-workflow-for-package-development/index.html#conclusion",
    "title": "usethis workflow for package development",
    "section": "Conclusion",
    "text": "Conclusion\nThis is the end of this post, and there are many more functions in usethis that I haven’t covered here, both for development and otherwise. One set of functions I would like to highlight in particular is the Helpers for tidyverse development which helps you follow tidyverse conventions which are generally a little stricter than the defaults. If you have any questions or additions you would like to have added please don’t refrain from contacting me!\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                           \n blogdown      1.3.2      2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22       2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   \n cachem        1.0.5      2021-05-15 [1] CRAN (R 4.1.0)                   \n callr         3.7.0      2021-04-20 [1] CRAN (R 4.1.0)                   \n cli           3.0.0      2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1      2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1      2021-02-08 [1] CRAN (R 4.1.0)                   \n desc          1.3.0      2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.1.0)                   \n devtools    * 2.4.1      2021-05-05 [1] CRAN (R 4.1.0)                   \n digest        0.6.27     2020-10-24 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.1.0)                   \n fastmap       1.1.0      2021-01-25 [1] CRAN (R 4.1.0)                   \n fs            1.5.0      2020-07-31 [1] CRAN (R 4.1.0)                   \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.1.0)                   \n highr         0.9        2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1    2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2      2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33       2021-04-24 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0      2021-02-15 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   \n memoise       2.0.0      2021-01-26 [1] CRAN (R 4.1.0)                   \n pkgbuild      1.2.0      2020-12-15 [1] CRAN (R 4.1.0)                   \n pkgload       1.2.1      2021-04-06 [1] CRAN (R 4.1.0)                   \n png           0.1-7      2013-12-03 [1] CRAN (R 4.1.0)                   \n prettyunits   1.1.1      2020-01-24 [1] CRAN (R 4.1.0)                   \n processx      3.5.2      2021-04-30 [1] CRAN (R 4.1.0)                   \n ps            1.6.0      2021-02-28 [1] CRAN (R 4.1.0)                   \n purrr         0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0      2020-10-28 [1] CRAN (R 4.1.0)                   \n remotes       2.4.0      2021-06-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11     2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9        2021-06-15 [1] CRAN (R 4.1.0)                   \n roxygen2    * 7.1.1.9001 2021-06-08 [1] Github (r-lib/roxygen2@e8cd313)  \n rprojroot     2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2      2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   \n testthat      3.0.2      2021-02-14 [1] CRAN (R 4.1.0)                   \n usethis     * 2.0.1      2021-02-10 [1] CRAN (R 4.1.0)                   \n withr         2.4.2      2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24       2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html",
    "href": "post/2018-02-14-rvision-a-first-look/index.html",
    "title": "Rvision: A first look",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16."
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#overview",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#overview",
    "title": "Rvision: A first look",
    "section": "Overview",
    "text": "Overview\nRecently I stumbled across the Rvision package, which frankly looks amazing so far (still in development at this time of writing). So I decided to take it for a spin and show you what I found."
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#setup",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#setup",
    "title": "Rvision: A first look",
    "section": "Setup",
    "text": "Setup\nSo for this, you will need a computer with a webcam and the Rvision package with its dependencies. It will use ROpenCVLite to access OpenCV’s functionalities. If not already installed, ROpenCVLite will be installed first by the command line below. Furthermore while not necessary for Rvision I have imported dplyr for general data manipulation.\n#devtools::install_github(\"swarm-lab/Rvision\")\nlibrary(Rvision)\nlibrary(dplyr)"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#minimal-setup---working-with-a-photo",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#minimal-setup---working-with-a-photo",
    "title": "Rvision: A first look",
    "section": "Minimal Setup - working with a photo",
    "text": "Minimal Setup - working with a photo\nWe will start by simply loading a picture of a parrot. This is done using the function image, which creates an object of class Image. Image objects are pointers toward C++ objects stored in memory and will therefore not work with some functions in base R such sum, %%, etc.\nimg &lt;- image(\"parrot.jpg\")\nIf we want to see the image we loaded we simply plot it:\nplot(img)\n\nFor more information about the Image object we can turn to the property functions:\ndim(img)\n## [1] 1595 1919    3\nnrow(img)\n## [1] 1595\nncol(img)\n## [1] 1919\nnchan(img)\n## [1] 3\nbitdepth(img)\n## [1] \"8U\"\ncolorspace(img)\n## [1] \"BGR\""
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#blurs",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#blurs",
    "title": "Rvision: A first look",
    "section": "Blurs",
    "text": "Blurs\nNow that we have an Image object we can use some of tools at our disposal, which includes standard things like blurs:\nboxFilter(img, k_height = 25, k_width = 25) %&gt;% plot()\n\ngaussianBlur(img, k_height = 25, k_width = 25, sigma_x = 5, sigma_y = 5) %&gt;% plot()\n\nmedianBlur(img, k_size = 25) %&gt;% plot()\n\nsqrBoxFilter(img, k_height = 25, k_width = 25) %&gt;% plot()"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#operators",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#operators",
    "title": "Rvision: A first look",
    "section": "Operators",
    "text": "Operators\nOther kinds of operations can be done, such as changing the color space:\nchangeColorSpace(img, \"GRAY\") %&gt;% plot()\n\nAnd apply edge detection algorithms such as sobel and laplacian.\nsobel(img) %&gt;% plot()"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#draws",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#draws",
    "title": "Rvision: A first look",
    "section": "Draws",
    "text": "Draws\nThe package also includes a number of drawing functions starting with the prefix draw, ending with Arrow, Circle, Ellipse, Line, Rectangle and text. These functions, unlike the others, modify the Image object that is taken in, instead of returning another Image object.\nimg1 &lt;- cloneImage(img)\ndrawCircle(img1, x = 750, y = 750, radius = 200, color = \"blue\", \n           thickness = 10)\nplot(img1)"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#blob-detection",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#blob-detection",
    "title": "Rvision: A first look",
    "section": "Blob detection",
    "text": "Blob detection\nBy now we looked at a bunch of different functions but all of them have been used separately. Now let’s combine them to detect something inside the picture.\nimg &lt;- image(\"balls.jpg\")\nplot(img)\n\nFor our further calculations, we need to know what color space this image is in\ncolorspace(img)\n## [1] \"BGR\"\nWhich is different than the correctly commonly used RGB. In the following code, I tried to find all the blue balls. For that, I used the split function to split the Image object into 3, one for each color channel. Then I used a do.call to return an object where the blue channel is more the 200, and the red and green are less than 200, in the hope that it would be enough to identify the blue color without also finding bright areas. This being a logical expression gives us an image file that is white when true and black when it isn’t. Lastly, we used the medianBlur to remove any rough edges and flicker. (you can try comment out the medianBlur and see what changes)\nimg %&gt;%\n  split() %&gt;%\n  do.call(function(B, G, R) B &gt; 200 & G &lt; 200 & R &lt; 200, .) %&gt;%\n  medianBlur() %&gt;%\n  plot()\nIf we would like to highlight these balls on the original image we have to detect where these white blobs are and use the draw functions to draw on our original image. We use simpleBlobDetector and play around with the settings till we get something reasonable.\nblue_balls &lt;- img %&gt;%\n  split() %&gt;%\n  do.call(function(B, G, R) B &gt; 200 & G &lt; 200 & R &lt; 200, .) %&gt;%\n  medianBlur() %&gt;%\n  simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255,\n                     filter_by_convexity = FALSE, \n                     filter_by_inertia = FALSE, min_threshold = 0)\nblue_balls\nWe use the cloneImage as it creates a new Image object such that the drawing doesn’t change the original Image object.\nimg1 &lt;- cloneImage(img)\n\nfor (i in seq_len(nrow(blue_balls))) {\n  drawRectangle(image = img1,\n                pt1_x = blue_balls$x[i] - 1 + blue_balls$size[i] / 2, \n                pt1_y = blue_balls$y[i] - 1 + blue_balls$size[i] / 2, \n                pt2_x = blue_balls$x[i] - 1 - blue_balls$size[i] / 2, \n                pt2_y = blue_balls$y[i] - 1 - blue_balls$size[i] / 2, \n                thickness = 3, color = \"blue\")\n}\n\nplot(img)\nWe see that it worked fairly well, it didn’t go all the way till the edges of the balls and it appeared to catch the blue artifact on the lower left side, but more careful ranges could take care of that problem."
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#streams",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#streams",
    "title": "Rvision: A first look",
    "section": "Streams",
    "text": "Streams\nRvision also has a Stream object that we can utilize. the stream function creates a Stream object from the camera connected to your computer. In my case, number 0 is the webcam in my Macbook. Its corresponding function is release which closes the stream object. To capture something we use the handy readNext function that reads the next frame and returns an Image object.\nmy_stream &lt;- stream(0)   # 0 will start your default webcam in general. \nmy_img &lt;- readNext(my_stream)\nrelease(my_stream)\nLet us take a look at the image that was captured on my webcam.\nplot(my_img)\n\nand what a coincidence!! Its a handful of distinctly colored m&m’s against a dark background. Lets try against to locate the different colors, but before we do that let us reuse what we did earlier and make it into some custom functions:\nblob_fun &lt;- function(img, fun, color = character()) {\n  img %&gt;%\n    split() %&gt;%\n    do.call(fun, .) %&gt;%\n    medianBlur(15) %&gt;%\n    simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255,\n                       filter_by_convexity = FALSE, \n                       filter_by_inertia = FALSE, min_threshold = 0) %&gt;%\n    mutate(color = color)\n} \n\nmulti_draw &lt;- function(img, blobs) {\n  if (nrow(blobs) &gt; 0) {\n    for (i in 1:nrow(blobs)) {\n      drawRectangle(img, \n                    blobs$x[i] - 1 + blobs$size[i], \n                    blobs$y[i] - 1 + blobs$size[i],\n                    blobs$x[i] - 1 - blobs$size[i], \n                    blobs$y[i] - 1 - blobs$size[i], \n                    thickness = 5, color = blobs$color[1])\n    }\n  }\n}\nLike before we found the blue balls by identifying the region in the BGR color space where it is blue, we expand the same idea to the other colors. (I have not attempted brown as it is fairly similar in color to the table)\nblue &lt;-   function(B, G, R) B &gt; 150 & R &lt; 200 & G &lt; 200\nred &lt;-    function(B, G, R) R &gt; 150 & B &lt; 200 & G &lt; 150\ngreen &lt;-  function(B, G, R) G &gt; 150 & B &lt; 200 & R &lt; 200\nyellow &lt;- function(B, G, R) G &gt; 150 & B &lt; 200 & B &gt; 150 & R &gt; 150\norange &lt;- function(B, G, R) G &gt; 150 & B &lt; 150 & R &gt; 150\nNow we just have to run our custom blob detection function and custom drawing function for each color and see the final result\nblue_mms &lt;-   blob_fun(my_img, blue, \"blue\")\nred_mms &lt;-    blob_fun(my_img, red, \"red\")\ngreen_mms &lt;-  blob_fun(my_img, green, \"green\")\nyellow_mms &lt;- blob_fun(my_img, yellow, \"yellow\")\norange_mms &lt;- blob_fun(my_img, orange, \"orange\")\n\nmulti_draw(my_img, blue_mms)\nmulti_draw(my_img, red_mms)\nmulti_draw(my_img, green_mms)\nmulti_draw(my_img, yellow_mms)\nmulti_draw(my_img, orange_mms)\n\nplot(my_img)\n\nAnd it is wonderful!"
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#displays",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#displays",
    "title": "Rvision: A first look",
    "section": "Displays",
    "text": "Displays\nThe last trip of the tour is a look at the Displays that Rvision facilitates. And in its simplest form, it creates a window where Image objects can be displayed. This means that we are able to do live m&m’s detection!!\nIn a minimal setup, you would have this following chunk of code, which sets up a stream, a display, and then populates that display with new images taken from the camera till you stop it. And then termination functions for the stream and display. However, this is no different than a video feed.\nmy_stream &lt;- stream(0)\nnewDisplay(\"Live test\", 360, 640)\nwhile(TRUE) {\n  img &lt;- readNext(my_stream)\n  display(img, \"Live test\", 25, 360, 640)\n}\ndestroyDisplay(\"Live test\")\nrelease(my_stream)\nSo instead we will use the functions from earlier to detect and highlight the colored m&m’s!\nmy_stream &lt;- stream(0)\nnewDisplay(\"Live test\", 360, 640)\n\nwhile(TRUE) {\n  img &lt;- readNext(my_stream)\n  \n  blue_mms &lt;- blob_fun(img, blue, \"blue\")\n  red_mms &lt;- blob_fun(img, red, \"red\")\n  green_mms &lt;- blob_fun(img, green, \"green\")\n  yellow_mms &lt;- blob_fun(img, yellow, \"yellow\")\n  orange_mms &lt;- blob_fun(img, orange, \"orange\")\n  \n  multi_draw(img, blue_mms)\n  multi_draw(img, red_mms)\n  multi_draw(img, green_mms)\n  multi_draw(img, yellow_mms)\n  multi_draw(img, orange_mms)\n  \n  display(img, \"Live test\", 25, 360, 640)\n}\ndestroyDisplay(\"Live test\")\nrelease(my_stream)\n\nIt’s a little choppy but that might be because of my now quite old Macbook."
  },
  {
    "objectID": "post/2018-02-14-rvision-a-first-look/index.html#conclusion",
    "href": "post/2018-02-14-rvision-a-first-look/index.html#conclusion",
    "title": "Rvision: A first look",
    "section": "Conclusion",
    "text": "Conclusion\nI had a blast working with Rvision and I look forward to using it in future projects! I would also recommend against using eatable data points as they tend to disappear over time.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                            \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.0)                    \n blogdown      1.3     2021-04-14 [1] CRAN (R 4.0.2)                    \n bookdown      0.22    2021-04-22 [1] CRAN (R 4.0.2)                    \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)                    \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.0.2)                    \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.0.2)                    \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.0.5)                    \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.0.2)                    \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.0.2)                    \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.0.2)                    \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)                    \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.0.2)                    \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)                    \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.0.2)                    \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.0)                    \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.0.2)                    \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.0.2)                    \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.0.2)                    \n highr         0.9     2021-04-16 [1] CRAN (R 4.0.2)                    \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)                    \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)                    \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.0.2)                    \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.0.2)                    \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.0.2)                    \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.0.2)                    \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.0.2)                    \n pbapply       1.4-3   2020-08-18 [1] CRAN (R 4.0.2)                    \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.0.2)                    \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.0)                    \n png           0.1-7   2013-12-03 [1] CRAN (R 4.0.0)                    \n purrr         0.3.4   2020-04-17 [1] CRAN (R 4.0.0)                    \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.0.2)                    \n Rcpp          1.0.6   2021-01-15 [1] CRAN (R 4.0.2)                    \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.0.2)                    \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.0.2)                    \n ROpenCVLite   4.52.0  2021-07-05 [1] CRAN (R 4.0.5)                    \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.2)                    \n Rvision     * 0.6.0   2021-07-05 [1] Github (swarm-lab/Rvision@0377e45)\n sass          0.4.0   2021-05-12 [1] CRAN (R 4.0.2)                    \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.0)                    \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.0.2)                    \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.0.0)                    \n tibble        3.1.2   2021-05-16 [1] CRAN (R 4.0.2)                    \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.0.2)                    \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.0.2)                    \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.0.2)                    \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.0.2)                    \n xfun          0.24    2021-06-15 [1] CRAN (R 4.0.2)                    \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.0)                    \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2018-08-14-what-are-the-reviews-telling-us/index.html",
    "href": "post/2018-08-14-what-are-the-reviews-telling-us/index.html",
    "title": "What are the reviews telling us?",
    "section": "",
    "text": "Warning\n\n\n\nThe data set for this blog post got lost and the code no longer runs.\nIn this post, we will look at a handful of English1 movies reviews from imdb which I have scraped and placed in this repository movie reviews. I took a look at the best and worst rated movies with their best and worst reviews respectively. From that, we will try to see if we can see how positive reviews on good movies are different than positive reviews on bad movies and so on.\nWe will use fairly standard packages with the inclusion of paletteer for the sole reason of self-promotion. (yay!!!)\nwe will read in the data using readr\nLet’s take a look at the data I prepared for us:\nIt includes 7 different variables. There is some redundancy, the url variable contains the URL of the movie, and id and title are just the extracts from the url variable. The rating variable is the average rating of the movie and will not be used in this analysis. Lastly, we have the review_rating and movie_rating which will denote if the review is positive or negative and if the movie being reviewed is good or bad respectively.\nLet’s start by unnesting the words and get the counts. We also don’t want to look at all the stopwords and words that contain numbers, this is likely not a great number of words but we will exclude them for now anyway.\nAnd lets have a quick looks at the result:\nAnd we notice that the word movie has been used quite a lot more in reviews of bad movies than in good movies."
  },
  {
    "objectID": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#log-odds",
    "href": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#log-odds",
    "title": "What are the reviews telling us?",
    "section": "Log odds",
    "text": "Log odds\nWe have a bunch of counts here and we would like to find a worthwhile transformation of them. Since we have the number of reviews for good movies and bad movies we would be able to find the percentage of words appearing in good movies. This would give us a number between 0 and 1, where the interesting words would be when the percentage is close to 0 and 1 as it would show that the word is being used more in one than another.\nBy doing this transformation to both the review scores and movie scores will give us the following plot:\ncounted_words %&gt;%\n  mutate(rating = str_c(movie_rating, \"_\", review_rating)) %&gt;%\n  select(-movie_rating, -review_rating) %&gt;%\n  spread(rating, n) %&gt;%\n  drop_na() %&gt;%\n  mutate(review_lo = (bad_good + good_good) / (bad_bad + good_bad + bad_good + good_good),\n         movie_lo = (good_bad + good_good) / (bad_bad + bad_good + good_bad + good_good)) %&gt;%\n  ggplot() +\n  aes(movie_lo, review_lo) +\n  geom_text(aes(label = word))\nAnother way to do this is to take the log of the odds of one event happening over the other event. We will create this little helper function for us.\nlog_odds &lt;- function(x, y) {\n  total &lt;- x + y\n  p &lt;- x / total\n  log(p / (1 - p))\n}\napplying this transformation instead expands the limit from 0 to 1 to the whole number range where the midpoint is 0, this has some nice properties from a visualization perspective, it will also compact the center points a little more allowing outliers to be more prominent.\nplot_data &lt;- counted_words %&gt;%\n  mutate(rating = str_c(movie_rating, \"_\", review_rating)) %&gt;%\n  select(-movie_rating, -review_rating) %&gt;%\n  spread(rating, n) %&gt;%\n  drop_na() %&gt;%\n  mutate(review_lo = log_odds(bad_good + good_good, bad_bad + good_bad),\n         movie_lo = log_odds(good_bad + good_good, bad_bad + bad_good))\nplot_data %&gt;%\n  ggplot() +\n  aes(movie_lo, review_lo, label = word) +\n  geom_text()\nWe have a good degree of overplotting in this plot, but part of that might be because of the text, a quick look at the scatterplot still reveals a good deal of overplotting. We will try to counter that later on.\nplot_data %&gt;%\n  ggplot() +\n  aes(movie_lo, review_lo) +\n  geom_point(alpha = 0.5)\nLet us stay in the scatterplot. Lets tighten up the theme and include guidelines at y = 0 and x = 0. We will also find the range of the data to make sure we include all the points.\nplot_data %&gt;% \n  select(movie_lo, review_lo) %&gt;%\n  range()\nplot_data %&gt;%\n  ggplot() +\n  aes(movie_lo, review_lo) +\n  geom_vline(xintercept = 0, color = \"grey\") +\n  geom_hline(yintercept = 0, color = \"grey\") +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(-4.6, 4.6),\n                  xlim = c(-4.6, 4.6)) +\n  labs(x = \"← Bad Movies - Good Movies →\", y = \"← Bad Reviews - Good Reviews →\")\nWe still have quite a bit of overplotting, I’m going to sample the points based on importance. The importance matrix I’m going to work with is the distance from the middle. In addition, we are going to display the number of times a word is used by the size of the points.\nset.seed(13)\nplot_data_v2 &lt;- plot_data %&gt;%\n  mutate(distance = review_lo ^ 2 + movie_lo ^ 2,\n         n = bad_bad + bad_good + good_bad + good_good) %&gt;%\n  sample_frac(0.1, weight = distance)\n\nplot_data_v2 %&gt;%  \n  ggplot() +\n  aes(movie_lo, review_lo, size = n) +\n  geom_vline(xintercept = 0, color = \"grey\") +\n  geom_hline(yintercept = 0, color = \"grey\") +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(-4.6, 4.6),\n                  xlim = c(-4.6, 4.6)) +\n  labs(x = \"← Bad Movies - Good Movies →\", y = \"← Bad Reviews - Good Reviews →\")\nLastly, we will make the whole thing interactive with plotly to allow hover text. We include some colors to indicate the distance to the center.\np &lt;- plot_data_v2 %&gt;%  \n  ggplot() +\n  aes(movie_lo, review_lo, size = n, color = distance, text = word) +\n  geom_vline(xintercept = 0, color = \"grey\") +\n  geom_hline(yintercept = 0, color = \"grey\") +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  coord_cartesian(ylim = c(-4.6, 4.6),\n                  xlim = c(-4.6, 4.6)) +\n  labs(x = \"← Bad Movies - Good Movies →\", \n       y = \"← Bad Reviews - Good Reviews →\",\n       title = \"What are people saying about the best and worst movies on IMDB?\") +\n  scale_color_paletteer_c(\"viridis::viridis\") +\n  guides(color = \"none\", size = \"none\")\n\nggplotly(p, width = 700, height = 700, displayModeBar = FALSE,\n         tooltip = \"text\") %&gt;% \n  config(displayModeBar = F)\nAnd we are done and it looks amazing! With this dataviz, we can see that the word overrated is mainly used in negative reviews about good movies. Likewise unfunny is used in bad reviews about bad movies. There are many more examples that I’ll let you explore by yourself.\nThanks for tagging along!"
  },
  {
    "objectID": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#references",
    "href": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#references",
    "title": "What are the reviews telling us?",
    "section": "References",
    "text": "References\n\nThe Words Men and Women Use When They Write About Love"
  },
  {
    "objectID": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#footnotes",
    "href": "post/2018-08-14-what-are-the-reviews-telling-us/index.html#footnotes",
    "title": "What are the reviews telling us?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "",
    "text": "This code has been lightly revised to make sure it works as of 2018-12-16.\nIn this post I’ll go through how I created the data visualization I posted yesterday on Twitter:"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#what-am-i-looking-at",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#what-am-i-looking-at",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "What am I looking at?",
    "text": "What am I looking at?\nSo for this particular data-viz I took the novel Emma by Jane Austen, extracted all the trigrams (sentences of length 3), took the 150 most frequent ones, and visualized those.\nThis visualization is a layered horizontal tree graph where the 3 levels (vertical columns of words) correspond to words that appear at the nth place in the trigrams, e.g. first column has the first words of the trigram, the second column has middle words of trigrams, etc. Up to 20 words in each column are kept and they are ordered and sized according to occurrence in the data.\nThe curves represent how often two words co-occur, with the color representing starting word and transparency related to frequency.\nAll code is presented in the following gist."
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#packages-and-parameters",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#packages-and-parameters",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Packages and parameters",
    "text": "Packages and parameters\nWe will be using the following packages:\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(purrrlyr)\nAnd the overall parameters outlined in description are defined here:\nn_word &lt;- 20\nn_top &lt;- 150\nn_gramming &lt;- 3"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#trigrams",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#trigrams",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Trigrams",
    "text": "Trigrams\nIf you have read Text Mining with R I’m sure you have encountered the janeaustenr package. We will use the Emma novel, and tidytext’s unnest_tokens to calculate the trigrams we need. We also specify the starting words.\ntrigrams &lt;- tibble(text = janeaustenr::emma) %&gt;%\n  unnest_tokens(trigram, text, token = \"ngrams\", n = n_gramming)\n\nstart_words &lt;- c(\"he\", \"she\")\nnext, we find the top 150 trigrams using count and some regex magic. And we use those top words to filter such that we only will be looking at the top 150.\npattern &lt;- str_c(\"^\", start_words, \" \", collapse = \"|\")\ntop_words &lt;- trigrams %&gt;%\n  filter(str_detect(trigram, pattern)) %&gt;%\n  count(trigram, sort = TRUE) %&gt;%\n  slice(seq_len(n_top)) %&gt;%\n  pull(trigram)\n\ntrigrams &lt;- trigrams %&gt;%\n  filter(trigram %in% top_words)"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#nodes",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#nodes",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Nodes",
    "text": "Nodes\nSince we know that each trigram has a sample format, we can create a simple function to extract the nth word in a string.\nstr_nth_word &lt;- function(x, n, sep = \" \") {\n  str_split(x, pattern = \" \") %&gt;%\n  map_chr(~ .x[n])\n}\nThe following purrr::map_df\n\nExtracts the nth word in the trigram\n\nCounts and sorts the occurrences\n\nGrabs the top 20 words\nEqually space them along the y-axis\n\nnodes &lt;- map_df(seq_len(n_gramming),\n       ~ trigrams %&gt;%\n           mutate(word = str_nth_word(trigram, .x)) %&gt;%\n           count(word, sort = TRUE) %&gt;%\n           slice(seq_len(n_word)) %&gt;% \n           mutate(y = seq(from = n_word + 1, to = 0, \n                          length.out = n() + 2)[seq_len(n()) + 1],\n                  x = .x))\n\nplot of node positions\nLets see the words so far:\nnodes %&gt;% \n  ggplot(aes(x, y, label = word)) +\n  geom_text()"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#edges",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#edges",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Edges",
    "text": "Edges\nWhen we look at the final visualization we see that the words are connected by curved lines. I achieved that by using a sigmoid curve and then transform it to match the starting and endpoints.\nsigmoid &lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) {\n  x &lt;- seq(-scale, scale, length = n)\n  y &lt;- exp(x) / (exp(x) + 1)\n  tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from,\n         y = y * (y_to - y_from) + y_from)\n}\nThe following function takes\n\na list of trigrams\na data.frame of “from” nodes\na data.frame of “to” nodes\n\nand returns a data.frame containing the data points for the curves we need to draw with correct starting and ending points.\negde_lines &lt;- function(trigram, from_word, to_word, scale = 5, n = 50, \n                       x_space = 0) {\n\n  from_word &lt;- from_word %&gt;%\n    select(-n) %&gt;%\n    set_names(c(\"from\", \"y_from\", \"x_from\"))\n  \n  to_word &lt;- to_word %&gt;%\n    select(-n) %&gt;%\n    set_names(c(\"to\", \"y_to\", \"x_to\"))\n  \n  links &lt;- crossing(from = from_word$from, \n                    to = to_word$to) %&gt;%\n    mutate(word_pair = paste(from, to),\n           number = map_dbl(word_pair, \n                            ~ sum(str_detect(trigram$trigram, .x)))) %&gt;%\n    left_join(from_word, by = \"from\") %&gt;%\n    left_join(to_word, by = \"to\")\n  \n  links %&gt;%\n    by_row(~ sigmoid(x_from = .x$x_from + 0.2 + x_space,\n                     x_to = .x$x_to - 0.05, \n                     y_from = .x$y_from, y_to = .x$y_to, \n                     scale = scale, n = n) %&gt;%\n    mutate(word_pair = .x$word_pair,\n           number = .x$number,\n           from = .x$from)) %&gt;%\n    pull(.out) %&gt;%\n    bind_rows()\n}\n\nplot of the first set of edges\nLet’s take a look at the first set of edges to see if it is working.\negde_lines(trigram = trigrams, \n           from_word = filter(nodes, x == 1), \n           to_word = filter(nodes, x == 2)) %&gt;%\n  filter(number &gt; 0) %&gt;%\n  ggplot(aes(x, y, group = word_pair, alpha = number, color = from)) +\n  geom_line()\n\n\n\nCalculating all egdes\nFor ease (and laziness) I have desired to calculate the edges in sections\n\nedges between the first and second column\nedges between the second and third column for words that start with “he”\nedges between the second and third columns for words that start with “she”\n\nand combine by the end.\n# egdes between first and second column\negde1 &lt;- egde_lines(trigram = trigrams, \n           from_word = filter(nodes, x == 1), \n           to_word = filter(nodes, x == 2), \n           n = 50) %&gt;%\n           filter(number &gt; 0) %&gt;%\n  mutate(id = word_pair)\n\n# Words in second colunm\n## That start with he\nsecond_word_he &lt;- nodes %&gt;%\n  filter(x == 2) %&gt;%\n  select(-n) %&gt;%\n  left_join(\n    trigrams %&gt;% \n      filter(str_nth_word(trigram, 1) == start_words[1]) %&gt;%\n      mutate(word = str_nth_word(trigram, 2)) %&gt;%\n      count(word), \n    by = \"word\"\n  ) %&gt;%\n  replace_na(list(n = 0))\n\n## That start with she\nsecond_word_she &lt;- nodes %&gt;%\n  filter(x == 2) %&gt;%\n  select(-n) %&gt;%\n  left_join(\n    trigrams %&gt;% \n      filter(str_nth_word(trigram, 1) == start_words[2]) %&gt;%\n      mutate(word = str_nth_word(trigram, 2)) %&gt;%\n      count(word), \n    by = \"word\"\n  ) %&gt;%\n  replace_na(list(n = 0))\n\n# Words in third colunm\n## That start with he\nthird_word_he &lt;- nodes %&gt;%\n  filter(x == 3) %&gt;%\n  select(-n) %&gt;%\n  left_join(\n    trigrams %&gt;% \n      filter(str_nth_word(trigram, 1) == start_words[1]) %&gt;%\n      mutate(word = str_nth_word(trigram, 3)) %&gt;%\n      count(word), \n    by = \"word\"\n  ) %&gt;%\n  replace_na(list(n = 0))\n\n## That start with she\nthird_word_she &lt;- nodes %&gt;%\n  filter(x == 3) %&gt;%\n  select(-n) %&gt;%\n  left_join(\n    trigrams %&gt;% \n      filter(str_nth_word(trigram, 1) == start_words[2]) %&gt;%\n      mutate(word = str_nth_word(trigram, 3)) %&gt;%\n      count(word), \n    by = \"word\"\n  ) %&gt;%\n  replace_na(list(n = 0))\n\n# egdes between second and third column that starts with he\negde2_he &lt;- egde_lines(filter(trigrams, \n                              str_detect(trigram, paste0(\"^\", start_words[1], \" \"))), \n             second_word_he, third_word_he, n = 50) %&gt;%\n  mutate(y = y + 0.05,\n         from = start_words[1],\n         id = str_c(from, word_pair, sep = \" \")) %&gt;%\n  filter(number &gt; 0)\n\n# egdes between second and third column that starts with she\negde2_she &lt;- egde_lines(filter(trigrams, \n                              str_detect(trigram, paste0(\"^\", start_words[2], \" \"))), \n             second_word_she, third_word_she, n = 50) %&gt;%\n  mutate(y = y - 0.05,\n         from = start_words[2],\n         id = str_c(from, word_pair, sep = \" \")) %&gt;%\n  filter(number &gt; 0)\n\n# All edges\nedges &lt;- bind_rows(egde1, egde2_he, egde2_she)"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#vizualisation",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#vizualisation",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "vizualisation",
    "text": "vizualisation\nNow we just add it all together. All labels, change colors, adjust xlim to fit words on the page.\np &lt;- nodes %&gt;% \n  ggplot(aes(x, y, label = word, size = n)) +\n  geom_text(hjust = 0, color = \"#DDDDDD\") +\n  theme_void() +\n  geom_line(data = edges,\n            aes(x, y, group = id, color = from, alpha = sqrt(number)),\n            inherit.aes = FALSE) +\n  theme(plot.background = element_rect(fill = \"#666666\", colour = 'black'),\n        text = element_text(color = \"#EEEEEE\", size = 15)) +\n  guides(alpha = \"none\", color = \"none\", size = \"none\") +\n  xlim(c(0.9, 3.2)) +\n  scale_color_manual(values = c(\"#5EF1F1\", \"#FA62D0\")) +\n  labs(title = \" Vizualizing trigrams in Jane Austen's, Emma\") + \n  scale_size(range = c(3, 8))\np"
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#notes",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#notes",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Notes",
    "text": "Notes\nThere are a couple of differences between the Viz I posted online yesterday and the result here in this post due to a couple of mistakes found in the code during cleanup."
  },
  {
    "objectID": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#extra-vizualisations",
    "href": "post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index.html#extra-vizualisations",
    "title": "Visualizing trigrams with the Tidyverse",
    "section": "Extra vizualisations",
    "text": "Extra vizualisations\nn_word &lt;- 20\nn_top &lt;- 150\nn_gramming &lt;- 3\n\ntrigrams &lt;- tibble(text = janeaustenr::emma) %&gt;%\n  unnest_tokens(trigram, text, token = \"ngrams\", n = n_gramming)\n\nstart_words &lt;- c(\"i\", \"you\")\n\nn_word &lt;- 20\nn_top &lt;- 150\nn_gramming &lt;- 3\n\nlibrary(rvest)\nsherlock_holmes &lt;- read_html(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\") %&gt;%\n  html_text() %&gt;% \n  str_split(\"\\n\") %&gt;%\n  unlist()\n\ntrigrams &lt;- tibble(text = sherlock_holmes) %&gt;%\n  unnest_tokens(trigram, text, token = \"ngrams\", n = n_gramming)\n\nstart_words &lt;- c(\"holmes\", \"watson\")\n\n\n\n session information \n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source        \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.0)\n backports     1.2.1   2020-12-09 [1] CRAN (R 4.0.2)\n blogdown      1.3     2021-04-14 [1] CRAN (R 4.0.2)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.0.2)\n broom         0.7.6   2021-04-05 [1] CRAN (R 4.0.2)\n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)\n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.0.0)\n cli           3.0.0   2021-06-30 [1] CRAN (R 4.0.2)\n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.0.2)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.0.5)\n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.0.2)\n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.0.2)\n curl          4.3.2   2021-06-23 [1] CRAN (R 4.0.2)\n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.0.2)\n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.0.2)\n desc          1.3.0   2021-03-05 [1] CRAN (R 4.0.2)\n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)\n digest        0.6.27  2020-10-24 [1] CRAN (R 4.0.2)\n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.0.2)\n evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.0)\n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.0.2)\n farver        2.1.0   2021-02-28 [1] CRAN (R 4.0.2)\n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.0.2)\n fs            1.5.0   2020-07-31 [1] CRAN (R 4.0.2)\n generics      0.1.0   2020-10-31 [1] CRAN (R 4.0.2)\n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.0.2)\n glue          1.4.2   2020-08-27 [1] CRAN (R 4.0.2)\n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.0.0)\n haven         2.4.1   2021-04-23 [1] CRAN (R 4.0.2)\n highr         0.9     2021-04-16 [1] CRAN (R 4.0.2)\n hms           1.1.0   2021-05-17 [1] CRAN (R 4.0.2)\n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)\n httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)\n janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 4.0.0)\n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.0.2)\n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.0.2)\n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.0.2)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.0.2)\n lattice       0.20-41 2020-04-02 [1] CRAN (R 4.0.5)\n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.0.2)\n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.0.2)\n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.0.2)\n Matrix        1.3-2   2021-01-06 [1] CRAN (R 4.0.5)\n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.0.0)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.0.0)\n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.0.2)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.0)\n png           0.1-7   2013-12-03 [1] CRAN (R 4.0.0)\n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)\n purrrlyr    * 0.0.7   2020-12-16 [1] CRAN (R 4.0.2)\n R6            2.5.0   2020-10-28 [1] CRAN (R 4.0.2)\n Rcpp          1.0.6   2021-01-15 [1] CRAN (R 4.0.2)\n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2)\n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.0.2)\n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.0.2)\n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.0.2)\n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.0.2)\n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.2)\n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.0.2)\n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.0.2)\n sass          0.4.0   2021-05-12 [1] CRAN (R 4.0.2)\n scales        1.1.1   2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.0)\n SnowballC     0.7.0   2020-04-01 [1] CRAN (R 4.0.0)\n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.0.2)\n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.0.0)\n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.0.2)\n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.0.2)\n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.0.2)\n tidytext    * 0.3.1   2021-04-10 [1] CRAN (R 4.0.2)\n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.0.2)\n tokenizers    0.2.1   2018-03-29 [1] CRAN (R 4.0.0)\n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.0.2)\n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.0.2)\n withr         2.4.2   2021-04-18 [1] CRAN (R 4.0.2)\n xfun          0.23    2021-05-15 [1] CRAN (R 4.0.2)\n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.0)\n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.0)\n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html",
    "href": "post/textrecipes-series-featurehashing/index.html",
    "title": "Textrecipes series: Feature Hashing",
    "section": "",
    "text": "This is the fourth blog post in the textrecipes series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be showcasing how to perform feature hashing) (also known as the hashing trick)."
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html#packages",
    "href": "post/textrecipes-series-featurehashing/index.html#packages",
    "title": "Textrecipes series: Feature Hashing",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe will be using tidymodels for modeling, tidyverse for EDA, and textrecipes for text preprocessing.\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(textrecipes)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html#exploring-the-data",
    "href": "post/textrecipes-series-featurehashing/index.html#exploring-the-data",
    "title": "Textrecipes series: Feature Hashing",
    "section": "Exploring the data ⛏",
    "text": "Exploring the data ⛏\nWe will use the some data from Kaggle containing English1 Women’s E-Commerce Clothing Reviews.\nreviews &lt;- read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n## Warning: Missing column names filled in: 'X1' [1]\nWe start with a quick glimpse() of the data.\nglimpse(reviews)\n## Rows: 23,486\n## Columns: 11\n## $ X1                        &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n## $ `Clothing ID`             &lt;dbl&gt; 767, 1080, 1077, 1049, 847, 1080, 858, 858, …\n## $ Age                       &lt;dbl&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, 34, 53, …\n## $ Title                     &lt;chr&gt; NA, NA, \"Some major design flaws\", \"My favor…\n## $ `Review Text`             &lt;chr&gt; \"Absolutely wonderful - silky and sexy and c…\n## $ Rating                    &lt;dbl&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, 5, 5, 3,…\n## $ `Recommended IND`         &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,…\n## $ `Positive Feedback Count` &lt;dbl&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2, 2, 0, 1…\n## $ `Division Name`           &lt;chr&gt; \"Initmates\", \"General\", \"General\", \"General …\n## $ `Department Name`         &lt;chr&gt; \"Intimate\", \"Dresses\", \"Dresses\", \"Bottoms\",…\n## $ `Class Name`              &lt;chr&gt; \"Intimates\", \"Dresses\", \"Dresses\", \"Pants\", …\nWe have a good split between text variables, numeric and categorical values. Let us also take a look at the distribution of the Rating variable\nreviews %&gt;%\n  ggplot(aes(Rating)) +\n  geom_bar()\n\nWhich is quite right-skewed. Let us collapse the ratings into 2 groups, 5 and less-then-5. Before we go on will I remove the row number variable X1 and clean the column names with the janitor package to remove cases and spaces.\nreviews &lt;- reviews %&gt;%\n  select(-X1) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(rating = factor(rating == 5, c(TRUE, FALSE), c(\"5\", \"&lt;5\")))\nBefore we do some exploratory analysis we will split the data into training and testing datasets. We do this to avoid learning this about the data that would only be available in the testing data set.\nset.seed(1234)\n\nreviews_split &lt;- initial_split(reviews)\n\nreviews_train &lt;- training(reviews_split)\nOur main objective is to predict the rating based on the text review. This will naturally exclude variables such as Recommended IND and Positive Feedback Count as that information is unlikely to be known before the rating is given. We will mostly be using the text variables (Review Text and Title) but I am going to take a look at some of the other variables before we move on in case they would be easy inclusions.\nThe ratings were pretty highly right-skewed and even when we collapsed them there are still more 5s.\nreviews_train %&gt;%\n  ggplot(aes(rating)) +\n  geom_bar()\n\nSince we have the age let us take a look to make sure it has a reasonable range.\nreviews_train %&gt;%\n  ggplot(aes(age)) +\n  geom_bar()\n\nNothing too out of the ordinary, we have some young people and old people but nothing weird. Out of curiosity let us take a look at that one age that is above the pack.\nreviews_train %&gt;%\n  count(age, sort = TRUE)\n## # A tibble: 77 x 2\n##      age     n\n##    &lt;dbl&gt; &lt;int&gt;\n##  1    39   932\n##  2    35   685\n##  3    36   640\n##  4    34   600\n##  5    38   583\n##  6    37   572\n##  7    33   552\n##  8    41   549\n##  9    46   543\n## 10    42   502\n## # … with 67 more rows\nSince we have the clothing id, then I want to know if any of the reviews apply to the same articles of clothing.\nreviews_train %&gt;%\n  count(clothing_id, sort = TRUE) \n## # A tibble: 1,068 x 2\n##    clothing_id     n\n##          &lt;dbl&gt; &lt;int&gt;\n##  1        1078   758\n##  2         862   603\n##  3        1094   580\n##  4         872   420\n##  5        1081   415\n##  6         829   409\n##  7        1110   347\n##  8         868   329\n##  9         895   309\n## 10         936   264\n## # … with 1,058 more rows\nSo out of 17614 we have 1068 clothing articles. Let us see how the reviews are split between the variables\nreviews_train %&gt;%\n  count(clothing_id, sort = TRUE) %&gt;%\n  mutate(x = row_number()) %&gt;%\n  ggplot(aes(x, n)) +\n  geom_point()\n\nAnd we see quite a fast drop-off.\nI’m trying to create a fairly simple model so I won’t be including much information."
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html#modeling",
    "href": "post/textrecipes-series-featurehashing/index.html#modeling",
    "title": "Textrecipes series: Feature Hashing",
    "section": "Modeling ⚙️",
    "text": "Modeling ⚙️\nWe will restrict ourselves to only use the two text fields and the age of the customer.\nWe tokenize the text fields and pass them to step_texthash() to perform feature hashing. The default number of variables created with step_texthash() is 1024 and is specified with the num_terms argument. We will leave this parameter to the default, but you can tune it like many other hyperparameters, see Textrecipes series: Term Frequency for an example of tuning a recipe parameter.\nrec_spec &lt;- recipe(rating ~ age + title + review_text, data = reviews_train) %&gt;%\n  step_tokenize(title, review_text) %&gt;%\n  step_tokenmerge(title, review_text, prefix = \"text\")\n\nrec_spec_fh &lt;- rec_spec %&gt;%\n  step_texthash(text)\nWe are using step_tokenmerge() to combine the tokens created in title and review_text into one list of tokens. There aren’t that many tokens in title alone for it to warrant treating it as a separate list of tokens.\nNext, we specify a lasso model.\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\nI have specified penalty = tune() because I want to use tune to find the best value of the penalty by doing hyperparameter tuning.\nWe set up a parameter grid using grid_regular()\nparam_grid &lt;- grid_regular(penalty(), levels = 50)\nsearching over 50 levels might seem like a lot. But remember that glmnet can calculate them all at once. This is because it relies on its warms starts for speed and it is often faster to fit a whole path than compute a single fit.\nWe will also set up some bootstraps of the data so we can evaluate the performance multiple times for each level.\nreviews_boot &lt;- bootstraps(reviews_train, times = 10)\nthe last thing we need to use is to create a workflow object to combine the preprocessing step with the model. This is important because we want the preprocessing steps to happen in the bootstraps.\nwf_fh &lt;- workflow() %&gt;%\n  add_recipe(rec_spec_fh) %&gt;%\n  add_model(lasso_spec)\nnow we are ready to perform the parameter tuning.\nset.seed(42)\nlasso_grid &lt;- tune_grid(\n  wf_fh,\n  resamples = reviews_boot,\n  grid = param_grid\n) \nOnce we have finished parameter tuning we can use the autoplot() function on the tuning results to get a nice chart showing the performance for different values of the penalty.\nlasso_grid %&gt;%\n  autoplot()\n\nand it appears that the best value for the penalty for this workflow is on the low end. Similarly, can we use the show_best() function from tune to show to the best performing hyperparameter.\nlasso_grid %&gt;%\n  show_best(\"roc_auc\")\n## # A tibble: 5 x 7\n##   penalty .metric .estimator  mean     n std_err .config              \n##     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n## 1 0.00356 roc_auc binary     0.840    10 0.00125 Preprocessor1_Model38\n## 2 0.00569 roc_auc binary     0.839    10 0.00129 Preprocessor1_Model39\n## 3 0.00222 roc_auc binary     0.837    10 0.00119 Preprocessor1_Model37\n## 4 0.00910 roc_auc binary     0.834    10 0.00147 Preprocessor1_Model40\n## 5 0.00139 roc_auc binary     0.833    10 0.00108 Preprocessor1_Model36\nWe will use the select_best() function to extract the best performing penalty and finalize the workflow with that value of penalty.\nwf_fh_final &lt;- wf_fh %&gt;%\n  finalize_workflow(parameters = select_best(lasso_grid, \"roc_auc\"))\nNow we can run last_fit() on our training/testing split to fit our final model.\nfinal_res &lt;- last_fit(wf_fh_final, reviews_split)\nWith our final model can we create a ROC curve of our final model.\nfinal_res %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(rating, .pred_5) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html#compare-feature-hashing-with-term-frequency",
    "href": "post/textrecipes-series-featurehashing/index.html#compare-feature-hashing-with-term-frequency",
    "title": "Textrecipes series: Feature Hashing",
    "section": "Compare Feature Hashing with Term frequency",
    "text": "Compare Feature Hashing with Term frequency\nOne of the benefits of using feature hashing is that it is only slightly worse than using a standard term frequency as we saw in the first post of the series. To demonstrate that idea will I fit a second workflow where we preprocess to term frequencies instead of feature hashing as the only chance. I will not be going over this chunk of code as it is very similar to what we have just seen.\nrec_spec_tf &lt;- rec_spec %&gt;%\n  step_tokenfilter(text, max_tokens = 1024) %&gt;%\n  step_tf(text)\n\nwf_tf &lt;- wf_fh %&gt;%\n  update_recipe(rec_spec_tf)\n\nset.seed(123456)\nlasso_grid_tf &lt;- tune_grid(\n  wf_tf,\n  resamples = reviews_boot,\n  grid = param_grid\n)\n\nwf_tf_final &lt;- wf_tf %&gt;%\n  finalize_workflow(parameters = select_best(lasso_grid_tf, \"roc_auc\"))\n\nfinal_res_tf &lt;- last_fit(wf_tf_final, reviews_split)\nNote how we can reuse parts of the original workflow by updating the recipe with update_recipe().\nNow that we have the two fitted models can we combine them and generate the ROC curve for both models together.\nbind_rows(\n  collect_predictions(final_res) %&gt;% mutate(model = \"Feature Hashing\"),\n  collect_predictions(final_res_tf) %&gt;%  mutate(model = \"Term Frequency\")\n) %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(rating, .pred_5) %&gt;%\n  autoplot()\n\nAnd we see that feature hashing is not that far behind Term frequency, despite its ease of use. Note that this ease of use comes with the downside that it is very difficult to access the model performance of individual tokens."
  },
  {
    "objectID": "post/textrecipes-series-featurehashing/index.html#footnotes",
    "href": "post/textrecipes-series-featurehashing/index.html#footnotes",
    "title": "Textrecipes series: Feature Hashing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html",
    "href": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html",
    "title": "ggplot2 trial and error - US trade data",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16.\nThis blog post will showcase an example of a workflow and its associated thought process when iterating through visualization styles working with ggplot2. For this reason, will this post include a lot of sub-par charts as you are seeing the steps, not just the final product.\nWe will use census data concerning US trade with other nations which we scrape as well."
  },
  {
    "objectID": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#setting-up",
    "href": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#setting-up",
    "title": "ggplot2 trial and error - US trade data",
    "section": "Setting up",
    "text": "Setting up\nWe will using a standard set of packages, tidyverse for general data manipulation, rvest and httr for scraping and manipulation.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)"
  },
  {
    "objectID": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#getting-the-data",
    "href": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#getting-the-data",
    "title": "ggplot2 trial and error - US trade data",
    "section": "Getting the data",
    "text": "Getting the data\nThis project started when I found the following link https://www.census.gov/foreign-trade/balance/c4099.html. It includes a month by month breakdown of U.S. trade in goods with Denmark from 1985 till the present. Unfortunately, the data is given in yearly tables, so we have a little bit of munching to do. First, we notice that the last part of the URL includes c4099, which after some googling reveals that 4099 is the country code for Denmark. The fill list of country trade codes is given on the following page https://www.census.gov/foreign-trade/schedules/c/countrycode.html which also include a .txt file so we don’t have to scrape. We will remove the first entry and last 6 since the US doesn’t trade with itself.\ncontinent_df &lt;- tibble(number = as.character(1:7),\n                       continent = c(\"North America\", \"Central America\", \n                                     \"South America\", \"Europe\", \"Asia\", \n                                     \"Australia and Oceania\", \"Africa\"))\n\ncode_df &lt;- read_lines(\"https://www.census.gov/foreign-trade/schedules/c/country.txt\",\n                      skip = 5) %&gt;%\n  tibble(code = .) %&gt;%\n  separate(code, c(\"code\", \"name\", \"iso\"), sep = \"\\\\|\") %&gt;%\n  mutate_all(trimws) %&gt;%\n  mutate(con_code = str_sub(code, 1, 1)) %&gt;%\n  filter(!is.na(iso), \n         name != \"United States of America\", \n         con_code != 9) %&gt;%\n  left_join(continent_df, by = c(\"con_code\" = \"number\")) %&gt;%\n  select(-con_code)\nWith these codes we create the targeted URLs we will be scraping\ntarget_urls &lt;- str_glue(\"https://www.census.gov/foreign-trade/balance/c{code_df$code}.html\")\nWe will be replication hrbrmstr’s scraping code found here since it works wonderfully.\ns_GET &lt;- safely(GET)\n\npb &lt;- progress_estimated(length(target_urls))\nmap(target_urls, ~{\n  pb$tick()$print()\n  Sys.sleep(5)\n  s_GET(.x)\n}) -&gt; httr_raw_responses\n\nwrite_rds(httr_raw_responses, \"data/2018-us-trade-raw-httr-responses.rds\")\n\ngood_responses &lt;- keep(httr_raw_responses, ~!is.null(.x$result))\nthen we wrangle all the HTML files by extracting all the tables, parse the numeric variables, and combining them into one table.\nwrangle &lt;- function(x, name) {\n  # Read html and extract tables\n  read_html(x[[1]]) %&gt;%\n  html_nodes(\"table\") %&gt;%\n  html_table() %&gt;%\n  # parse numeric columns\n  map(~ mutate_at(.x, vars(Exports:Balance), funs(parse_number))) %&gt;%\n  bind_rows() %&gt;%\n  mutate(Country = name)\n}\n\nfull_data &lt;- map2_df(good_responses, code_df$code, wrangle)\nLastly, we do some polishing up with the date variables and join in the country information.\ntrade_data &lt;- full_data %&gt;%\n  filter(!str_detect(Month, \"TOTAL\")) %&gt;%\n  mutate(Date = parse_date(Month, format = \"%B %Y\"), \n         Month = lubridate::month(Date),\n         Year = lubridate::year(Date)) %&gt;%\n  left_join(code_df, by = c(\"Country\" = \"code\"))\nGiving us this data to work with:\nglimpse(trade_data)\n## Rows: 75,379\n## Columns: 10\n## $ Month     &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, …\n## $ Exports   &lt;dbl&gt; 0.1, 0.4, 1.8, 0.2, 0.1, 1.3, 3.2, 0.6, 0.3, 0.6, 0.7, 1.4, …\n## $ Imports   &lt;dbl&gt; 1.0, 2.4, 2.2, 0.8, 0.2, 0.5, 0.8, 0.9, 0.4, 2.4, 0.5, 1.0, …\n## $ Balance   &lt;dbl&gt; -0.8, -2.0, -0.4, -0.6, -0.1, 0.7, 2.4, -0.3, -0.1, -1.8, 0.…\n## $ Country   &lt;dbl&gt; 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, …\n## $ Date      &lt;date&gt; 2018-01-01, 2018-02-01, 2018-03-01, 2018-04-01, 2017-01-01,…\n## $ Year      &lt;dbl&gt; 2018, 2018, 2018, 2018, 2017, 2017, 2017, 2017, 2017, 2017, …\n## $ name      &lt;chr&gt; \"Greenland\", \"Greenland\", \"Greenland\", \"Greenland\", \"Greenla…\n## $ iso       &lt;chr&gt; \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", \"GL\", …\n## $ continent &lt;chr&gt; \"North America\", \"North America\", \"North America\", \"North Am…"
  },
  {
    "objectID": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#lets-get-visualizing",
    "href": "post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index.html#lets-get-visualizing",
    "title": "ggplot2 trial and error - US trade data",
    "section": "Let’s get visualizing!",
    "text": "Let’s get visualizing!\nLet’s set a different theme for now.\ntheme_set(theme_minimal())\nLet’s start nice and simple by plotting a simple scatter plot for just a single country to get a feel for the data.\ntrade_data %&gt;% \n  filter(name == \"Greenland\") %&gt;%\n  ggplot(aes(Date, Balance)) +\n  geom_point() +\n  labs(title = \"United States Trade Balance in Goods with Greenland\")\n\nWhich looks good already! Let’s see how it would look with as a line chart instead\ntrade_data %&gt;% \n  filter(name == \"Greenland\") %&gt;%\n  ggplot(aes(Date, Balance)) +\n  geom_line() +\n  labs(title = \"United States Trade Balance in Goods with Greenland\")\n\nit sure is quite jagged! Let’s take a look at the 4 biggest spiked to see if it is an indication of a trend\ntrade_data %&gt;% \n  filter(name == \"Greenland\", Balance &gt; 5)\n## # A tibble: 4 x 10\n##   Month Exports Imports Balance Country Date        Year name   iso   continent \n##   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;     \n## 1     3     7.9     0.5     7.4    1010 2014-03-01  2014 Green… GL    North Ame…\n## 2     3    10.4     1       9.4    1010 2013-03-01  2013 Green… GL    North Ame…\n## 3     3    10.5     0.6     9.9    1010 2012-03-01  2012 Green… GL    North Ame…\n## 4     9    20       1.3    18.8    1010 2008-09-01  2008 Green… GL    North Ame…\nWhich didn’t give us much, 3 of the spikes happened in March, and the last one was a random September. It was worth a try, back to plotting! Let’s see how a smooth curve would look overlaid the line chart\ntrade_data %&gt;% \n  filter(name == \"Greenland\") %&gt;%\n  ggplot(aes(Date, Balance)) +\n  geom_line() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"United States Trade Balance in Goods with Greenland\")\n\nThis looks nice in and off itself, however, since this chart looks at the trade balance between two countries is the value 0 is quite important and should be highlighted better. I will add a line behind the data points such that it highlights rather than hides\ntrade_data %&gt;% \n  filter(name == \"Greenland\") %&gt;%\n  ggplot(aes(Date, Balance)) +\n  geom_abline(slope = 0, intercept = 0, color = \"orange\") +\n  geom_line() +\n  geom_smooth(se = FALSE) +\n  labs(title = \"United States Trade Balance in Goods with Greenland\")\n\nThis gives us a better indication of when the trade is positive or negative with respect to the United States. Let’s take it up a notch and include a couple more countries. We remove the filter and add Country as the group aesthetic.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line() +\n  labs(title = \"United States Trade Balance in Goods with all countries\")\n\nSo we have 3 different problems I would like to fix right now. The scale between these different countries is massively different! The very negative balance of one country is making it hard to see what happens to the other countries. Secondly, it is hard to distinguish the different countries since they are all the same color. And lastly, there is some serious overplotting, this point is tired to the other problems so let us see if we can fix them one at a time.\nFirst, let us transform the scales on the y axis such that we better can identify individual lines. We do this with the square root transformation which gives weights to values close to 0 and shrinks values far away from 0.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line() + \n  scale_y_sqrt() +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With square root transformation\")\n## Warning in self$trans$transform(x): NaNs produced\n## Warning: Transformation introduced infinite values in continuous y-axis\n## Warning: Removed 11918 row(s) containing missing values (geom_path).\n\nOh no! We lost all the negative values. This happened because the normal square root operation only works with positive numbers. We fix this by using the signed square root which applies the square root to both the positive and negative as if they were positive and then sign them accordingly. For this, we create a new transformation with the scales package.\nS_sqrt &lt;- function(x) sign(x) * sqrt(abs(x))\nIS_sqrt &lt;- function(x) x ^ 2 * sign(x) \nS_sqrt_trans &lt;- function() scales::trans_new(\"S_sqrt\", S_sqrt, IS_sqrt)\n\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line() + \n  coord_trans(y = \"S_sqrt\") +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nMuch better! We will fix the breaks a little bit too.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line() + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nNow let’s solve the problem with overplotting, a standard trick is to introduce transparency, this is done using the alpha aesthetic. Let us start with 0.5 alpha.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line(alpha = 0.5) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nslightly better but not good enough, lets try 0.2\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nmuch better! Another thing we could do is coloring depending on the continent.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country, color = continent)) +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nThis is quite messy, however, we notice that the data for the African countries don’t cover the same range as the other countries. Let’s see if there are some overall trends within each continent.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  geom_line(alpha = 0.2) + \n  geom_smooth(aes(Date, Balance, group = continent, color = continent), se = FALSE) +\n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation\")\n\nThis gives some more tangible information. There is an upwards trend within North America for the last 10 years, where Asia has had a slow decline since the beginning of the data collection.\nNext, let’s see what happens when you facet depending on continent\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country)) +\n  facet_wrap(~ continent) +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation faceted depending on continent\")\n\nThese look nice, lets free up the scale on the y axis within each facet such that we can differentiate the lines better, on top of that let us reintroduce the colors.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country, color = continent)) +\n  facet_wrap(~ continent, scales = \"free_y\") +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation faceted depending on continent\")\n\nlet us remove the color legend as the information is already present in the facet labels.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country, color = continent)) +\n  facet_wrap(~ continent, scales = \"free_y\") +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation faceted depending on continent\") +\n  guides(color = \"none\")\n\nLastly, lets overlay the smooth continent average\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country, color = continent)) +\n  facet_wrap(~ continent, scales = \"free_y\") +\n  geom_line(alpha = 0.2) + \n  geom_smooth(aes(group = continent), color = \"grey40\", se = FALSE) +\n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation faceted depending on continent\") +\n  guides(color = \"none\")\n## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\nUnfortunately doesn’t add too much information so let’s remove it again. Lastly, let’s update the labels to reflect the unit.\ntrade_data %&gt;% \n  ggplot(aes(Date, Balance, group = Country, color = continent)) +\n  facet_wrap(~ continent, scales = \"free_y\") +\n  geom_line(alpha = 0.2) + \n  coord_trans(y = \"S_sqrt\") +\n  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),\n                     minor_breaks = NULL) +\n  labs(title = \"United States Trade Balance in Goods with all countries\",\n       subtitle = \"With signed square root transformation faceted depending on continent\",\n       y = \"Balance (in millions of U.S. dollars on a nominal basis)\") +\n  guides(color = \"none\")\n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr        * 1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lattice       0.20-44 2021-05-02 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n Matrix        1.3-3   2021-05-04 [1] CRAN (R 4.1.0)                   \n mgcv          1.8-35  2021-04-18 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n nlme          3.1-152 2021-02-04 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-01-08-purrr-tips-and-tricks/index.html",
    "href": "post/2018-01-08-purrr-tips-and-tricks/index.html",
    "title": "Purrr - tips and tricks",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16."
  },
  {
    "objectID": "post/2018-01-08-purrr-tips-and-tricks/index.html#purrr-tips-and-tricks",
    "href": "post/2018-01-08-purrr-tips-and-tricks/index.html#purrr-tips-and-tricks",
    "title": "Purrr - tips and tricks",
    "section": "Purrr tips and tricks",
    "text": "Purrr tips and tricks\nIf you like me started by only using map() and its cousins (map_df, map_dbl, etc) you are missing out on a lot of what purrr have to offer! With the advent of #purrrresolution on Twitter, I’ll throw my 2 cents in in form of my bag of tips and tricks (which I’ll update in the future).\nFirst we load the packages:\nlibrary(tidyverse)\nlibrary(repurrrsive) # datasets used in some of the examples.\n\nloading files\nMultiple files can be read and combined at once using map_df and read_cvs.\nfiles &lt;- c(\"2015.cvs\", \"2016.cvs\", \"2017.cvs\")\nmap_df(files, read_csv)\nCombine with list.files to create magic1.\nfiles &lt;- list.files(\"../open-data/\", pattern = \"^2017\", full.names = TRUE)\nfull &lt;- map_df(files, read_csv)\n\n\ncombine if you forget *_df the first time around.\nIf you like me sometimes forget to end my map() with my desired output. The last resort is to manually combine it in a second line if you don’t want to replace map() with map_df() (which is properly the better advice, but can be handy in a pinch).\nX &lt;- map(1:10000, ~ data.frame(x = .x))\nX &lt;- bind_rows(X)\n\n\nname shortcut in map\nprovide “TEXT” to extract the element named “TEXT”. Follow 3 lines are equivalent.\nmap(got_chars, function(x) x[[\"name\"]]) \nmap(got_chars, ~ .x[[\"name\"]])\nmap(got_chars, \"name\")\nworks the same with indexes.2\nmap(got_chars, function(x) x[[1]]) \nmap(got_chars, ~ .x[[1]])\nmap(got_chars, 1)\n\n\nuse {} inside map\nIf you don’t know how to write the proper anonymous function or you want some counter in your map(), you can use {} to construct your anonymous function.\nHere is a simple toy example that shows that you can write multiple lines inside map.\nmap(1:3, ~ {\n  h &lt;- .x + 2\n  g &lt;- .x - 2\n  h + g\n})\nmap(1:3, ~ {\n  Sys.sleep(10)\n  cat(.x)\n  .x\n})\nThis can be very handy if you want to be a responsible (web scraping) pirate3.\nlibrary(httr)\ns_GET &lt;- safely(GET)\n\npb &lt;- progress_estimated(length(target_urls))\nmap(target_urls, ~{\n  pb$tick()$print()\n  Sys.sleep(5)\n  s_GET(.x)\n}) -&gt; httr_raw_responses\n\n\ndiscard, keep and compact\ndiscard() and keep() will provide very valuable since they help you filter your list/vector based on certain predictors.\nThey can be useful in cases of web scraping where certain lines are to be ignored.\nlibrary(rvest)\nurl &lt;- \"http://www.imdb.com/chart/boxoffice\"\n\nread_html(url) %&gt;%\n  html_nodes('tr') %&gt;%\n  html_text() %&gt;%\n  str_replace_all(\"\\n +\", \" \") %&gt;%\n  trimws() %&gt;%\n  keep(~ str_extract(.x, \".$\") %in% 0:9) %&gt;%\n  discard(~ as.numeric(str_extract(.x, \".$\")) &gt; 5)\nWhere we here scrape Top Box Office (US) from IMDb.com and we use keep() to keeps all lines that end in an integer and discards() to discards all lines where the integer is more than 5.\ncompact() is a handy wrapper that removed all elements that are NULL.\n\n\nsafely + compact\nIf you have a function that sometimes throws an error, warning, or for whatever reason isn’t entirely stable, you can use the wonder of safely() and compact(). safely() is a function that takes a function f() and returns a function safe_f() that returns a list with the elements result and error where result is the output of f() if it is able to run, and NULL otherwise. This means that we can create a function that will always work!\nunstable_function &lt;- function() {\n  ...\n}\n\nsafe_function &lt;- safely(unstable_function)\n\nmap(data, ~ safe_function(.x)) %&gt;%\n  map(\"result\") %&gt;%\n  compact()\ncombining this with compact which removes all NULL values thus returning only the successful calls.\n\n\nReduce\npurrr includes an little group of functions called reduce() (with its cousins reduce_right(), reduce2() and reduce2_right()) which iteratively combines from the left (right for reduce_right()) making\nreduce(list(x1, x2, x3), f)\nf(f(x1, x2), x3)\nequivalent.\nThis example4 comes from Colin Fay shows how to use reduce().\nregex_build &lt;- function(list){\n    reduce(list, ~ paste(.x, .y, sep = \"|\"))\n}\n\nregex_build(letters[1:5])\n## [1] \"a|b|c|d|e\"\nThis example by Jason Becker5 shows how to easier label data using reduce_right.\n# Load a directory of .csv files that has each of the lookup tables\nlookups &lt;- map(dir('data/lookups'), read.csv, stringsAsFactors = FALSE)\n# Alternatively if you have a single lookup table with code_type as your\n# data attribute you're looking up\n# lookups &lt;- split(lookups, code_type)\nlookups$real_data &lt;- read.csv('data/real_data.csv', \n                              stringsAsFactors = FALSE)\nreal_data &lt;- reduce_right(lookups, left_join)\n\n\npluck\nI find that a subsetting list can be a hassle more often than not. But pluck() have really helped to alleviate those problems quite a bit.\nlist(A = list(\"a1\",\"a2\"), \n     B = list(\"b1\", \"b2\"),\n     C = list(\"c1\", \"c2\"),\n     D = list(\"d1\", \"d2\", \"d3\")) %&gt;% \n  pluck(1)\n\n\nhead_while, tail_while\npurrr includes the twins head_while and tail_while which will give you all the elements that satisfy the condition until the first time it doesn’t.\nX &lt;- sample(1:100)\n\n# This\np &lt;- function(X) !(X &gt;= 10)\nX[seq(Position(p, X) - 1)]\n\n# is the same as this\nhead_while(X, ~ .x &gt;= 10)\n\n\nrerun\nif you need to do some simulation studies rerun could prove very useful. It takes 2 arguments. .n is the number of times to run, and ... is the expression that has to be rerun.\nrerun(.n = 10, rnorm(10)) %&gt;%\n  map_df(~ tibble(mean = mean(.x),\n                  sd = sd(.x),\n                  median = median(.x)))\n\n\ncompose\nThis little wonder of a function composes multiple functions to be applied in order from right to left.\nThis toy examples show how it works:\nsample(x = 1:6, size =  50, replace = TRUE) %&gt;%\n  table %&gt;% \n  sort %&gt;%\n  names\n\ndice1 &lt;- function(n) sample(size = n, x = 1:6, replace = TRUE)\ndice_rank &lt;- compose(names, sort, table, dice1)\ndice_rank(50)\nA more informative is found here6:\nlibrary(broom)\ntidy_lm &lt;- compose(tidy, lm)\ntidy_lm(Sepal.Length ~ Species, data = iris)\n## # A tibble: 3 x 5\n##   term              estimate std.error statistic   p.value\n##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)          5.01     0.0728     68.8  1.13e-113\n## 2 Speciesversicolor    0.930    0.103       9.03 8.77e- 16\n## 3 Speciesvirginica     1.58     0.103      15.4  2.21e- 32\n\n\nimap\nimap() is a handy little wrapper that acts as the indexed map(). Thus making it shorthand for map2(x, names(x), ...) when x have named and map2(x, seq_along(x), ...) when it doesn’t have names.\nimap_dbl(sample(10), ~ {\n  cat(\"draw nr\", .y, \"is\", .x, \"\\n\")\n  .x\n  })\nor it could be used in conjunction with rerun() to easily add an id to each sample.\nrerun(.n = 10, rnorm(10)) %&gt;%\n  imap_dfr(~ tibble(run = .y, \n                    mean = mean(.x),\n                    sd = sd(.x),\n                    median = median(.x)))"
  },
  {
    "objectID": "post/2018-01-08-purrr-tips-and-tricks/index.html#sources",
    "href": "post/2018-01-08-purrr-tips-and-tricks/index.html#sources",
    "title": "Purrr - tips and tricks",
    "section": "Sources",
    "text": "Sources\nhttp://ghement.ca/purrr.html\nhttp://statwonk.com/purrr.html\nhttps://maraaverick.rbind.io/2017/09/purrr-ty-posts/\nhttp://serialmentor.com/blog/2016/6/13/reading-and-combining-many-tidy-data-files-in-R\nhttp://colinfay.me/purrr-web-mining/\nhttp://colinfay.me/purrr-text-wrangling/\nhttp://colinfay.me/purrr-set-na/\nhttp://colinfay.me/purrr-mappers/\nhttp://colinfay.me/purrr-code-optim/\nhttp://colinfay.me/purrr-statistics/"
  },
  {
    "objectID": "post/2018-01-08-purrr-tips-and-tricks/index.html#footnotes",
    "href": "post/2018-01-08-purrr-tips-and-tricks/index.html#footnotes",
    "title": "Purrr - tips and tricks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nColinFay/df a list↩︎\njennybc.github.io - Introduction to map(): extract elements↩︎\nPirating Web Content Responsibly With R↩︎\nA Crazy Little Thing Called {purrr} - Part 2 : Text Wrangling↩︎\nLabeling Data with purrr↩︎\nA Crazy Little Thing Called {purrr} - Part 5: code optimization↩︎"
  },
  {
    "objectID": "post/2019-07-30-creating-rstudio-addin-to-modify-selection/index.html",
    "href": "post/2019-07-30-creating-rstudio-addin-to-modify-selection/index.html",
    "title": "Creating RStudio addin to modify selection",
    "section": "",
    "text": "The problem\nLately, there has been some well-deserved buzz around addins in RStudio, datapasta being one and hrbraddins being another highly liked one.\n\n\nI find datapasta helpful for creating little tibbles for teaching. I'll find some interesting data online and just copy and paste the table directly into the correct format. You can also set up keyboard shortcuts, because who doesn't love a keyboard shortcut. Thanks @MilesMcBain pic.twitter.com/deaZVVYYDu\n\n— We are R-Ladies (@WeAreRLadies) July 22, 2019\n\n\n\n\nMy keyboard shortcut for this lil' function gets quite the workout…📺 \"hrbraddins::bare_combine()\" by @hrbrmstr https://t.co/8dwqNEso0B #rstats pic.twitter.com/gyqz2mUE0Y\n\n— Mara Averick (@dataandme) July 29, 2019\n\n\nAll of this is done with RStudio Addins using the rstudioapi r package.\nA lot of the popular addins follows the same simple formula\n\nextract highlighted text\nmodify extracted text\nreplace highlighted text with modified text.\n\nif your problem can be solved with the above steps, then this post is for you.\n\n\nThe solution\nOnce you have found the name of your addin, go to your package directory, or create a new package. Then we use usethis to create a .R file for the function and to create the necessary infrastructure to let RStudio know it is a Addin.\nuse_r(\"name_of_your_addin\")\nuse_addin(\"name_of_your_addin\")\nThe inst/rstudio/addins.dcf file will be populated to make a binding between your function to the addins menu. From here you will in Name to change the text of the button in the drop-down menu and change the description to change the hover text.\nName: New Addin Name\nDescription: New Addin Description\nBinding: name_of_your_addin\nInteractive: false\nnow you can go back to the .R to write your function. Below is the minimal code needed. Just replace any_function with a function that takes a string and returns a modified string. build the package and you are done!\nexample &lt;- function() {\n  \n  # Gets The active Documeent\n  ctx &lt;- rstudioapi::getActiveDocumentContext()\n\n  # Checks that a document is active\n  if (!is.null(ctx)) {\n    \n    # Extracts selection as a string\n    selected_text &lt;- ctx$selection[[1]]$text\n\n    # modify string\n    selected_text &lt;- any_function(selected_text)\n    \n    # replaces selection with string\n    rstudioapi::modifyRange(ctx$selection[[1]]$range, selected_text)\n  }\n}\n\n\nExamples - slugify\nWhile I was writing this post I created an addin to turn the title of the blog post into a slug I could use. I replaced\nselected_text &lt;- any_function(selected_text)\nwith\nselected_text &lt;- stringr::str_to_lower(selected_text)\nselected_text &lt;- stringr::str_replace_all(selected_text, \" \", \"-\")\nWhich gave me this little gem of an addin!\n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html",
    "href": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html",
    "title": "Use prismatic with after_scale() for finer control of colors in ggplot2",
    "section": "",
    "text": "With the release of version 3.3.0 of ggplot2 came the ability to have more control over the aesthetic evaluation. This allows us to modify the colors of the mapped palettes with prismatic now easier than ever."
  },
  {
    "objectID": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html#packages",
    "href": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html#packages",
    "title": "Use prismatic with after_scale() for finer control of colors in ggplot2",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe load the essential packages to wrangle, collect data (we will use tweets), scrape websites, and handle emojis.\nlibrary(ggplot2)\nlibrary(prismatic)"
  },
  {
    "objectID": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html#examples",
    "href": "post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index.html#examples",
    "title": "Use prismatic with after_scale() for finer control of colors in ggplot2",
    "section": "Examples",
    "text": "Examples\nSuppose you have a simple bar chart and you have added colors to each bar.\nggplot(diamonds, aes(cut)) +\n  geom_bar(aes(fill = cut))\n\nNext, suppose you would like to add a border around each bar. Traditionally you could add a single color like black but it isn’t that satisfying as it doesn’t have any relation to the mapped colors.\nggplot(diamonds, aes(cut)) +\n  geom_bar(aes(fill = cut), color = \"black\")\n[])index_files/figure-html/unnamed-chunk-2-1.png)\nnow that after_scale() is available for us we can base the color based on the mapped fill colors. Below I have used clr_darken() to create a border that is just slightly darker than the fill color.\nggplot(diamonds, aes(cut)) +\n  geom_bar(aes(fill = cut, color = after_scale(clr_darken(fill, 0.3))))\n\nthis could also have been done in reverse by supplying the color and modifying the fill after. Notice how we can chain multiple color modifications together. Here we are taking the color, then desaturating it followed by some lighting.\nggplot(diamonds, aes(cut)) +\n  geom_bar(aes(color = cut, \n               fill = after_scale(clr_lighten(clr_desaturate(color), \n                                              space = \"combined\"))))\n\nIf you only need to specify one color directly you can use the stage() function.\nggplot(diamonds, aes(cut)) +\n  geom_bar(aes(fill = stage(start = cut, \n                            after_scale = clr_lighten(fill, space = \"combined\"))))\n\n\n\n current session info \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                           \n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2      2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22       2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0      2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1      2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2      2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1      2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1      2021-01-15 [1] CRAN (R 4.1.0)                   \n desc          1.3.0      2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27     2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr         1.0.7      2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   \n emo           0.0.0.9000 2021-07-17 [1] Github (hadley/emo@3f03b11)      \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0      2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0      2021-02-28 [1] CRAN (R 4.1.0)                   \n generics      0.1.0      2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                   \n highr         0.9        2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1    2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2      2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33       2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0      2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10     2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1      2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7      2013-12-03 [1] CRAN (R 4.1.0)                   \n prismatic   * 1.0.0      2021-01-05 [1] CRAN (R 4.1.0)                   \n purrr         0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0      2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7      2021-07-07 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11     2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9        2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2      2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble        3.1.2      2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1      2021-04-30 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1      2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                   \n viridisLite   0.4.0      2021-04-13 [1] CRAN (R 4.1.0)                   \n withr         2.4.2      2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24       2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2017-04-26-2017-world-press-freedom-index-with-emojis/index.html",
    "href": "post/2017-04-26-2017-world-press-freedom-index-with-emojis/index.html",
    "title": "2017 World Press Freedom Index with emojis",
    "section": "",
    "text": "Note\n\n\n\nEmojis are now fully supported in {ggplot2} thanks to the {ragg} package. Read more about it here: Modern Text Features in R.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16.\n\n\nWith Reporters Without Borders coming out with its 2017 World Press Freedom Index in the same week as Hadley Wickham coming out with the emo(ji) package, I had no choice but to explore both of them at the same time.\nDisclaimer! This post is not an exercise in statistical inference but rather a proof of concept of how to use the emo(ji) package with ggplot2.\n\nLoading packages\nlibrary(hrbrthemes)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(tibble)\n# remotes::install_github('hadley/emo')\nlibrary(emo)\nThe hrbrthemes is not necessary for this project but it is one of my personal favorite ggplot2 themes.\n\n\nGathering data\nHere we collect the data from Reporters Without Borders, emoji flags, and The World Bank (so we have something to plot against).\n\n2017 World Press Freedom Index\nWe have the 2017 World Press Freedom Index (direct download link) data which we load in as normal.\n(freedom_index &lt;- read_csv(\"https://rsf.org/sites/default/files/index_format_upload_2017-v2_1_0.csv\"))\n## # A tibble: 180 x 12\n##    ISO    Rank FR_Country EN_country  ES_country   `Underlying situation score …\n##    &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;                                &lt;dbl&gt;\n##  1 NOR       1 Norvège    Norway      Noruega                                760\n##  2 SWE       2 Suede      Sweden      Suecia                                 759\n##  3 FIN       3 Finlande   Finland     Finlandia                              892\n##  4 DNK       4 Danemark   Denmark     Dinamarca                             1036\n##  5 NLD       5 Pays-Bas   Netherlands Países Bajos                           963\n##  6 CRI       6 Costa Rica Costa Rica  Costa Rica                            1193\n##  7 CHE       7 Suisse     Switzerland Suiza                                 1213\n##  8 JAM       8 Jamaïque   Jamaica     Jamaica                               1273\n##  9 BEL       9 Belgique   Belgium     Bélgica                               1247\n## 10 ISL      10 Islande    Iceland     Islandia                              1303\n## # … with 170 more rows, and 6 more variables: Abuse score 2016 &lt;chr&gt;,\n## #   Overall Score 2016 &lt;dbl&gt;, Progression RANK &lt;dbl&gt;, Rank 2015 &lt;dbl&gt;,\n## #   Score 2015 &lt;dbl&gt;, Zone &lt;chr&gt;\nand we see that a total of 180 countries have a score (Overall Score 2016).\n\n\nGDP per capita\nTo have something somehow meaningful to compare the freedom index to. I’ve found some data about GDP per capita, mostly because I figured it would have data for quite a lot of the countries covered by the freedom index. So from The World Bank (direct download link) which we load in as normal.\n(gdp_pcap &lt;- read_csv(\"API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv\", skip = 3))\n## # A tibble: 264 x 62\n##    `Country Name` `Country Code` `Indicator Name` `Indicator Code` `1960` `1961`\n##    &lt;chr&gt;          &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Aruba          ABW            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  2 Afghanistan    AFG            GDP per capita … NY.GDP.PCAP.CD     59.8   59.9\n##  3 Angola         AGO            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  4 Albania        ALB            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  5 Andorra        AND            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  6 Arab World     ARB            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  7 United Arab E… ARE            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  8 Argentina      ARG            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n##  9 Armenia        ARM            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n## 10 American Samoa ASM            GDP per capita … NY.GDP.PCAP.CD     NA     NA  \n## # … with 254 more rows, and 56 more variables: 1962 &lt;dbl&gt;, 1963 &lt;dbl&gt;,\n## #   1964 &lt;dbl&gt;, 1965 &lt;dbl&gt;, 1966 &lt;dbl&gt;, 1967 &lt;dbl&gt;, 1968 &lt;dbl&gt;, 1969 &lt;dbl&gt;,\n## #   1970 &lt;dbl&gt;, 1971 &lt;dbl&gt;, 1972 &lt;dbl&gt;, 1973 &lt;dbl&gt;, 1974 &lt;dbl&gt;, 1975 &lt;dbl&gt;,\n## #   1976 &lt;dbl&gt;, 1977 &lt;dbl&gt;, 1978 &lt;dbl&gt;, 1979 &lt;dbl&gt;, 1980 &lt;dbl&gt;, 1981 &lt;dbl&gt;,\n## #   1982 &lt;dbl&gt;, 1983 &lt;dbl&gt;, 1984 &lt;dbl&gt;, 1985 &lt;dbl&gt;, 1986 &lt;dbl&gt;, 1987 &lt;dbl&gt;,\n## #   1988 &lt;dbl&gt;, 1989 &lt;dbl&gt;, 1990 &lt;dbl&gt;, 1991 &lt;dbl&gt;, 1992 &lt;dbl&gt;, 1993 &lt;dbl&gt;,\n## #   1994 &lt;dbl&gt;, 1995 &lt;dbl&gt;, 1996 &lt;dbl&gt;, 1997 &lt;dbl&gt;, 1998 &lt;dbl&gt;, 1999 &lt;dbl&gt;,\n## #   2000 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2002 &lt;dbl&gt;, 2003 &lt;dbl&gt;, 2004 &lt;dbl&gt;, 2005 &lt;dbl&gt;,\n## #   2006 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2008 &lt;dbl&gt;, 2009 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2011 &lt;dbl&gt;,\n## #   2012 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2014 &lt;dbl&gt;, 2015 &lt;dbl&gt;, 2016 &lt;dbl&gt;, X62 &lt;lgl&gt;\nwhich have quite a few variables but for now we will just focus on the 2015 variable as the 2016 appears empty. Now that we have two data sets that we would like to combine, a general question would be if the gdp_pcap data have information matching our 180 countries. So with the following bit of code, we join the two datasets together by the ICO ALPHA-3 Code available in both datasets and select the countries that don’t have a value for the year 2015.\nleft_join(freedom_index, gdp_pcap, by = c(\"ISO\" = \"Country Code\")) %&gt;% \n  filter(is.na(`2015`)) %&gt;% \n  select(EN_country)\n## # A tibble: 12 x 1\n##    EN_country                           \n##    &lt;chr&gt;                                \n##  1 Liechtenstein                        \n##  2 Andorra                              \n##  3 OECS                                 \n##  4 Taiwan                               \n##  5 Papua New Guinea                     \n##  6 Cyprus North                         \n##  7 Kosovo                               \n##  8 Venezuela                            \n##  9 Libya                                \n## 10 Syrian Arab Republic                 \n## 11 Eritrea                              \n## 12 Democratic People's Republic of Korea\nwhich leaves us with 166 countries. I could have looked for the data for these countries, but that is outside the reach of this post.\n\n\nFlag emoji\nI would like to use the different flag emojis\n## 🇦🇨🇦🇩🇦🇪🇦🇫🇦🇬🇦🇮🇦🇱🇦🇲🇦🇴🇦🇶🇦🇷🇦🇸🇦🇹🇦🇺🇦🇼🇦🇽🇦🇿🇧🇦🇧🇧🇧🇩🇧🇪🇧🇫🇧🇬🇧🇭🇧🇮🇧🇯🇧🇱🇧🇲🇧🇳🇧🇴🇧🇶🇧🇷🇧🇸🇧🇹🇧🇻🇧🇼🇧🇾🇧🇿🇨🇦🇨🇨🇨🇩🇨🇫🇨🇬🇨🇭🇨🇮🇨🇰🇨🇱🇨🇲🇨🇳🇨🇴🇨🇵🇨🇷🇨🇺🇨🇻🇨🇼🇨🇽🇨🇾🇨🇿🇩🇪🇩🇪🇩🇬🇩🇯🇩🇰🇩🇲🇩🇴🇩🇿🇪🇦🇪🇨🇪🇪🇪🇬🇪🇭🇪🇷🇪🇸🇪🇹🇪🇺🇫🇮🇫🇯🇫🇰🇫🇲🇫🇴🇫🇷🇬🇦🇬🇧🇬🇧🇬🇩🇬🇪🇬🇫🇬🇬🇬🇭🇬🇮🇬🇱🇬🇲🇬🇳🇬🇵🇬🇶🇬🇷🇬🇸🇬🇹🇬🇺🇬🇼🇬🇾🇭🇰🇭🇲🇭🇳🇭🇷🇭🇹🇭🇺🇮🇨🇮🇩🇮🇪🇮🇱🇮🇲🇮🇳🇮🇴🇮🇶🇮🇷🇮🇸🇮🇹🇯🇪🇯🇲🇯🇴🇯🇵🇰🇪🇰🇬🇰🇭🇰🇮🇰🇲🇰🇳🇰🇵🇰🇷🇰🇼🇰🇾🇰🇿🇱🇦🇱🇧🇱🇨🇱🇮🇱🇰🇱🇷🇱🇸🇱🇹🇱🇺🇱🇻🇱🇾🇲🇦🇲🇨🇲🇩🇲🇪🇲🇫🇲🇬🇲🇭🇲🇰🇲🇱🇲🇲🇲🇳🇲🇴🇲🇵🇲🇶🇲🇷🇲🇸🇲🇹🇲🇺🇲🇻🇲🇼🇲🇽🇲🇾🇲🇿🇳🇦🇳🇨🇳🇪🇳🇫🇳🇬🇳🇮🇳🇱🇳🇴🇳🇵🇳🇷🇳🇺🇳🇿🇴🇲🇵🇦🇵🇪🇵🇫🇵🇬🇵🇭🇵🇰🇵🇱🇵🇲🇵🇳🇵🇷🇵🇸🇵🇹🇵🇼🇵🇾🇶🇦🇷🇪🇷🇴🇷🇸🇷🇺🇷🇼🇸🇦🇸🇧🇸🇨🇸🇩🇸🇪🇸🇬🇸🇭🇸🇮🇸🇯🇸🇰🇸🇱🇸🇲🇸🇳🇸🇴🇸🇷🇸🇸🇸🇹🇸🇻🇸🇽🇸🇾🇸🇿🇹🇦🇹🇨🇹🇩🇹🇫🇹🇬🇹🇭🇹🇯🇹🇰🇹🇱🇹🇲🇹🇳🇹🇴🇹🇷🇹🇹🇹🇻🇹🇼🇹🇿🇺🇦🇺🇬🇺🇲🇺🇳🇺🇸🇺🇸🇺🇾🇺🇿🇻🇦🇻🇨🇻🇪🇻🇬🇻🇮🇻🇳🇻🇺🇼🇫🇼🇸🇽🇰🇾🇪🇾🇹🇿🇦🇿🇲🇿🇼🏴󠁧󠁢󠁥󠁮󠁧󠁿🏴󠁧󠁢󠁳󠁣󠁴󠁿🏴󠁧󠁢󠁷󠁬󠁳󠁿\nwhich all can be found with the new emo(ji) package\nemo::ji_find(\"flag\")\n## # A tibble: 264 x 2\n##    name                 emoji\n##    &lt;chr&gt;                &lt;chr&gt;\n##  1 Ascension_Island     🇦🇨   \n##  2 andorra              🇦🇩   \n##  3 united_arab_emirates 🇦🇪   \n##  4 afghanistan          🇦🇫   \n##  5 antigua_barbuda      🇦🇬   \n##  6 anguilla             🇦🇮   \n##  7 albania              🇦🇱   \n##  8 armenia              🇦🇲   \n##  9 angola               🇦🇴   \n## 10 antarctica           🇦🇶   \n## # … with 254 more rows\nwe first notice that the first two emojis are not country flags and that the name of the countries are not in the same format as what we have from earlier, so we replace the underscores with spaces and translate everything to lowercase before joining. This time by country name. Again we check for missed joints.\nleft_join(freedom_index, gdp_pcap, by = c(\"ISO\" = \"Country Code\")) %&gt;% \n  mutate(EN_country = tolower(EN_country)) %&gt;% \n  left_join(emo::ji_find(\"flag\") %&gt;% \n              mutate(name = str_replace_all(name, \"_\", \" \")) %&gt;% \n              filter(name != \"japan\", name != \"crossed flags\"), \n            by = c(\"EN_country\" = \"name\"))  %&gt;% \n  filter(!is.na(`2015`)) %&gt;% \n  filter(is.na(emoji)) %&gt;% \n  select(EN_country)\n## # A tibble: 22 x 1\n##    EN_country            \n##    &lt;chr&gt;                 \n##  1 germany               \n##  2 spain                 \n##  3 trinidad and tobago   \n##  4 france                \n##  5 united kingdom        \n##  6 united states         \n##  7 italy                 \n##  8 south korea           \n##  9 bosnia and herzegovina\n## 10 japan                 \n## # … with 12 more rows\nWhich is quite a few. It turns out that the naming convention for the emoji names has not been that consistent, “de” used instead of “germany” etc. To clear up code later on we make a new emoji tibble with all the changes.\nnewemoji &lt;- emo::ji_find(\"flag\") %&gt;% \n              mutate(name = str_replace_all(string = name,\n                                            pattern = \"_\",\n                                            replacement =  \" \")) %&gt;% \n  filter(name != \"japan\", name != \"crossed flags\") %&gt;% \n  mutate(name = str_replace(name, \"^de$\", \"germany\"),\n         name = str_replace(name, \"^es$\", \"spain\"),\n         name = str_replace(name, \"^trinidad tobago$\", \"trinidad and tobago\"),\n         name = str_replace(name, \"^fr$\", \"france\"),\n         name = str_replace(name, \"^uk$\", \"united kingdom\"),\n         name = str_replace(name, \"^us$\", \"united states\"),\n         name = str_replace(name, \"^it$\", \"italy\"),\n         name = str_replace(name, \"^kr$\", \"south korea\"),\n         name = str_replace(name, \"^bosnia herzegovina$\", \"bosnia and herzegovina\"),\n         name = str_replace(name, \"^guinea bissau$\", \"guinea-bissau\"),\n         name = str_replace(name, \"^cote divoire$\", \"ivory coast\"),\n         name = str_replace(name, \"^timor leste$\", \"east timor\"),\n         name = str_replace(name, \"^congo brazzaville$\", \"congo\"),\n         name = str_replace(name, \"^palestinian territories$\", \"palestine\"),\n         name = str_replace(name, \"^ru$\", \"russian federation\"),\n         name = str_replace(name, \"^congo kinshasa$\", \"the democratic republic of the congo\"),\n         name = str_replace(name, \"^tr$\", \"turkey\"),\n         name = str_replace(name, \"^brunei$\", \"brunei darussalam\"),\n         name = str_replace(name, \"^laos$\", \"lao people's democratic republic\"),\n         name = str_replace(name, \"^cn$\", \"china\"),\n         name = str_replace(name, \"^jp$\", \"japan\"))\nnewemoji\n## # A tibble: 264 x 2\n##    name                 emoji\n##    &lt;chr&gt;                &lt;chr&gt;\n##  1 Ascension Island     🇦🇨   \n##  2 andorra              🇦🇩   \n##  3 united arab emirates 🇦🇪   \n##  4 afghanistan          🇦🇫   \n##  5 antigua barbuda      🇦🇬   \n##  6 anguilla             🇦🇮   \n##  7 albania              🇦🇱   \n##  8 armenia              🇦🇲   \n##  9 angola               🇦🇴   \n## 10 antarctica           🇦🇶   \n## # … with 254 more rows\n\n\n\nPlotting it all with ggplot2\nNow with all the preparation done we do a naive first plot.\nleft_join(freedom_index, gdp_pcap, by = c(\"ISO\" = \"Country Code\")) %&gt;% \n  mutate(EN_country = tolower(EN_country)) %&gt;% \n  left_join(newemoji, by = c(\"EN_country\" = \"name\")) %&gt;% \n  ggplot(aes(x = `2015`, y = `Overall Score 2016`)) +\n  geom_text(aes(label = emoji))\n## Warning: Removed 14 rows containing missing values (geom_text).\n\nBut wait, we have a couple of problems:\n\nThe emojis don’t show up.\nThe freedom score is 100 times too much as the actual.\nThe gdp_pcap is quite skewed.\n\nBut these are not problems too great for us. It turns out that R’s graphical devices don’t support AppleColorEmoji font. We can alleviate that problem by saving the plot as a svg file. And we will do a simple log transformation of the gdp_pcap.\nOur final plot is thus the following:\nleft_join(freedom_index, gdp_pcap, by = c(\"ISO\" = \"Country Code\")) %&gt;% \n  mutate(EN_country = tolower(EN_country),\n         `Overall Score 2016` = `Overall Score 2016` / 100) %&gt;% \n  left_join(newemoji, by = c(\"EN_country\" = \"name\")) %&gt;% \n  ggplot(aes(x = `2015`, y = `Overall Score 2016`)) +\n  stat_smooth(method = \"lm\", color = \"grey\", se = FALSE) +\n  geom_text(aes(label = emoji)) +\n  scale_x_log10() +\n  annotation_logticks(sides = \"b\")  +\n  theme_ipsum() +\n  labs(x = \"GDP per capita (current US$)\", y = \"2017 World Press Freedom Index\",\n       title = \"Countries with high GDP per capita\\ntend to have low Freedom Index\",\n       subtitle = \"Visualized with emojis\")\n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-02                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                            \n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.0.0)                    \n backports     1.2.1      2020-12-09 [1] CRAN (R 4.0.2)                    \n blogdown      1.3.2      2021-06-06 [1] Github (rstudio/blogdown@00a2090) \n bookdown      0.22       2021-04-22 [1] CRAN (R 4.0.2)                    \n broom         0.7.6      2021-04-05 [1] CRAN (R 4.0.2)                    \n bslib         0.2.4.9003 2021-05-05 [1] Github (rstudio/bslib@ba6a80d)    \n cellranger    1.1.0      2016-07-27 [1] CRAN (R 4.0.0)                    \n cli           2.5.0      2021-04-26 [1] CRAN (R 4.0.2)                    \n clipr         0.7.1      2020-10-08 [1] CRAN (R 4.0.2)                    \n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.0.5)                    \n colorspace    2.0-1      2021-05-04 [1] CRAN (R 4.0.2)                    \n crayon        1.4.1      2021-02-08 [1] CRAN (R 4.0.2)                    \n DBI           1.1.1      2021-01-15 [1] CRAN (R 4.0.2)                    \n dbplyr        2.1.1      2021-04-06 [1] CRAN (R 4.0.2)                    \n desc          1.3.0      2021-03-05 [1] CRAN (R 4.0.2)                    \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.0.0)                    \n digest        0.6.27     2020-10-24 [1] CRAN (R 4.0.2)                    \n dplyr       * 1.0.7      2021-06-18 [1] CRAN (R 4.0.2)                    \n ellipsis      0.3.2      2021-04-29 [1] CRAN (R 4.0.2)                    \n emo         * 0.0.0.9000 2020-05-12 [1] Github (hadley/emo@3f03b11)       \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.0.0)                    \n extrafont     0.17       2014-12-08 [1] CRAN (R 4.0.0)                    \n extrafontdb   1.0        2012-06-11 [1] CRAN (R 4.0.0)                    \n fansi         0.5.0      2021-05-25 [1] CRAN (R 4.0.2)                    \n farver        2.1.0      2021-02-28 [1] CRAN (R 4.0.2)                    \n forcats     * 0.5.1      2021-01-27 [1] CRAN (R 4.0.2)                    \n fs            1.5.0      2020-07-31 [1] CRAN (R 4.0.2)                    \n gdtools       0.2.3      2021-01-06 [1] CRAN (R 4.0.2)                    \n generics      0.1.0      2020-10-31 [1] CRAN (R 4.0.2)                    \n ggplot2     * 3.3.3      2020-12-30 [1] CRAN (R 4.0.2)                    \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.0.2)                    \n gtable        0.3.0      2019-03-25 [1] CRAN (R 4.0.0)                    \n haven         2.4.1      2021-04-23 [1] CRAN (R 4.0.2)                    \n highr         0.9        2021-04-16 [1] CRAN (R 4.0.2)                    \n hms           1.1.0      2021-05-17 [1] CRAN (R 4.0.2)                    \n hrbrthemes  * 0.8.0      2020-03-06 [1] CRAN (R 4.0.2)                    \n htmltools     0.5.1.1    2021-01-22 [1] CRAN (R 4.0.2)                    \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.0.2)                    \n jquerylib     0.1.4      2021-04-26 [1] CRAN (R 4.0.2)                    \n jsonlite      1.7.2      2020-12-09 [1] CRAN (R 4.0.2)                    \n knitr       * 1.33       2021-04-24 [1] CRAN (R 4.0.2)                    \n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.0.2)                    \n lifecycle     1.0.0      2021-02-15 [1] CRAN (R 4.0.2)                    \n lubridate     1.7.10     2021-02-26 [1] CRAN (R 4.0.2)                    \n magrittr      2.0.1      2020-11-17 [1] CRAN (R 4.0.2)                    \n modelr        0.1.8      2020-05-19 [1] CRAN (R 4.0.0)                    \n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.0.0)                    \n pillar        1.6.1      2021-05-16 [1] CRAN (R 4.0.2)                    \n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.0.0)                    \n png           0.1-7      2013-12-03 [1] CRAN (R 4.0.0)                    \n purrr       * 0.3.4      2020-04-17 [1] CRAN (R 4.0.0)                    \n R6            2.5.0      2020-10-28 [1] CRAN (R 4.0.2)                    \n Rcpp          1.0.6      2021-01-15 [1] CRAN (R 4.0.2)                    \n readr       * 1.4.0      2020-10-05 [1] CRAN (R 4.0.2)                    \n readxl        1.3.1      2019-03-13 [1] CRAN (R 4.0.2)                    \n reprex        2.0.0      2021-04-02 [1] CRAN (R 4.0.2)                    \n rlang         0.4.11     2021-04-30 [1] CRAN (R 4.0.2)                    \n rmarkdown     2.8.6      2021-06-06 [1] Github (rstudio/rmarkdown@9dc5d97)\n rprojroot     2.0.2      2020-11-15 [1] CRAN (R 4.0.2)                    \n rstudioapi    0.13       2020-11-12 [1] CRAN (R 4.0.2)                    \n Rttf2pt1      1.3.8      2020-01-10 [1] CRAN (R 4.0.0)                    \n rvest         1.0.0      2021-03-09 [1] CRAN (R 4.0.2)                    \n sass          0.3.1.9003 2021-05-05 [1] Github (rstudio/sass@6166162)     \n scales        1.1.1      2020-05-11 [1] CRAN (R 4.0.0)                    \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.0.0)                    \n stringi       1.6.2      2021-05-17 [1] CRAN (R 4.0.2)                    \n stringr     * 1.4.0      2019-02-10 [1] CRAN (R 4.0.0)                    \n systemfonts   1.0.1      2021-02-09 [1] CRAN (R 4.0.2)                    \n tibble      * 3.1.2      2021-05-16 [1] CRAN (R 4.0.2)                    \n tidyr       * 1.1.3      2021-03-03 [1] CRAN (R 4.0.2)                    \n tidyselect    1.1.1      2021-04-30 [1] CRAN (R 4.0.2)                    \n tidyverse   * 1.3.1      2021-04-15 [1] CRAN (R 4.0.2)                    \n utf8          1.2.1      2021-03-12 [1] CRAN (R 4.0.2)                    \n vctrs         0.3.8      2021-04-29 [1] CRAN (R 4.0.2)                    \n withr         2.4.2      2021-04-18 [1] CRAN (R 4.0.2)                    \n xfun          0.23       2021-05-15 [1] CRAN (R 4.0.2)                    \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.0.0)                    \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.0.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2019-11-25-refactoring-tests/index.html",
    "href": "post/2019-11-25-refactoring-tests/index.html",
    "title": "Refactoring Tests",
    "section": "",
    "text": "We all know the saying\nHowever, I would like to expand that to\nDuring my lasting packages such as prismatic, I found myself copy-pasting tests around whenever I needed to test a new function. I realized that the refactoring practices I try to apply in my general code writing weren’t carried over to the way I was writing my tests. I would frequently copy-paste hundreds of lines of tests only to replace the function name. This post will go over a refactoring scenario I am working on at the moment."
  },
  {
    "objectID": "post/2019-11-25-refactoring-tests/index.html#fix-1---plays-well-with-error-messages",
    "href": "post/2019-11-25-refactoring-tests/index.html#fix-1---plays-well-with-error-messages",
    "title": "Refactoring Tests",
    "section": "Fix #1 - Plays well with error messages",
    "text": "Fix #1 - Plays well with error messages\nThe first solution is to wrap the inside of your test into a function. The above test would create the refactored testing-function\ntest_wrong_input &lt;- function(clr_) {\n  expect_error(clr_(\"not a color\"))\n\n  expect_error(clr_(list(pal = \"#000000\")))\n\n  expect_error(clr_(character()))\n}\nand the test would be changed to\ntest_that(\"it complains when col type is wrong.\", {\n  test_wrong_input(clr_alpha)\n})\nthis change will perform the tests, and adding tests for the new function would only need 1 change in the test instead of 3.\ntest_that(\"it complains when col type is wrong.\", {\n  test_wrong_input(clr_mix)\n})\nMore importantly, let’s imagine that we want to extend the types of wrong inputs we what to screen. Now we simply just need to add it once and it propagates to all the functions.\nThe main benefit of this refactoring style is that when an error appears, It will denote the line where the test broke."
  },
  {
    "objectID": "post/2019-11-25-refactoring-tests/index.html#fix-2---less-typing-worse-error-message",
    "href": "post/2019-11-25-refactoring-tests/index.html#fix-2---less-typing-worse-error-message",
    "title": "Refactoring Tests",
    "section": "Fix #2 - Less typing, worse error message",
    "text": "Fix #2 - Less typing, worse error message\nThe second solution is to wrap the entire testing statement inside a function. For this example, the function would look like this\ntest_wrong_input &lt;- function(clr_) {\n  test_that(\"it complains when col type is wrong.\", {\n  expect_error(clr_(\"not a color\"))\n\n  expect_error(clr_(list(pal = \"#000000\")))\n\n  expect_error(clr_(character()))\n  })\n}\nand the testing would look like\ntest_wrong_input(clr_mix)\nThis reduces the number of lines needed for each test from 3 down to 1. However, it comes with a downside. When an error appears testthat will give the location of the definition of the test function, not the location from where it was called.\n We can still see that the error happens inside the “alpha” Context, but it is slightly harder to track down."
  },
  {
    "objectID": "post/2019-11-25-refactoring-tests/index.html#fix-2.1---ugly-hack-to-give-me-the-location",
    "href": "post/2019-11-25-refactoring-tests/index.html#fix-2.1---ugly-hack-to-give-me-the-location",
    "title": "Refactoring Tests",
    "section": "Fix #2.1 - ugly hack to give me the location",
    "text": "Fix #2.1 - ugly hack to give me the location\nThe second solution can be made slightly better by making the description of the test more informative.\ntest_wrong_input &lt;- function(clr_) {\n  test_that(paste0(\"test_wrong_input: \",\n                   deparse(substitute(clr_)),\n                   \"() complains when col type is wrong.\"), {\n  expect_error(clr_(\"not a color\"))\n\n  expect_error(clr_(list(pal = \"#000000\")))\n\n  expect_error(clr_(\"pink\"))\n  })\n}\n\nIt takes more work upfront when writing the test functions. But it gives a compromise between the brevity of test files and the clarity of the debugging page.\nThanks for reading along! I hope you found it useful!\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/smltar-story/index.html",
    "href": "post/smltar-story/index.html",
    "title": "Supervised Machine Learning for Text Analysis in R",
    "section": "",
    "text": "The last couple of years of my life have been quite a journey. I’m standing here now with a book in production, available for preorder, and I couldn’t be more proud of the work Julia Silge and I have done. If you had told me 3 years ago that I would be here now I wouldn’t have believed in you.\nThis post serves three different purposes;\n\nThank the people and community that helped me get to this point\nVisualize and talk about the book writing process itself\nTell you about how to preorder the book\n\nFirst, let us get to the thanks! First and foremost I’m thankful to my family and my wife Athena for supporting me through these times. Every time I start a new project Athena has been supporting me in the ways that only she knows how to, no matter how outlandish it may seem (“Hey I’m gonna write a book”).\nI want to especially give thanks to the #rstats community and all the people who are here to nourish it. I’m especially thankful for the people lifting up the little people. Notable people in this space is Mara Averick and Jesse Mostipak who lift up many newcomers including myself.\nI want to thank Julia for working with me through this long project, without her expertise, kindness, and work ethic I doubt I would have finished the book yet alone start it.\nI thank our editors at Chapman & Hall; John Kimmel and Robin Lloyd-Starkes. The wonderful and helpful technical reviewers (stick around for the pre-order links to read some of the extracts).\nYihui Xie is wonderful and working tirelessly to make document creating easier and easier, this includes his work on bookdown.\nDesirée De Leon for the site beautiful design of the book’s website.\nMax Kuhn, Davis Vaughan, and Hannah Frick for the amazing work on tidymodels which we using in the second section of the book. There are far too many people to list here for thanks so I will end with “Thank you all!”.\nNow let us she some stories. I have used the gitsum package to gather information about all the commits that we made when building the book.\nThere are many files involved with creating a book. bookdown helps a lot with organization and there is plenty of documentation. Below is a chart of what happened to the main files, which is the rmarkdown files containing the content of each of the chapters in the book. You see how we worked through each of the chapters, one by one. You can see when we sent off a section of the book for review and started working on the next set of chapters.\n\nNext, we see how the size of the chapters slowly but surely growing in size. I’ll point out one thing here, you see around March of 2021 how chapters 6 and 7 diverge in size from each other. This was a tough decision on our side where we swapped the data and models around between the two chapters, leading to a lot more work, but what we think ended up being a better flow.\n\nLast we have another overview of changes, here looking at both the rmarkdown files and everything else. I find this to be a neat view of how we are working together to develop the content.\n\nI want to give some tips and tricks we learned along the way. The biggest advice I have for you is to make a reprex, however, that can be harder when you are working with a book. Julia created this reprex repository here that mimics the main repository as closely as possible but with minimal code so we can render the book in a minute instead of hours. We have used this repository properly a dozen times for debugging and fast iteration.\nThe second piece of advice I have is to use _common.R file as we do here. This helps with uniformity and consistency. Here we set knitr chunk option templates to make sure we are using consistent figure sizing. And here we are setting to default colors and color scales for the whole book.\nThe book is available for pre-order at all the major online bookstores. We have the whole book freely available for you to read online at smltar.com/. We want to make sure that this book is accessible for as many people as possible.\nWe wanted to make sure that this book not only teaching you about natural language processing at the implementation level but to make sure you start thinking about why you are doing what you are doing and the effects it has on the stakeholders of your project. Algorithmic bias and adverse effects of a machine learning model can have large effects and we want to make sure that it is in the back of your mind when working with models like this.\nWhen It came to giving back to the community, Black Girls Code was an easy choice. BGC is a not-for-profit organization that focuses on providing technology education for African-American girls that we are happy to support. We are donating all the author proceeds of the presale books.\nI’m hoping you will enjoy the book as much as we enjoyed writing it. I don’t know what my next big project will be, but I know I’ll make sure that it gives back to the community."
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "",
    "text": "This is the fifth blog post in the textrecipes series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be showcasing how to use pretrained word embeddings."
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html#packages",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html#packages",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe will be using tidymodels for modeling, tidyverse for EDA, textrecipes for text preprocessing, and textdata to load the pretrained word embedding.\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(textrecipes)\nlibrary(textdata)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html#exploring-the-data",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html#exploring-the-data",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "Exploring the data ⛏",
    "text": "Exploring the data ⛏\nWe will the same data from the previous blogpost. This data comes from Kaggle and contains English1 Women’s E-Commerce Clothing Reviews.\nThe following section is unchanged from the last post, if you are already familiar with this section then skip to the modeling section.\nreviews &lt;- read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n## Warning: Missing column names filled in: 'X1' [1]\nWe start by a quick glimpse() of the data.\nglimpse(reviews)\n## Rows: 23,486\n## Columns: 11\n## $ X1                        &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n## $ `Clothing ID`             &lt;dbl&gt; 767, 1080, 1077, 1049, 847, 1080, 858, 858, …\n## $ Age                       &lt;dbl&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, 34, 53, …\n## $ Title                     &lt;chr&gt; NA, NA, \"Some major design flaws\", \"My favor…\n## $ `Review Text`             &lt;chr&gt; \"Absolutely wonderful - silky and sexy and c…\n## $ Rating                    &lt;dbl&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, 5, 5, 3,…\n## $ `Recommended IND`         &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,…\n## $ `Positive Feedback Count` &lt;dbl&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2, 2, 0, 1…\n## $ `Division Name`           &lt;chr&gt; \"Initmates\", \"General\", \"General\", \"General …\n## $ `Department Name`         &lt;chr&gt; \"Intimate\", \"Dresses\", \"Dresses\", \"Bottoms\",…\n## $ `Class Name`              &lt;chr&gt; \"Intimates\", \"Dresses\", \"Dresses\", \"Pants\", …\nWe have a good split between text variables, numeric and categorical values. Let us also take a look at the distribution of the Rating variable\nreviews %&gt;%\n  ggplot(aes(Rating)) +\n  geom_bar()\n\nWhich is quite right-skewed. Let us collapse the ratings into 2 groups, 5 and less-then-5. Before we go on will I remove the row number variable X1 and clean the column names with the janitor package to remove cases and spaces.\nreviews &lt;- reviews %&gt;%\n  select(-X1) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(rating = factor(rating == 5, c(TRUE, FALSE), c(\"5\", \"&lt;5\")))\nBefore we do some exploratory analysis we will split the data into training and testing datasets. We do this to avoid learning this about the data that would only be available in the testing data set.\nset.seed(1234)\n\nreviews_split &lt;- initial_split(reviews)\n\nreviews_train &lt;- training(reviews_split)\nOur main objective is to predict the rating based on the text review. This will naturally exclude variables such as Recommended IND and Positive Feedback Count as that information is unlikely to be known before the rating is given. We will mostly be using the text variables (Review Text and Title) but I am going to take a look at some of the other variables before we move on in case they would be easy inclusions.\nThe ratings were pretty highly right-skewed and even when we collapsed them there are still more 5s.\nreviews_train %&gt;%\n  ggplot(aes(rating)) +\n  geom_bar()\n\nSince we have the age let us take a look to make sure it has a reasonable range.\nreviews_train %&gt;%\n  ggplot(aes(age)) +\n  geom_bar()\n\nNothing too out of the ordinary, we have some young people and old people but nothing weird. Out of curiosity let us take a look at that one age that is above the pack.\nreviews_train %&gt;%\n  count(age, sort = TRUE)\n## # A tibble: 77 x 2\n##      age     n\n##    &lt;dbl&gt; &lt;int&gt;\n##  1    39   932\n##  2    35   685\n##  3    36   640\n##  4    34   600\n##  5    38   583\n##  6    37   572\n##  7    33   552\n##  8    41   549\n##  9    46   543\n## 10    42   502\n## # … with 67 more rows\nSince we have the clothing id, then I want to know if any of the reviews apply to the same articles of clothing.\nreviews_train %&gt;%\n  count(clothing_id, sort = TRUE) \n## # A tibble: 1,068 x 2\n##    clothing_id     n\n##          &lt;dbl&gt; &lt;int&gt;\n##  1        1078   758\n##  2         862   603\n##  3        1094   580\n##  4         872   420\n##  5        1081   415\n##  6         829   409\n##  7        1110   347\n##  8         868   329\n##  9         895   309\n## 10         936   264\n## # … with 1,058 more rows\nSo out of 17614 we have 1068 clothing articles. Let us see how the reviews are split between the variables\nreviews_train %&gt;%\n  count(clothing_id, sort = TRUE) %&gt;%\n  mutate(x = row_number()) %&gt;%\n  ggplot(aes(x, n)) +\n  geom_point()\n\nAnd we see quite a fast drop-off.\nI’m trying to create a fairly simple model so I won’t be including much information."
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html#modeling",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html#modeling",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "Modeling ⚙️",
    "text": "Modeling ⚙️\nWe will restrict ourselves to only use the two text fields and the age of the customer. We are doing this so we can compare results with previous results.\nBefore we go on, let us take a look at the pre-trained word embeddings. The embedding_glove6b() function gives us access to the 6B tokens glove embedding from the Stanford NLP Group.\nYou will get a prompt the first time you use this function. This is expected and done to make sure that the user agrees to the license and agreements of any given resource.\nI have specified dimensions = 100 to get word vectors with 100 dimensions.\nglove6b &lt;- textdata::embedding_glove6b(dimensions = 100)\nglove6b\n## # A tibble: 400,000 x 101\n##    token      d1      d2      d3      d4      d5      d6      d7      d8      d9\n##    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n##  1 \"the\" -0.0382 -0.245   0.728  -0.400   0.0832  0.0440 -0.391   0.334  -0.575 \n##  2 \",\"   -0.108   0.111   0.598  -0.544   0.674   0.107   0.0389  0.355   0.0635\n##  3 \".\"   -0.340   0.209   0.463  -0.648  -0.384   0.0380  0.171   0.160   0.466 \n##  4 \"of\"  -0.153  -0.243   0.898   0.170   0.535   0.488  -0.588  -0.180  -1.36  \n##  5 \"to\"  -0.190   0.0500  0.191  -0.0492 -0.0897  0.210  -0.550   0.0984 -0.201 \n##  6 \"and\" -0.0720  0.231   0.0237 -0.506   0.339   0.196  -0.329   0.184  -0.181 \n##  7 \"in\"   0.0857 -0.222   0.166   0.134   0.382   0.354   0.0129  0.225  -0.438 \n##  8 \"a\"   -0.271   0.0440 -0.0203 -0.174   0.644   0.712   0.355   0.471  -0.296 \n##  9 \"\\\"\"  -0.305  -0.236   0.176  -0.729  -0.283  -0.256   0.266   0.0253 -0.0748\n## 10 \"'s\"   0.589  -0.202   0.735  -0.683  -0.197  -0.180  -0.392   0.342  -0.606 \n## # … with 399,990 more rows, and 91 more variables: d10 &lt;dbl&gt;, d11 &lt;dbl&gt;,\n## #   d12 &lt;dbl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;, d15 &lt;dbl&gt;, d16 &lt;dbl&gt;, d17 &lt;dbl&gt;,\n## #   d18 &lt;dbl&gt;, d19 &lt;dbl&gt;, d20 &lt;dbl&gt;, d21 &lt;dbl&gt;, d22 &lt;dbl&gt;, d23 &lt;dbl&gt;,\n## #   d24 &lt;dbl&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;, d27 &lt;dbl&gt;, d28 &lt;dbl&gt;, d29 &lt;dbl&gt;,\n## #   d30 &lt;dbl&gt;, d31 &lt;dbl&gt;, d32 &lt;dbl&gt;, d33 &lt;dbl&gt;, d34 &lt;dbl&gt;, d35 &lt;dbl&gt;,\n## #   d36 &lt;dbl&gt;, d37 &lt;dbl&gt;, d38 &lt;dbl&gt;, d39 &lt;dbl&gt;, d40 &lt;dbl&gt;, d41 &lt;dbl&gt;,\n## #   d42 &lt;dbl&gt;, d43 &lt;dbl&gt;, d44 &lt;dbl&gt;, d45 &lt;dbl&gt;, d46 &lt;dbl&gt;, d47 &lt;dbl&gt;,\n## #   d48 &lt;dbl&gt;, d49 &lt;dbl&gt;, d50 &lt;dbl&gt;, d51 &lt;dbl&gt;, d52 &lt;dbl&gt;, d53 &lt;dbl&gt;,\n## #   d54 &lt;dbl&gt;, d55 &lt;dbl&gt;, d56 &lt;dbl&gt;, d57 &lt;dbl&gt;, d58 &lt;dbl&gt;, d59 &lt;dbl&gt;,\n## #   d60 &lt;dbl&gt;, d61 &lt;dbl&gt;, d62 &lt;dbl&gt;, d63 &lt;dbl&gt;, d64 &lt;dbl&gt;, d65 &lt;dbl&gt;,\n## #   d66 &lt;dbl&gt;, d67 &lt;dbl&gt;, d68 &lt;dbl&gt;, d69 &lt;dbl&gt;, d70 &lt;dbl&gt;, d71 &lt;dbl&gt;,\n## #   d72 &lt;dbl&gt;, d73 &lt;dbl&gt;, d74 &lt;dbl&gt;, d75 &lt;dbl&gt;, d76 &lt;dbl&gt;, d77 &lt;dbl&gt;,\n## #   d78 &lt;dbl&gt;, d79 &lt;dbl&gt;, d80 &lt;dbl&gt;, d81 &lt;dbl&gt;, d82 &lt;dbl&gt;, d83 &lt;dbl&gt;,\n## #   d84 &lt;dbl&gt;, d85 &lt;dbl&gt;, d86 &lt;dbl&gt;, d87 &lt;dbl&gt;, d88 &lt;dbl&gt;, d89 &lt;dbl&gt;,\n## #   d90 &lt;dbl&gt;, d91 &lt;dbl&gt;, d92 &lt;dbl&gt;, d93 &lt;dbl&gt;, d94 &lt;dbl&gt;, d95 &lt;dbl&gt;,\n## #   d96 &lt;dbl&gt;, d97 &lt;dbl&gt;, d98 &lt;dbl&gt;, d99 &lt;dbl&gt;, d100 &lt;dbl&gt;\nThe format of these word vectors is perfectly tailored to work with textrecipes. The first column has the tokens, and the remaining numerical columns are the word vectors.\nwe need to specify a tokenizer that closely matches the tokenizer that was used in the pre-trained word embedding. Otherwise, will you get mismatches between words. I was not able to find the same tokenizer used in this case. but I found that the default tokenizers::tokenize_words() with strip_punct = FALSE gives very similar results.\nwe can pass arguments to the underlying tokenizer in step_tokenize() by passing a named list to the options = argument.\nWe will be using the default method of aggregating the vectors within each observation which is to sum them together. This can be changed using the aggregation = argument.\nrec_spec &lt;- recipe(rating ~ age + title + review_text, data = reviews_train) %&gt;%\n  step_tokenize(title, review_text, options = list(strip_punct = FALSE)) %&gt;%\n  step_tokenmerge(title, review_text, prefix = \"text\") %&gt;%\n  step_word_embeddings(text, embeddings = glove6b)\nWe are using step_tokenmerge() to combine the tokens created in title and review_text into one list of tokens. There aren’t that many tokens in title alone for it to warrant treating it as a separate list of tokens.\nNext, we specify a lasso model.\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\nI have specified penalty = tune() because I want to use tune to find the best value of the penalty by doing hyperparameter tuning.\nWe set up a parameter grid using grid_regular()\nparam_grid &lt;- grid_regular(penalty(), levels = 50)\nsearching over 50 levels might seem like a lot. But remember that glmnet can calculate them all at once. This is because it relies on its warms starts for speed and it is often faster to fit a whole path than compute a single fit.\nWe will also set up some bootstraps of the data so we can evaluate the performance multiple times for each level.\nreviews_boot &lt;- bootstraps(reviews_train, times = 10)\nthe last thing we need to use is to create a workflow object to combine the preprocessing step with the model. This is important because we want the preprocessing steps to happen in the bootstraps.\nwf_fh &lt;- workflow() %&gt;%\n  add_recipe(rec_spec) %&gt;%\n  add_model(lasso_spec)\nnow we are ready to perform the parameter tuning.\nset.seed(42)\nlasso_grid &lt;- tune_grid(\n  wf_fh,\n  resamples = reviews_boot,\n  grid = param_grid\n) \nOnce we have finished parameter tuning we can use the autoplot() function on the tuning results to get a nice chart showing the performance for different values of the penalty.\nlasso_grid %&gt;%\n  autoplot()\n\nIt appears that the best value for the penalty for this workflow is on the low end.\nIt is worth noting that this data coming out of recipes is dense since we are using word vectors. We will only be having 100 predictors in this model (since we choose dimensions = 100). This is an order of magnitude less than the last time where we had 1024 sparse predictors. This more dense data representation also allows us to use models that we normally can’t use when doing count-based preprocessing since some models don’t handle sparseness that well.\nSimilarly, can we use the show_best() function from tune to show to the best performing hyperparameter.\nlasso_grid %&gt;%\n  show_best(\"roc_auc\")\n## # A tibble: 5 x 7\n##    penalty .metric .estimator  mean     n std_err .config              \n##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n## 1 3.24e- 5 roc_auc binary     0.821    10 0.00129 Preprocessor1_Model28\n## 2 1   e-10 roc_auc binary     0.821    10 0.00129 Preprocessor1_Model01\n## 3 1.60e-10 roc_auc binary     0.821    10 0.00129 Preprocessor1_Model02\n## 4 2.56e-10 roc_auc binary     0.821    10 0.00129 Preprocessor1_Model03\n## 5 4.09e-10 roc_auc binary     0.821    10 0.00129 Preprocessor1_Model04\nWe will use the select_best() function to extract the best performing penalty and finalize the workflow with that value of penalty.\nwf_fh_final &lt;- wf_fh %&gt;%\n  finalize_workflow(parameters = select_best(lasso_grid, \"roc_auc\"))\nNow we can run last_fit() on our training/testing split to fit our final model.\nfinal_res &lt;- last_fit(wf_fh_final, reviews_split)\nWith our final model can we create a ROC curve of our final model.\nfinal_res %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(rating, .pred_5) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html#thoughts",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html#thoughts",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "Thoughts 🤔",
    "text": "Thoughts 🤔\nThis example is mostly a showcase of how to use pre-trained word embeddings. We are not getting as good performance as we did with feature hashing of simple term frequencies from the last post.\nWe can improve the performance in a couple of ways. The first way is to use bigger embeddings. We can increase the number of dimensions we are using. This embedding comes with as many as 300 dimensions. We can also use an embedding with a larger vocabulary. I used the smallest pre-trained glove embedding, but bigger ones are available too.\nAnother way to get improved performance is to use word vectors that more closely matched the target domain you are working in. Many pre-trained word vectors and trained on text for the general web, Wikipedia and, news articles. The more removed your text is from these kinds of text, the less likely it is that the embedding will be helpful.\nI will showcase in a later post how you can train your own word embedding.\nIf you want to know more about word embeddings, Julia Silge and I have a chapter in our upcoming book Supervised Machine Learning for Text Analysis in R which goes more in dept on Word Embeddings."
  },
  {
    "objectID": "post/textrecipes-series-pretrained-word-embeddings/index.html#footnotes",
    "href": "post/textrecipes-series-pretrained-word-embeddings/index.html#footnotes",
    "title": "Textrecipes series: Pretrained Word Embedding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html",
    "href": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html",
    "title": "Using PCA for word embedding in R",
    "section": "",
    "text": "In my earlier post on binary text classification was one of the problems that occurred was the sheer size of the data when trying to fit a model. The bag of words method of having each column describe the occurrence of a specific word in each document (row) is appealing from a mathematical perspective but gives rise to large sparse matrices which aren’t handled well by some models in R. This leads to slow running code at best and crashing at worst.\nWe will try to combat this problem by using something called word embedding which is a general term for the process of mapping textural information to a lower-dimensional space. This is a special case of dimensionality reduction, and we will use the simple well-known method Principal component analysis for our word embedding today. We are essentially trying to squeeze as much information into as little space as possible such that our models can run in a reasonable time.\nWe will use the same data as in the earlier post, and the PCA procedure is very inspired by Julia Silge recent post Understanding PCA using Stack Overflow data which you should read if you haven’t already!!"
  },
  {
    "objectID": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#data-prepossessing",
    "href": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#data-prepossessing",
    "title": "Using PCA for word embedding in R",
    "section": "Data prepossessing",
    "text": "Data prepossessing\nWe will use the standard tidyverse toolset for this post. We will use randomForest model as this approach should be much faster.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(broom)\nlibrary(randomForest)\nThe data we will be using for this demonstration will be some English1 social media disaster tweets discussed in this article. It consists of several tweets regarding accidents mixed in with a selection of control tweets (not about accidents). We start by loading the data.\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv\")\nAnd for this exercise, we will only look at the body of the text. Furthermore, a handful of the tweets weren’t classified, marked \"Can't Decide\" so we are removing those as well. Since we are working with tweet data we have the constraint that most tweets don’t have that much information in them as they are limited in characters and some only contain a couple of words.\nWe will at this stage remove what appears to be URLs using some regex and str_replace_all, and we will select the columns id, disaster, and text.\ndata_clean &lt;- data %&gt;%\n  filter(choose_one != \"Can't Decide\") %&gt;%\n  mutate(id = `_unit_id`,\n         disaster = choose_one == \"Relevant\",\n         text = str_replace_all(text, \" ?(f|ht)tp(s?)://(.*)[.][a-z]+\", \"\")) %&gt;%\n  select(id, disaster, text)\nWe then extract all unigrams, bigrams and remove stopwords.\ndata_counts &lt;- map_df(1:2,\n                      ~ unnest_tokens(data_clean, word, text, \n                                      token = \"ngrams\", n = .x)) %&gt;%\n  anti_join(stop_words, by = \"word\")\nWe will only focus on the top 10000 most used words for the remainder of the analysis.\ntop10000 &lt;- data_counts %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  slice(1:10000) %&gt;%\n  select(word)\nwe will then count the words again, but this time we will count the word occurrence within each document and remove the underused words.\nunnested_words &lt;- data_counts %&gt;%\n  count(id, word, sort = TRUE) %&gt;%\n  inner_join(top10000, by = \"word\")\nWe then cast the data.frame to a sparse matrix.\nsparse_word_matrix &lt;- unnested_words %&gt;%\n  cast_sparse(id, word, n)\nIn the last post, we used this matrix for the modeling, but the size was quite an obstacle.\ndim(sparse_word_matrix)\n## [1] 10829 10000\nWe have a row for each document and a row for each of the top 10000 words, but most of the elements are empty so each of the variables doesn’t contain much information. We will do word embedding by applying PCA to the sparse word count matrix. Like Julia Silge we will use the wonderful irlba package that facilities PCA on sparse matrices. First, we scale the matrix and then we apply PCA where we request 64 columns.\nThis stage will take some time, but that is the trade-off we will be making when using word embedding. We take some computation time upfront in exchange for quick computation later down the line.\nword_scaled &lt;- scale(sparse_word_matrix)\nword_pca &lt;- irlba::prcomp_irlba(word_scaled, n = 64)\nThen we will create a meta data.frame to take care of tweets that disappeared when we cleaned them earlier.\nmeta &lt;- tibble(id = as.numeric(dimnames(sparse_word_matrix)[[1]])) %&gt;%\n  left_join(data_clean[!duplicated(data_clean$id), ], by = \"id\")\nNow we combine the PCA matrix with the proper response variable (disaster/non-disaster) with the addition of a training/testing split variable.\nclass_df &lt;- data.frame(word_pca$x) %&gt;%\n  mutate(response = factor(meta$disaster),\n         split = sample(0:1, NROW(meta), replace = TRUE, prob = c(0.2, 0.8)))\nWe now have a data frame with 64 explanatory variables instead of the 10000 we started with. This a huge reduction which hopefully should pay off. For this demonstration will we try using two kinds of models. Standard logistic regression and a random forest model. Logistic regression is a good baseline which should be blazing fast now since the reduction has taken place and the random forest model which generally was quite slow should be more manageable this time around.\nmodel &lt;- glm(response ~ ., \n             data = filter(class_df, split == 1), \n             family = binomial)\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\ny_pred &lt;- predict(model, \n                  type = \"response\",\n                  newdata = filter(class_df, split == 0) %&gt;% select(-response))\n## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\n## prediction from a rank-deficient fit may be misleading\n\ny_pred_logical &lt;- if_else(y_pred &gt; 0.5, 1, 0)\n(con &lt;- table(y_pred_logical, filter(class_df, split == 0) %&gt;% pull(response)))\n##               \n## y_pred_logical FALSE TRUE\n##              0  1163  591\n##              1    70  356\nsum(diag(con)) / sum(con)\n## [1] 0.696789\nit work fairly quickly and we get a decent accuracy of 70%. Remember this method isn’t meant to improve the accuracy but rather to improve the computational time.\nmodel &lt;- randomForest(response ~ ., \n                      data = filter(class_df, split == 1))\n\ny_pred &lt;- predict(model, \n                  type = \"class\",\n                  newdata = filter(class_df, split == 0) %&gt;% select(-response))\n\n(con &lt;- table(y_pred, filter(class_df, split == 0) %&gt;% pull(response)))\n##        \n## y_pred  FALSE TRUE\n##   FALSE  1086  383\n##   TRUE    147  564\nsum(diag(con)) / sum(con)\n## [1] 0.7568807\nThis one takes slightly longer to run due to the number of trees, but it does give us the nifty 76% accuracy which is pretty good considering we only look at tweets.\nAnd this is all that there is to it! The dimensionality reduction method was able to reduce the number of variables while retaining most of the information within those variables such that we can run our procedures at a faster phase without much loss. There is still a lot of individual improvements to be done if this was to be used further, both in terms of hyper-parameter selection in the modeling choices but also the number of PCA variables that should be used in the final modeling. Remember that this is just one of the simpler methods, with more advanced word representation methods being glove and word2vec."
  },
  {
    "objectID": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#data-viz",
    "href": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#data-viz",
    "title": "Using PCA for word embedding in R",
    "section": "Data viz",
    "text": "Data viz\nSince Julia did most of the legwork for the visualizations so we will take a look at how each of the words contributes to the first four components.\ntidied_pca &lt;- bind_cols(Tag = colnames(word_scaled),\n                        word_pca$rotation) %&gt;%\n    gather(PC, Contribution, PC1:PC4)\n\ntidied_pca %&gt;% \n    filter(PC %in% paste0(\"PC\", 1:4)) %&gt;%\n    ggplot(aes(Tag, Contribution, fill = Tag)) +\n    geom_col(show.legend = FALSE) +\n    theme(axis.text.x = element_blank(), \n          axis.ticks.x = element_blank()) + \n    labs(x = \"Words\",\n         y = \"Relative importance in each principal component\") +\n    facet_wrap(~ PC, ncol = 2)\n\nWhat we see is quite different then what Julia found in her study. We have just a few words doing most of the contributions in each of component. Lets zoom in to take a look at the words with the most influence on the different components:\nmap_df(c(-1, 1) * 20,\n    ~ tidied_pca %&gt;%\n        filter(PC == \"PC1\") %&gt;% \n        top_n(.x, Contribution)) %&gt;%\n    mutate(Tag = reorder(Tag, Contribution)) %&gt;%\n    ggplot(aes(Tag, Contribution, fill = Tag)) +\n    geom_col(show.legend = FALSE, alpha = 0.8) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), \n          axis.ticks.x = element_blank()) +\n    labs(x = \"Words\",\n         y = \"Relative importance in principle component\",\n         title = \"PC1\")\n\nWe would like to see some sensible separation between the positive words and the negative words (concerning contribution). However, I haven’t been able to come up with a meaningful full grouping for the first 3 components. The fourth on the other hand have all the positive influencing words containing numbers in one way or another.\nmap_df(c(-1, 1) * 20,\n    ~ tidied_pca %&gt;%\n        filter(PC == \"PC4\") %&gt;% \n        top_n(.x, Contribution)) %&gt;%\n    mutate(Tag = reorder(Tag, Contribution)) %&gt;%\n    ggplot(aes(Tag, Contribution, fill = Tag)) +\n    geom_col(show.legend = FALSE, alpha = 0.8) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), \n          axis.ticks.x = element_blank()) +\n    labs(x = \"Words\",\n         y = \"Relative importance in principle component\",\n         title = \"PC4\")\n\nThis is all I have for this time. Hope you enjoyed it!\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-13                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date       lib source                           \n assertthat     0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports      1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown       1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown       0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom        * 0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib          0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger     1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli            3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr          0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools      0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace     2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon         1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI            1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr         2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc           1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details      * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest         0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr        * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate       0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi          0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver         2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats      * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs             1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics       0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2      * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue           1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable         0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven          2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr          0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms            1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools      0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr           1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n janeaustenr    0.1.5   2017-06-10 [1] CRAN (R 4.1.0)                   \n jquerylib      0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite       1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr        * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling       0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lattice        0.20-44 2021-05-02 [1] CRAN (R 4.1.0)                   \n lifecycle      1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate      1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr       2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n Matrix         1.3-3   2021-05-04 [1] CRAN (R 4.1.0)                   \n modelr         0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar         1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png            0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6             2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n randomForest * 4.6-14  2018-03-25 [1] CRAN (R 4.1.0)                   \n Rcpp           1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr        * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl         1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex         2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang          0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown      2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot      2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi     0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rvest          1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass           0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales         1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n SnowballC      0.7.0   2020-04-01 [1] CRAN (R 4.1.0)                   \n stringi        1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr      * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble       * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr        * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect     1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidytext     * 0.3.1   2021-04-10 [1] CRAN (R 4.1.0)                   \n tidyverse    * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n tokenizers     0.2.1   2018-03-29 [1] CRAN (R 4.1.0)                   \n utf8           1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs          0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr          2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun           0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2           1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml           2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#footnotes",
    "href": "post/2018-05-22-using-pca-for-word-embedding-in-r/index.html#footnotes",
    "title": "Using PCA for word embedding in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16."
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#overview",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#overview",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Overview",
    "text": "Overview\nIn this post, we will\n\ntalk about The Federalist Papers\naccess and tidy the text using the tidytext package\napply our model to the data to predict the author of the disputed papers"
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#the-federalist-papers",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#the-federalist-papers",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "The Federalist Papers",
    "text": "The Federalist Papers\nIn the early days of The United States of America around the time when the Constitution was being signed did a series of articles were published in various newspapers. These papers were writing under the false name Publius. It was later revealed to have been the collected works of Alexander Hamilton, James Madison, and John Jay.\nThe Interesting thing in this was that the authorship of these papers was not consistent. This is where we come in, in this blog post will we try to see if we are able to classify the troublesome papers.\nIf you would like to read more about this story including past attempts to solve this problem please read How Statistics Solved a 175-Year-Old Mystery About Alexander Hamilton by Ben Christopher."
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#libraries",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#libraries",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Libraries",
    "text": "Libraries\nWe will start by loading the libraries which includes glmnet that will be used to construct the predictive model later.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(gutenbergr)\nlibrary(glmnet)"
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#data",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#data",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Data",
    "text": "Data\nWe are lucky today because all of The Federalist Papers happens to be on Gutenberg, which is written in English1\npapers &lt;- gutenberg_download(1404)\nhead(papers, n = 10)\n## # A tibble: 10 x 2\n##    gutenberg_id text                                                \n##           &lt;int&gt; &lt;chr&gt;                                               \n##  1         1404 \"THE FEDERALIST PAPERS\"                             \n##  2         1404 \"\"                                                  \n##  3         1404 \"By Alexander Hamilton, John Jay, and James Madison\"\n##  4         1404 \"\"                                                  \n##  5         1404 \"\"                                                  \n##  6         1404 \"\"                                                  \n##  7         1404 \"\"                                                  \n##  8         1404 \"FEDERALIST No. 1\"                                  \n##  9         1404 \"\"                                                  \n## 10         1404 \"General Introduction\"\nFor the predictive modeling we are going to do later, I would like to divide each paper up into sentences. This is a rather complicated affair, but I will take a rather ad hoc approach that will be good enough for the purpose of this post. We will do this by collapsing all the lines together and splitting them by ., ! and ?’s.\npapers_sentences &lt;- pull(papers, text) %&gt;% \n  str_c(collapse = \" \") %&gt;%\n  str_split(pattern = \"\\\\.|\\\\?|\\\\!\") %&gt;%\n  unlist() %&gt;%\n  tibble(text = .) %&gt;%\n  mutate(sentence = row_number())\nWe would like to assign each of these sentences to the corresponding article number and author. Thus we will first assign each of the 85 papers to the 3 authors and a group for the papers of interest.\nhamilton &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)\nmadison &lt;- c(10, 14, 18:20, 37:48)\njay &lt;- c(2:5, 64)\nunknown &lt;- c(49:58, 62:63)\nNext, we will simply look for lines that include “FEDERALIST No” as they would indicate the start of a paper and then label them accordingly.\npapers_words &lt;- papers_sentences %&gt;%\n  mutate(no = cumsum(str_detect(text, regex(\"FEDERALIST No\",\n                                            ignore_case = TRUE)))) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  mutate(author = case_when(no %in% hamilton ~ \"hamilton\",\n                            no %in% madison ~ \"madison\",\n                            no %in% jay ~ \"jay\",\n                            no %in% unknown ~ \"unknown\"),\n         id = paste(no, sentence, sep = \"-\"))\nlet us take a quick count before we move on\npapers_words %&gt;%\n  count(author)\n## # A tibble: 4 x 2\n##   author        n\n##   &lt;chr&gt;     &lt;int&gt;\n## 1 hamilton 114688\n## 2 jay        8539\n## 3 madison   45073\n## 4 unknown   24471\nWe see that Jay didn’t post as many articles as the other two gentlemen so we will exclude him from further analysis\npapers_words &lt;- papers_words %&gt;%\n  filter(author != \"jay\")"
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#predictive-modeling",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#predictive-modeling",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Predictive modeling",
    "text": "Predictive modeling\nTo make this predictive model we will use the term-frequency matrix as our input and as the response will be an indicator that Hamilton wrote the paper. For this modeling we will use the glmnet package which fits a generalized linear model via penalized maximum likelihood. It is quite fast and works great with sparse matrix input, hence the term-frequency matrix.\nThe response is set to the binomial family because of the binary nature of the response (did Hamilton write the sentence).\nFirst, we get the term-frequency matrix with the cast_ family in tidytext.\npapers_dtm &lt;- papers_words %&gt;%\n  count(id, word, sort = TRUE) %&gt;%\n  cast_sparse(id, word, n)\nWe will need to define a response variable, which we will do with a simple mutate, along with an indicator for our training set which will be the articles with known authors.\nmeta &lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %&gt;%\n  left_join(papers_words[!duplicated(papers_words$id), ], by = \"id\") %&gt;%\n  mutate(y = as.numeric(author == \"hamilton\"),\n         train = author != \"unknown\")\nWe will use cross-validation to obtain the best value of the model tuning parameter. This part takes a couple of minutes.\npredictor &lt;- papers_dtm[meta$train, ]\nresponse &lt;- meta$y[meta$train]\n\nmodel &lt;- cv.glmnet(predictor, response, family = \"binomial\", alpha = 0.9)\nAfter running the model, we will add the predicted values to our meta data.frame.\nmeta &lt;- meta %&gt;%\n  mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = \"response\",\n                        s = model$lambda.1se) %&gt;% as.numeric())\nIt is now time to visualize the results. First, we will look at how the training set have been separated.\nmeta %&gt;%\n  filter(train) %&gt;%\n  ggplot(aes(factor(no), pred)) + \n  geom_boxplot(aes(fill = author)) +\n  theme_minimal() +\n  labs(y = \"predicted probability\",\n       x = \"Article number\") +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"#304890\", \"#6A7E50\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nThe box plot of predicted probabilities, one value for each sentence, for the 68 papers by Alexander Hamilton and James Madison. The probability represents the extent to which the model believes the sentence was written by Alexander Hamilton.\nLet us see if this model can settle the dispute of the 12 papers. We will plot the predicted probabilities of the unknown papers alongside the training set.\nmeta %&gt;%\n  ggplot(aes(factor(no), pred)) + \n  geom_boxplot(aes(fill = author)) +\n  theme_minimal() +\n  labs(y = \"predicted probability\",\n       x = \"Article number\") +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"#304890\", \"#6A7E50\", \"#D6BBD0\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nwe notice that the predicted probabilities don’t quite make up able to determine who the original author is. This can be due to a variety of different reasons. One of them could be that Madison wrote them and Hamilton edited them.\nDespite the unsuccessful attempt to predict the secret author we still managed to showcase the method which while being unsuccessful in this case could provide useful in other cases."
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#working-showcase",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#working-showcase",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Working showcase",
    "text": "Working showcase\nSince the method proved unsuccessful in determining the secret author did I decide to add an example where the authorship is known. We will use the same data from earlier, only look at known Hamilton and Madison papers, train on some of them, and show that the algorithm is able to detect the authorship of the other.\npapers_dtm &lt;- papers_words %&gt;%\n  filter(author != \"unknown\") %&gt;%\n  count(id, word, sort = TRUE) %&gt;% \n  cast_dtm(id, word, n)\nhere we let the first 16 papers that they wrote be the test set and the rest be the training set.\nmeta &lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %&gt;%\n  left_join(papers_words[!duplicated(papers_words$id), ], by = \"id\") %&gt;%\n  mutate(y = as.numeric(author == \"hamilton\"),\n         train = no &gt; 20)\npredictor &lt;- papers_dtm[meta$train, ] %&gt;% as.matrix()\nresponse &lt;- meta$y[meta$train]\n\nmodel &lt;- cv.glmnet(predictor, response, family = \"binomial\", alpha = 0.9)\nmeta &lt;- meta %&gt;%\n  mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = \"response\",\n                        s = model$lambda.1se) %&gt;% as.numeric())\nmeta %&gt;%\n  ggplot(aes(factor(no), pred)) + \n  geom_boxplot(aes(fill = author)) +\n  theme_minimal() +\n  labs(y = \"predicted probability\",\n       x = \"Article number\") +\n  theme(legend.position = \"top\") +\n  scale_fill_manual(values = c(\"#304890\", \"#6A7E50\")) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  geom_vline(aes(xintercept = 16.5), color = \"red\")\n\nSo we see that while it isn’t as crystal clear what the test set predictions are giving us, they still give a pretty good indication.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source        \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.0)\n backports     1.2.1   2020-12-09 [1] CRAN (R 4.0.2)\n blogdown      1.3     2021-04-14 [1] CRAN (R 4.0.2)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.0.2)\n broom         0.7.6   2021-04-05 [1] CRAN (R 4.0.2)\n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)\n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.0.0)\n cli           3.0.0   2021-06-30 [1] CRAN (R 4.0.2)\n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.0.2)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.0.5)\n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.0.2)\n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.0.2)\n curl          4.3.2   2021-06-23 [1] CRAN (R 4.0.2)\n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.0.2)\n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.0.2)\n desc          1.3.0   2021-03-05 [1] CRAN (R 4.0.2)\n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)\n digest        0.6.27  2020-10-24 [1] CRAN (R 4.0.2)\n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.0.2)\n evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.0)\n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.0.2)\n farver        2.1.0   2021-02-28 [1] CRAN (R 4.0.2)\n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.0.2)\n foreach       1.5.1   2020-10-15 [1] CRAN (R 4.0.2)\n fs            1.5.0   2020-07-31 [1] CRAN (R 4.0.2)\n generics      0.1.0   2020-10-31 [1] CRAN (R 4.0.2)\n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.0.2)\n glmnet      * 4.1-1   2021-02-21 [1] CRAN (R 4.0.2)\n glue          1.4.2   2020-08-27 [1] CRAN (R 4.0.2)\n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.0.0)\n gutenbergr  * 0.1.5   2019-09-10 [1] CRAN (R 4.0.0)\n haven         2.4.1   2021-04-23 [1] CRAN (R 4.0.2)\n highr         0.9     2021-04-16 [1] CRAN (R 4.0.2)\n hms           1.1.0   2021-05-17 [1] CRAN (R 4.0.2)\n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)\n httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)\n iterators     1.0.13  2020-10-15 [1] CRAN (R 4.0.2)\n janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 4.0.0)\n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.0.2)\n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.0.2)\n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.0.2)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.0.2)\n lattice       0.20-41 2020-04-02 [1] CRAN (R 4.0.5)\n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.0.2)\n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.0.2)\n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.0.2)\n Matrix      * 1.3-2   2021-01-06 [1] CRAN (R 4.0.5)\n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.0.0)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.0.0)\n NLP           0.2-0   2018-10-18 [1] CRAN (R 4.0.0)\n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.0.2)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.0)\n png           0.1-7   2013-12-03 [1] CRAN (R 4.0.0)\n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)\n R6            2.5.0   2020-10-28 [1] CRAN (R 4.0.2)\n Rcpp          1.0.6   2021-01-15 [1] CRAN (R 4.0.2)\n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2)\n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.0.2)\n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.0.2)\n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.0.2)\n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.0.2)\n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.2)\n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.0.2)\n rvest         1.0.0   2021-03-09 [1] CRAN (R 4.0.2)\n sass          0.4.0   2021-05-12 [1] CRAN (R 4.0.2)\n scales        1.1.1   2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.0)\n shape         1.4.5   2020-09-13 [1] CRAN (R 4.0.2)\n slam          0.1-47  2019-12-21 [1] CRAN (R 4.0.0)\n SnowballC     0.7.0   2020-04-01 [1] CRAN (R 4.0.0)\n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.0.2)\n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.0.0)\n survival      3.2-10  2021-03-16 [1] CRAN (R 4.0.5)\n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.0.2)\n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.0.2)\n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.0.2)\n tidytext    * 0.3.1   2021-04-10 [1] CRAN (R 4.0.2)\n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.0.2)\n tm            0.7-7   2019-12-12 [1] CRAN (R 4.0.0)\n tokenizers    0.2.1   2018-03-29 [1] CRAN (R 4.0.0)\n triebeard     0.3.0   2016-08-04 [1] CRAN (R 4.0.0)\n urltools      1.7.3   2019-04-14 [1] CRAN (R 4.0.0)\n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.0.2)\n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.0.2)\n withr         2.4.2   2021-04-18 [1] CRAN (R 4.0.2)\n xfun          0.24    2021-06-15 [1] CRAN (R 4.0.2)\n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.0)\n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.0)\n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#footnotes",
    "href": "post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index.html#footnotes",
    "title": "Predicting authorship in The Federalist Papers with tidytext",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "",
    "text": "Caution\n\n\n\nThis post was written with early versions of tidymodels packages. And in some ways have not aged perfectly. The general idea about this post is still valid, but if you want more up-to-date code please refer to https://www.tidymodels.org/.\nIn this post, we will revisit one of my earlier blogposts where I tried to use tidytext and glmnet to predict the authorship of the anonymous Federalist Papers. If you want more information regarding the data, please read the old post. In the post, we will try to achieve the same goal but use the tidymodels framework."
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#loading-packages",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#loading-packages",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Loading Packages",
    "text": "Loading Packages\nlibrary(tidymodels) # Modeling framework\nlibrary(textrecipes) # extension to preprocessing engine to handle text\nlibrary(stringr) # String modification\nlibrary(gutenbergr) # Portal to download the Federalist Papers\nlibrary(tokenizers) # Tokenization engine\nlibrary(furrr) # to be able to fit the models in parallel"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#fetching-the-data",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#fetching-the-data",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Fetching the Data",
    "text": "Fetching the Data\nThe text is provided from the Gutenberg Project. A simple search reveals that the Federalist Papers have the id of 1404. Note that the text is in English1.\npapers &lt;- gutenberg_download(1404)\npapers"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#shaping-data",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#shaping-data",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "shaping data",
    "text": "shaping data\nThis is the first we will deviate from the original post in that we will divide the text into paragraphs instead of sentences as we did in the last post. Hopefully, this will strike a good balance between the size of each observation and the number of observations.\nIn the following pipe we: - pull() out the text vector - paste together the strings with \\n to denote line-breaks - tokenize into paragraphs - put it in a tibble - create a variable no to denote which paper the paragraph is in - add author variable to denote the author - remove preamble text\n# attribution numbers\nhamilton &lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)\nmadison &lt;- c(10, 14, 18:20, 37:48)\njay &lt;- c(2:5, 64)\nunknown &lt;- c(49:58, 62:63)\n\npapers_paragraphs &lt;- papers %&gt;%\n  pull(text) %&gt;%\n  str_c(collapse = \"\\n\") %&gt;%\n  tokenize_paragraphs() %&gt;%\n  unlist() %&gt;%\n  tibble(text = .) %&gt;%\n  mutate(no = cumsum(str_detect(text, regex(\"FEDERALIST No\",\n                                            ignore_case = TRUE)))) %&gt;%\n  mutate(author = case_when(no %in% hamilton ~ \"hamilton\",\n                            no %in% madison ~ \"madison\",\n                            no %in% jay ~ \"jay\",\n                            no %in% unknown ~ \"unknown\")) %&gt;%\n  filter(no &gt; 0)"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#class-balance",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#class-balance",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Class Balance",
    "text": "Class Balance\nThere is quite a bit of imbalance between the classes. For the remainder of the analysis will we exclude all the papers written by Jay, partly because it is a small class, but more importantly because he isn’t suspected to be the mystery author.\npapers_paragraphs %&gt;%\n  count(author) %&gt;%\n  ggplot(aes(author, n)) +\n  geom_col()\nIt is worth remembering that we don’t have the true answer, much more like in real-world problems."
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#splitting-the-data",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#splitting-the-data",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nHere we will use the rsample package to split the data into testing, validation, and training data sets. We will let the testing dataset be all the paragraphs where author == \"unknown\" and the training and validation datasets being the paragraphs written by Hamilton and Madison. intial_split() will insure that each dataset has the same proportions with respect to the author column.\ndata_split &lt;- papers_paragraphs %&gt;%\n  filter(author %in% c(\"hamilton\", \"madison\")) %&gt;%\n  initial_split(strata = author)\n\ntraining_data &lt;- training(data_split)\n\nvalidation_data &lt;- testing(data_split)\n\ntesting_data &lt;- papers_paragraphs %&gt;%\n  filter(author == \"unknown\")"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#specifying-data-preprocessing",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#specifying-data-preprocessing",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "specifying data preprocessing",
    "text": "specifying data preprocessing\nWe will go with a rather simple preprocessing. start by specifying a recipe where author is to be predicted, and we only want to use the text data. Here we make sure to use the training dataset. We then\n\ntokenize according to (n-grams)[https://www.tidytextmining.com/ngrams.html]\nonly keep the 250 most frequent tokens\ncalculate the (term frequency-inverse document frequency)[https://en.wikipedia.org/wiki/Tf%E2%80%93idf]\nup-sample the observation to achieve class balance\n\nand finally prep the recipe.\nrec &lt;- recipe(author ~ text, data = training_data) %&gt;%\n  step_tokenize(text, token = \"ngrams\", options = list(n = 3)) %&gt;%\n  step_tokenfilter(text, max_tokens = 250) %&gt;%\n  step_tfidf(text) %&gt;%\n  step_upsample(author) %&gt;%\n  prep()"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#apply-preprocessing",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#apply-preprocessing",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Apply Preprocessing",
    "text": "Apply Preprocessing\nNow we apply the prepped recipe to get back the processed datasets. Note that I have used shorter names for processed datasets (train_data vs training_data).\ntrain_data &lt;- juice(rec)\nval_data &lt;- bake(rec, new_data = validation_data)\ntest_data &lt;- bake(rec, new_data = testing_data)"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#fitting-the-models",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#fitting-the-models",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Fitting the Models",
    "text": "Fitting the Models\nThis time I’m going to try to run some (random forests)[https://en.wikipedia.org/wiki/Random_forest]. And that would be fairly easy to use. First, we specify the model type (rand_forest) the type (classification), and the engine (randomForest). Next, we fit the model to the training dataset, predict it on the validation datasets, add the true value and calculate the accuracy\nrand_forest(\"classification\") %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  fit(author ~ ., data = train_data) %&gt;%\n  predict(new_data = val_data) %&gt;%\n  mutate(truth = val_data$author) %&gt;%\n  accuracy(truth, .pred_class)\nHowever, we want to try some different hyperparameter values to make sure we are using the best we can. The dials allow us to do hyper-parameter searching in a fairly easy way. First, we will create a parameter_grid, where we will vary the number of trees in our forest (trees()) and the number of predictors to be randomly sampled. We give it some reasonable ranges and say that we want 5 levels for each parameter, resulting in 5 * 5 = 25 parameter pairs.\nparam_grid &lt;- grid_regular(range_set(trees(), c(50, 250)), \n                           range_set(mtry(), c(1, 15)), levels = 5)\nNext, we create a model specification where we use varying() to denote that these parameters are to be varying. Then we merge() the model specification into the parameter grid such that we have a tibble of model specifications\nrf_spec &lt;- rand_forest(\"classification\", mtry = varying(), trees = varying()) %&gt;%\n  set_engine(\"randomForest\")\n\nparam_grid &lt;- param_grid %&gt;%\n  mutate(specs = merge(., rf_spec))\n\nparam_grid\nNext, we want to iterate through the model specification. We will here create a function that will take a model specification, fit it to the training data, predict according to the validation data, calculate the accuracy and return it as a single number. Create this function makes so we can use map() over all the model specifications.\nfit_one_spec &lt;- function(model) {\n  model %&gt;%\n    fit(author ~ ., data = train_data) %&gt;%\n    predict(new_data = val_data) %&gt;%\n    mutate(truth = val_data$author) %&gt;%\n    accuracy(truth, .pred_class) %&gt;%\n    pull(.estimate)\n}\nWhile this is a fairly small dataset, I’ll showcase how we can parallelize the calculations. Since we have a framework where are we map()’ing over the specification it is an obvious case for the furrr package. (if you don’t want or isn’t able to to run your models on multiple cores, simply delete plan(multicore) and turn future_map_dbl() to map_dbl()).\nplan(multicore)\nfinal &lt;- param_grid %&gt;%\n  mutate(accuracy = future_map_dbl(specs, fit_one_spec))\nNow we can try to visualize the optimal hyper-parameters\nfinal %&gt;%\n  mutate_at(vars(trees:mtry), factor) %&gt;%\n  ggplot(aes(mtry, trees, fill = accuracy)) +\n  geom_tile() +\n  scale_fill_viridis_c()\nand we see that only having 1 predictor to split with it is sub-optimal, but otherwise having a low number of predictors are to be preferred. We can use arrange() to look at the top parameter pairs\narrange(final, desc(accuracy)) %&gt;%\n  slice(1:5)\nand we pick trees == 100 and mtry == 4 as our hyper-parameters. And we use these to fit our final model\nfinal_model &lt;- rf_spec %&gt;%\n  update(trees = 100, mtry = 4) %&gt;%\n  fit(author ~ ., data = train_data)"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#predicting-the-unknown-papers",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#predicting-the-unknown-papers",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Predicting the unknown papers",
    "text": "Predicting the unknown papers\nLastly, we predict on the unknown papers.\nfinal_predict &lt;- testing_data %&gt;% \n  bind_cols(predict(final_model, new_data = test_data)) \nWe can’t calculate accuracy or any other metric, as we don’t know the true value. However, we can see how the different paragraphs have been classified within each paper.\nfinal_predict %&gt;%\n  count(no, .pred_class) %&gt;%\n  mutate(no = factor(no)) %&gt;%\n  group_by(no) %&gt;%\n  mutate(highest = n == max(n))\nNow we can visualize the results, and it looks like from this limited analysis that Hamilton is the author of mysterious papers.\nfinal_predict %&gt;%\n  count(no, .pred_class) %&gt;%\n  mutate(no = factor(no),\n         .pred_class = factor(.pred_class, \n                              levels = c(\"hamilton\", \"madison\"), \n                              labels = c(\"Hamilton\", \"Madison\"))) %&gt;%\n  group_by(no) %&gt;%\n  mutate(highest = n == max(n)) %&gt;%\n  ggplot(aes(no, n, fill = .pred_class, alpha = highest)) +\n  scale_alpha_ordinal(range = c(0.5, 1)) +\n  geom_col(position = \"dodge\", color = \"black\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#304890\", \"#6A7E50\")) +\n  guides(alpha = \"none\") +\n  theme(legend.position = \"top\") +\n  labs(x = \"Federalist Paper Number\",\n       y = \"Number of paragraphs\",\n       fill = \"Author\",\n       title = \"Hamilton were predicted more often to be the author then\\nMadison in all but 1 Paper\")"
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#conclusion",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#conclusion",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we have touched on a lot of different topics; tidymodels, text preprocessing, parallel computing, etc. Since we have covered so many topics have left each section not covered in a lot of detail. In a more proper analysis, you would want to try some different models and different ways to do the preprocessing."
  },
  {
    "objectID": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#footnotes",
    "href": "post/2019-08-09-authorship-classification-with-tidymodels-and-textrecipes/index.html#footnotes",
    "title": "Authorship classification with tidymodels and textrecipes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/textrecipes-series-tf/index.html",
    "href": "post/textrecipes-series-tf/index.html",
    "title": "Textrecipes series: Term Frequency",
    "section": "",
    "text": "This is the first blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes. This post will we start simple with term frequencies.\nToday we are lucky with timing to be able to use the current #tidytuesday dataset about Animal Crossing - New Horizons. There are a lot of different datasets available for us this week but we will only be looking at the user reviews."
  },
  {
    "objectID": "post/textrecipes-series-tf/index.html#packages",
    "href": "post/textrecipes-series-tf/index.html#packages",
    "title": "Textrecipes series: Term Frequency",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe are going fairly light package-wise this time only needing tidymodels, textrecipes, and lastly tidytext for EDA.\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tidytext)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "post/textrecipes-series-tf/index.html#exploring-the-data",
    "href": "post/textrecipes-series-tf/index.html#exploring-the-data",
    "title": "Textrecipes series: Term Frequency",
    "section": "Exploring the data ⛏",
    "text": "Exploring the data ⛏\nWe start by reading in the data. I want to set the goal of this modeling task to predict if a review is positive or not. More specifically I (somehow arbitrarily) denote a review with a grade of 8 or higher to be a “High” review and everything else “Low”. I split the data into a training and testing dataset right away before looking at the data.\nuser_reviews &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')\nuser_reviews &lt;- user_reviews %&gt;%\n  mutate(grade = factor(grade &gt; 7, c(TRUE, FALSE), c(\"High\", \"Low\")))\n\nset.seed(1234)\nreview_split &lt;- initial_split(user_reviews)\n\nreview_training &lt;- training(review_split)\nreview_testing &lt;- training(review_split)\nTaking a glimpse() of the data reveals 4 variables. grade is a factor with the two levels “High” and “Low”. This is what we are trying to predict. Next, we have user_name which we won’t be using for this analysis, text is the most important variable as it contains the reviews. We also get a date variable denoting the date the review was submitted.\nglimpse(review_training)\n## Rows: 2,250\n## Columns: 4\n## $ grade     &lt;fct&gt; Low, Low, Low, Low, Low, Low, Low, Low, Low, Low, Low, Low,…\n## $ user_name &lt;chr&gt; \"mds27272\", \"lolo2178\", \"Roachant\", \"Houndf\", \"ProfessorFox…\n## $ text      &lt;chr&gt; \"My gf started playing before me. No option to create my ow…\n## $ date      &lt;date&gt; 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-20…\nWe First look at the distribution of “High” and “Low” scoring reviews.\nreview_training %&gt;%\n  ggplot(aes(grade)) +\n  geom_bar()\n\nit is slightly skewed but we will soldier on and ignore it. Next, let us take a look at the distribution of the dates. Remember that the game release on March 20th of this year, so we can plot the distribution of days since release. This provides us with an upper bound of how many days they have access to the game before leaving the review.\nreview_training %&gt;%\n  transmute(date = as.numeric(date - as.Date(\"2020-03-20\"))) %&gt;%\n  ggplot(aes(date)) +\n  geom_bar() +\n  labs(title = \"Number of days since release\")\n\nWe see a spike on the 1st day as well as the 5th. This is a little worrisome considering Animal Crossing games tend to be played casually over a long period of time and are tend to be slow and take a long time to beat.\nLastly, let us get ourselves some summary stats of the text itself.\nreview_tokens &lt;- review_training %&gt;%\n  unnest_tokens(tokens, text)\nWe can look at the distribution of the number of tokens in the reviews\nreview_tokens %&gt;%\n  count(user_name) %&gt;%\n  ggplot(aes(n)) +\n  geom_histogram(binwidth = 10) +\n  geom_vline(xintercept = 100, color = \"firebrick\") +\n  annotate(\"text\", x = 105, y = 205, hjust = 0, color = \"firebrick\",\n           label = \"Sharp clif between reviews with less and more then 100 word\") +\n  labs(title = \"Distribution of number of words in user reviews\")\n\nThis is a highly bimodal distribution. This is something we should include in our model. But we are sadly too early in our textrecipes series to address this kind of preprocessing, in a later post will we take a look at step_textfeature() that can do that."
  },
  {
    "objectID": "post/textrecipes-series-tf/index.html#modeling",
    "href": "post/textrecipes-series-tf/index.html#modeling",
    "title": "Textrecipes series: Term Frequency",
    "section": "Modeling ⚙️",
    "text": "Modeling ⚙️\nIn the preprocessing, we are including both the date and text variables. There are many different things we could do with the data variable. I’ll go simple and calculate the difference in time between the release day and the date the review was submitted. For the text we will again go simple, I’ll start by tokenizing to words with the default tokenizers engine. Next, we will remove stopwords, being conservative to only use the snowball stopword list which removes the least amount of words. Let us take a look at the words we are trying to remove to verify they ate appropriate.\nstopwords::data_stopwords_snowball$en\n##   [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n##   [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n##  [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n##  [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n##  [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n##  [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n##  [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n##  [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n##  [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n##  [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n##  [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n##  [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n##  [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n##  [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n##  [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n##  [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n##  [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n##  [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n##  [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n##  [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n## [101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n## [106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n## [111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n## [116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n## [121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n## [126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n## [131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n## [136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n## [141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n## [146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n## [151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n## [156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n## [161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n## [166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n## [171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"\nThe list contains a good amount of pronouns and negations, but considering the subject matter, I don’t think it will introduce much bias. We will apply a step_tokenfilter() to filter the tokens we keep based on frequency. We use tune() to indicate that we want to do hyperparameter tuning to determine the optimal number of tokens to keep. We end our recipe with step_tf() to calculate term frequencies from the tokens we kept.\nrec_spec &lt;- recipe(grade ~ text + date, review_training) %&gt;%\n  # Days since release\n  step_mutate(date = as.numeric(date - as.Date(\"2020-03-20\"))) %&gt;%\n  # Tokenize to words\n  step_tokenize(text) %&gt;%\n  # Remove stopwords\n  step_stopwords(text) %&gt;%\n  # Remove less frequent words\n  step_tokenfilter(text, max_tokens = tune()) %&gt;%\n  # Calculate term frequencies\n  step_tf(text, weight_scheme = \"binary\")\nstep_tf() has one main argument weight_scheme which determines how the term frequencies should be represented. I will be using “binary” This will return a 1 if the word is present in the review an 0. This is a kind of scaling since we are assuming that having the word “good” in the document once is providing as much evidence as if it appeared 10 times. See ?step_tf() for more detail and other options.\nThe modeling will be using a radial basis function support vector machines (SVM).\nmod_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\nWe will be tuning both the cost and rbf_sigma arguments.\nTo calculate the performance over the hyperparameter space will we do some V-fold Cross-Validation on our data.\nset.seed(1234)\nreview_folds &lt;- vfold_cv(review_training, v = 5)\nNow we are pretty much ready to run our model. We combine our model specification with the recipe specification to create a workflow object. This way we can tune the recipe and model jointly.\nreview_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_spec) %&gt;%\n  add_model(mod_spec)\nWith everything in place will we go to tune_grid() to perform the model tuning via a grid search.\ntune_res &lt;- tune_grid(\n  review_wf,\n  resamples = review_folds,\n  grid = 25,\n  control = control_grid(verbose = TRUE)\n)\nNow that we have the rune results we can find the best candidates we can simply use show_best()\nshow_best(tune_res, \"accuracy\")\n## # A tibble: 5 x 8\n##      cost rbf_sigma max_tokens .metric  .estimator  mean     n std_err\n##     &lt;dbl&gt;     &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1  1.31    0.000382        876 accuracy binary     0.891     5 0.00337\n## 2  2.75    0.000164        314 accuracy binary     0.864     5 0.00575\n## 3  0.286   0.00113         193 accuracy binary     0.838     5 0.00504\n## 4 24.3     0.109           620 accuracy binary     0.642     5 0.00897\n## 5  0.0918  0.00185         736 accuracy binary     0.641     5 0.00879\nWe notice that the top candidates have very similar .estimators but the 3 parameters vary quite a bit. Lets do a little vizualizations to see what is happening:\ncollect_metrics(tune_res) %&gt;%\n  filter(.metric == \"accuracy\") %&gt;%\n  ggplot(aes(cost, rbf_sigma, size = max_tokens, color = mean)) +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_log10() +\n  scale_color_viridis_c()\n\nwhen we are evaluating the accuracy it seems that a combination of high rbf_sigma and high cost yields to high accuracy.\nWhen we are evaluating the roc_auc then it appears that both max_tokens and cost don’t have too much influence and a high value of rbf_sigma is really bad.\ncollect_metrics(tune_res) %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  ggplot(aes(cost, rbf_sigma, size = max_tokens, color = mean)) +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_log10() +\n  scale_color_viridis_c()\n\nI’ll select a model with the highest accuracy in this use case. We can do that with the select_best() function. Then we can use the finalize_workflow() function to update our workflow with the new hyperparameters.\nbest_accuracy &lt;- select_best(tune_res, \"accuracy\")\n\nfinal_wf &lt;- finalize_workflow(\n  review_wf,\n  best_accuracy\n)\n\nfinal_wf\n## ══ Workflow ════════════════════════════════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_rbf()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## ● step_mutate()\n## ● step_tokenize()\n## ● step_stopwords()\n## ● step_tokenfilter()\n## ● step_tf()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────────────────────────────────\n## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = 1.31257376115796\n##   rbf_sigma = 0.000382104630228968\n## \n## Computational engine: kernlab\nlastly, we will do a list fit and see how well the final model did on our testing data:\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(review_split)\n\nfinal_res %&gt;%\n  collect_metrics()\n## # A tibble: 2 x 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.869\n## 2 roc_auc  binary         0.935\nWe did decently well on both accounts.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.0 (2019-04-26)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2020-05-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package       * version    date       lib source                            \n P assertthat      0.2.1      2019-03-21 [?] CRAN (R 3.6.0)                    \n P backports       1.1.6      2020-04-05 [?] CRAN (R 3.6.0)                    \n P base64enc       0.1-3      2015-07-28 [?] CRAN (R 3.6.0)                    \n P bayesplot       1.7.1      2019-12-01 [?] CRAN (R 3.6.0)                    \n P blogdown        0.18       2020-03-04 [?] CRAN (R 3.6.0)                    \n P bookdown        0.18       2020-03-05 [?] CRAN (R 3.6.0)                    \n P boot            1.3-24     2019-12-20 [?] CRAN (R 3.6.0)                    \n P broom         * 0.5.6      2020-04-20 [?] CRAN (R 3.6.2)                    \n P callr           3.4.3      2020-03-28 [?] CRAN (R 3.6.2)                    \n P class           7.3-16     2020-03-25 [?] CRAN (R 3.6.0)                    \n P cli             2.0.2      2020-02-28 [?] CRAN (R 3.6.0)                    \n P clipr           0.7.0      2019-07-23 [?] CRAN (R 3.6.0)                    \n P codetools       0.2-16     2018-12-24 [?] CRAN (R 3.6.0)                    \n P colorspace      1.4-1      2019-03-18 [?] CRAN (R 3.6.0)                    \n P colourpicker    1.0        2017-09-27 [?] CRAN (R 3.6.0)                    \n   crayon          1.3.4      2017-09-16 [1] CRAN (R 3.6.0)                    \n P crosstalk       1.1.0.1    2020-03-13 [?] CRAN (R 3.6.0)                    \n P curl            4.3        2019-12-02 [?] CRAN (R 3.6.0)                    \n P desc            1.2.0      2018-05-01 [?] CRAN (R 3.6.0)                    \n P details       * 0.2.1      2020-01-12 [?] CRAN (R 3.6.0)                    \n P dials         * 0.0.6      2020-04-03 [?] CRAN (R 3.6.0)                    \n P DiceDesign      1.8-1      2019-07-31 [?] CRAN (R 3.6.0)                    \n P digest          0.6.25     2020-02-23 [?] CRAN (R 3.6.0)                    \n P dplyr         * 0.8.5      2020-03-07 [?] CRAN (R 3.6.0)                    \n P DT              0.13       2020-03-23 [?] CRAN (R 3.6.0)                    \n P dygraphs        1.1.1.6    2018-07-11 [?] CRAN (R 3.6.0)                    \n P ellipsis        0.3.0      2019-09-20 [?] CRAN (R 3.6.0)                    \n   emo             0.0.0.9000 2020-05-05 [1] Github (Hadley/emo@3f03b11)       \n P evaluate        0.14       2019-05-28 [?] CRAN (R 3.6.0)                    \n P fansi           0.4.1      2020-01-08 [?] CRAN (R 3.6.0)                    \n P farver          2.0.3      2020-01-16 [?] CRAN (R 3.6.0)                    \n P fastmap         1.0.1      2019-10-08 [?] CRAN (R 3.6.0)                    \n P foreach         1.5.0      2020-03-30 [?] CRAN (R 3.6.2)                    \n P fs              1.4.1      2020-04-04 [?] CRAN (R 3.6.0)                    \n P furrr           0.1.0      2018-05-16 [?] CRAN (R 3.6.0)                    \n P future          1.17.0     2020-04-18 [?] CRAN (R 3.6.2)                    \n P generics        0.0.2      2018-11-29 [?] CRAN (R 3.6.0)                    \n P ggplot2       * 3.3.0      2020-03-05 [?] CRAN (R 3.6.0)                    \n P ggridges        0.5.2      2020-01-12 [?] CRAN (R 3.6.0)                    \n P globals         0.12.5     2019-12-07 [?] CRAN (R 3.6.0)                    \n P glue            1.4.0      2020-04-03 [?] CRAN (R 3.6.0)                    \n P gower           0.2.1      2019-05-14 [?] CRAN (R 3.6.0)                    \n P GPfit           1.0-8      2019-02-08 [?] CRAN (R 3.6.0)                    \n P gridExtra       2.3        2017-09-09 [?] CRAN (R 3.6.0)                    \n P gtable          0.3.0      2019-03-25 [?] CRAN (R 3.6.0)                    \n P gtools          3.8.2      2020-03-31 [?] CRAN (R 3.6.2)                    \n P hardhat         0.1.2      2020-02-28 [?] CRAN (R 3.6.0)                    \n P hms             0.5.3      2020-01-08 [?] CRAN (R 3.6.0)                    \n P htmltools       0.4.0      2019-10-04 [?] CRAN (R 3.6.0)                    \n P htmlwidgets     1.5.1      2019-10-08 [?] CRAN (R 3.6.0)                    \n P httpuv          1.5.2      2019-09-11 [?] CRAN (R 3.6.0)                    \n P httr            1.4.1      2019-08-05 [?] CRAN (R 3.6.0)                    \n P igraph          1.2.5      2020-03-19 [?] CRAN (R 3.6.0)                    \n P infer         * 0.5.1      2019-11-19 [?] CRAN (R 3.6.0)                    \n P inline          0.3.15     2018-05-18 [?] CRAN (R 3.6.0)                    \n P ipred           0.9-9      2019-04-28 [?] CRAN (R 3.6.0)                    \n P iterators       1.0.12     2019-07-26 [?] CRAN (R 3.6.0)                    \n P janeaustenr     0.1.5      2017-06-10 [?] CRAN (R 3.6.0)                    \n P kernlab         0.9-29     2019-11-12 [?] CRAN (R 3.6.0)                    \n P knitr         * 1.28       2020-02-06 [?] CRAN (R 3.6.0)                    \n P labeling        0.3        2014-08-23 [?] CRAN (R 3.6.0)                    \n P later           1.0.0      2019-10-04 [?] CRAN (R 3.6.0)                    \n P lattice         0.20-41    2020-04-02 [?] CRAN (R 3.6.0)                    \n P lava            1.6.7      2020-03-05 [?] CRAN (R 3.6.0)                    \n P lhs             1.0.2      2020-04-13 [?] CRAN (R 3.6.2)                    \n P lifecycle       0.2.0      2020-03-09 [?] Github (r-lib/lifecycle@1b13d96)  \n P listenv         0.8.0      2019-12-05 [?] CRAN (R 3.6.0)                    \n P lme4            1.1-23     2020-04-07 [?] CRAN (R 3.6.0)                    \n P loo             2.2.0      2019-12-19 [?] CRAN (R 3.6.0)                    \n P lubridate       1.7.8      2020-04-06 [?] CRAN (R 3.6.0)                    \n P magrittr        1.5        2014-11-22 [?] CRAN (R 3.6.0)                    \n P markdown        1.1        2019-08-07 [?] CRAN (R 3.6.0)                    \n P MASS            7.3-51.5   2019-12-20 [?] CRAN (R 3.6.0)                    \n P Matrix          1.2-18     2019-11-27 [?] CRAN (R 3.6.0)                    \n P matrixStats     0.56.0     2020-03-13 [?] CRAN (R 3.6.0)                    \n P mime            0.9        2020-02-04 [?] CRAN (R 3.6.0)                    \n   miniUI          0.1.1.1    2018-05-18 [1] CRAN (R 3.6.0)                    \n P minqa           1.2.4      2014-10-09 [?] CRAN (R 3.6.0)                    \n P munsell         0.5.0      2018-06-12 [?] CRAN (R 3.6.0)                    \n P nlme            3.1-145    2020-03-04 [?] CRAN (R 3.6.0)                    \n P nloptr          1.2.2.1    2020-03-11 [?] CRAN (R 3.6.0)                    \n P nnet            7.3-13     2020-02-25 [?] CRAN (R 3.6.0)                    \n P parsnip       * 0.1.0      2020-04-09 [?] CRAN (R 3.6.2)                    \n P pillar          1.4.3      2019-12-20 [?] CRAN (R 3.6.0)                    \n P pkgbuild        1.0.7      2020-04-25 [?] CRAN (R 3.6.2)                    \n P pkgconfig       2.0.3      2019-09-22 [?] CRAN (R 3.6.0)                    \n P plyr            1.8.6      2020-03-03 [?] CRAN (R 3.6.0)                    \n P png             0.1-7      2013-12-03 [?] CRAN (R 3.6.0)                    \n P prettyunits     1.1.1      2020-01-24 [?] CRAN (R 3.6.0)                    \n P pROC            1.16.2     2020-03-19 [?] CRAN (R 3.6.0)                    \n P processx        3.4.2      2020-02-09 [?] CRAN (R 3.6.0)                    \n P prodlim         2019.11.13 2019-11-17 [?] CRAN (R 3.6.0)                    \n P promises        1.1.0      2019-10-04 [?] CRAN (R 3.6.0)                    \n P ps              1.3.2      2020-02-13 [?] CRAN (R 3.6.0)                    \n P purrr         * 0.3.4      2020-04-17 [?] CRAN (R 3.6.2)                    \n P R6              2.4.1      2019-11-12 [?] CRAN (R 3.6.0)                    \n P Rcpp            1.0.4.6    2020-04-09 [?] CRAN (R 3.6.0)                    \n P readr           1.3.1      2018-12-21 [?] CRAN (R 3.6.0)                    \n P recipes       * 0.1.12     2020-05-01 [?] CRAN (R 3.6.2)                    \n   renv            0.9.3      2020-02-10 [1] CRAN (R 3.6.0)                    \n P reshape2        1.4.4      2020-04-09 [?] CRAN (R 3.6.2)                    \n P rlang           0.4.6      2020-05-02 [?] CRAN (R 3.6.2)                    \n P rmarkdown       2.1        2020-01-20 [?] CRAN (R 3.6.0)                    \n P rpart           4.1-15     2019-04-12 [?] CRAN (R 3.6.0)                    \n P rprojroot       1.3-2      2018-01-03 [?] CRAN (R 3.6.0)                    \n P rsample       * 0.0.6      2020-03-31 [?] CRAN (R 3.6.2)                    \n P rsconnect       0.8.16     2019-12-13 [?] CRAN (R 3.6.0)                    \n P rstan           2.19.3     2020-02-11 [?] CRAN (R 3.6.0)                    \n P rstanarm        2.19.3     2020-02-11 [?] CRAN (R 3.6.0)                    \n P rstantools      2.0.0      2019-09-15 [?] CRAN (R 3.6.0)                    \n   rstudioapi      0.11       2020-02-07 [1] CRAN (R 3.6.0)                    \n P scales        * 1.1.0      2019-11-18 [?] CRAN (R 3.6.0)                    \n P sessioninfo     1.1.1      2018-11-05 [?] CRAN (R 3.6.0)                    \n   shiny           1.4.0.2    2020-03-13 [1] CRAN (R 3.6.0)                    \n P shinyjs         1.1        2020-01-13 [?] CRAN (R 3.6.0)                    \n P shinystan       2.5.0      2018-05-01 [?] CRAN (R 3.6.0)                    \n P shinythemes     1.1.2      2018-11-06 [?] CRAN (R 3.6.0)                    \n P SnowballC       0.7.0      2020-04-01 [?] CRAN (R 3.6.2)                    \n P StanHeaders     2.21.0-1   2020-01-19 [?] CRAN (R 3.6.0)                    \n P statmod         1.4.34     2020-02-17 [?] CRAN (R 3.6.0)                    \n P stopwords       2.0        2020-04-14 [?] CRAN (R 3.6.2)                    \n P stringi         1.4.6      2020-02-17 [?] CRAN (R 3.6.0)                    \n P stringr         1.4.0      2019-02-10 [?] CRAN (R 3.6.0)                    \n P survival        3.1-12     2020-03-28 [?] Github (therneau/survival@c55af18)\n P textrecipes   * 0.2.1      2020-05-04 [?] CRAN (R 3.6.0)                    \n P threejs         0.3.3      2020-01-21 [?] CRAN (R 3.6.0)                    \n P tibble        * 3.0.1      2020-04-20 [?] CRAN (R 3.6.2)                    \n P tidymodels    * 0.1.0      2020-02-16 [?] CRAN (R 3.6.0)                    \n P tidyposterior   0.0.2      2018-11-15 [?] CRAN (R 3.6.0)                    \n P tidypredict     0.4.5      2020-02-10 [?] CRAN (R 3.6.0)                    \n P tidyr           1.0.2      2020-01-24 [?] CRAN (R 3.6.0)                    \n P tidyselect      1.0.0      2020-01-27 [?] CRAN (R 3.6.0)                    \n P tidytext      * 0.2.4      2020-04-17 [?] CRAN (R 3.6.2)                    \n P timeDate        3043.102   2018-02-21 [?] CRAN (R 3.6.0)                    \n P tokenizers      0.2.1      2018-03-29 [?] CRAN (R 3.6.0)                    \n P tune          * 0.1.0      2020-04-02 [?] CRAN (R 3.6.0)                    \n P usethis         1.6.1      2020-04-29 [?] CRAN (R 3.6.2)                    \n P utf8            1.1.4      2018-05-24 [?] CRAN (R 3.6.0)                    \n P vctrs           0.2.4      2020-03-10 [?] CRAN (R 3.6.0)                    \n P viridisLite     0.3.0      2018-02-01 [?] CRAN (R 3.6.0)                    \n P withr           2.2.0      2020-04-20 [?] CRAN (R 3.6.2)                    \n P workflows     * 0.1.1      2020-03-17 [?] CRAN (R 3.6.0)                    \n P xfun            0.13       2020-04-13 [?] CRAN (R 3.6.2)                    \n P xml2            1.3.2      2020-04-23 [?] CRAN (R 3.6.2)                    \n   xtable          1.8-4      2019-04-21 [1] CRAN (R 3.6.0)                    \n P xts             0.12-0     2020-01-19 [?] CRAN (R 3.6.0)                    \n P yaml            2.2.1      2020-02-01 [?] CRAN (R 3.6.0)                    \n P yardstick     * 0.0.6      2020-03-17 [?] CRAN (R 3.6.0)                    \n P zoo             1.8-8      2020-05-02 [?] CRAN (R 3.6.2)                    \n\n[1] /Users/emilhvitfeldthansen/Github/hvitfeldt.me/renv/library/R-3.6/x86_64-apple-darwin15.6.0\n[2] /private/var/folders/m0/zmxymdmd7ps0q_tbhx0d_26w0000gn/T/RtmpTCz51a/renv-system-library\n\n P ── Loaded and on-disk path mismatch."
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html",
    "href": "post/2018-03-15-tidy-text-summarization/index.html",
    "title": "Tidy Text Summarization using TextRank",
    "section": "",
    "text": "This code has been lightly revised to make sure it works as of 2018-12-19."
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#text-summarization",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#text-summarization",
    "title": "Tidy Text Summarization using TextRank",
    "section": "Text summarization",
    "text": "Text summarization\nIn the realm of text summarization there two main paths:\n\nextractive summarization\nabstractive summarization\n\nWhere extractive scoring words and sentences according to some metric and then using that information to summarize the text. Usually done by copy/pasting (extracting) the most informative parts of the text.\nThe abstractive methods aim to build a semantic representation of the text and then use natural language generation techniques to generate text describing the informative parts.\nExtractive summarization is primarily the simpler task, with a handful of algorithms do will do the scoring. While with the advent of deep learning did NLP has a boost in abstractive summarization methods.\nThis post will focus on an example of an extractive summarization method called TextRank which is based on the PageRank algorithm that is used by Google to rank websites by their importance."
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#textrank-algorithm",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#textrank-algorithm",
    "title": "Tidy Text Summarization using TextRank",
    "section": "TextRank Algorithm",
    "text": "TextRank Algorithm\nThe TextRank algorithm is based on a graph-based ranking algorithm. Generally used in web searches at Google, but have many other applications. Graph-based ranking algorithms try to decide the importance of a vertex by taking into account information about the entire graph rather than the vertex-specific information. A typical piece of information would be the information between relationships (edges) between the vertices.\nIn the NLP case, we need to define what we want to use as vertices and edges. In our case will we be using sentences as the vertices and words as the connection edges. So sentences with words that appear in many other sentences are seen as more important."
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#data-preparation",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#data-preparation",
    "title": "Tidy Text Summarization using TextRank",
    "section": "Data preparation",
    "text": "Data preparation\nWe start by loading the appropriate packages, which include tidyverse for general tasks, tidytext for text manipulations, textrank for the implementation of the TextRank algorithm, and finally rvest to scrape an article to use as an example. The GitHub for the textrank package can be found here.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textrank)\nlibrary(rvest)\nTo showcase this method I have randomly (EXTENSIVELY filtered political and controversial) selected an article as our guinea pig. The main body is selected using the html_nodes.\nurl &lt;- \"http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\"\narticle &lt;- read_html(url) %&gt;%\n  html_nodes('div[class=\"padded\"]') %&gt;%\n  html_text()\nnext, we load the article into a tibble (since tidytext required the input as a data.frame). We start by tokenizing according to sentences which are done by setting token = \"sentences\" in unnest_tokens. The tokenization is not always perfect using this tokenizer, but it has a low number of dependencies and is sufficient for this showcase. Lastly, we add a sentence number column and switch the order of the columns (textrank_sentences prefers the columns in a certain order).\narticle_sentences &lt;- tibble(text = article) %&gt;%\n  unnest_tokens(sentence, text, token = \"sentences\") %&gt;%\n  mutate(sentence_id = row_number()) %&gt;%\n  select(sentence_id, sentence)\nnext, we will tokenize again but this time to get words. In doing this we will retain the sentence_id column in our data.\narticle_words &lt;- article_sentences %&gt;%\n  unnest_tokens(word, sentence)\nnow we have all the sufficient input for the textrank_sentences function. However, we will go one step further and remove the stop words in article_words since they would appear in most of the sentences and don’t really carry any information in themself.\narticle_words &lt;- article_words %&gt;%\n  anti_join(stop_words, by = \"word\")"
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#running-textrank",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#running-textrank",
    "title": "Tidy Text Summarization using TextRank",
    "section": "Running TextRank",
    "text": "Running TextRank\nRunning the TextRank algorithm is easy, the textrank_sentences function only required 2 inputs.\n\nA data.frame with sentences\nA data.frame with tokens (in our case words) which are part of each sentence\n\nSo we are ready to run\narticle_summary &lt;- textrank_sentences(data = article_sentences, \n                                      terminology = article_words)\nThe output has its own printing method that displays the top 5 sentences:\narticle_summary\n## Textrank on sentences, showing top 5 most important sentences found:\n##   1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\n##   2. fitbit says the tracker is designed for children eight years old and up.\n##   3. the fitbit ace looks a lot like the company’s alta tracker, but with a few child-friendly tweaks.\n##   4. like many of fitbit’s other products, the fitbit ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long.\n##   5. the most important of which is fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the children’s online privacy protection act, or coppa.\nWhich in itself is pretty good."
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#digging-deeper",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#digging-deeper",
    "title": "Tidy Text Summarization using TextRank",
    "section": "Digging deeper",
    "text": "Digging deeper\nWhile the printing method is good, we can extract the information to good some further analysis. The information about the sentences is stored in sentences. It includes the information article_sentences plus the calculated textrank score.\narticle_summary[[\"sentences\"]]\nLet’s begging by extracting the top 3 and bottom 3 sentences to see how they differ.\narticle_summary[[\"sentences\"]] %&gt;%\n  arrange(desc(textrank)) %&gt;% \n  slice(1:3) %&gt;%\n  pull(sentence)\n## [1] \"fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\"\n## [2] \"fitbit says the tracker is designed for children eight years old and up.\"                                                                                   \n## [3] \"the fitbit ace looks a lot like the company’s alta tracker, but with a few child-friendly tweaks.\"\nAs expected these are the same sentences as we saw earlier. However, the button sentences don’t include the word Fitbit (properly a rather important word) and focus more on “other” things, like the reference to another product in the second sentence.\narticle_summary[[\"sentences\"]] %&gt;%\n  arrange(textrank) %&gt;% \n  slice(1:3) %&gt;%\n  pull(sentence)\n## [1] \"conversations with the most influential leaders in business and tech.\"           \n## [2] \"please try again later.\"                                                         \n## [3] \"click the link to confirm your subscription and begin receiving our newsletters.\"\nIf we look at the article over time, it would be interesting to see where the important sentences appear.\narticle_summary[[\"sentences\"]] %&gt;%\n  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n  geom_col() +\n  theme_minimal() +\n  scale_fill_viridis_c() +\n  guides(fill = \"none\") +\n  labs(x = \"Sentence\",\n       y = \"TextRank score\",\n       title = \"4 Most informative sentences appear within first half of sentences\",\n       subtitle = 'In article \"Fitbits Newest Fitness Tracker Is Just for Kids\"',\n       caption = \"Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\")"
  },
  {
    "objectID": "post/2018-03-15-tidy-text-summarization/index.html#working-with-books",
    "href": "post/2018-03-15-tidy-text-summarization/index.html#working-with-books",
    "title": "Tidy Text Summarization using TextRank",
    "section": "Working with books???",
    "text": "Working with books???\nSummaries help cut down the reading when used on articles. Would the same approach work on books? Let’s see what happens when you exchange “sentence” in “article” with “chapter” in “book”. I’ll go to my old friend emma from the janeaustenr package. We will borrow some code from the Text Mining with R book to create the chapters. Remember that we want 1 chapter per row.\nemma_chapters &lt;- janeaustenr::emma %&gt;%\n  tibble(text = .) %&gt;%\n  mutate(chapter_id = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE)))) %&gt;%\n  filter(chapter_id &gt; 0) %&gt;%\n  group_by(chapter_id) %&gt;%\n  summarise(text = paste(text, collapse = ' '))\nand proceed as before to find the words and remove the stop words.\nemma_words &lt;- emma_chapters %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words, by = \"word\")\nWe run the textrank_sentences function again. It should still be very quick, as the bottleneck of the algorithm is more with the number of vertices rather than their individual size.\nemma_summary &lt;- textrank_sentences(data = emma_chapters, \n                                   terminology = emma_words)\nWe will be careful not to use the standard printing method as it would print 5 whole chapters!!\nInstead, we will look at the bar chart again to see if the important chapters appear in any particular order.\nemma_summary[[\"sentences\"]] %&gt;%\n  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +\n  geom_col() +\n  theme_minimal() +\n  scale_fill_viridis_c(option = \"inferno\") +\n  guides(fill = \"none\") +\n  labs(x = \"Chapter\",\n       y = \"TextRank score\",\n       title = \"Chapter importance in the novel Emma by Jane Austen\") +\n  scale_x_continuous(breaks = seq(from = 0, to = 55, by = 5))\n\nWhich doesn’t appear to be the case in this particular text (which is properly good since skipping a chapter would be discouraged in a book like Emma). however, it might prove helpful in non-chronological texts.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source        \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.0)\n backports     1.2.1   2020-12-09 [1] CRAN (R 4.0.2)\n blogdown      1.3     2021-04-14 [1] CRAN (R 4.0.2)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.0.2)\n broom         0.7.6   2021-04-05 [1] CRAN (R 4.0.2)\n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)\n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.0.0)\n cli           3.0.0   2021-06-30 [1] CRAN (R 4.0.2)\n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.0.2)\n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.0.5)\n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.0.2)\n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.0.2)\n curl          4.3.2   2021-06-23 [1] CRAN (R 4.0.2)\n data.table    1.14.0  2021-02-21 [1] CRAN (R 4.0.2)\n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.0.2)\n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.0.2)\n desc          1.3.0   2021-03-05 [1] CRAN (R 4.0.2)\n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)\n digest        0.6.27  2020-10-24 [1] CRAN (R 4.0.2)\n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.0.2)\n evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.0)\n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.0.2)\n farver        2.1.0   2021-02-28 [1] CRAN (R 4.0.2)\n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.0.2)\n fs            1.5.0   2020-07-31 [1] CRAN (R 4.0.2)\n generics      0.1.0   2020-10-31 [1] CRAN (R 4.0.2)\n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.0.2)\n glue          1.4.2   2020-08-27 [1] CRAN (R 4.0.2)\n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.0.0)\n haven         2.4.1   2021-04-23 [1] CRAN (R 4.0.2)\n highr         0.9     2021-04-16 [1] CRAN (R 4.0.2)\n hms           1.1.0   2021-05-17 [1] CRAN (R 4.0.2)\n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)\n httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)\n igraph        1.2.6   2020-10-06 [1] CRAN (R 4.0.2)\n janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 4.0.0)\n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.0.2)\n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.0.2)\n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.0.2)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.0.2)\n lattice       0.20-41 2020-04-02 [1] CRAN (R 4.0.5)\n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.0.2)\n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.0.2)\n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.0.2)\n Matrix        1.3-2   2021-01-06 [1] CRAN (R 4.0.5)\n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.0.0)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.0.0)\n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.0.2)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.0)\n png           0.1-7   2013-12-03 [1] CRAN (R 4.0.0)\n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)\n R6            2.5.0   2020-10-28 [1] CRAN (R 4.0.2)\n Rcpp          1.0.6   2021-01-15 [1] CRAN (R 4.0.2)\n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2)\n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.0.2)\n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.0.2)\n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.0.2)\n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.0.2)\n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.2)\n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.0.2)\n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.0.2)\n sass          0.4.0   2021-05-12 [1] CRAN (R 4.0.2)\n scales        1.1.1   2020-05-11 [1] CRAN (R 4.0.0)\n selectr       0.4-2   2019-11-20 [1] CRAN (R 4.0.0)\n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.0)\n SnowballC     0.7.0   2020-04-01 [1] CRAN (R 4.0.0)\n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.0.2)\n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.0.0)\n textrank    * 0.3.1   2020-10-12 [1] CRAN (R 4.0.2)\n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.0.2)\n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.0.2)\n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.0.2)\n tidytext    * 0.3.1   2021-04-10 [1] CRAN (R 4.0.2)\n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.0.2)\n tokenizers    0.2.1   2018-03-29 [1] CRAN (R 4.0.0)\n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.0.2)\n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.0.2)\n viridisLite   0.4.0   2021-04-13 [1] CRAN (R 4.0.2)\n withr         2.4.2   2021-04-18 [1] CRAN (R 4.0.2)\n xfun          0.24    2021-06-15 [1] CRAN (R 4.0.2)\n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.0)\n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.0)\n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2018-06-04-emoji-use-on-twitter/index.html",
    "href": "post/2018-06-04-emoji-use-on-twitter/index.html",
    "title": "Emoji use on Twitter",
    "section": "",
    "text": "This code has been lightly revised to make sure it works as of 2018-12-16.\n\nThis post will be a short demonstration of how the occurrence of emojis on Twitter can be analyzed using tidytools. We start loading the necessary packages.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(rtweet)\nI have decided that for this example that I would focus on tweets that include the hashtags #happy and #sad in the hope that both would include a similar number of emojis but hopefully of different groups. We will use the rtweet package which already conforms to the tidy principles. Notice the retryonratelimit = TRUE argument as the combined number of tweets (10000 + 10000 = 20000) is larger than the 15 min limit of 18000.\ntweets_happy &lt;- search_tweets(\"#happy\", n = 10000, include_rts = FALSE)\ntweets_sad &lt;- search_tweets(\"#sad\", n = 10000, include_rts = FALSE, \n                            retryonratelimit = TRUE)\nwe will safely save these tweets.\nwrite_as_csv(tweets_happy, \"tweets_happy.csv\")\nwrite_as_csv(tweets_sad, \"tweets_sad.csv\")\nNow we load this data.frame that contains information regarding the various emojis.\nemoji &lt;- readr::read_csv(\"https://raw.githubusercontent.com/EmilHvitfeldt/Emoji-table/master/emoji.csv\")\nNext, we add the hash tag label as the emotion variable, next we tokenize all the tweets according to characters (this is done since a lot of the tweets didn’t use spaces emojis rendering them hard to detect.) and left join with the emoji data.frame such that we get the descriptions.\ntweets_all &lt;- bind_rows(\n  tweets_happy %&gt;% mutate(emotion = \"#happy\"),\n  tweets_sad %&gt;% mutate(emotion = \"#sad\")\n)\n\nemoji_all &lt;- unnest_tokens(tweets_all, word, text, \n                           token = \"characters\") %&gt;%\n  select(word, emotion) %&gt;%\n  left_join(emoji, by = c(\"word\" = \"utf\")) %&gt;%\n  filter(!is.na(shortname))\nLastly, we create a simple faceted bar chart of the number of emojis used within each hashtag.\nemoji_all %&gt;%\n  count(word, emotion, shortname) %&gt;%\n  group_by(emotion) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(10, n) %&gt;%\n  ungroup() %&gt;%\n  mutate(emoji = reorder(shortname, n)) %&gt;%\n  ggplot(aes(emoji, n)) +\n  geom_col() +\n  facet_grid(emotion ~ ., scales = \"free_y\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Emojis used in #happy and #sad tweets\",\n       y = \"Count\", x = \"\")\n\nUsing the emoji data.frame allows us to gain quick insight with the descriptive short names.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n lattice       0.20-44 2021-05-02 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n Matrix        1.3-3   2021-05-04 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rtweet      * 0.7.0   2020-01-08 [1] CRAN (R 4.1.0)                   \n rvest         1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n SnowballC     0.7.0   2020-04-01 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidytext    * 0.3.1   2021-04-10 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n tokenizers    0.2.1   2018-03-29 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html",
    "title": "Binary text classification with tidytext and caret",
    "section": "",
    "text": "the scope of this blog post is to show how to do binary text classification using standard tools such as tidytext and caret packages. One of if not the most common binary text classification task is the spam detection (spam vs non-spam) that happens in most email services but has many other application such as language identification (English vs non-English).\nIn this post I’ll showcase 5 different classification methods to see how they compare with this data. The methods all land on the less complex side of the spectrum and thus does not include creating complex deep neural networks.\nAn expansion of this subject is multiclass text classification which I might write about in the future."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#packages",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#packages",
    "title": "Binary text classification with tidytext and caret",
    "section": "Packages",
    "text": "Packages\nWe load the packages we need for this project. tidyverse for general data science work, tidytext for text manipulation and caret for modeling.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(caret)"
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#data",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#data",
    "title": "Binary text classification with tidytext and caret",
    "section": "Data",
    "text": "Data\nThe data we will be using for this demonstration will be some English1 social media disaster tweets discussed in this article. It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv\")\nAnd for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren’t classified, marked \"Can't Decide\" so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don’t actually have that much information in them as they are limited in characters and some only contain a couple of words.\nWe will at this stage remove what appears to be urls using some regex and str_replace_all, and we will select the columns id, disaster and text.\ndata_clean &lt;- data %&gt;%\n  filter(choose_one != \"Can't Decide\") %&gt;%\n  mutate(id = `_unit_id`,\n         disaster = choose_one == \"Relevant\",\n         text = str_replace_all(text, \" ?(f|ht)tp(s?)://(.*)[.][a-z]+\", \"\")) %&gt;%\n  select(id, disaster, text)\nFirst we take a quick look at the distribution of classes and we see if the classes are balanced\ndata_clean %&gt;%\n  ggplot(aes(disaster)) +\n  geom_bar()\nAnd we see that is fairly balanced so we don’t have to worry about sampling this time.\nThe representation we will be using in this post will be the bag-of-words representation in which we just count how many times each word appears in each tweet disregarding grammar and even word order (mostly).\nWe will construct a tf-idf vector model in which each unique word is represented as a column and each document (tweet in our case) is a row of the tf-idf values. This will create a very large matrix/data.frame (a column of each unique word in the total data set) which will overload a lot of the different models we can implement, furthermore will a lot of the words (or features in ML slang) not add considerably information. We have a trade off between information and computational speed.\nFirst we will remove all the stop words, this will insure that common words that usually don’t carry meaning doesn’t take up space (and time) in our model. Next will we only look at words that appear in 10 different tweets. Lastly we will be looking at both unigrams and bigrams to hopefully get a better information extraction.\ndata_counts &lt;- map_df(1:2,\n                      ~ unnest_tokens(data_clean, word, text, \n                                      token = \"ngrams\", n = .x)) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  count(id, word, sort = TRUE)\nWe will only look at words at appear in at least 10 different tweets.\nwords_10 &lt;- data_counts %&gt;%\n  group_by(word) %&gt;%\n  summarise(n = n()) %&gt;% \n  filter(n &gt;= 10) %&gt;%\n  select(word)\nwe will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix.\ndata_dtm &lt;- data_counts %&gt;%\n  right_join(words_10, by = \"word\") %&gt;%\n  bind_tf_idf(word, id, n) %&gt;%\n  cast_dtm(id, word, tf_idf)\nThis leaves us with 2993 features. We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction.\nmeta &lt;- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %&gt;%\n  left_join(data_clean[!duplicated(data_clean$id), ], by = \"id\")\nWe also create the index (based on the meta data.frame) to separate the data into a training and test set.\nset.seed(1234)\ntrainIndex &lt;- createDataPartition(meta$disaster, p = 0.8, list = FALSE, times = 1)\nsince a lot of the methods take data.frames as inputs we will take the time and create these here:\ndata_df_train &lt;- data_dtm[trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame()\ndata_df_test &lt;- data_dtm[-trainIndex, ] %&gt;% as.matrix() %&gt;% as.data.frame()\n\nresponse_train &lt;- meta$disaster[trainIndex]\nNow each row in the data.frame is a document/tweet (yay tidy principles!!)."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#missing-tweets",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#missing-tweets",
    "title": "Binary text classification with tidytext and caret",
    "section": "Missing tweets",
    "text": "Missing tweets\nIn the feature selection earlier we decided to turn our focus towards certain words and word-pairs, with that we also turned our focus AWAY from certain words. Since the tweets are fairly short in length it wouldn’t be surprising if a handful of the tweets completely skipped out focus as we noted earlier. Lets take a look at those tweets here.\ndata_clean %&gt;%\n  anti_join(meta, by = \"id\") %&gt;%\n  head(25) %&gt;%\n  pull(text)\nWe see that a lot of them appears to be part of urls that our regex didn’t detect, furthermore it appears that in those tweet the sole text was the url which wouldn’t have helped us in this case anyways."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#modeling",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#modeling",
    "title": "Binary text classification with tidytext and caret",
    "section": "Modeling",
    "text": "Modeling\nNow that we have the data all clean and tidy we will turn our heads towards modeling. We will be using the wonderful caret package which we will use to employ the following models\n\nSupport vector machine\nNaive Bayes\nLogitBoost\nRandom forest\nfeed-forward neural networks\n\nThese where chosen because of their frequent use ( why SVM are good at text classification ) or because they are common in the classification field. They were also chosen because they where able to work with data with this number of variables in a reasonable time.\nFirst time around we will not use a resampling method.\ntrctrl &lt;- trainControl(method = \"none\")"
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#svm",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#svm",
    "title": "Binary text classification with tidytext and caret",
    "section": "SVM",
    "text": "SVM\nThe first model will be the svmLinearWeights2 model from the LiblineaR package. Where we specify default parameters.\nsvm_mod &lt;- train(x = data_df_train,\n                 y = as.factor(response_train),\n                 method = \"svmLinearWeights2\",\n                 trControl = trctrl,\n                 tuneGrid = data.frame(cost = 1, \n                                       Loss = 0, \n                                       weight = 1))\nWe predict on the test data set based on the fitted model.\nsvm_pred &lt;- predict(svm_mod,\n                    newdata = data_df_test)\nlastly we calculate the confusion matrix using the confusionMatrix function in the caret package.\nsvm_cm &lt;- confusionMatrix(svm_pred, meta[-trainIndex, ]$disaster)\nsvm_cm\nand we get an accuracy of 0.7461646."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#naive-bayes",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#naive-bayes",
    "title": "Binary text classification with tidytext and caret",
    "section": "Naive-Bayes",
    "text": "Naive-Bayes\nThe second model will be the naive_bayes model from the naivebayes package. Where we specify default parameters.\nnb_mod &lt;- train(x = data_df_train,\n                y = as.factor(response_train),\n                method = \"naive_bayes\",\n                trControl = trctrl,\n                tuneGrid = data.frame(laplace = 0,\n                                      usekernel = FALSE,\n                                      adjust = FALSE))\nWe predict on the test data set based on the fitted model.\nnb_pred &lt;- predict(nb_mod,\n                   newdata = data_df_test)\ncalculate the confusion matrix\nnb_cm &lt;- confusionMatrix(nb_pred, meta[-trainIndex, ]$disaster)\nnb_cm\nand we get an accuracy of 0.5564854."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#logitboost",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#logitboost",
    "title": "Binary text classification with tidytext and caret",
    "section": "LogitBoost",
    "text": "LogitBoost\nThe third model will be the LogitBoost model from the caTools package. We don’t have to specify any parameters.\nlogitboost_mod &lt;- train(x = data_df_train,\n                        y = as.factor(response_train),\n                        method = \"LogitBoost\",\n                        trControl = trctrl)\nWe predict on the test data set based on the fitted model.\nlogitboost_pred &lt;- predict(logitboost_mod,\n                           newdata = data_df_test)\ncalculate the confusion matrix\nlogitboost_cm &lt;- confusionMatrix(logitboost_pred, meta[-trainIndex, ]$disaster)\nlogitboost_cm\nand we get an accuracy of 0.632729."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#random-forest",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#random-forest",
    "title": "Binary text classification with tidytext and caret",
    "section": "Random forest",
    "text": "Random forest\nThe fourth model will be the ranger model from the caTools package. Where we specify default parameters.\nrf_mod &lt;- train(x = data_df_train, \n                y = as.factor(response_train), \n                method = \"ranger\",\n                trControl = trctrl,\n                tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])),\n                                      splitrule = \"gini\",\n                                      min.node.size = 1))\nWe predict on the test data set based on the fitted model.\nrf_pred &lt;- predict(rf_mod,\n                   newdata = data_df_test)\ncalculate the confusion matrix\nrf_cm &lt;- confusionMatrix(rf_pred, meta[-trainIndex, ]$disaster)\nrf_cm\nand we get an accuracy of 0.7777778."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#nnet",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#nnet",
    "title": "Binary text classification with tidytext and caret",
    "section": "nnet",
    "text": "nnet\nThe fifth and final model will be the nnet model from the caTools package. Where we specify default parameters. We will also specify MaxNWts = 5000 such that it will work. It will need to be more then the number of columns multiplied the size.\nnnet_mod &lt;- train(x = data_df_train,\n                    y = as.factor(response_train),\n                    method = \"nnet\",\n                    trControl = trctrl,\n                    tuneGrid = data.frame(size = 1,\n                                          decay = 5e-4),\n                    MaxNWts = 5000)\nWe predict on the test data set based on the fitted model.\nnnet_pred &lt;- predict(nnet_mod,\n                     newdata = data_df_test)\ncalculate the confusion matrix\nnnet_cm &lt;- confusionMatrix(nnet_pred, meta[-trainIndex, ]$disaster)\nnnet_cm\nand we get an accuracy of 0.7173408."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#comparing-models",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#comparing-models",
    "title": "Binary text classification with tidytext and caret",
    "section": "Comparing models",
    "text": "Comparing models\nTo see how the different models stack out we combine the metrics together in a data.frame.\nmod_results &lt;- rbind(\n  svm_cm$overall, \n  nb_cm$overall,\n  logitboost_cm$overall,\n  rf_cm$overall,\n  nnet_cm$overall\n  ) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(model = c(\"SVM\", \"Naive-Bayes\", \"LogitBoost\", \"Random forest\", \"Neural network\"))\nvisualizing the accuracy for the different models with the red line being the “No Information Rate” that is, having a model that just picks the model common class.\nmod_results %&gt;%\n  ggplot(aes(model, Accuracy)) +\n  geom_point() +\n  ylim(0, 1) +\n  geom_hline(yintercept = mod_results$AccuracyNull[1],\n             color = \"red\")\nAs you can see all but one approach does better then the “No Information Rate” on its first try before tuning the hyperparameters."
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#tuning-hyperparameters",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#tuning-hyperparameters",
    "title": "Binary text classification with tidytext and caret",
    "section": "Tuning hyperparameters",
    "text": "Tuning hyperparameters\nAfter trying out the different models we saw quite a spread in performance. But it important to remember that the results might be because of good/bad default hyperparameters. There are a few different ways to handle this problem. I’ll show on of them here, grid search, on the SVM model so you get the idea.\nWe will be using 10-fold cross-validation and 3 repeats, which will slow down the procedure, but will try to limit and reduce overfitting. We will be using grid search approach to find optimal hyperparameters. For the sake of time have to fixed 2 of the hyperparameters and only let one vary. Remember that the time it takes to search though all combinations take a long time when then number of hyperparameters increase.\nfitControl &lt;- trainControl(method = \"repeatedcv\",\n                           number = 3,\n                           repeats = 3,\n                           search = \"grid\")\nWe have decided to limit the search around the weight parameter’s default value 1.\nsvm_mod &lt;- train(x = data_df_train,\n                 y = as.factor(response_train),\n                 method = \"svmLinearWeights2\",\n                 trControl = fitControl,\n                 tuneGrid = data.frame(cost = 0.01, \n                                       Loss = 0, \n                                       weight = seq(0.5, 1.5, 0.1)))\nand once it have finished running we can plot the train object to see which value is highest.\nplot(svm_mod)\nAnd we see that it appear to be just around 1. It is important to search multiple parameters at the SAME TIME as it can not be assumed that the parameters are independent of each others. Only reason I didn’t do that here was to same the time.\nI will leave to you the reader to find out which of the models have the highest accuracy after doing parameter tuning.\nI hope you have enjoyed this overview of binary text classification.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-13                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#footnotes",
    "href": "post/2018-03-31-binary-text-classification-with-tidytext-and-caret/index.html#footnotes",
    "title": "Binary text classification with tidytext and caret",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html",
    "href": "post/bookdown-netlify-github-actions/index.html",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "",
    "text": "With the new Github Actions comes many possibilities. Some new and some old. One of the benefits is that you don’t have to use third-party applications to do continuous integration.\nThis post will show you how you can set up a bookdown site with Netlify using Github Actions. This was previously and still is possible to do with Travis-CI.\nThis post wouldn’t have been possible without Jim Hester’s work on Github Actions.\nIf you are transferring a book from Travis-CI build look at the notes at the end of this post."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#create-repository",
    "href": "post/bookdown-netlify-github-actions/index.html#create-repository",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Create Repository",
    "text": "Create Repository\nFirst, you need to create a bookdown repository. For this, I suggest you follow the Getting Started chapter from the Bookdown Book and download the GitHub repository https://github.com/rstudio/bookdown-demo as a Zip file, then unzip it locally. I recommend that you change the name of the .Rproj file so isn’t the default value.\nThe next step isn’t necessary but is still highly recommended. Go fill in the information in the DESCRIPTION file. Most importantly the Package and Title fields. The Package field will be used as the name of the repository and the Title field will be the description of the repository once it hits Github."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#connect-to-github",
    "href": "post/bookdown-netlify-github-actions/index.html#connect-to-github",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Connect to Github",
    "text": "Connect to Github\nNow we want to connect our repository to Github. For this, I will use the usethis package which is wonderful for things like this. If you haven’t used usethis before please go do the usethis setup before moving forward.\nSimply add Git to the repository by running usethis::use_git() and connect it to Github with usethis::use_github(). This should open up a webpage with the newly linked repository."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#create-netlify-account",
    "href": "post/bookdown-netlify-github-actions/index.html#create-netlify-account",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Create Netlify account",
    "text": "Create Netlify account\nIf you haven’t already got a Netlify account, go to netlify.com/ to create one for free. I have it set up with Github for easier interaction."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#create-netlify-site",
    "href": "post/bookdown-netlify-github-actions/index.html#create-netlify-site",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Create Netlify site",
    "text": "Create Netlify site\nOnce you have logged into Netlify go to your team page and create a “New site from Git”\n\nSelect Github for Continuous Deployment\n\nNow we need to select the GitHub repository. Depending on how many repositories you have you can find it in the list or search for it with the search bar. Once you have found it click the little arrow to the right of it.\n\nDon’t touch any of the settings.\n\nAnd voila! Here is your new site, it is currently empty. Now click on the “Site settings” button\n\ncopy the API ID and save it, you will need it in a little bit. If you lose it you can always come back here and copy it again.\n\nYou might have noticed that the website is completely random. If you click on the “Change site name” button you can set a new prefix name.\n\nScroll down to get the Status Badge, you can copy this too and put it at the top of your README.md file if you want."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#get-a-netlify-personal-access-token",
    "href": "post/bookdown-netlify-github-actions/index.html#get-a-netlify-personal-access-token",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Get a Netlify personal access token",
    "text": "Get a Netlify personal access token\nScroll all the way up and click on your icon in the top right corner. Then go to “User settings”\n\ngo to “Applications”\n\nAnd click on “New access token” to create a personal access token\n\nThe description of your token isn’t important but try to make it related to your book so you remember. Click “Generate token” when you are done.\n\nHere is your authentication token. Copy it and don’t lose it! Once you leave this site you can not get it back."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#store-your-secrets",
    "href": "post/bookdown-netlify-github-actions/index.html#store-your-secrets",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Store Your Secrets",
    "text": "Store Your Secrets\nNow that you have the API ID and personal access token go back to your Github repository and go to “Settings”\n\ngo to “Secrets”\n\nClick on “Add a new secret”\n\nYou need to do this twice.\n\none named “NETLIFY_AUTH_TOKEN” where you put the personal access token as the value and,\none named “NETLIFY_SITE_ID” where you put the API ID as the value."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#create-github-workflow",
    "href": "post/bookdown-netlify-github-actions/index.html#create-github-workflow",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Create Github workflow",
    "text": "Create Github workflow\nNow add the GitHub workflow file.\nFor this, you will need version 1.5.1.9000 or higher of usethis for it to work. You can get the newest version of usethis from github with\n# install.packages(\"devtools\")\ndevtools::install_github(\"r-lib/usethis\")\nthen you run use_github_action(\"bookdown.yaml\") which will create the .yaml file in the right directory for you."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#run-renvsnapshot",
    "href": "post/bookdown-netlify-github-actions/index.html#run-renvsnapshot",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Run renv::snapshot",
    "text": "Run renv::snapshot\nInstall the renv package and run renv::snapshot(). This will ensure the package versions remain consistent across builds.\nOnce you need more packages, add them to the description like you normally would with an R package."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#push-changes",
    "href": "post/bookdown-netlify-github-actions/index.html#push-changes",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Push changes",
    "text": "Push changes\nAnd that is everything you need to do, just commit the workflow file and the renv files you created and the website should build for you.\nMy example can be found here with the repository."
  },
  {
    "objectID": "post/bookdown-netlify-github-actions/index.html#notes",
    "href": "post/bookdown-netlify-github-actions/index.html#notes",
    "title": "Deploy your bookdown project to Netlify with Github Actions",
    "section": "Notes",
    "text": "Notes\nthe line\nnetlify deploy --prod --dir _book\nthe workflow files it the one that deploys the built book to Netlify. It defaults to the _book folder. In the _bookdown.yml file you can change the output folder. So if you have set it to output_dir: \"docs\" then you need to change the deploy option to\nnetlify deploy --prod --dir docs\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.2 (2020-06-22)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2020-09-04                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date       lib source                       \n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.0.0)               \n backports     1.1.8      2020-06-17 [1] CRAN (R 4.0.0)               \n blogdown      0.20       2020-06-23 [1] CRAN (R 4.0.0)               \n bookdown      0.20       2020-06-23 [1] CRAN (R 4.0.0)               \n cli           2.0.2      2020-02-28 [1] CRAN (R 4.0.0)               \n clipr         0.7.0      2019-07-23 [1] CRAN (R 4.0.0)               \n crayon        1.3.4.9000 2020-08-22 [1] Github (r-lib/crayon@6b3f0c6)\n desc          1.2.0      2018-05-01 [1] CRAN (R 4.0.0)               \n details     * 0.2.1      2020-01-12 [1] CRAN (R 4.0.0)               \n digest        0.6.25     2020-02-23 [1] CRAN (R 4.0.0)               \n evaluate      0.14       2019-05-28 [1] CRAN (R 4.0.0)               \n fansi         0.4.1      2020-01-08 [1] CRAN (R 4.0.0)               \n glue          1.4.2      2020-08-27 [1] CRAN (R 4.0.2)               \n htmltools     0.5.0      2020-06-16 [1] CRAN (R 4.0.0)               \n httr          1.4.2      2020-07-20 [1] CRAN (R 4.0.2)               \n knitr       * 1.29       2020-06-23 [1] CRAN (R 4.0.0)               \n magrittr      1.5        2014-11-22 [1] CRAN (R 4.0.0)               \n png           0.1-7      2013-12-03 [1] CRAN (R 4.0.0)               \n R6            2.4.1      2019-11-12 [1] CRAN (R 4.0.0)               \n rlang         0.4.7      2020-07-09 [1] CRAN (R 4.0.2)               \n rmarkdown     2.3        2020-06-18 [1] CRAN (R 4.0.0)               \n rprojroot     1.3-2      2018-01-03 [1] CRAN (R 4.0.0)               \n sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 4.0.0)               \n stringi       1.4.6      2020-02-17 [1] CRAN (R 4.0.0)               \n stringr       1.4.0      2019-02-10 [1] CRAN (R 4.0.0)               \n withr         2.2.0      2020-04-20 [1] CRAN (R 4.0.0)               \n xfun          0.16       2020-07-24 [1] CRAN (R 4.0.2)               \n xml2          1.3.2      2020-04-23 [1] CRAN (R 4.0.0)               \n yaml          2.2.1      2020-02-01 [1] CRAN (R 4.0.0)               \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/xaringancolor-announcement/index.html",
    "href": "post/xaringancolor-announcement/index.html",
    "title": "xaringancolor announcement",
    "section": "",
    "text": "I’m happy to announce a small new package of mine: xaringancolor. xaringancolor allows you to specify shared text colors in the text, equations and code sections of xaringan slides.\nIt has already been possible to set the text colors with custom css classes and change the color of code with the flair package. I haven’t been able to see anyone else being able to extend this to work with the LaTeX equations in xaringan, let alone unifying the color specification across all 3.\n\nSetup\nYou start with using setup_colors() to specify the colors you want to use along with the name to want to reference them as\nlibrary(xaringancolor)\nsetup_colors(\n  yellow = \"#ffcc29\",\n  orange = \"#f58634\",\n  green = \"#007965\"\n)\nCalling this function (preferably in the console, you won’t need to keep it) copies a section of text into your clipboard that you will have to include in the beginning of your xaringan document.\nAnd that is all the setup you need to do to have color support across your document.\n\n\nHTML\nxaringancolor sets up a css class for each of the colors you specify with the sole purpose of changing the color. So In our example, writing .orange[hello] would produce a orange hello\n\n\n\n\nSource Code\n\n\nCode highlighting\nCode highlighting using flair is done the same as you normally would, see this article for more examples.\nYou now just use color = green in flair() instead of supplying a hex-color and it will match the color used in the css classes. Furthermore since we have the the green variable, you can use it to specify colors in charts as well for maximum effect!\n\n\n\n\nSource Code\n\n\nEquations\nTo use the colors in LaTeX equations you need to use LaTeX macros/functions. So you have a green X you would write \\green{X}.\n\n\n\n\nSource Code\n\n\nAdvanced Examples\nLast example is a re-implementation of Andrew Heiss absolutely amazing colorful slide\n\n\n\n\nSource Code"
  },
  {
    "objectID": "post/2019-10-01-manipulating-colors-with-prismatic/index.html",
    "href": "post/2019-10-01-manipulating-colors-with-prismatic/index.html",
    "title": "Manipulating colors with {prismatic}",
    "section": "",
    "text": "I’m happy to announce my newest package prismatic which facilitates simple manipulations of colors. I had been working on this package online and offline for some time, but the promise of easy manipulation of mapped data in ggplot2 forced me to get some work done to get this package out before ggplot2 version 3.3.0. (as of the time of writing.)\nThis post will go over some of the finer details with lots of pretty pictures!"
  },
  {
    "objectID": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#loading-packages",
    "href": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#loading-packages",
    "title": "Manipulating colors with {prismatic}",
    "section": "Loading Packages",
    "text": "Loading Packages\nThe prismatic package is a fairly low dependency with only 1 import being farver for lightning-fast conversion between color spaces. I have also loaded the colorspace package, from which some of the following functions have been inspired. I will use colorspace to enable the plotting of multiple color palettes side by side, but I will not showcase the code each time. Go to the end of the post for example code for comparison plots.\nlibrary(prismatic)\nlibrary(colorspace) # for plotting functions\n## \n## Attaching package: 'colorspace'\n## The following object is masked from 'package:prismatic':\n## \n##     contrast_ratio\nlibrary(magrittr) # for the glorious pipe"
  },
  {
    "objectID": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#let-me-see-the-colors",
    "href": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#let-me-see-the-colors",
    "title": "Manipulating colors with {prismatic}",
    "section": "Let me see the colors!!",
    "text": "Let me see the colors!!\nIf you have seen my work, you will properly know that I like colors alot! But being also to quickly inspect some colors have always been a little too much work. Now all you have to do it pass your colors to color() (or colour() for our friends across the pond) to get a  object which has a nice plot() method\nrainbow(10) %&gt;% color() %&gt;% plot()\n\nhcl.colors(25) %&gt;% color() %&gt;% plot()\n\nscico::scico(256, palette = \"buda\") %&gt;% color() %&gt;% plot()\n\nWhich I would like to think is one of the main features of the package. If you happen to have crayon available you will see an approximation of the colors with a filled-in background (this is limited to 256 colors so your mileage might vary, when in doubt use plot())\n\nThis is the extent of what the color object can do."
  },
  {
    "objectID": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#manipulations",
    "href": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#manipulations",
    "title": "Manipulating colors with {prismatic}",
    "section": "Manipulations",
    "text": "Manipulations\nThe second star of the package is the collection of functions to manipulate the colors. All these functions have a couple of things in common.\n\nThey all start with clr_ for easy auto-completion in your favorite IDE.\nThey all take a vector of colors as the first argument and results in a colors object of the same length.\n\nthese two facts make the function super pipe friendly.\n\nSaturation\nThe two functions clr_saturate() and clr_desaturate() both modifies the saturation of a color. It takes a single additional argument to specify the degree to which the (de)saturation should occur. These values should be between 0(nothing happens) and 1(full on power!).\nnotice how you don’t have to call color() on the output of clr_desaturate() as it already returns a colors object.\nhcl.colors(10, \"plasma\") %&gt;%\n  clr_desaturate(0.8) %&gt;%\n  plot()\n\n\nExamples are done with Mango palette from LaCroixColoR package.\n\n\n\nSeeing life in black and white\nTurns out there is a lot of different ways to turn colors into grayscale. Prismatic has implemented a handful of these. Notice how the viridis palette is still working once you have it transformed to black and white.\nhcl.colors(10) %&gt;%\n  clr_greyscale() %&gt;%\n  plot()\n\nBe advised that not all of these methods are meant to be perceptually uniform.\n\n\n\nNegate\nNegation of color is pretty simple. it will just pick the opposite color in RGB space.\nterrain.colors(10) %&gt;%\n  clr_negate() %&gt;%\n  plot()\n\n\n\n\nMixing\nMixing is just adding colors together. Thus my mixing a color with red would make the color redder.\nrainbow(10) %&gt;%\n  clr_mix(\"red\") %&gt;%\n  plot()\n\n\n\n\n\nRotation\nthe clr_rotate() function will take a color and rotate its hue, which is a way to walk around the rainbow.\nterrain.colors(10) %&gt;%\n  clr_rotate(90) %&gt;%\n  plot()\n\n\n\n\nColorblindness\nalso includes 3 functions (clr_protan(), clr_deutan() and clr_tritan()) to simulate colorblindness. These functions has a severity argument to control the strength of the deficiency.\nhcl.colors(10) %&gt;%\n  clr_deutan() %&gt;%\n  plot()\n\n\n\n\n\n\nLight and darkness\nLastly, we have functions to simulate lightness and darkness. This is surprisingly hard to do and no one way works great all the time. Please refer to the excellent colorspace paper for more information. These functions (clr_lighten() and clr_darken()) also include a space argument to determine the space in which to perform the transformation. Please try each of these to find the optimal method for your use case.\nrainbow(10) %&gt;%\n  clr_darken() %&gt;%\n  plot()"
  },
  {
    "objectID": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#comparison-code",
    "href": "post/2019-10-01-manipulating-colors-with-prismatic/index.html#comparison-code",
    "title": "Manipulating colors with {prismatic}",
    "section": "Comparison Code",
    "text": "Comparison Code\nswatchplot(\n  list(\n    saturate = rbind(\"0\" = clr_rotate(terrain.colors(10),  0),\n                     \"60\" = clr_rotate(terrain.colors(10),  60),\n                     \"120\" = clr_rotate(terrain.colors(10),  120),\n                     \"180\" = clr_rotate(terrain.colors(10),  180),\n                     \"240\" = clr_rotate(terrain.colors(10),  240),\n                     \"300\" = clr_rotate(terrain.colors(10),  300)),\n    desaturate = rbind(\"0\" = clr_rotate(hcl.colors(10),  0),\n                       \"60\" = clr_rotate(hcl.colors(10),  60),\n                       \"120\" = clr_rotate(hcl.colors(10),  120),\n                       \"180\" = clr_rotate(hcl.colors(10),  180),\n                       \"240\" = clr_rotate(hcl.colors(10),  240),\n                       \"300\" = clr_rotate(hcl.colors(10),  300))\n  ),\n  nrow = 7, line = 2.5\n)\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date       lib\n blogdown       1.3.2   2021-06-09 [1]\n bookdown       0.22    2021-04-22 [1]\n bslib          0.2.5.1 2021-05-18 [1]\n cli            3.0.0   2021-06-30 [1]\n clipr          0.7.1   2020-10-08 [1]\n codetools      0.2-18  2020-11-04 [1]\n colorspace   * 2.0-2   2021-06-24 [1]\n crayon         1.4.1   2021-02-08 [1]\n desc           1.3.0   2021-03-05 [1]\n details      * 0.2.1   2020-01-12 [1]\n digest         0.6.27  2020-10-24 [1]\n evaluate       0.14    2019-05-28 [1]\n farver         2.1.0   2021-02-28 [1]\n highr          0.9     2021-04-16 [1]\n htmltools      0.5.1.1 2021-01-22 [1]\n httr           1.4.2   2020-07-20 [1]\n jquerylib      0.1.4   2021-04-26 [1]\n jsonlite       1.7.2   2020-12-09 [1]\n knitr        * 1.33    2021-04-24 [1]\n LaCroixColoR   0.1.0   2021-07-17 [1]\n magrittr     * 2.0.1   2020-11-17 [1]\n png            0.1-7   2013-12-03 [1]\n prismatic    * 1.0.0   2021-01-05 [1]\n R6             2.5.0   2020-10-28 [1]\n rlang          0.4.11  2021-04-30 [1]\n rmarkdown      2.9     2021-06-15 [1]\n rprojroot      2.0.2   2020-11-15 [1]\n sass           0.4.0   2021-05-12 [1]\n sessioninfo    1.1.1   2018-11-05 [1]\n stringi        1.6.2   2021-05-17 [1]\n stringr        1.4.0   2019-02-10 [1]\n withr          2.4.2   2021-04-18 [1]\n xfun           0.24    2021-06-15 [1]\n xml2           1.3.2   2020-04-23 [1]\n yaml           2.2.1   2020-02-01 [1]\n source                                     \n Github (rstudio/blogdown@00a2090)          \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n Github (johannesbjork/LaCroixColoR@57e6c09)\n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n CRAN (R 4.1.0)                             \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "",
    "text": "This post was written before the change to textrecipes to support spacyr as an engine to step_tokenize(). It is still a good demonstration of how to use a custom tokenizer.\nI’m ready for my second #tidytuesday and as a massive The Office fan this dataset is right up my alley. In this post, you will read how to\nI’ll put a little more effort into the explorative charts than I usually do. I’ll not be explaining each line of code for those, but you are encouraged to play around with them yourself."
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html#packages",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html#packages",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "Packages 📦",
    "text": "Packages 📦\nlibrary(schrute)\nlibrary(tidytext)\nlibrary(tidymodels)\nlibrary(tidyr)\nlibrary(tokenizers)\nlibrary(textrecipes)\nlibrary(spacyr)\nlibrary(paletteer)\nWe will be using the schrute package which includes the dataset for the week. tidytext and tokenizers to do data exploration for the text. spacyr to access the spacy to perform part of speech tagging. tidymodels and textrecipes to do the preprocessing and modeling. And lastly, we use paletteer to get pretty color palettes."
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html#exploring-the-data",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html#exploring-the-data",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "Exploring the data ⛏",
    "text": "Exploring the data ⛏\nThe data comes with a lot of different variables. We will be focusing on character and text which contains the character and what they said, in English1. First, let us take a look at how many lines each character has\ntheoffice %&gt;%\n  count(character, sort = TRUE)\n## # A tibble: 773 x 2\n##    character     n\n##    &lt;chr&gt;     &lt;int&gt;\n##  1 Michael   10921\n##  2 Dwight     6847\n##  3 Jim        6303\n##  4 Pam        5031\n##  5 Andy       3754\n##  6 Angela     1569\n##  7 Kevin      1564\n##  8 Erin       1440\n##  9 Oscar      1368\n## 10 Ryan       1198\n## # … with 763 more rows\nMicheal, Dwight, Jim, and Pam are dominating the charts. This is unsurprising since they are some of the main characters having a central role in the episodes they appear in. This will be too many classes for the scope of this post so I’ll limit it to the top 5 characters with the most lines since the number drops off more after the first 5.\nsmall_office &lt;- theoffice %&gt;%\n  select(character, text) %&gt;%\n  filter(character %in% c(\"Michael\", \"Dwight\", \"Jim\", \"Pam\", \"Andy\"))\nLet us take a lot at how many words each line in the script is. This is going to be a problem for us later on as predicting with shorter text is harder than longer text as there is less information in it.\nsmall_office %&gt;%\n  mutate(n_words = count_words(text)) %&gt;%\n  ggplot(aes(n_words, color = character)) +\n  geom_density(binwidth = 1, key_glyph = draw_key_timeseries) +\n  xlim(c(0, 50)) +\n  scale_color_paletteer_d(\"nord::aurora\") +\n  labs(x = \"Number of words\", y = \"Density\", color = NULL,\n       title = \"Distribution of line length in The Office\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", \n        plot.title.position = \"plot\") \n\nThese lines are thankfully pretty similar, which will make it easier for us to make a good predictive model. However, we can still see some differences. Pam and Jim both have shorter lines than the rest, and Michael and Andy both have fewer shorter lines in exchange for more long lines.\nWe will be also be exploring part of speech tagging and for that, we will be using the spacyr package. It isn’t always needed but I’m going to explicitly initialize the spacy model\nspacy_initialize(model = \"en_core_web_sm\")\nthe spacyr package outputs in this nice format with doc_id, sentence_id, token_id, token and pos.\nspacy_parse(small_office$text[1], entity = FALSE, lemma = FALSE)\n##    doc_id sentence_id token_id       token   pos\n## 1   text1           1        1         All   ADV\n## 2   text1           1        2       right   ADV\n## 3   text1           1        3         Jim PROPN\n## 4   text1           1        4           . PUNCT\n## 5   text1           2        1        Your  PRON\n## 6   text1           2        2 quarterlies  NOUN\n## 7   text1           2        3        look  VERB\n## 8   text1           2        4        very   ADV\n## 9   text1           2        5        good   ADJ\n## 10  text1           2        6           . PUNCT\n## 11  text1           3        1         How   ADV\n## 12  text1           3        2         are   AUX\n## 13  text1           3        3      things  NOUN\n## 14  text1           3        4          at   ADP\n## 15  text1           3        5         the   DET\n## 16  text1           3        6     library  NOUN\n## 17  text1           3        7           ? PUNCT\nNormally I would just analyze the data in this format. But since I have to create a custom wrapper for textrecipes anyway I’ll do the remaining of the text mining in tidytext. textrecipes requires that the tokenizer returns the tokens in a list format similar to the tokenizers in tokenizers. The following function takes a character vector and returns the part of speech tags in a list format.\nspacy_pos &lt;- function(x) {\n  tokens &lt;- spacy_parse(x, entity = FALSE, lemma = FALSE)\n  token_list &lt;- split(tokens$pos, tokens$doc_id)\n  names(token_list) &lt;- gsub(\"text\", \"\", names(token_list))\n  res &lt;- unname(token_list[as.character(seq_along(x))])\n  empty &lt;- lengths(res) == 0\n  res[empty] &lt;- lapply(seq_len(sum(empty)), function(x) character(0))\n  res\n}\nLittle example to showcase the function\nexample_string &lt;- c(\"Hello there pig\", \"\", \"One more pig here\")\n\nspacy_pos(x = example_string)\n## [[1]]\n## [1] \"INTJ\" \"ADV\"  \"NOUN\"\n## \n## [[2]]\n## character(0)\n## \n## [[3]]\n## [1] \"NUM\"  \"ADJ\"  \"NOUN\" \"ADV\"\nWe can use a custom tokenizer by simply passing it to the token argument. This is going to take a little longer than normal since POS tagging takes longer than simply tokenizing.\nsmall_office_tokens &lt;- small_office %&gt;%\n  unnest_tokens(text, text, token = spacy_pos, to_lower = FALSE)\nBelow is a chart of the number of each part of speech tags. The meaning of the acronyms can be found here if you click on the Universal Part-of-speech Tags button.\ncolors &lt;- rep(paletteer_d(\"rcartocolor::Pastel\"), length.out = 18)\n\nsmall_office_tokens %&gt;%\n  count(text) %&gt;%\n  ggplot(aes(n, reorder(text, n), fill = reorder(text, n))) +\n  geom_col() +\n  labs(x = NULL, y = NULL, title = \"Part of Speech tags in The Office\") +\n  scale_fill_manual(values = colors) +\n  guides(fill = \"none\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\nI found it initially surprising that punctuation (PUNCT) was leading the chart. But after thinking about it a little bit, I can imagine it has something to do with all the lines being very short and having to end in some kind of punctuation.\nWe can facet this by the character to see who uses what part of speech.\nsmall_office_tokens %&gt;%\n  count(character, text) %&gt;%\n  group_by(character) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(forcats::fct_rev(reorder(text, n)), prop, fill = character)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_paletteer_d(\"nord::aurora\") +\n  labs(x = NULL, y = NULL, fill = NULL,\n       title = \"Part of speech tags by main character in The Office\") +\n  theme_minimal() +\n  theme(legend.position = \"top\", \n        plot.title.position = \"plot\") \n\nI don’t immediately see anything popping out at me, but it is a very pretty chart otherwise. I feel like I have seen enough, lets get to modeling!"
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html#modeling",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html#modeling",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "Modeling ⚙️",
    "text": "Modeling ⚙️\nNot that we have gotten a look at the data let’s get to modeling. First, we need to do a test/train split which we can do with yardstick.\nset.seed(1234)\noffice_split &lt;- initial_split(small_office, strata = character)\noffice_test &lt;- testing(office_split)\noffice_train &lt;- training(office_split)\nNext, we are going to prepare the preprocessing steps. We will be using the custom part of speech tokenizer we defined earlier to include part of speech tag counts as features in our model. Since this data is going to be a little sparse will we also include bi-grams of the data. To this, we first create a copy of the text variable and apply the tokenizers to each copy. Lastly will be also be doing some downsampling of the data to handle the imbalance in the data. This calculation will once again take a little while since the part of speech calculations takes a minute or two.\nrec &lt;- recipe(character ~ text, data = small_office) %&gt;%\n  # Deal with imbalance\n  step_downsample(character) %&gt;%\n  # Create copy of text variable\n  step_mutate(text_copy = text) %&gt;%\n  # Tokenize the two text columns\n  step_tokenize(text, token = \"ngrams\", options = list(n = 2)) %&gt;%\n  step_tokenize(text_copy, custom_token = spacy_pos) %&gt;%\n  # Filter to only keep the most 100 frequent n-grams\n  step_tokenfilter(text, max_tokens = 100) %&gt;%\n  # Calculate tf-idf for both sets of tokens\n  step_tfidf(text, text_copy) %&gt;%\n  prep()\n## Warning: `step_downsample()` was deprecated in recipes 0.1.13.\n## Please use `themis::step_downsample()` instead.\n## Found 'spacy_condaenv'. spacyr will use this environment\n## successfully initialized (spaCy Version: 3.0.5, language model: en_core_web_sm)\n## (python options: type = \"condaenv\", value = \"spacy_condaenv\")\nWe can now extract the processed data\noffice_test_prepped &lt;- bake(rec, office_test)\noffice_train_prepped &lt;- juice(rec)\nTo do the actual modeling we will be using multinom_reg() with \"glmnet\" as the engine. This model has two hyperparameters, which we will be doing a grid search over to find optimal values. We specify that we want to tune these parameters by passing tune() to them.\ntune_spec &lt;- multinom_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\ntune_spec\n## Multinomial Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = tune()\n## \n## Computational engine: glmnet\nNext, we set up a bootstrap sampler and grid to optimize over.\nset.seed(12345)\noffice_boot &lt;- bootstraps(office_train_prepped, strata = character, times = 10)\n\nhyper_grid &lt;- grid_regular(penalty(), mixture(), levels = 10)\n\nWe are experiencing a little bit of data leakage since we don’t perform the preprocessing within each bootstrap.\n\nNow we pass all the objects to tune_grid(). It is also possible to combine our recipe and model object into a workflow object to pass to tune_grid instead. However, since the preprocessing step took so long and we didn’t vary anything it makes more sense time-wise to use tune_grid() with a formula instead. I also set control = control_grid(verbose = TRUE) so I get a live update of how far the calculations are going.\nset.seed(123456)\nfitted_grid &lt;- tune_grid(\n  object = tune_spec,\n  preprocessor = character ~ .,\n  resamples = office_boot,\n  grid = hyper_grid,\n  control = control_grid(verbose = TRUE)\n)\nWe can now look at the best performing models with show_best()\nfitted_grid %&gt;%\n  show_best(\"roc_auc\")\n## # A tibble: 5 x 8\n##   penalty mixture .metric .estimator  mean     n  std_err .config               \n##     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                 \n## 1 0.00599   0.667 roc_auc hand_till  0.575    10 0.00105  Preprocessor1_Model068\n## 2 0.00599   0.778 roc_auc hand_till  0.575    10 0.00102  Preprocessor1_Model078\n## 3 0.00599   0.556 roc_auc hand_till  0.575    10 0.00105  Preprocessor1_Model058\n## 4 0.00599   0.889 roc_auc hand_till  0.575    10 0.00102  Preprocessor1_Model088\n## 5 0.00599   0.444 roc_auc hand_till  0.575    10 0.000918 Preprocessor1_Model048\nAnd we can use the values from the best performing model to fit our final model.\nfinal_model &lt;- tune_spec %&gt;%\n  update(penalty = 0.005994843, mixture = 1 / 3) %&gt;%\n  fit(character ~ ., data = office_train_prepped)"
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html#evaluation",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html#evaluation",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "Evaluation 📐",
    "text": "Evaluation 📐\nNow that we have our final model we can predict on our test set and look at the confusion matrix to see how well we did.\nbind_cols(\n  predict(final_model, office_test_prepped),\n  office_test_prepped\n) %&gt;%\n  conf_mat(truth = character, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\")\n\nThese are not going too well. It is doing best at predicting Michael correctly, and it seems to confuse Dwight and Michael a little bit.\nLet us investigate the cases that didn’t go too well. We can get the individual class probabilities by setting type = \"prob\" in predict()\nclass_predictions &lt;- predict(final_model, office_test_prepped, type = \"prob\")\nclass_predictions\n## # A tibble: 8,216 x 5\n##    .pred_Andy .pred_Dwight .pred_Jim .pred_Michael .pred_Pam\n##         &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n##  1      0.236        0.160     0.240         0.250     0.114\n##  2      0.206        0.285     0.176         0.173     0.159\n##  3      0.227        0.183     0.181         0.257     0.151\n##  4      0.183        0.218     0.182         0.205     0.212\n##  5      0.159        0.118     0.233         0.207     0.283\n##  6      0.249        0.206     0.181         0.175     0.189\n##  7      0.223        0.223     0.164         0.198     0.191\n##  8      0.224        0.242     0.186         0.177     0.172\n##  9      0.222        0.214     0.209         0.188     0.167\n## 10      0.234        0.233     0.197         0.187     0.149\n## # … with 8,206 more rows\nWe can do some wrangling to get the 5 worst predicted texts for each character:\nbind_cols(\n  class_predictions,\n  office_test\n) %&gt;%\n  pivot_longer(starts_with(\".pred_\")) %&gt;%\n  filter(gsub(\".pred_\", \"\", name) == character) %&gt;%\n  group_by(character) %&gt;%\n  arrange(value) %&gt;%\n  slice(1:5) %&gt;%\n  ungroup() %&gt;%\n  select(-name, -value) %&gt;%\n  reactable::reactable()\n\n\n\n\n\nSo the first striking thing here is that many of the lines are quite short, with most of Pam’s being 5 words or less. On the other hand, all the wrongly predicted lines for Michael are quite a bit longer than the rest.\nWe can also get the best predicted lines for each character by flipping the sign with desc()\nbind_cols(\n  class_predictions,\n  office_test\n) %&gt;%\n  pivot_longer(starts_with(\".pred_\")) %&gt;%\n  filter(gsub(\".pred_\", \"\", name) == character) %&gt;%\n  group_by(character) %&gt;%\n  arrange(desc(value)) %&gt;%\n  slice(1:5) %&gt;%\n  ungroup() %&gt;%\n  select(-name, -value) %&gt;%\n  reactable::reactable()\n\n\n\n\n\nOne thing I noticed is that many of Pam’s lines start with “Oh my” and that might have been a unique character trait that got picked up in the bi-grams.\n\n\n session information \n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date       lib source                           \n assertthat     0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                   \n backports      1.2.1      2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown       1.3.2      2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown       0.22       2021-04-22 [1] CRAN (R 4.1.0)                   \n broom        * 0.7.8      2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib          0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   \n class          7.3-19     2021-05-03 [1] CRAN (R 4.1.0)                   \n cli            3.0.0      2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr          0.7.1      2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools      0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace     2.0-2      2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon         1.4.1      2021-02-08 [1] CRAN (R 4.1.0)                   \n crosstalk      1.1.1      2021-01-12 [1] CRAN (R 4.1.0)                   \n data.table     1.14.0     2021-02-21 [1] CRAN (R 4.1.0)                   \n DBI            1.1.1      2021-01-15 [1] CRAN (R 4.1.0)                   \n desc           1.3.0      2021-03-05 [1] CRAN (R 4.1.0)                   \n details      * 0.2.1      2020-01-12 [1] CRAN (R 4.1.0)                   \n dials        * 0.0.9      2020-09-16 [1] CRAN (R 4.1.0)                   \n DiceDesign     1.9        2021-02-13 [1] CRAN (R 4.1.0)                   \n digest         0.6.27     2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr        * 1.0.7      2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis       0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   \n emo            0.0.0.9000 2021-07-17 [1] Github (hadley/emo@3f03b11)      \n evaluate       0.14       2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi          0.5.0      2021-05-25 [1] CRAN (R 4.1.0)                   \n farver         2.1.0      2021-02-28 [1] CRAN (R 4.1.0)                   \n foreach        1.5.1      2020-10-15 [1] CRAN (R 4.1.0)                   \n furrr          0.2.3      2021-06-25 [1] CRAN (R 4.1.0)                   \n future         1.21.0     2020-12-10 [1] CRAN (R 4.1.0)                   \n generics       0.1.0      2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2      * 3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                   \n glmnet       * 4.1-2      2021-06-24 [1] CRAN (R 4.1.0)                   \n globals        0.14.0     2020-11-22 [1] CRAN (R 4.1.0)                   \n glue           1.4.2      2020-08-27 [1] CRAN (R 4.1.0)                   \n gower          0.2.2      2020-06-23 [1] CRAN (R 4.1.0)                   \n GPfit          1.0-8      2019-02-08 [1] CRAN (R 4.1.0)                   \n gtable         0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                   \n hardhat        0.1.5      2020-11-09 [1] CRAN (R 4.1.0)                   \n highr          0.9        2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools      0.5.1.1    2021-01-22 [1] CRAN (R 4.1.0)                   \n htmlwidgets    1.5.3      2020-12-10 [1] CRAN (R 4.1.0)                   \n httr           1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   \n infer        * 0.5.4      2021-01-13 [1] CRAN (R 4.1.0)                   \n ipred          0.9-11     2021-03-12 [1] CRAN (R 4.1.0)                   \n iterators      1.0.13     2020-10-15 [1] CRAN (R 4.1.0)                   \n janeaustenr    0.1.5      2017-06-10 [1] CRAN (R 4.1.0)                   \n jquerylib      0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite       1.7.2      2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr        * 1.33       2021-04-24 [1] CRAN (R 4.1.0)                   \n lattice        0.20-44    2021-05-02 [1] CRAN (R 4.1.0)                   \n lava           1.6.9      2021-03-11 [1] CRAN (R 4.1.0)                   \n lhs            1.1.1      2020-10-05 [1] CRAN (R 4.1.0)                   \n lifecycle      1.0.0      2021-02-15 [1] CRAN (R 4.1.0)                   \n listenv        0.8.0      2019-12-05 [1] CRAN (R 4.1.0)                   \n lubridate      1.7.10     2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr       2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   \n MASS           7.3-54     2021-05-03 [1] CRAN (R 4.1.0)                   \n Matrix       * 1.3-3      2021-05-04 [1] CRAN (R 4.1.0)                   \n modeldata    * 0.1.0      2020-10-22 [1] CRAN (R 4.1.0)                   \n munsell        0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                   \n nnet           7.3-16     2021-05-03 [1] CRAN (R 4.1.0)                   \n paletteer    * 1.3.0      2021-01-06 [1] CRAN (R 4.1.0)                   \n parallelly     1.26.1     2021-06-30 [1] CRAN (R 4.1.0)                   \n parsnip      * 0.1.6      2021-05-27 [1] CRAN (R 4.1.0)                   \n pillar         1.6.1      2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                   \n plyr           1.8.6      2020-03-03 [1] CRAN (R 4.1.0)                   \n png            0.1-7      2013-12-03 [1] CRAN (R 4.1.0)                   \n pROC           1.17.0.1   2021-01-13 [1] CRAN (R 4.1.0)                   \n prodlim        2019.11.13 2019-11-17 [1] CRAN (R 4.1.0)                   \n purrr        * 0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                   \n R6             2.5.0      2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp           1.0.7      2021-07-07 [1] CRAN (R 4.1.0)                   \n reactable      0.2.3      2020-10-04 [1] CRAN (R 4.1.0)                   \n reactR         0.4.4      2021-02-22 [1] CRAN (R 4.1.0)                   \n recipes      * 0.1.16     2021-04-16 [1] CRAN (R 4.1.0)                   \n rematch2       2.1.2      2020-05-01 [1] CRAN (R 4.1.0)                   \n rlang        * 0.4.11     2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown      2.9        2021-06-15 [1] CRAN (R 4.1.0)                   \n rpart          4.1-15     2019-04-12 [1] CRAN (R 4.1.0)                   \n rprojroot      2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   \n rsample      * 0.1.0      2021-05-08 [1] CRAN (R 4.1.0)                   \n rstudioapi     0.13       2020-11-12 [1] CRAN (R 4.1.0)                   \n sass           0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   \n scales       * 1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                   \n schrute      * 0.2.2      2020-06-30 [1] CRAN (R 4.1.0)                   \n sessioninfo    1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   \n shape          1.4.6      2021-05-19 [1] CRAN (R 4.1.0)                   \n SnowballC      0.7.0      2020-04-01 [1] CRAN (R 4.1.0)                   \n spacyr       * 1.2.1      2020-03-04 [1] CRAN (R 4.1.0)                   \n stringi        1.6.2      2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr        1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   \n survival       3.2-11     2021-04-26 [1] CRAN (R 4.1.0)                   \n textrecipes  * 0.4.1      2021-07-11 [1] CRAN (R 4.1.0)                   \n tibble       * 3.1.2      2021-05-16 [1] CRAN (R 4.1.0)                   \n tidymodels   * 0.1.3      2021-04-19 [1] CRAN (R 4.1.0)                   \n tidyr        * 1.1.3      2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect     1.1.1      2021-04-30 [1] CRAN (R 4.1.0)                   \n tidytext     * 0.3.1      2021-04-10 [1] CRAN (R 4.1.0)                   \n timeDate       3043.102   2018-02-21 [1] CRAN (R 4.1.0)                   \n tokenizers   * 0.2.1      2018-03-29 [1] CRAN (R 4.1.0)                   \n tune         * 0.1.5      2021-04-23 [1] CRAN (R 4.1.0)                   \n utf8           1.2.1      2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs        * 0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                   \n withr          2.4.2      2021-04-18 [1] CRAN (R 4.1.0)                   \n workflows    * 0.2.2      2021-03-10 [1] CRAN (R 4.1.0)                   \n workflowsets * 0.0.2      2021-04-16 [1] CRAN (R 4.1.0)                   \n xfun           0.24       2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2           1.3.2      2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml           2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   \n yardstick    * 0.0.8      2021-03-28 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/tidytuesday-pos-textrecipes-the-office/index.html#footnotes",
    "href": "post/tidytuesday-pos-textrecipes-the-office/index.html#footnotes",
    "title": "tidytuesday: Part-of-Speech and textrecipes with The Office",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html",
    "href": "post/textrecipes-series-lexicon/index.html",
    "title": "Textrecipes series: lexicons",
    "section": "",
    "text": "This is the second blog post in the textrecipes series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be covering how to use lexicons to create features. This post will not cover end-to-end modeling but will only look at how to add lexicons information into your recipe."
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html#packages",
    "href": "post/textrecipes-series-lexicon/index.html#packages",
    "title": "Textrecipes series: lexicons",
    "section": "Packages 📦",
    "text": "Packages 📦\nWe are going fairly light package wise this time only needing tidymodels, textrecipes, and lastly tidytext for EDA. We will also be using textdata to provide lexicons.\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tidytext)\nlibrary(textdata)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html#what-is-a-lexicon",
    "href": "post/textrecipes-series-lexicon/index.html#what-is-a-lexicon",
    "title": "Textrecipes series: lexicons",
    "section": "What is a lexicon?",
    "text": "What is a lexicon?\nA lexicon is a list of words with one or more corresponding values for each word. You could imagine a sentiment lexicon having entries such as “awesome = 1”, “terrible = -1” and “okay = 0”. Having this information could be useful if you want to predict if some text is positively charged or negatively charged.\nOne real-world lexicon is the AFINN lexicon. It rates English words on a scale from -5 (negative) to 5 (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. It is available in textdata as the function lexicon_afinn()\nlexicon_afinn()\n## # A tibble: 2,477 x 2\n##    word       value\n##    &lt;chr&gt;      &lt;dbl&gt;\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # … with 2,467 more rows\n\nThe first time you use a function in textdata you are given a prompt to download. Please carefully read the prompt to make sure you are able to conform to the license and the demands of the authors.\n\nAnd we have plenty of words. Note that this list doesn’t give every possible word-value pair, this is partly because words with no apparent sentiment such as (cat, house, government) haven’t been encluded. Always make sure to manually inspect a premade lexicon before using it in your application. Make sure that the domain you are working in is similar to the domain the lexicon was created for. An example of a domain-specific lexicon is the Loughran-McDonald sentiment lexicon (lexicon_loughran()) which was created for use with financial documents."
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html#the-data",
    "href": "post/textrecipes-series-lexicon/index.html#the-data",
    "title": "Textrecipes series: lexicons",
    "section": "The data",
    "text": "The data\nWe will be using the data Animal Crossing data from the last post again.\nuser_reviews &lt;- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')\n\nuser_reviews &lt;- user_reviews %&gt;%\n  mutate(grade = factor(grade &gt; 7, c(TRUE, FALSE), c(\"High\", \"Low\")))\n\nset.seed(1234)\nreview_split &lt;- initial_split(user_reviews)\n\nreview_training &lt;- training(review_split)\nreview_testing &lt;- training(review_split)\nWe can use lexicons in our text mining with tidytext too. First, we will tokenize\nreview_tokens &lt;- review_training %&gt;%\n  select(grade, user_name, text) %&gt;%\n  unnest_tokens(tokens, text)\n\nreview_tokens\n## # A tibble: 270,013 x 3\n##    grade user_name tokens \n##    &lt;fct&gt; &lt;chr&gt;     &lt;chr&gt;  \n##  1 Low   mds27272  my     \n##  2 Low   mds27272  gf     \n##  3 Low   mds27272  started\n##  4 Low   mds27272  playing\n##  5 Low   mds27272  before \n##  6 Low   mds27272  me     \n##  7 Low   mds27272  no     \n##  8 Low   mds27272  option \n##  9 Low   mds27272  to     \n## 10 Low   mds27272  create \n## # … with 270,003 more rows\nthen we can use a left_join() to add a sentiment variable\nreview_tokens %&gt;%\n  left_join(lexicon_afinn(), by = c(\"tokens\" = \"word\"))\n## # A tibble: 270,013 x 4\n##    grade user_name tokens  value\n##    &lt;fct&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;\n##  1 Low   mds27272  my         NA\n##  2 Low   mds27272  gf         NA\n##  3 Low   mds27272  started    NA\n##  4 Low   mds27272  playing    NA\n##  5 Low   mds27272  before     NA\n##  6 Low   mds27272  me         NA\n##  7 Low   mds27272  no         -1\n##  8 Low   mds27272  option     NA\n##  9 Low   mds27272  to         NA\n## 10 Low   mds27272  create     NA\n## # … with 270,003 more rows\nIf we want to look at the overall document-wise sentiment level we can sum the values within each document\nreview_tokens_sentiment &lt;- review_tokens %&gt;%\n  left_join(lexicon_afinn(), by = c(\"tokens\" = \"word\")) %&gt;%\n  group_by(user_name, grade) %&gt;% \n  summarise(sentiment = sum(value, na.rm = TRUE))\n\nreview_tokens_sentiment\n## # A tibble: 2,250 x 3\n## # Groups:   user_name [2,250]\n##    user_name     grade sentiment\n##    &lt;chr&gt;         &lt;fct&gt;     &lt;dbl&gt;\n##  1 11_11         Low          16\n##  2 12hwilso      Low           3\n##  3 1mooey        High         10\n##  4 24ths         Low           1\n##  5 3nd3r02       Low          25\n##  6 425_Flex      Low         -11\n##  7 7kurtis7      Low          -3\n##  8 7Swords       Low          -2\n##  9 8bheotapus    Low          -7\n## 10 A_Mighty_Pleb Low           0\n## # … with 2,240 more rows\nSince the AFINN lexicon is centered around 0 we can very generally say that positive scores tend to be more positive and a negative score will tend to accompany negative texts.\n\nThere are many oversimplifications going on here. We are not taking sentence length into account. There is no reason to believe a 100-word review with a score of 10 is any less positive than a 1000-word review with a score of 100. It is also not obvious that “a breathtaking(5) bastard(-5)” is a neutral statement.\n\nWe can visualize the final distribution\nreview_tokens_sentiment %&gt;% \n  ggplot(aes(sentiment)) +\n  geom_bar()\n\nBut it would be more informative if we include grade to see if there is a difference\nreview_tokens_sentiment %&gt;% \n  ggplot(aes(sentiment, fill = grade)) +\n  geom_boxplot()\n\nIt appears that the lexicon is not entirely useless. The sentiments for highly-rated reviews are a little bit higher."
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html#reshaping-a-lexicon",
    "href": "post/textrecipes-series-lexicon/index.html#reshaping-a-lexicon",
    "title": "Textrecipes series: lexicons",
    "section": "Reshaping a lexicon",
    "text": "Reshaping a lexicon\nA lexicon needs to be in a specific format to be used in textrecipes. We need a tibble with the first column containing tokens and any additional columns should contain the numerics. lexicon_afinn() already meets the demand and can be used directly. The lexicon_loughran() doesn’t give us the information we want.\nlexicon_loughran()\n## # A tibble: 4,150 x 2\n##    word         sentiment\n##    &lt;chr&gt;        &lt;chr&gt;    \n##  1 abandon      negative \n##  2 abandoned    negative \n##  3 abandoning   negative \n##  4 abandonment  negative \n##  5 abandonments negative \n##  6 abandons     negative \n##  7 abdicated    negative \n##  8 abdicates    negative \n##  9 abdicating   negative \n## 10 abdication   negative \n## # … with 4,140 more rows\nWith the sentiment being a character denoting the sentiment of the word. What might not be obvious at first glance of this lexicon is that a word can have multiple sentiments such as the word “encumber” which has 3\nlexicon_loughran() %&gt;%\n  filter(word == \"encumber\")\n## # A tibble: 3 x 2\n##   word     sentiment   \n##   &lt;chr&gt;    &lt;chr&gt;       \n## 1 encumber negative    \n## 2 encumber litigious   \n## 3 encumber constraining\nWe can use tidyr to turn this into a wide format.\nlexicon_loughran_wide &lt;- lexicon_loughran() %&gt;%\n  mutate(var = 1) %&gt;% \n  tidyr::pivot_wider(names_from = sentiment, \n                     values_from = var, \n                     values_fill = list(var = 0))\n\nlexicon_loughran_wide\n## # A tibble: 3,917 x 7\n##    word         negative positive uncertainty litigious constraining superfluous\n##    &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n##  1 abandon             1        0           0         0            0           0\n##  2 abandoned           1        0           0         0            0           0\n##  3 abandoning          1        0           0         0            0           0\n##  4 abandonment         1        0           0         0            0           0\n##  5 abandonments        1        0           0         0            0           0\n##  6 abandons            1        0           0         0            0           0\n##  7 abdicated           1        0           0         0            0           0\n##  8 abdicates           1        0           0         0            0           0\n##  9 abdicating          1        0           0         0            0           0\n## 10 abdication          1        0           0         0            0           0\n## # … with 3,907 more rows\nThis is now be used. Textrecipes are able to handle multi-axis lexicons with no problems."
  },
  {
    "objectID": "post/textrecipes-series-lexicon/index.html#using-textrecipes",
    "href": "post/textrecipes-series-lexicon/index.html#using-textrecipes",
    "title": "Textrecipes series: lexicons",
    "section": "Using textrecipes",
    "text": "Using textrecipes\nTo use these lexicons in our modeling step will we use the step_word_embeddings() step. This is normally used for word embeddings, but you can treat a lexicon (when transformed according to the last section) as a selection of word vector or in other words a word embedding.\nTo see the effect lets create a minimal recipe that only sums along the lexicons using the AFINN lexicon\nrecipe(~ text, data = review_training) %&gt;%\n  step_tokenize(text) %&gt;%\n  step_word_embeddings(text, embeddings = lexicon_afinn()) %&gt;%\n  prep() %&gt;%\n  juice()\n## # A tibble: 2,250 x 1\n##    w_embed_sum_value\n##                &lt;dbl&gt;\n##  1               -11\n##  2                 7\n##  3                -5\n##  4                 9\n##  5                 2\n##  6                16\n##  7                11\n##  8                 0\n##  9                 4\n## 10                -3\n## # … with 2,240 more rows\nThis gives us 1 column of the sum of the values. If we instead used the lexicon_loughran_wide lexicon the get back 6 variables.\nrecipe(~ text, data = review_training) %&gt;%\n  step_tokenize(text) %&gt;%\n  step_word_embeddings(text, embeddings = lexicon_loughran_wide, prefix = \"loughran\") %&gt;%\n  prep() %&gt;%\n  juice()\n## # A tibble: 2,250 x 6\n##    loughran_sum_ne… loughran_sum_po… loughran_sum_un… loughran_sum_li…\n##               &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n##  1                2                0                0                0\n##  2                2                8                0                0\n##  3                2                0                0                0\n##  4                0                3                0                0\n##  5                2                2                1                0\n##  6                2                5                6                0\n##  7                0                4                0                0\n##  8                1                0                0                0\n##  9                2                5                0                0\n## 10                0                0                0                0\n## # … with 2,240 more rows, and 2 more variables:\n## #   loughran_sum_constraining &lt;dbl&gt;, loughran_sum_superfluous &lt;dbl&gt;\nTo use the lexicon values along with side term frequencies can we use step_mutate() to create a separate variable to be used for lexicon calculations.\nrec_spec &lt;- recipe(grade ~ text + date, review_training) %&gt;%\n  # Days since release\n  step_mutate(date = as.numeric(date - as.Date(\"2020-03-20\"))) %&gt;%\n  # Tokenize to words\n  step_tokenize(text) %&gt;%\n  \n  # Create copy of text variable\n  step_mutate(text_lexicon = text) %&gt;%\n  # Apply lexicon counting\n  step_word_embeddings(text_lexicon, embeddings = lexicon_afinn(), prefix = \"afinn\") %&gt;%\n  \n  # Remove stopwords\n  step_stopwords(text) %&gt;%\n  # Remove less frequent words\n  step_tokenfilter(text, max_tokens = 100) %&gt;%\n  # Calculate term frequencies\n  step_tf(text, weight_scheme = \"binary\")\n\nrec_spec\n## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          2\n## \n## Operations:\n## \n## Variable mutation for date\n## Tokenization for text\n## Variable mutation for text_lexicon\n## Word embeddings aggregated from text_lexicon\n## Stop word removal for text\n## Text filtering for text\n## Term frequency with text\nBy inspectiong the results we get:\nrec_spec %&gt;%\n  prep() %&gt;%\n  juice()\n## # A tibble: 2,250 x 103\n##     date grade afinn_sum_value tf_text_1 tf_text_10 tf_text_2 tf_text_able\n##    &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt;     &lt;lgl&gt;       \n##  1     0 Low               -11 FALSE     FALSE      FALSE     FALSE       \n##  2     0 Low                 7 TRUE      FALSE      FALSE     TRUE        \n##  3     0 Low                -5 FALSE     FALSE      FALSE     FALSE       \n##  4     0 Low                 9 FALSE     FALSE      FALSE     FALSE       \n##  5     0 Low                 2 FALSE     FALSE      FALSE     TRUE        \n##  6     0 Low                16 FALSE     FALSE      FALSE     FALSE       \n##  7     0 Low                11 FALSE     FALSE      FALSE     FALSE       \n##  8     0 Low                 0 FALSE     FALSE      FALSE     FALSE       \n##  9     0 Low                 4 FALSE     FALSE      FALSE     FALSE       \n## 10     0 Low                -3 FALSE     FALSE      FALSE     FALSE       \n## # … with 2,240 more rows, and 96 more variables: tf_text_absolutely &lt;lgl&gt;,\n## #   tf_text_account &lt;lgl&gt;, tf_text_actually &lt;lgl&gt;, tf_text_also &lt;lgl&gt;,\n## #   tf_text_amazing &lt;lgl&gt;, tf_text_animal &lt;lgl&gt;, tf_text_another &lt;lgl&gt;,\n## #   tf_text_anything &lt;lgl&gt;, tf_text_back &lt;lgl&gt;, tf_text_bad &lt;lgl&gt;,\n## #   tf_text_best &lt;lgl&gt;, tf_text_bought &lt;lgl&gt;, tf_text_buy &lt;lgl&gt;,\n## #   tf_text_can &lt;lgl&gt;, tf_text_console &lt;lgl&gt;, tf_text_crossing &lt;lgl&gt;,\n## #   tf_text_day &lt;lgl&gt;, tf_text_else &lt;lgl&gt;, tf_text_enjoy &lt;lgl&gt;,\n## #   tf_text_even &lt;lgl&gt;, tf_text_ever &lt;lgl&gt;, tf_text_every &lt;lgl&gt;,\n## #   tf_text_everyone &lt;lgl&gt;, tf_text_everything &lt;lgl&gt;, tf_text_expand &lt;lgl&gt;,\n## #   tf_text_experience &lt;lgl&gt;, tf_text_fact &lt;lgl&gt;, tf_text_family &lt;lgl&gt;,\n## #   tf_text_feel &lt;lgl&gt;, tf_text_first &lt;lgl&gt;, tf_text_full &lt;lgl&gt;,\n## #   tf_text_fun &lt;lgl&gt;, tf_text_game &lt;lgl&gt;, tf_text_games &lt;lgl&gt;,\n## #   tf_text_get &lt;lgl&gt;, tf_text_gets &lt;lgl&gt;, tf_text_give &lt;lgl&gt;,\n## #   tf_text_go &lt;lgl&gt;, tf_text_going &lt;lgl&gt;, tf_text_good &lt;lgl&gt;,\n## #   tf_text_great &lt;lgl&gt;, tf_text_horizons &lt;lgl&gt;, tf_text_island &lt;lgl&gt;,\n## #   tf_text_islands &lt;lgl&gt;, `tf_text_it’s` &lt;lgl&gt;, tf_text_just &lt;lgl&gt;,\n## #   tf_text_let &lt;lgl&gt;, tf_text_like &lt;lgl&gt;, tf_text_little &lt;lgl&gt;,\n## #   tf_text_lot &lt;lgl&gt;, tf_text_love &lt;lgl&gt;, tf_text_made &lt;lgl&gt;,\n## #   tf_text_make &lt;lgl&gt;, tf_text_makes &lt;lgl&gt;, tf_text_many &lt;lgl&gt;,\n## #   tf_text_money &lt;lgl&gt;, tf_text_much &lt;lgl&gt;, tf_text_multiplayer &lt;lgl&gt;,\n## #   tf_text_multiple &lt;lgl&gt;, tf_text_never &lt;lgl&gt;, tf_text_new &lt;lgl&gt;,\n## #   tf_text_nintendo &lt;lgl&gt;, tf_text_now &lt;lgl&gt;, tf_text_one &lt;lgl&gt;,\n## #   tf_text_people &lt;lgl&gt;, tf_text_per &lt;lgl&gt;, tf_text_person &lt;lgl&gt;,\n## #   tf_text_play &lt;lgl&gt;, tf_text_played &lt;lgl&gt;, tf_text_player &lt;lgl&gt;,\n## #   tf_text_players &lt;lgl&gt;, tf_text_playing &lt;lgl&gt;, tf_text_progress &lt;lgl&gt;,\n## #   tf_text_really &lt;lgl&gt;, tf_text_review &lt;lgl&gt;, tf_text_save &lt;lgl&gt;,\n## #   tf_text_second &lt;lgl&gt;, tf_text_see &lt;lgl&gt;, tf_text_series &lt;lgl&gt;,\n## #   tf_text_share &lt;lgl&gt;, tf_text_since &lt;lgl&gt;, tf_text_single &lt;lgl&gt;,\n## #   tf_text_start &lt;lgl&gt;, tf_text_still &lt;lgl&gt;, tf_text_switch &lt;lgl&gt;,\n## #   tf_text_system &lt;lgl&gt;, tf_text_thing &lt;lgl&gt;, tf_text_things &lt;lgl&gt;,\n## #   tf_text_think &lt;lgl&gt;, tf_text_time &lt;lgl&gt;, tf_text_two &lt;lgl&gt;,\n## #   tf_text_us &lt;lgl&gt;, tf_text_want &lt;lgl&gt;, tf_text_way &lt;lgl&gt;,\n## #   tf_text_well &lt;lgl&gt;, tf_text_wife &lt;lgl&gt;\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.0 (2020-04-24)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2020-05-11                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version    date       lib source                     \n assertthat      0.2.1      2019-03-21 [1] CRAN (R 4.0.0)             \n backports       1.1.6      2020-04-05 [1] CRAN (R 4.0.0)             \n base64enc       0.1-3      2015-07-28 [1] CRAN (R 4.0.0)             \n bayesplot       1.7.1      2019-12-01 [1] CRAN (R 4.0.0)             \n blogdown        0.18       2020-03-04 [1] CRAN (R 4.0.0)             \n bookdown        0.18       2020-03-05 [1] CRAN (R 4.0.0)             \n boot            1.3-25     2020-04-26 [1] CRAN (R 4.0.0)             \n broom         * 0.5.6      2020-04-20 [1] CRAN (R 4.0.0)             \n callr           3.4.3      2020-03-28 [1] CRAN (R 4.0.0)             \n class           7.3-17     2020-04-26 [1] CRAN (R 4.0.0)             \n cli             2.0.2      2020-02-28 [1] CRAN (R 4.0.0)             \n clipr           0.7.0      2019-07-23 [1] CRAN (R 4.0.0)             \n codetools       0.2-16     2018-12-24 [1] CRAN (R 4.0.0)             \n colorspace      1.4-1      2019-03-18 [1] CRAN (R 4.0.0)             \n colourpicker    1.0        2017-09-27 [1] CRAN (R 4.0.0)             \n crayon          1.3.4      2017-09-16 [1] CRAN (R 4.0.0)             \n crosstalk       1.1.0.1    2020-03-13 [1] CRAN (R 4.0.0)             \n desc            1.2.0      2018-05-01 [1] CRAN (R 4.0.0)             \n details       * 0.2.1      2020-01-12 [1] CRAN (R 4.0.0)             \n dials         * 0.0.6      2020-04-03 [1] CRAN (R 4.0.0)             \n DiceDesign      1.8-1      2019-07-31 [1] CRAN (R 4.0.0)             \n digest          0.6.25     2020-02-23 [1] CRAN (R 4.0.0)             \n dplyr         * 0.8.5      2020-03-07 [1] CRAN (R 4.0.0)             \n DT              0.13       2020-03-23 [1] CRAN (R 4.0.0)             \n dygraphs        1.1.1.6    2018-07-11 [1] CRAN (R 4.0.0)             \n ellipsis        0.3.0      2019-09-20 [1] CRAN (R 4.0.0)             \n emo             0.0.0.9000 2020-05-12 [1] Github (hadley/emo@3f03b11)\n evaluate        0.14       2019-05-28 [1] CRAN (R 4.0.0)             \n fansi           0.4.1      2020-01-08 [1] CRAN (R 4.0.0)             \n fastmap         1.0.1      2019-10-08 [1] CRAN (R 4.0.0)             \n foreach         1.5.0      2020-03-30 [1] CRAN (R 4.0.0)             \n fs              1.4.1      2020-04-04 [1] CRAN (R 4.0.0)             \n furrr           0.1.0      2018-05-16 [1] CRAN (R 4.0.0)             \n future          1.17.0     2020-04-18 [1] CRAN (R 4.0.0)             \n generics        0.0.2      2018-11-29 [1] CRAN (R 4.0.0)             \n ggplot2       * 3.3.0      2020-03-05 [1] CRAN (R 4.0.0)             \n ggridges        0.5.2      2020-01-12 [1] CRAN (R 4.0.0)             \n globals         0.12.5     2019-12-07 [1] CRAN (R 4.0.0)             \n glue            1.4.0      2020-04-03 [1] CRAN (R 4.0.0)             \n gower           0.2.1      2019-05-14 [1] CRAN (R 4.0.0)             \n GPfit           1.0-8      2019-02-08 [1] CRAN (R 4.0.0)             \n gridExtra       2.3        2017-09-09 [1] CRAN (R 4.0.0)             \n gtable          0.3.0      2019-03-25 [1] CRAN (R 4.0.0)             \n gtools          3.8.2      2020-03-31 [1] CRAN (R 4.0.0)             \n htmltools       0.4.0      2019-10-04 [1] CRAN (R 4.0.0)             \n htmlwidgets     1.5.1      2019-10-08 [1] CRAN (R 4.0.0)             \n httpuv          1.5.2      2019-09-11 [1] CRAN (R 4.0.0)             \n httr            1.4.1      2019-08-05 [1] CRAN (R 4.0.0)             \n igraph          1.2.5      2020-03-19 [1] CRAN (R 4.0.0)             \n infer         * 0.5.1      2019-11-19 [1] CRAN (R 4.0.0)             \n inline          0.3.15     2018-05-18 [1] CRAN (R 4.0.0)             \n ipred           0.9-9      2019-04-28 [1] CRAN (R 4.0.0)             \n iterators       1.0.12     2019-07-26 [1] CRAN (R 4.0.0)             \n janeaustenr     0.1.5      2017-06-10 [1] CRAN (R 4.0.0)             \n knitr         * 1.28       2020-02-06 [1] CRAN (R 4.0.0)             \n later           1.0.0      2019-10-04 [1] CRAN (R 4.0.0)             \n lattice         0.20-41    2020-04-02 [1] CRAN (R 4.0.0)             \n lava            1.6.7      2020-03-05 [1] CRAN (R 4.0.0)             \n lhs             1.0.2      2020-04-13 [1] CRAN (R 4.0.0)             \n lifecycle       0.2.0      2020-03-06 [1] CRAN (R 4.0.0)             \n listenv         0.8.0      2019-12-05 [1] CRAN (R 4.0.0)             \n lme4            1.1-23     2020-04-07 [1] CRAN (R 4.0.0)             \n loo             2.2.0      2019-12-19 [1] CRAN (R 4.0.0)             \n lubridate       1.7.8      2020-04-06 [1] CRAN (R 4.0.0)             \n magrittr        1.5        2014-11-22 [1] CRAN (R 4.0.0)             \n markdown        1.1        2019-08-07 [1] CRAN (R 4.0.0)             \n MASS            7.3-51.6   2020-04-26 [1] CRAN (R 4.0.0)             \n Matrix          1.2-18     2019-11-27 [1] CRAN (R 4.0.0)             \n matrixStats     0.56.0     2020-03-13 [1] CRAN (R 4.0.0)             \n mime            0.9        2020-02-04 [1] CRAN (R 4.0.0)             \n miniUI          0.1.1.1    2018-05-18 [1] CRAN (R 4.0.0)             \n minqa           1.2.4      2014-10-09 [1] CRAN (R 4.0.0)             \n munsell         0.5.0      2018-06-12 [1] CRAN (R 4.0.0)             \n nlme            3.1-147    2020-04-13 [1] CRAN (R 4.0.0)             \n nloptr          1.2.2.1    2020-03-11 [1] CRAN (R 4.0.0)             \n nnet            7.3-14     2020-04-26 [1] CRAN (R 4.0.0)             \n parsnip       * 0.1.1      2020-05-06 [1] CRAN (R 4.0.0)             \n pillar          1.4.4      2020-05-05 [1] CRAN (R 4.0.0)             \n pkgbuild        1.0.8      2020-05-07 [1] CRAN (R 4.0.0)             \n pkgconfig       2.0.3      2019-09-22 [1] CRAN (R 4.0.0)             \n plyr            1.8.6      2020-03-03 [1] CRAN (R 4.0.0)             \n png             0.1-7      2013-12-03 [1] CRAN (R 4.0.0)             \n prettyunits     1.1.1      2020-01-24 [1] CRAN (R 4.0.0)             \n pROC            1.16.2     2020-03-19 [1] CRAN (R 4.0.0)             \n processx        3.4.2      2020-02-09 [1] CRAN (R 4.0.0)             \n prodlim         2019.11.13 2019-11-17 [1] CRAN (R 4.0.0)             \n promises        1.1.0      2019-10-04 [1] CRAN (R 4.0.0)             \n ps              1.3.3      2020-05-08 [1] CRAN (R 4.0.0)             \n purrr         * 0.3.4      2020-04-17 [1] CRAN (R 4.0.0)             \n R6              2.4.1      2019-11-12 [1] CRAN (R 4.0.0)             \n Rcpp            1.0.4.6    2020-04-09 [1] CRAN (R 4.0.0)             \n recipes       * 0.1.12     2020-05-01 [1] CRAN (R 4.0.0)             \n reshape2        1.4.4      2020-04-09 [1] CRAN (R 4.0.0)             \n rlang           0.4.6      2020-05-02 [1] CRAN (R 4.0.0)             \n rmarkdown       2.1        2020-01-20 [1] CRAN (R 4.0.0)             \n rpart           4.1-15     2019-04-12 [1] CRAN (R 4.0.0)             \n rprojroot       1.3-2      2018-01-03 [1] CRAN (R 4.0.0)             \n rsample       * 0.0.6      2020-03-31 [1] CRAN (R 4.0.0)             \n rsconnect       0.8.16     2019-12-13 [1] CRAN (R 4.0.0)             \n rstan           2.19.3     2020-02-11 [1] CRAN (R 4.0.0)             \n rstanarm        2.19.3     2020-02-11 [1] CRAN (R 4.0.0)             \n rstantools      2.0.0      2019-09-15 [1] CRAN (R 4.0.0)             \n rstudioapi      0.11       2020-02-07 [1] CRAN (R 4.0.0)             \n scales        * 1.1.1      2020-05-11 [1] CRAN (R 4.0.0)             \n sessioninfo     1.1.1      2018-11-05 [1] CRAN (R 4.0.0)             \n shiny           1.4.0.2    2020-03-13 [1] CRAN (R 4.0.0)             \n shinyjs         1.1        2020-01-13 [1] CRAN (R 4.0.0)             \n shinystan       2.5.0      2018-05-01 [1] CRAN (R 4.0.0)             \n shinythemes     1.1.2      2018-11-06 [1] CRAN (R 4.0.0)             \n SnowballC       0.7.0      2020-04-01 [1] CRAN (R 4.0.0)             \n StanHeaders     2.19.2     2020-02-11 [1] CRAN (R 4.0.0)             \n statmod         1.4.34     2020-02-17 [1] CRAN (R 4.0.0)             \n stopwords       2.0        2020-04-14 [1] CRAN (R 4.0.0)             \n stringi         1.4.6      2020-02-17 [1] CRAN (R 4.0.0)             \n stringr         1.4.0      2019-02-10 [1] CRAN (R 4.0.0)             \n survival        3.1-12     2020-04-10 [1] CRAN (R 4.0.0)             \n textrecipes   * 0.2.2      2020-05-10 [1] CRAN (R 4.0.0)             \n threejs         0.3.3      2020-01-21 [1] CRAN (R 4.0.0)             \n tibble        * 3.0.1      2020-04-20 [1] CRAN (R 4.0.0)             \n tidymodels    * 0.1.0      2020-02-16 [1] CRAN (R 4.0.0)             \n tidyposterior   0.0.2      2018-11-15 [1] CRAN (R 4.0.0)             \n tidypredict     0.4.5      2020-02-10 [1] CRAN (R 4.0.0)             \n tidyr           1.0.3      2020-05-07 [1] CRAN (R 4.0.0)             \n tidyselect      1.1.0      2020-05-11 [1] CRAN (R 4.0.0)             \n tidytext      * 0.2.4      2020-04-17 [1] CRAN (R 4.0.0)             \n timeDate        3043.102   2018-02-21 [1] CRAN (R 4.0.0)             \n tokenizers      0.2.1      2018-03-29 [1] CRAN (R 4.0.0)             \n tune          * 0.1.0      2020-04-02 [1] CRAN (R 4.0.0)             \n usethis         1.6.1      2020-04-29 [1] CRAN (R 4.0.0)             \n vctrs           0.3.0      2020-05-11 [1] CRAN (R 4.0.0)             \n withr           2.2.0      2020-04-20 [1] CRAN (R 4.0.0)             \n workflows     * 0.1.1      2020-03-17 [1] CRAN (R 4.0.0)             \n xfun            0.13       2020-04-13 [1] CRAN (R 4.0.0)             \n xml2            1.3.2      2020-04-23 [1] CRAN (R 4.0.0)             \n xtable          1.8-4      2019-04-21 [1] CRAN (R 4.0.0)             \n xts             0.12-0     2020-01-19 [1] CRAN (R 4.0.0)             \n yaml            2.2.1      2020-02-01 [1] CRAN (R 4.0.0)             \n yardstick     * 0.0.6      2020-03-17 [1] CRAN (R 4.0.0)             \n zoo             1.8-8      2020-05-02 [1] CRAN (R 4.0.0)             \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/xaringan-first-letter/index.html",
    "href": "post/xaringan-first-letter/index.html",
    "title": "xaringan first-letter",
    "section": "",
    "text": "I recently saw the use of the ::first-letter pseudo selector and I was hooked! and I was hooked! This selector allows you to style the first letter in a block, sometimes called a drop cap, and can be used to add a little flair to your xaringan slides. These selectors can just as well be used in any other Html output, they are not limited to xaringan.\nYou can either add the pseudo selector to an existing class, but I’m going to add a new CSS class so I can apply it whenever I want. Below is a class called .drop-cap that doesn’t do anything by itself, but has a ::first-letter pseudo selector that doubles the font-size of the first letter.\n.drop-cap::first-letter {\n  font-size: 200%;\n}\n\nNext, we have the information we are putting in the slide\n.drop-cap[ # Sample Header]\n.drop-cap[ &gt; All the .drop-cap[statistics] in the world can’t measure the warmth of a smile.]\n.drop-cap[ Duis vel viverra elit, eget hendrerit odio. Curabitur cursus elit nec diam vulputate, nec sollicitudin nunc ornare. Ut mi lectus, aliquet non ligula sed, lobortis vehicula erat. Morbi porttitor orci ut semper dapibus. Donec sodales tellus varius tortor varius, ornare commodo augue maximus. Vestibulum quis bibendum mi, sit amet lobortis leo. Morbi vulputate orci arcu, ac lobortis sapien gravida eget. Nulla non interdum orci, nec congue ligula.]\nAnd the result:\n\n\n\n\nand look, each of the first letters of the selected blocks has had their size doubled. Notice that while the word statistics had a .drop-cap around it, it remained unchanged, this is because ::first-letter only works on blocks.\nWe can also get a little bit fancier with our styling to create nice-looking drop caps, next example is inspired by this video by Ethan Marcotte.\n.drop-cap::first-letter {\n  color: #B22222;\n  float: left;\n  font-size: 75px;\n  line-height: 60px;\n  padding-top: 4px;\n  padding-right: 8px;\n  padding-left: 3px;\n}\n\nHere we get a nice drop cap that spans multiple lines.\n\n\n\n\nLastly, there is also a related selector called first-line that works in just the same way as ::first-letter except that it applies the style to the entire first line.\n.super-line::first-line {\n  color: #B22222;\n  font-size: 120%;\n  font-weight: bold;\n}\n\n\n\n\n\nI hope you can use these two selectors to add a little something special to your next set of xaringan slides!"
  },
  {
    "objectID": "post/textrecipes-series-tfidf/index.html",
    "href": "post/textrecipes-series-tfidf/index.html",
    "title": "Textrecipes series: TF-IDF",
    "section": "",
    "text": "This is the third blog post in the textrecipes series where I go over the various text preprocessing workflows you can do with textrecipes. This post will be showcasing how to perform term frequency-inverse document frequency (Tf-IDF for short)."
  },
  {
    "objectID": "post/textrecipes-series-tfidf/index.html#packages",
    "href": "post/textrecipes-series-tfidf/index.html#packages",
    "title": "Textrecipes series: TF-IDF",
    "section": "Packages 📦",
    "text": "Packages 📦\nThe packages used in the post shouldn’t come as any surprise if you have been following the series. tidymodels for modeling, tidyverse for EDA, textrecipes for text preprocessing, and vip for visualizing variable importance.\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(textrecipes)\nlibrary(vip)\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "post/textrecipes-series-tfidf/index.html#exploring-the-data",
    "href": "post/textrecipes-series-tfidf/index.html#exploring-the-data",
    "title": "Textrecipes series: TF-IDF",
    "section": "Exploring the data ⛏",
    "text": "Exploring the data ⛏\nWe will be using a #tidytuesday dataset from almost a year ago, it contains a lot of wine reviews. David Robinson did a very nice screen about this dataset\n\n\nDavid goes into a lot of detail explaining what he is doing and I highly recommend watching this one if you are interested in using text in regression. Fortunately, he didn’t use tidymodels so this post will bring a little something new. Our goal for this post is to build a model that predicts the score (denotes points) a particular wine has.\nwine_ratings &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv\")\n## Warning: Missing column names filled in: 'X1' [1]\nWe load in the data with read_csv() and immediately use glimpse() to get an idea of the data we have to work with\nglimpse(wine_ratings)\n## Rows: 129,971\n## Columns: 14\n## $ X1                    &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n## $ country               &lt;chr&gt; \"Italy\", \"Portugal\", \"US\", \"US\", \"US\", \"Spain\",…\n## $ description           &lt;chr&gt; \"Aromas include tropical fruit, broom, brimston…\n## $ designation           &lt;chr&gt; \"Vulkà Bianco\", \"Avidagos\", NA, \"Reserve Late H…\n## $ points                &lt;dbl&gt; 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87,…\n## $ price                 &lt;dbl&gt; NA, 15, 14, 13, 65, 15, 16, 24, 12, 27, 19, 30,…\n## $ province              &lt;chr&gt; \"Sicily & Sardinia\", \"Douro\", \"Oregon\", \"Michig…\n## $ region_1              &lt;chr&gt; \"Etna\", NA, \"Willamette Valley\", \"Lake Michigan…\n## $ region_2              &lt;chr&gt; NA, NA, \"Willamette Valley\", NA, \"Willamette Va…\n## $ taster_name           &lt;chr&gt; \"Kerin O’Keefe\", \"Roger Voss\", \"Paul Gregutt\", …\n## $ taster_twitter_handle &lt;chr&gt; \"@kerinokeefe\", \"@vossroger\", \"@paulgwine \", NA…\n## $ title                 &lt;chr&gt; \"Nicosia 2013 Vulkà Bianco  (Etna)\", \"Quinta do…\n## $ variety               &lt;chr&gt; \"White Blend\", \"Portuguese Red\", \"Pinot Gris\", …\n## $ winery                &lt;chr&gt; \"Nicosia\", \"Quinta dos Avidagos\", \"Rainstorm\", …\nThis dataset barely contains any numeric variables. The only numeric is the price. As with many prices in data, it is a good idea to log transform them since they are highly skewed\nwine_ratings %&gt;%\n  mutate(price_log = log(price)) %&gt;%\n  pivot_longer(c(price, price_log)) %&gt;%\n  ggplot(aes(value)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(~name, scales = \"free_x\") \n## Warning: Removed 17992 rows containing non-finite values (stat_bin).\n\nSince most of the data most likely will be factors let us take a look at the cardinality of each variable\nmap_int(wine_ratings, n_distinct)\n##                    X1               country           description \n##                129971                    44                119955 \n##           designation                points                 price \n##                 37980                    21                   391 \n##              province              region_1              region_2 \n##                   426                  1230                    18 \n##           taster_name taster_twitter_handle                 title \n##                    20                    16                118840 \n##               variety                winery \n##                   708                 16757\nBut wait! the number of unique descriptions is not the same as the number of rows. This seems very odd since they will be multiple sentences long and the likelihood of two different people writing the same description is very low.\nlet us take a selection of duplicated descriptions and see if anything stands out.\nwine_ratings %&gt;%\n  filter(duplicated(description)) %&gt;%\n  slice(1:3) %&gt;%\n  pull(description)\n## [1] \"This is weighty, creamy and medium to full in body. It has plenty of lime and pear flavors, plus slight brown sugar and vanilla notes.\"                                                             \n## [2] \"There's a touch of toasted almond at the start, but then this Grillo revs up in the glass to deliver notes of citrus, stone fruit, crushed stone and lemon tart. The mouthfeel is crisp and simple.\"\n## [3] \"Lightly herbal strawberry and raspberry aromas are authentic and fresh. On the palate, this is light and juicy, with snappy, lean flavors of red fruit and dry spice. The finish is dry and oaky.\"\nas we feared these are pretty specific and would be unlikely to be duplications at random. We will assume that this problem is a scraping error and remove the duplicate entries. Additionally, some of the point values are missing, since this is our target variable will I remove those data points as well.\nIf you are working on a real project you shouldn’t simply delete observations like that. Both of these errors smell a little bit like bad scraping so your first course of action should be testing your data pipeline for errors.\nI found out about the issue with duplicate descriptions when I was browsing through other people’s analyses of the dataset.\nBefore we do any more analysis, let us remove the troublesome observations.\nwine_ratings &lt;- wine_ratings %&gt;%\n  filter(!duplicated(description), !is.na(price))\n\nwine_ratings\n## # A tibble: 111,567 x 14\n##       X1 country description designation points price province region_1 region_2\n##    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n##  1     1 Portug… This is ri… Avidagos        87    15 Douro    &lt;NA&gt;     &lt;NA&gt;    \n##  2     2 US      Tart and s… &lt;NA&gt;            87    14 Oregon   Willame… Willame…\n##  3     3 US      Pineapple … Reserve La…     87    13 Michigan Lake Mi… &lt;NA&gt;    \n##  4     4 US      Much like … Vintner's …     87    65 Oregon   Willame… Willame…\n##  5     5 Spain   Blackberry… Ars In Vit…     87    15 Norther… Navarra  &lt;NA&gt;    \n##  6     6 Italy   Here's a b… Belsito         87    16 Sicily … Vittoria &lt;NA&gt;    \n##  7     7 France  This dry a… &lt;NA&gt;            87    24 Alsace   Alsace   &lt;NA&gt;    \n##  8     8 Germany Savory dri… Shine           87    12 Rheinhe… &lt;NA&gt;     &lt;NA&gt;    \n##  9     9 France  This has g… Les Natures     87    27 Alsace   Alsace   &lt;NA&gt;    \n## 10    10 US      Soft, supp… Mountain C…     87    19 Califor… Napa Va… Napa    \n## # … with 111,557 more rows, and 5 more variables: taster_name &lt;chr&gt;,\n## #   taster_twitter_handle &lt;chr&gt;, title &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;\nCountries look like it would be important to include, doing a little bar chart reveals a high imbalance in where the wines are coming from. We will need to weed out some of the low count countries\nwine_ratings %&gt;% \n  count(country, sort = TRUE) %&gt;%\n  ggplot(aes(n, fct_reorder(country, n), label = country)) +\n  geom_col() +\n  geom_text(hjust = 0, nudge_x = 1000)\n## Warning: Removed 1 rows containing missing values (geom_text).\n\nThis dataset is restricted to review of wine that scored 80 points or more,\nwine_ratings %&gt;% \n  ggplot(aes(points)) +\n  geom_bar()\n\nIt looks like the 80 wasn’t as hard cutoff, and the points even look bell-shaped.\nI’ll be using tester_name and variety as well in the final analysis."
  },
  {
    "objectID": "post/textrecipes-series-tfidf/index.html#modeling",
    "href": "post/textrecipes-series-tfidf/index.html#modeling",
    "title": "Textrecipes series: TF-IDF",
    "section": "Modeling ⚙️",
    "text": "Modeling ⚙️\nWe start by doing a simple training test split of the data using the yardstick package.\nset.seed(1234)\nwine_split &lt;- initial_split(wine_ratings)\n\nwine_training &lt;- training(wine_split)\nwine_testing &lt;- training(wine_split)\nNext will we use recipes and textrecipes to specify the preprocessing of the data. We - Use step_log() to take the logarithm of price - Use step_uknowm() to turn missing values in factors into levels with name “unknown” - Use step_other() to lump together factor levels that don’t take up more the 1% of the counts. - Use step_dummy() to dummify the factor variables - Use step_tokenize() to turn the descriptions into tokens - Use step_stopwords() to remove stop words from the tokens (ALWAYS manually verify your stop word list) - Use step_tokenfilter() to limit the number of tokens we will use when calculating tf-idf. We will only keep tokens if they appear more then 100 times and of those will be at most take the 1000 most frequent tokens. - Use step_tfidf() to calculate the term frequency-inverse document frequency of the tokens. - Use step_normalize() to normalize all the predictors to have a standard deviation of one and a mean of zero. We need to do this because it’s important for lasso regularization.\nrec_spec &lt;- recipe(points ~ description + price + country + variety + taster_name, \n                   data = wine_training) %&gt;%\n  step_log(price) %&gt;%\n  step_unknown(country, variety, taster_name) %&gt;%\n  step_other(country, variety, threshold = 0.01) %&gt;%\n  step_dummy(country, variety, taster_name) %&gt;%\n  step_tokenize(description) %&gt;%\n  step_stopwords(description) %&gt;%\n  step_tokenfilter(description, min_times = 100, max_tokens = 1000) %&gt;%\n  step_tfidf(description) %&gt;%\n  step_normalize(all_predictors())\nWe will use lasso regression and we will use the “glmnet” engine.\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\nI have specified penalty = tune() because I want to use tune to find the best value of the penalty by doing hyperparameter tuning.\nWe set up a parameter grid using grid_regular()\nparam_grid &lt;- grid_regular(penalty(), levels = 50)\nsearching over 50 levels might seem like a lot. But remember that glmnet is able to calculate them all at once. This is because it relies on its warms starts for speed and it is often faster to fit a whole path than compute a single fit.\nWe will also set up some bootstraps of the data so we can evaluate the performance multiple times for each level.\nwine_boot &lt;- bootstraps(wine_training, times = 10)\nthe last thing we need to use is to create a workflow object to combine the preprocessing step with the model. This is important because we want the preprocessing steps to happen in the bootstraps.\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_spec) %&gt;%\n  add_model(lasso_spec)\nnow we are ready to perform the parameter tuning.\nset.seed(42)\nlasso_grid &lt;- tune_grid(\n  lasso_wf,\n  resamples = wine_boot,\n  grid = param_grid\n) \n## ! Bootstrap01: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap01: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap02: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap02: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap03: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap03: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap04: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap04: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap05: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap05: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap06: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap06: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap07: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap07: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap08: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap08: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap09: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap09: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n## ! Bootstrap10: preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after...\n## ! Bootstrap10: preprocessor 1/1, model 1/1 (predictions): There are new levels in a fac...\n\nlasso_grid\n## Warning: This tuning result has notes. Example notes on model fitting include:\n## preprocessor 1/1, model 1/1 (predictions): There are new levels in a factor: Armenia, Slovakia\n## New levels will be coerced to `NA` by `step_unknown()`.\n## Consider using `step_novel()` before `step_unknown()`., There are new levels in a factor: Vilana, Tinta Miúda, Cesanese, Marsanne-Viognier, Kinali Yapincak, Syrah-Petit Verdot, Moscadello, Loureiro-Arinto, Kangoun, Clairette, Marzemino, Freisa, Barbera-Nebbiolo, Piquepoul Blanc, Cesanese d'Affile, Forcallà, Cabernet Sauvignon-Barbera, Grignolino, \n## preprocessor 1/1, model 1/1 (predictions): There are new levels in a factor: Cesanese, Kinali Yapincak, Karasakiz, Marzemino, Mazuelo, Tinta del Pais, Treixadura, Forcallà, Aleatico, Prunelard, Mourvèdre-Syrah, Ugni Blanc, Groppello, Irsai Oliver, Alvarelhão, Rosenmuskateller, Malbec-Cabernet, Mansois, Mavroudi, Casavecchia, Tinto Velasco, Terrantez, Rebula, Malvasia di Candia, Erbaluce, Orangetraube, Pinotage-Merlot, Chardonnay-Riesling, Bastardo, Vidadillo, Souzao, Mandilaria, Albarossa, Baga-\n## preprocessor 1/1: 'keep_original_cols' was added to `step_dummy()` after this recipe was created.\n## Regenerate your recipe to avoid this warning.\n## # Tuning results\n## # Bootstrap sampling \n## # A tibble: 10 x 4\n##    splits                id          .metrics           .notes          \n##    &lt;list&gt;                &lt;chr&gt;       &lt;list&gt;             &lt;list&gt;          \n##  1 &lt;split [83676/30847]&gt; Bootstrap01 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  2 &lt;split [83676/30813]&gt; Bootstrap02 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  3 &lt;split [83676/30782]&gt; Bootstrap03 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  4 &lt;split [83676/30629]&gt; Bootstrap04 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  5 &lt;split [83676/30777]&gt; Bootstrap05 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  6 &lt;split [83676/30725]&gt; Bootstrap06 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  7 &lt;split [83676/30792]&gt; Bootstrap07 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  8 &lt;split [83676/30777]&gt; Bootstrap08 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n##  9 &lt;split [83676/30895]&gt; Bootstrap09 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\n## 10 &lt;split [83676/30752]&gt; Bootstrap10 &lt;tibble [100 × 5]&gt; &lt;tibble [2 × 1]&gt;\nNow that the grid search has finished we can look at the best performing models with show_best().\nshow_best(lasso_grid, metric =  \"rmse\")\n## # A tibble: 5 x 6\n##    penalty .metric .estimator  mean     n std_err\n##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 0.00356  rmse    standard    1.64    10 0.00222\n## 2 0.00222  rmse    standard    1.64    10 0.00220\n## 3 0.00139  rmse    standard    1.64    10 0.00219\n## 4 0.00569  rmse    standard    1.64    10 0.00227\n## 5 0.000869 rmse    standard    1.64    10 0.00218\nWe are quite satisfied with these results! Use select_best() to extract the best performing one\nbest_penalty &lt;- select_best(lasso_grid, metric =  \"rmse\")\nand we will use that value of penalty in our final workflow object\nfinal_wf &lt;- finalize_workflow(\n  lasso_wf,\n  best_penalty\n)\nNow all there is to do is to fit the workflow on the real training dataset.\nfinal_lasso &lt;- final_wf %&gt;%\n  fit(data = wine_training)\nAnd then, finally, let’s return to our test data. The tune package has a function last_fit() which is nice for situations when you have tuned and finalized a model or workflow and want to fit it one last time on your training data and evaluate it on your testing data. You only have to pass this function your finalized model/workflow and your split.\nFinally can we return to our testing dataset. We can use the last_fit() function to apply our finalized workflow to the testing dataset and see what performance we are getting.\nlast_fit(final_lasso, wine_split) %&gt;%\n  collect_metrics()\n## ! Resample1: model (predictions): There are new levels in a factor: NA\n## # A tibble: 2 x 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       1.65 \n## 2 rsq     standard       0.721"
  },
  {
    "objectID": "post/ggpage-version-0-2-0-showcase/index.html",
    "href": "post/ggpage-version-0-2-0-showcase/index.html",
    "title": "ggpage version 0.2.0 showcase",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16."
  },
  {
    "objectID": "post/ggpage-version-0-2-0-showcase/index.html#ggpage-version-0.2.0",
    "href": "post/ggpage-version-0-2-0-showcase/index.html#ggpage-version-0.2.0",
    "title": "ggpage version 0.2.0 showcase",
    "section": "ggpage version 0.2.0",
    "text": "ggpage version 0.2.0\nIn this post I will highlight a couple of the new features in the new update of my package ggpage.\nfirst we load the packages we need, which is tidyverse for general tidy tools, ggpage for visualization and finally rtweet and rvest for data collection.\nlibrary(tidyverse)\nlibrary(ggpage)\nlibrary(rtweet)\nlibrary(rvest)"
  },
  {
    "objectID": "post/ggpage-version-0-2-0-showcase/index.html#the-basics",
    "href": "post/ggpage-version-0-2-0-showcase/index.html#the-basics",
    "title": "ggpage version 0.2.0 showcase",
    "section": "The basics",
    "text": "The basics\nThe packages includes 2 main functions, ggpage_build and ggpage_plot that will transform the data in the right way and plot it respectively. The reason for the split of the functions is to allow additional transformations to be done on the tokenized data to be used in the plotting.\nThe package includes a example data set of the text Tinderbox by H.C. Andersen\ntinderbox %&gt;%\n  head()\n## # A tibble: 6 x 2\n##   text                                                              book        \n##   &lt;chr&gt;                                                             &lt;chr&gt;       \n## 1 \"A soldier came marching along the high road: \\\"Left, right - le… The tinder-…\n## 2 \"had his knapsack on his back, and a sword at his side; he had b… The tinder-…\n## 3 \"and was now returning home. As he walked on, he met a very frig… The tinder-…\n## 4 \"witch in the road. Her under-lip hung quite down on her breast,… The tinder-…\n## 5 \"and said, \\\"Good evening, soldier; you have a very fine sword, … The tinder-…\n## 6 \"knapsack, and you are a real soldier; so you shall have as much… The tinder-…\nThis data set can be used directly with ggpage_build and ggpage_plot.\nggpage_build(tinderbox) %&gt;%\n  ggpage_plot()\n\nggpage_build expects the column containing the text to be named “text” which it is in the tinderbox object. This visualization gets exiting when you start combining it with the other tools. We can show where the word “tinderbox” appears along with adding some page numbers.\nggpage_build(tinderbox) %&gt;%\n  mutate(tinderbox = word == \"tinderbox\") %&gt;%\n  ggpage_plot(mapping = aes(fill = tinderbox), page.number = \"top-left\")\n\nAnd we see that the word tinderbox only appear 3 times in the middle of the story."
  },
  {
    "objectID": "post/ggpage-version-0-2-0-showcase/index.html#vizualizing-tweets",
    "href": "post/ggpage-version-0-2-0-showcase/index.html#vizualizing-tweets",
    "title": "ggpage version 0.2.0 showcase",
    "section": "Vizualizing tweets",
    "text": "Vizualizing tweets\nWe can also use this to showcase a number of tweets. For this we will use the rtweet package. We will load in 100 tweets that contain the hash tag #rstats.\n## whatever name you assigned to your created app\nappname &lt;- \"********\"\n\n## api key (example below is not a real key)\nkey &lt;- \"**********\"\n\n## api secret (example below is not a real key)\nsecret &lt;- \"********\"\n\n## create token named \"twitter_token\"\ntwitter_token &lt;- create_token(\n  app = appname,\n  consumer_key = key,\n  consumer_secret = secret)\nrstats_tweets &lt;- rtweet::search_tweets(\"#rstats\") %&gt;%\n  mutate(status_id = as.numeric(as.factor(status_id)))\nSince each tweet is too long by itself will we use the nest_paragraphs function to wrap the texts within each tweet. By passing the tweet id to page.col we will make it such that we have a tweet per page. Additionally we can indicate both whether the tweet is a retweet by coloring the paper blue if it is and green if it isn’t. Lastly we highlight where “rstats” is used.\nrstats_tweets %&gt;%\n  select(status_id, text) %&gt;%\n  nest_paragraphs(text) %&gt;%\n  ggpage_build(page.col = \"status_id\", lpp = 4, ncol = 6) %&gt;%\n  mutate(rstats = word == \"rstats\") %&gt;%\n  ggpage_plot(mapping = aes(fill = rstats), paper.show = TRUE, \n              paper.color = ifelse(rstats_tweets$is_retweet, \"lightblue\", \"lightgreen\")) +\n  scale_fill_manual(values = c(\"grey60\", \"black\")) +\n  labs(title = \"100 #rstats tweets\",\n       subtitle = \"blue = retweet, green = original\")"
  },
  {
    "objectID": "post/ggpage-version-0-2-0-showcase/index.html#vizualizing-documents",
    "href": "post/ggpage-version-0-2-0-showcase/index.html#vizualizing-documents",
    "title": "ggpage version 0.2.0 showcase",
    "section": "Vizualizing documents",
    "text": "Vizualizing documents\nNext we will look at the Convention on the Rights of the Child which we will scrape with rvest.\nurl &lt;- \"http://www.ohchr.org/EN/ProfessionalInterest/Pages/CRC.aspx\"\n\nrights_text &lt;- read_html(url) %&gt;%\n  html_nodes('div[class=\"boxtext\"]') %&gt;%\n  html_text() %&gt;%\n  str_split(\"\\n\") %&gt;%\n  unlist() %&gt;%\n  str_wrap() %&gt;%\n  str_split(\"\\n\") %&gt;%\n  unlist() %&gt;%\n  data.frame(text = ., stringsAsFactors = FALSE)\nIn this case will we remove the vertical space between the pages have it appear as a long paper like the website.\nThe wonderful case_when comes in vary handy here when we want to highlight multiple different words.\nfor the purpose of the “United Nations” was it necessary to check that the words “united” and “nations” only appeared in pairs.\nrights_text %&gt;%\n  ggpage_build(wtl = FALSE, y_space_pages = 0, ncol = 7) %&gt;%\n  mutate(highlight = case_when(word %in% c(\"child\", \"children\") ~ \"child\",\n                               word %in% c(\"right\", \"rights\") ~ \"rights\",\n                               word %in% c(\"united\", \"nations\") ~ \"United Nations\",\n                               TRUE ~ \"other\")) %&gt;%\n  ggpage_plot(mapping = aes(fill = highlight)) +\n  scale_fill_manual(values = c(\"darkgreen\", \"grey\", \"darkblue\", \"darkred\")) +\n  labs(title = \"Word highlights in the 'Convention on the Rights of the Child'\",\n       fill = NULL)\n\nThis is just a couple of different ways to use this package. I look forward to see what you guys can come up with.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-13                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggpage      * 0.2.3   2019-06-13 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lattice       0.20-44 2021-05-02 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n Matrix        1.3-3   2021-05-04 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rtweet      * 0.7.0   2020-01-08 [1] CRAN (R 4.1.0)                   \n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n SnowballC     0.7.0   2020-04-01 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidytext      0.3.1   2021-04-10 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n tokenizers    0.2.1   2018-03-29 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html",
    "title": "Text Classification with Tidymodels",
    "section": "",
    "text": "Caution\n\n\n\nThis post was written with early versions of tidymodels packages. And in some ways have not aged perfectly. The general idea about this post is still valid, but if you want more up-to-date code please refer to https://www.tidymodels.org/."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#introduction",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#introduction",
    "title": "Text Classification with Tidymodels",
    "section": "Introduction",
    "text": "Introduction\nI have previously used this blog to talk about text classification a couple of times. tidymodels have since then seen quite a bit of progress. I did in addition get the textrecipes package on CRAN, which provides extra steps to recipes package from tidymodels.\nSeeing the always wonderful post by Julia Silge on text classification with tidy data principles encouraged me to show how the same workflow also can be accomplished in tidymodels.\nTo give this post a little spice will only be using stop words. Yes, you read that right, we will only keep stopping words. Words you are often encouraged to exclude as they don’t provide much information. We will challenge that assumption in this post! To have a baseline for our stop word model will I be using the same data as Julia used in her post."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#data",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#data",
    "title": "Text Classification with Tidymodels",
    "section": "Data",
    "text": "Data\nThe data we will be using is the text from Pride and Prejudice and the text from The War of the Worlds. These texts can be get from Project Gutenberg using the gutenbergr package. Note that both works are in English1.\nlibrary(tidyverse)\nlibrary(gutenbergr)\n\ntitles &lt;- c(\n  \"The War of the Worlds\",\n  \"Pride and Prejudice\"\n)\nbooks &lt;- gutenberg_works(title %in% titles) %&gt;%\n  gutenberg_download(meta_fields = \"title\") %&gt;%\n  mutate(title = as.factor(title)) %&gt;%\n  select(-gutenberg_id)\n\nbooks\n(deviating from Julia, will we drop the gutenberg_id variable as it is redundant, remove the document variable as it isn’t needed in the tidymodels framework, and set the title variable as a factor as it works better with tidymodels used later on.)\nI’m going to quote Julia to explain the modeling problem we are facing;\n\nWe have the text data now and let’s frame the kind of prediction problem we are going to work on. Imagine that we take each book and cut it up into lines, like strips of paper (✨ confetti ✨) with an individual line on each paper. Let’s train a model that can take an individual line and give us a probability that this book comes from Pride and Prejudice vs. from The War of the Worlds.\n\nSo that is a fairly straightforward task, we already have the data as we want in books. Before we go on let’s investigate the class imbalance.\nbooks %&gt;%\n  ggplot(aes(title)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = NULL,\n       y = \"Count\",\n       title = \"Number of Strips in 'Pride and Prejudice' and 'The War of the Worlds'\")\nIt is a little uneven, but we will carry on."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#stop-words",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#stop-words",
    "title": "Text Classification with Tidymodels",
    "section": "Stop words",
    "text": "Stop words\nLet’s first have a talk about stop words. These are the words that are needed for the sentences to be structurally sound but don’t add any information. however, such a concept as “non-informational” is quite abstract and is bound to be highly domain-specific. We will be using the English snowball stop word lists provided by the stopwords package (because that is what textrecipes naively uses).\nlibrary(stopwords)\nstopwords(language = \"en\", source = \"snowball\") %&gt;% sort()\nthis list contains 175 words. Many of these words will at first glance pass the “non-informational” test. However, if you look at it more you will realize that many of these can have meaning in certain contexts. The word “i” for example will be used more in blog posts than legal documents. Secondly, there appear to be quite a lot of negation words, “wouldn’t”, “don’t”, “doesn’t” and “mustn’t” just to list a few. This is another reminder that constructing your own stop word list can be highly beneficial for your project as the default list might not work in your field.\nWhile these words are assumed to have little information, the distribution of them and the relational information contained with how the stop word is used compared to each other might give us some information anyways. One author might use negations more often than another, maybe someone really likes to use the word “nor”. These kinds of features can be extracted as the distributional information, or in other words “counts”. We will count how often each stop word appears and hope that some of the words can divide the authors. Next, we have the order in which words appear in. This is related to writing style, some authors might write “… will you please…” while others might use “… you will handle…”. The way each word combination is used might be worth a little bit of information. We will capture the relational information with ngrams.\nWe will briefly showcase how this works with an example.\nsentence &lt;- \"This is an example sentence that is used to explain the concept of ngrams.\"\nto extract the ngrams we will use the tokenizers package (also default in textrecipes). Here we can get all the trigrams (ngrams of length 3).\nlibrary(tokenizers)\ntokenize_ngrams(sentence, n = 3)\nhowever, we would also like to the singular word counts (unigrams) and bigrams (ngrams of length 2). This can easily be done by setting the n_min argument.\ntokenize_ngrams(sentence, n = 3, n_min = 1)\nNow we get unigrams, bigrams, and trigrams in one. But wait, we wanted to limit our focus to stop words. Here is how the end result will look once we exclude all non-stop words and perform the ngram operation.\ntokenize_words(sentence) %&gt;%\n  unlist() %&gt;%\n  intersect(stopwords(language = \"en\", source = \"snowball\")) %&gt;%\n  paste(collapse = \" \") %&gt;%\n  print() %&gt;%\n  tokenize_ngrams(n = 3, n_min = 1)\nWe have quite a reduction in ngrams than the full sentence, but hopefully there is some information within."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#training-testing-split",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#training-testing-split",
    "title": "Text Classification with Tidymodels",
    "section": "Training & testing split",
    "text": "Training & testing split\nBefore we start modeling we need to split our data into a testing and training set. This is easily done using the rsample package from tidymodels.\nlibrary(tidymodels)\nset.seed(1234) \n\nbooks_split &lt;- initial_split(books, strata = \"title\", p = 0.75)\ntrain_data &lt;- training(books_split)\ntest_data &lt;- testing(books_split)"
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#preprocessing",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#preprocessing",
    "title": "Text Classification with Tidymodels",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe next step is to do the preprocessing. For this will we use the recipes from tidymodels. This allows us to specify a preprocessing design that can be train on the training data and applied to the training and testing data alike. I created textrecipes as recipes don’t naively support text preprocessing.\nI’m are going to replicate Julia’s preprocessing here to make comparisons easier for myself. Notice the step_filter() call, the original text have quite a lot of empty lines and these don’t contain any textual information at all so we will filter away these observations. Note also that we could have used all_predictors() instead of text as it is the only predictor we have.\nlibrary(textrecipes)\njulia_rec &lt;- recipe(title ~ ., data = train_data) %&gt;%\n  step_filter(text != \"\") %&gt;%\n  step_tokenize(text) %&gt;%\n  step_tokenfilter(text, min_times = 11) %&gt;%\n  step_tf(text) %&gt;%\n  prep(training = train_data)\njulia_rec\nThis recipe will remove empty texts, tokenize to words (default in step_tokenize()), keeping words that appear 10 times or more in the training set, and then count how many times each word appears. The processed data looks like this\njulia_train_data &lt;- juice(julia_rec)\njulia_test_data  &lt;- bake(julia_rec, test_data)\n\nstr(julia_train_data, list.len = 10)\nThe reason we get features and Julia got 1652 is because she did her filtering on the full dataset where we only did the filtering on the training set and that Julia didn’t explicitly remove empty observations.\nBack to stop words!! In this case, we need a slightly more complicated recipe\nstopword_rec &lt;- recipe(title ~ ., data = train_data) %&gt;%\n  step_filter(text != \"\") %&gt;%\n  step_tokenize(text) %&gt;%\n  step_stopwords(text, keep = TRUE) %&gt;%\n  step_untokenize(text) %&gt;%\n  step_tokenize(text, token = \"ngrams\", options = list(n = 3, n_min = 1)) %&gt;%\n  step_tokenfilter(text, min_times = 10) %&gt;%\n  step_tf(text) %&gt;%\n  prep(training = train_data)\nstopword_rec\nFirst, we tokenize to words, remove all non-stop words, untokenize (which is basically just paste() with a fancy name), tokenize to ngrams, remove ngrams that appear less than 10 times, and lastly we count how often each ngram appear.\n# Processed data\nstopword_train_data &lt;- juice(stopword_rec)\nstopword_test_data  &lt;- bake(stopword_rec, test_data)\n\nstr(stopword_train_data, list.len = 10)\nAnd we are left with features."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#modeling",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#modeling",
    "title": "Text Classification with Tidymodels",
    "section": "Modeling",
    "text": "Modeling\nFor modeling, we will be using the parsnip package from tidymodels. First, we start by defining a model specification. This defines the intent of our model, what we want to do, not what we want to do it on. Meaning we don’t include the data yet, just the kind of model, its hyperparameters, and the engine (the package that will do the work). We will be using glmnet package here so we will specify a logistic regression model\nglmnet_model &lt;- logistic_reg(mixture = 0, penalty = 0.1) %&gt;%\n  set_engine(\"glmnet\")\nglmnet_model\nHere we will fit the models using both our training data, first using the stop words, then using the simple would count approach.\nstopword_model &lt;- glmnet_model %&gt;%\n  fit(title ~ ., data = stopword_train_data)\n\njulia_model &lt;- glmnet_model %&gt;%\n  fit(title ~ ., data = julia_train_data)\nThis is the part of the workflow where one should do hyperparameter optimization and explore different models to find the best model for the task. For the interest of the length of this post will this step be excluded, possibly to be explored in a future post 😉."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#evaluation",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#evaluation",
    "title": "Text Classification with Tidymodels",
    "section": "Evaluation",
    "text": "Evaluation\nNow that we have fitted the data based on the training data we can evaluate based on the testing data set. Here we will use the parsnip functions predict_class() and predict_classprob() to give us the predicted class and predicted probabilities for the two models. Neatly collecting the whole thing in one tibble.\neval_tibble &lt;- stopword_test_data %&gt;%\n  select(title) %&gt;%\n  mutate(\n    class_stopword = parsnip:::predict_class(stopword_model, stopword_test_data),\n    class_julia    = parsnip:::predict_class(julia_model, julia_test_data),\n    prop_stopword  = parsnip:::predict_classprob(stopword_model, stopword_test_data) %&gt;% pull(`The War of the Worlds`),\n    prop_julia     = parsnip:::predict_classprob(julia_model, julia_test_data) %&gt;% pull(`The War of the Worlds`)\n  )\n\neval_tibble\nTidymodels includes the yardstick package which makes evaluation calculations much easier and tidy. It can allow us to calculate the accuracy by calling the accuracy() function\naccuracy(eval_tibble, truth = title, estimate = class_stopword)\naccuracy(eval_tibble, truth = title, estimate = class_julia)\nAnd we see that the stop words model beats the naive model (one that always picks the majority class) while lacking behind the word count model.\ntest_data %&gt;%\n  filter(text != \"\") %&gt;%\n  summarise(mean(title == \"Pride and Prejudice\"))\nWe are also able to plot the ROC curve using roc_curve()(notice how we are using the predicted probabilities instead of class) and autoplot()\neval_tibble %&gt;%\n  roc_curve(title, prop_stopword) %&gt;%\n  autoplot()\nTo superimpose both ROC curve we are going to tidyr our data a little bit.\neval_tibble %&gt;%\n  rename(`Word Count` = prop_julia, `Stopwords` = prop_stopword) %&gt;%\n  gather(\"Stopwords\", \"Word Count\", key = \"Model\", value = \"Prop\") %&gt;%\n  group_by(Model) %&gt;%\n  roc_curve(title, Prop) %&gt;%\n  autoplot() +\n  labs(title = \"ROC curve for text classification using word count or stopwords\",\n       subtitle = \"Predicting whether text was written by Jane Austen or H.G. Wells\") +\n  paletteer::scale_color_paletteer_d(\"ggsci::category10_d3\")"
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#conclusion",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#conclusion",
    "title": "Text Classification with Tidymodels",
    "section": "Conclusion",
    "text": "Conclusion\nI’m not going to tell you that you should run an “all stop words only” model every time you want to do text classification. But I hope this exercise shows you that stop words which are assumed to have no information do indeed have some degree of information. Please always look at your stop word list, check if you even need to remove them, some studies show that removal of stop words might not provide the benefit you thought.\nFurthermore, I hope to have shown the power of tidymodels. Tidymodels is still growing, so if you have any feedback/bug reports/suggests please go to the respective repositories, we would highly appreciate it!"
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#comments",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#comments",
    "title": "Text Classification with Tidymodels",
    "section": "Comments",
    "text": "Comments\nThis plot was suggested in the comments, Thanks Isaiah!\nstopword_model$fit %&gt;% \n  tidy() %&gt;%\n  mutate(term = str_replace(term, \"tf_text_\", \"\")) %&gt;%\n  group_by(estimate &gt; 0) %&gt;%\n  top_n(10, abs(estimate)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &gt; 0)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = NULL,\n  title = \"Coefficients that increase/decrease probability the most\",\n  subtitle = \"Stopwords only\")\nAnd Isaiah notes that\n\nWhereas Julia’s analysis using nonstop words showed that Elizabeth is the opposite of a Martian, stop words show that Pride and Prejudice talks of men and women, and War of the Worlds makes declarations about existence.\n\nWhich I would like to say looks pretty spot on."
  },
  {
    "objectID": "post/2018-12-29-text-classification-with-tidymodels/index.html#footnotes",
    "href": "post/2018-12-29-text-classification-with-tidymodels/index.html#footnotes",
    "title": "Text Classification with Tidymodels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16."
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#what-are-we-doing",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#what-are-we-doing",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "What are we doing?",
    "text": "What are we doing?\nThe inspiration for this post is this beautiful vizualization from Mike Bostock. It nicely visualizes the co-occurrence of characters (when two characters appear in the same chapter) in the English1 version of the novel Les Misérables by Victor Hugo using the data collected by Jacques Bertin (and his assistants).\nThe way this post will differentiate itself from this is that we are going to collect the data ourselves using named entity recognition. Named entity recognition is the discipline of location and classifying named entities in text. Furthermore, will we also try to cluster the characters according to their appearance in the novel.\ndisclaimer! I have of the time of writing this analysis not read of familiarized myself with Les Misérables in an attempt to show how a blind text analysis would run."
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#loading-package-and-backend",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#loading-package-and-backend",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "Loading package and backend",
    "text": "Loading package and backend\nfor this we will need tidyverse for general data science tasks, spacyr for the named entity recognition, and igraph for some graph related transformation.\nlibrary(tidyverse)\nlibrary(spacyr)\nlibrary(igraph)\nWe will be using the spacy NLP back-end as the parser for this analysis since it provides named entity recognition as one of its functionalities."
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#data",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#data",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "Data",
    "text": "Data\nLes Miserable is quite a long novel, in the terms of words and pages, however, due to its age is it in the public domain and is easily available on Project Gutenberg.\nlesmis_raw &lt;- gutenbergr::gutenberg_download(135)\nLooking through the beginning of the text we notice how a large part of the beginning of the document is a table of content and other information that isn’t of interest in this analysis. Manually checking leads us to discard the first 650 lines of the data. We will also add a chapter column using a regex.\nlesmis_line &lt;- lesmis_raw %&gt;%\n  slice(-(1:650)) %&gt;%\n  mutate(chapter = cumsum(str_detect(text, \"CHAPTER \")))\nFor the use in cnlp_annotate() we need a data.frame where each row is a full chapter, with the 2 necessary columns id and text. This is accomplished using a simple map.\nlesmis &lt;- map_df(seq_len(max(lesmis_line$chapter)),\n                 ~ tibble(id = .x,\n                          text = lesmis_line %&gt;% \n                                   filter(chapter == .x) %&gt;% \n                                   pull(text) %&gt;% \n                                   paste(collapse = \" \")))\nNow we are all ready to run the spacy parser which will only take a couple of minutes.\nlesmis_obj &lt;- spacy_parse(lesmis$text)\n## Found 'spacy_condaenv'. spacyr will use this environment\n## successfully initialized (spaCy Version: 3.0.5, language model: en_core_web_sm)\n## (python options: type = \"condaenv\", value = \"spacy_condaenv\")\nthe output we are given nothing more than a simple tibble\nlesmis_obj\n##   doc_id sentence_id token_id   token   lemma   pos entity\n## 1  text1           1        1                 SPACE       \n## 2  text1           1        2 CHAPTER chapter  NOUN       \n## 3  text1           1        3       I       i  NOUN       \n## 4  text1           1        4       —       — PUNCT       \n## 5  text1           1        5       A       a   DET       \n## 6  text1           1        6   WOUND   WOUND PROPN\nthe entity information can be extracted using entity_extract()\nentity_extract(lesmis_obj)\n##   doc_id sentence_id           entity entity_type\n## 1  text2           1       CHAPTER_II         LAW\n## 2  text2           2 THE_END_OF_WHICH         ORG\n## 3  text4           1       CHAPTER_II         LAW\n## 4  text5           1        TOUSSAINT      PERSON\n## 5  text6           1            STONE         ORG\n## 6  text8           1       CHAPTER_VI         LAW\nWe see quite a few different entity_types, in fact, lets take a quick look at the different types that are in this text\nentity_extract(lesmis_obj) %&gt;%\n  pull(entity_type) %&gt;%\n  unique()\n##  [1] \"LAW\"      \"ORG\"      \"PERSON\"   \"EVENT\"    \"GPE\"      \"PRODUCT\" \n##  [7] \"WORK\"     \"FAC\"      \"NORP\"     \"LOC\"      \"LANGUAGE\"\nThis labeling is explained here. After a bit of investigating I have decided that we only will look at “PERSON” and “ORG” (which is due in part to Napoleon being classified as an organization.) Furthermore, I will limit further analysis to about the 50 most mentioned characters. The rationale behind this is that it hopefully would capture most of the important characters, with the weight that characters that are mentioned sparingly but consistently are more important than characters with high density in a few chapters. We will include a few more characters in case we have to exclude some of them after looking.\ntop_person_df &lt;- entity_extract(lesmis_obj) %&gt;%\n  filter(entity_type %in% c(\"ORG\", \"PERSON\")) %&gt;%\n  count(entity, sort = TRUE) %&gt;%\n  slice(seq_len(60))\n\ntop_person_vec &lt;- top_person_df %&gt;% pull(entity)\ntop_person_vec\n##  [1] \"Jean_Valjean\"              \"Thénardier\"               \n##  [3] \"Javert\"                    \"Gavroche\"                 \n##  [5] \"Cosette\"                   \"Monsieur\"                 \n##  [7] \"M._Madeleine\"              \"Courfeyrac\"               \n##  [9] \"Fantine\"                   \"Bishop\"                   \n## [11] \"M._Gillenormand\"           \"Waterloo\"                 \n## [13] \"Montparnasse\"              \"M._Leblanc\"               \n## [15] \"Rue\"                       \"Napoleon\"                 \n## [17] \"Madeleine\"                 \"Jean_Valjean_’s\"          \n## [19] \"Enjolras\"                  \"Éponine\"                  \n## [21] \"Jondrette\"                 \"M._Mabeuf\"                \n## [23] \"Champmathieu\"              \"Toussaint\"                \n## [25] \"Monseigneur\"               \"Seine\"                    \n## [27] \"Austerlitz\"                \"Madame_Magloire\"          \n## [29] \"Théodule\"                  \"Bahorel\"                  \n## [31] \"Louis_Philippe\"            \"Nicolette\"                \n## [33] \"Gillenormand\"              \"M._Fauchelevent\"          \n## [35] \"Mademoiselle_Gillenormand\" \"Mayor\"                    \n## [37] \"Magnon\"                    \"Toulon\"                   \n## [39] \"Cosette_’s\"                \"Pontmercy\"                \n## [41] \"Jean_Prouvaire\"            \"Thou\"                     \n## [43] \"Grantaire\"                 \"l’Homme_Armé\"             \n## [45] \"Mademoiselle_Baptistine\"   \"Sister_Simplice\"          \n## [47] \"Cæsar\"                     \"Blücher\"                  \n## [49] \"Blachevelle\"               \"Genappe\"                  \n## [51] \"Laigle\"                    \"Ursule\"                   \n## [53] \"the_National_Guard\"        \"Aunt_Gillenormand\"        \n## [55] \"Bishop_’s\"                 \"Bossuet\"                  \n## [57] \"Louis_XVIII\"               \"M._Myriel\"                \n## [59] \"Mestienne\"                 \"Monsieur_Madeleine\"\nAfter looking we see a few things we would like to fix before moving on. Firstly is “CHAPTER IV” and “CHAPTER VI” wrongly both classified as “ORG”s. ” “,”-” and “exclaimed:–” and “Monsieur” have also been misclassified. “Jean Valjean’s” have been classified differently than “Jean Valjean” which is also the case with “Fauchelevent” and “M. Fauchelevent”, “M. Madeleine” and “Madeleine”, “M. Gillenormand”, “Gillenormand” and “Mademoiselle Gillenormand”. We will remove the miss-classifications here, and create a list of all the characters with all of their names. The list is named with the character’s main name for later subsetting.\ntop_person_vec_clean &lt;- top_person_vec[-c(9, 13, 29, 34, 42, 56)] \n\ncomplications &lt;- list(c(\"Jean Valjean\", \"Jean Valjean's\"),\n                      c(\"Fauchelevent\", \"M. Fauchelevent\"),\n                      c(\"Madeleine\", \"M. Madeleine\"),\n                      c(\"Gillenormand\", \"M. Gillenormand\", \"Mademoiselle Gillenormand\"))\n\ncharacters &lt;- setdiff(top_person_vec_clean, unlist(complications)) %&gt;%\n  as.list() %&gt;%\n  c(complications)\n\nnames(characters) &lt;- map_chr(characters, ~ .x[1])\nWe expand the grid of all possible co-occurrences and count how many times they both occur within a chapter.\nco_occurrence &lt;- expand.grid(map_chr(characters, ~ .x[1]), \n                             map_chr(characters, ~ .x[1])) %&gt;%\n  set_names(c(\"person1\", \"person2\")) %&gt;%\n  mutate(cooc = map2_dbl(person1, person2,\n                         ~ sum(str_detect(lesmis$text, str_c(.x, collapse = \"|\")) & \n                               str_detect(lesmis$text, str_c(.y, collapse = \"|\")))))"
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#visualize",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#visualize",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "Visualize",
    "text": "Visualize\nnow that we have the co-occurrence data we can make some visualizations!! (I will take care of labels etc in the end. Hang on!)\nco_occurrence %&gt;%\n  ggplot(aes(person1, person2, fill = cooc)) +\n  geom_tile()\n\nSo at a first glance is it hard to see anything due to the default color scale and the fact that a couple of people, Jean Valjean and Marius, appear in a much higher number of chapters (perhaps they are main characters?). To get a more manageable scale we disregard co-occurrence if they have been in less than 5 chapters together(remember that there are a total of 365 chapters in the novel).\nco_occurrence_1 &lt;- co_occurrence %&gt;%\n  mutate(cooc = ifelse(cooc &gt; 5, log(cooc), NA))\n\nco_occurrence_1 %&gt;%\n    ggplot(aes(person1, person2, fill = cooc)) +\n  geom_tile()\n\nNow we finally see some of the fruit of our earlier work. It is definitely clear that there are groups of people that might form communities but it is unclear which and how many from this heat-map by itself. We would like to reorder the axis’s in the hope that it would create more clarity.\nThis data here can be seen as a Adjacency matrix here the row numbers are vertices and the tiles-values are the edges connecting them. So in a sense, we would like to do some cluster analysis on this graph. This can be done by doing some Spectral Graph Partitioning in which we calculate the eigenvectors and sort the vertices by the second smallest eigenvector.\neigen &lt;- co_occurrence_1 %&gt;%\n#  mutate(cooc = !is.na(cooc)) %&gt;%\n  igraph::graph_from_data_frame() %&gt;%\n  igraph::as_adj() %&gt;%\n  eigen()\n\neigenvec2_sort &lt;- data.frame(eigen = eigen$vectors[, length(eigen$values) - 1]) %&gt;%\n  mutate(row = row_number(),\n         names = names(characters)) %&gt;%\n  arrange(eigen)\n\neigen_names &lt;- eigenvec2_sort %&gt;% pull(names)\nWe use sorted names to re-level the factors in the co-occurrence data and see if it reveals more structure.\nco_occurrence_1 %&gt;%\n  mutate(person1 = factor(person1, levels = eigen_names),\n         person2 = factor(person2, levels = eigen_names)) %&gt;%\n    ggplot(aes(person1, person2, fill = cooc)) +\n  geom_tile()\n\nit isn’t much but it appears to have moved the data slightly closer to the diagonal. We will still need to locate some communities in this data. this can be done using the plotted eigenvector.\neigenvec2_sort %&gt;% pull(eigen) %&gt;% plot(type = \"o\")\n\nAnd what we are looking at is not their position but at the jumps. There can more easily be seen when we look at the diffs\neigenvec2_sort %&gt;% pull(eigen) %&gt;% diff() %&gt;% plot()\nabline(h = 0.02)\n\nAnd after playing around a little it seems that 0.02 is an appropriate cutoff.\ncummunity_df &lt;- eigenvec2_sort %&gt;%\n  mutate(community = c(0, diff(eigen) &gt; 0.02) %&gt;% cumsum()) %&gt;%\n  select(names, community)\nWe will color-code the final visualization according to this clustering. So with a couple of joins\nco_occurrence_comm &lt;- co_occurrence_1 %&gt;%\n  filter(!is.na(cooc)) %&gt;%\n  mutate(person1_chr = as.character(person1),\n         person2_chr = as.character(person2),\n         person1 = factor(person1, levels = eigen_names),\n         person2 = factor(person2, levels = eigen_names)) %&gt;%\n  left_join(cummunity_df, by = c(\"person1_chr\" = \"names\")) %&gt;%\n  left_join(cummunity_df, by = c(\"person2_chr\" = \"names\")) %&gt;%\n  mutate(community = ifelse(community.x == community.y, community.x, NA),\n         community = ifelse(!is.na(cooc), community, NA))\nWith a couple of final touch-ups and we arrive at the final result:\nco_occurrence_comm %&gt;%\n  ggplot(aes(person1, person2, alpha = cooc, fill = factor(community))) +\n  geom_tile(color = \"grey50\") +\n  scale_alpha(range = c(0.5, 1)) +\n  scale_fill_brewer(palette = \"Set1\", na.value = \"grey50\") +\n  theme_minimal() + \n  theme(panel.grid.major = element_blank(),\n        axis.text.x = element_text(angle = 90, hjust = 1)) +\n  guides(fill = \"none\", alpha = \"none\") +\n  coord_fixed() +\n  labs(x = NULL, y = NULL, \n       title = \"Les Misérables Co-occurrence\", \n       subtitle = \"with color-coded communities\")"
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#conclusion",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#conclusion",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "Conclusion",
    "text": "Conclusion\nWhile I wasn’t able to find as full clusters as Jacques Bertin I still managed to get quite a lot of information out of the text regardless. I had fun in the progress and there are many more things I see myself doing with this new data set and spacyr.\nAnd while I couldn’t find a good way to include it in the main body of the text. I almost finished the main analysis before realizing what Monsieur means. Mention your mistakes in your posts so others can learn from them!\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date       lib source        \n assertthat     0.2.1   2019-03-21 [1] CRAN (R 4.0.0)\n backports      1.2.1   2020-12-09 [1] CRAN (R 4.0.2)\n blogdown       1.3     2021-04-14 [1] CRAN (R 4.0.2)\n bookdown       0.22    2021-04-22 [1] CRAN (R 4.0.2)\n broom          0.7.6   2021-04-05 [1] CRAN (R 4.0.2)\n bslib          0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)\n cellranger     1.1.0   2016-07-27 [1] CRAN (R 4.0.0)\n cli            3.0.0   2021-06-30 [1] CRAN (R 4.0.2)\n clipr          0.7.1   2020-10-08 [1] CRAN (R 4.0.2)\n codetools      0.2-18  2020-11-04 [1] CRAN (R 4.0.5)\n colorspace     2.0-2   2021-06-24 [1] CRAN (R 4.0.2)\n crayon         1.4.1   2021-02-08 [1] CRAN (R 4.0.2)\n curl           4.3.2   2021-06-23 [1] CRAN (R 4.0.2)\n data.table     1.14.0  2021-02-21 [1] CRAN (R 4.0.2)\n DBI            1.1.1   2021-01-15 [1] CRAN (R 4.0.2)\n dbplyr         2.1.1   2021-04-06 [1] CRAN (R 4.0.2)\n desc           1.3.0   2021-03-05 [1] CRAN (R 4.0.2)\n details      * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)\n digest         0.6.27  2020-10-24 [1] CRAN (R 4.0.2)\n dplyr        * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)\n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.0.2)\n evaluate       0.14    2019-05-28 [1] CRAN (R 4.0.0)\n fansi          0.5.0   2021-05-25 [1] CRAN (R 4.0.2)\n farver         2.1.0   2021-02-28 [1] CRAN (R 4.0.2)\n forcats      * 0.5.1   2021-01-27 [1] CRAN (R 4.0.2)\n fs             1.5.0   2020-07-31 [1] CRAN (R 4.0.2)\n generics       0.1.0   2020-10-31 [1] CRAN (R 4.0.2)\n ggplot2      * 3.3.5   2021-06-25 [1] CRAN (R 4.0.2)\n glue           1.4.2   2020-08-27 [1] CRAN (R 4.0.2)\n gtable         0.3.0   2019-03-25 [1] CRAN (R 4.0.0)\n gutenbergr     0.1.5   2019-09-10 [1] CRAN (R 4.0.0)\n haven          2.4.1   2021-04-23 [1] CRAN (R 4.0.2)\n highr          0.9     2021-04-16 [1] CRAN (R 4.0.2)\n hms            1.1.0   2021-05-17 [1] CRAN (R 4.0.2)\n htmltools      0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)\n httr           1.4.2   2020-07-20 [1] CRAN (R 4.0.2)\n igraph       * 1.2.6   2020-10-06 [1] CRAN (R 4.0.2)\n jquerylib      0.1.4   2021-04-26 [1] CRAN (R 4.0.2)\n jsonlite       1.7.2   2020-12-09 [1] CRAN (R 4.0.2)\n knitr        * 1.33    2021-04-24 [1] CRAN (R 4.0.2)\n labeling       0.4.2   2020-10-20 [1] CRAN (R 4.0.2)\n lattice        0.20-41 2020-04-02 [1] CRAN (R 4.0.5)\n lifecycle      1.0.0   2021-02-15 [1] CRAN (R 4.0.2)\n lubridate      1.7.10  2021-02-26 [1] CRAN (R 4.0.2)\n magrittr       2.0.1   2020-11-17 [1] CRAN (R 4.0.2)\n Matrix         1.3-2   2021-01-06 [1] CRAN (R 4.0.5)\n modelr         0.1.8   2020-05-19 [1] CRAN (R 4.0.0)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.0.0)\n pillar         1.6.1   2021-05-16 [1] CRAN (R 4.0.2)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.0.0)\n png            0.1-7   2013-12-03 [1] CRAN (R 4.0.0)\n purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)\n R6             2.5.0   2020-10-28 [1] CRAN (R 4.0.2)\n RColorBrewer   1.1-2   2014-12-07 [1] CRAN (R 4.0.2)\n Rcpp           1.0.6   2021-01-15 [1] CRAN (R 4.0.2)\n readr        * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2)\n readxl         1.3.1   2019-03-13 [1] CRAN (R 4.0.2)\n reprex         2.0.0   2021-04-02 [1] CRAN (R 4.0.2)\n reticulate     1.20    2021-05-03 [1] CRAN (R 4.0.2)\n rlang          0.4.11  2021-04-30 [1] CRAN (R 4.0.2)\n rmarkdown      2.9     2021-06-15 [1] CRAN (R 4.0.2)\n rprojroot      2.0.2   2020-11-15 [1] CRAN (R 4.0.2)\n rstudioapi     0.13    2020-11-12 [1] CRAN (R 4.0.2)\n rvest          1.0.0   2021-03-09 [1] CRAN (R 4.0.2)\n sass           0.4.0   2021-05-12 [1] CRAN (R 4.0.2)\n scales         1.1.1   2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 4.0.0)\n spacyr       * 1.2.1   2020-03-04 [1] CRAN (R 4.0.2)\n stringi        1.6.2   2021-05-17 [1] CRAN (R 4.0.2)\n stringr      * 1.4.0   2019-02-10 [1] CRAN (R 4.0.0)\n tibble       * 3.1.2   2021-05-16 [1] CRAN (R 4.0.2)\n tidyr        * 1.1.3   2021-03-03 [1] CRAN (R 4.0.2)\n tidyselect     1.1.1   2021-04-30 [1] CRAN (R 4.0.2)\n tidyverse    * 1.3.1   2021-04-15 [1] CRAN (R 4.0.2)\n triebeard      0.3.0   2016-08-04 [1] CRAN (R 4.0.0)\n urltools       1.7.3   2019-04-14 [1] CRAN (R 4.0.0)\n utf8           1.2.1   2021-03-12 [1] CRAN (R 4.0.2)\n vctrs          0.3.8   2021-04-29 [1] CRAN (R 4.0.2)\n withr          2.4.2   2021-04-18 [1] CRAN (R 4.0.2)\n xfun           0.24    2021-06-15 [1] CRAN (R 4.0.2)\n xml2           1.3.2   2020-04-23 [1] CRAN (R 4.0.0)\n yaml           2.2.1   2020-02-01 [1] CRAN (R 4.0.0)\n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#footnotes",
    "href": "post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index.html#footnotes",
    "title": "Co Occurrence of Characters in Les Miserable",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/paletteer-version-1-0-0/index.html",
    "href": "post/paletteer-version-1-0-0/index.html",
    "title": "Paletteer version 1.0.0",
    "section": "",
    "text": "I’m over-the-moon excited to announce the release of version 1.0.0 of paletteer. This version comes with breaking changes and major quality of life improvements. I will unironically name this the “first useable version” for reasons that will be obvious later in this post."
  },
  {
    "objectID": "post/paletteer-version-1-0-0/index.html#breaking-changes",
    "href": "post/paletteer-version-1-0-0/index.html#breaking-changes",
    "title": "Paletteer version 1.0.0",
    "section": "Breaking Changes 💥",
    "text": "Breaking Changes 💥\nThere has been a significant change in syntax for this version. For versions &lt;= 0.2.1 the way to specify a palette was done using the arguments package and palette. Both could be taken as both strings or unquoted strings.\n# versions &lt;= 0.2.1\npaletteer_c(\"gameofthrones\", \"baratheon\", 10)\n\npaletteer_d(nord, halifax_harbor)\nWhile convinient and cool to use NSE, tt was not very useful and I had several people complaining. I realized that using NSE wasn’t a good fit at all for this package. This means that from version 1.0.0 and moving forward only strings will be used to specify palettes.\nSecondly, I have eliminated the package argument and from now on all specification is done on the form package::palette\n# versions &gt;= 1.0.0\npaletteer_c(\"gameofthrones::baratheon\", 10)\n\npaletteer_d(\"nord::halifax_harbor\")\nThe above change is the most likely to break your earlier code."
  },
  {
    "objectID": "post/paletteer-version-1-0-0/index.html#autocomplete",
    "href": "post/paletteer-version-1-0-0/index.html#autocomplete",
    "title": "Paletteer version 1.0.0",
    "section": "Autocomplete 🎉",
    "text": "Autocomplete 🎉\nThe biggest downside to the original version of paletteer and the later version was the lack of discoverability. Unless you knew the palette you wanted and the EXACT spelling you couldn’t use paletteer. Sure you could browse palettes_c_names and palettes_d_names like some caveman, but to be honest the package felt more like a novelty project than a useful tool.\nAll of this changes with version 1.0.0 🎉! Simply starting by typing paletteer_d() or any of the other related functions and simply hit tab. This will prompt all the names of available palettes which you then can search through using fuzzy search.\n\nThis change is the single biggest improvement to this package.\n\nDiscoverability ✅\nNo more missspellings ✅\nTotal awesomeness ✅\n\nAnd yes, it also work with the scale_*_paletteer() functions 🙌"
  },
  {
    "objectID": "post/paletteer-version-1-0-0/index.html#prismatic-integration",
    "href": "post/paletteer-version-1-0-0/index.html#prismatic-integration",
    "title": "Paletteer version 1.0.0",
    "section": "Prismatic integration 💎",
    "text": "Prismatic integration 💎\nYou can see from the first gif that the output is a little more colorful then what you are used to. This all comes from the prismatic package I released earlier this year. The prismatic colors objects that are returned from all paletteer functions will be printed with colorful backgrounds provided that the crayon package is available, otherwise, it will just print normally. This is great for when you want to take a quick look at the colors you are about to use. Please note that the background can only take 256 different colors. Some palettes will fit nicely inside these 256 values and will display nicely (viridis::magma) below, while other palettes with a lot of value will show weird jumps in colors (gameofthrones::greyjoy)\n\nIf you want more accurate color depictions you can simply plot() the output to see the real colors\nplot(paletteer_c(\"viridis::magma\", 10))\n\nplot(paletteer_c(\"gameofthrones::greyjoy\", 100))"
  },
  {
    "objectID": "post/paletteer-version-1-0-0/index.html#more-color-palettes",
    "href": "post/paletteer-version-1-0-0/index.html#more-color-palettes",
    "title": "Paletteer version 1.0.0",
    "section": "More color palettes 🌈",
    "text": "More color palettes 🌈\nIt wouldn’t be a paletteer release without more palettes. And this release is no different! This update brings us 654 new palettes!!! from 19 different packages bringing out a total of 1759. I did a little live-tweeting while implementing these packages so you can take a look at the newly included palettes here:\n\n\nI'll be adding a whole bunch of new palettes to {paletteer} tonight! 🌈Read this thread if you want to see the new colorful goodies coming your way!❤️💙💚🧡💛💜#rstats pic.twitter.com/c0qK27nc4N\n\n— Emil Hvitfeldt (@Emil_Hvitfeldt) December 8, 2019\n\n\nThat is all I have for you this time around if you create or find more palette packages please go over and file an issue so they can be included as well. Thank you!\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version    date       lib source                           \n assertthat      0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                   \n blogdown        1.3.2      2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown        0.22       2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib           0.2.5.1    2021-05-18 [1] CRAN (R 4.1.0)                   \n cli             3.0.0      2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr           0.7.1      2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools       0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace      2.0-2      2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon          1.4.1      2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI             1.1.1      2021-01-15 [1] CRAN (R 4.1.0)                   \n desc            1.3.0      2021-03-05 [1] CRAN (R 4.1.0)                   \n details       * 0.2.1      2020-01-12 [1] CRAN (R 4.1.0)                   \n digest          0.6.27     2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr           1.0.7      2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis        0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                   \n emo             0.0.0.9000 2021-07-17 [1] Github (hadley/emo@3f03b11)      \n evaluate        0.14       2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi           0.5.0      2021-05-25 [1] CRAN (R 4.1.0)                   \n farver          2.1.0      2021-02-28 [1] CRAN (R 4.1.0)                   \n gameofthrones   1.0.2      2020-02-23 [1] CRAN (R 4.1.0)                   \n generics        0.1.0      2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2         3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                   \n glue            1.4.2      2020-08-27 [1] CRAN (R 4.1.0)                   \n gridExtra       2.3        2017-09-09 [1] CRAN (R 4.1.0)                   \n gtable          0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                   \n highr           0.9        2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools       0.5.1.1    2021-01-22 [1] CRAN (R 4.1.0)                   \n httr            1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib       0.1.4      2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite        1.7.2      2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr         * 1.33       2021-04-24 [1] CRAN (R 4.1.0)                   \n lifecycle       1.0.0      2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate       1.7.10     2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr        2.0.1      2020-11-17 [1] CRAN (R 4.1.0)                   \n MASS            7.3-54     2021-05-03 [1] CRAN (R 4.1.0)                   \n munsell         0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                   \n paletteer     * 1.3.0      2021-01-06 [1] CRAN (R 4.1.0)                   \n pillar          1.6.1      2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig       2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                   \n png             0.1-7      2013-12-03 [1] CRAN (R 4.1.0)                   \n prismatic       1.0.0      2021-01-05 [1] CRAN (R 4.1.0)                   \n purrr           0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                   \n R6              2.5.0      2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp            1.0.7      2021-07-07 [1] CRAN (R 4.1.0)                   \n rematch2        2.1.2      2020-05-01 [1] CRAN (R 4.1.0)                   \n rlang           0.4.11     2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown       2.9        2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot       2.0.2      2020-11-15 [1] CRAN (R 4.1.0)                   \n sass            0.4.0      2021-05-12 [1] CRAN (R 4.1.0)                   \n scales          1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo     1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi         1.6.2      2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr         1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble          3.1.2      2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyselect      1.1.1      2021-04-30 [1] CRAN (R 4.1.0)                   \n utf8            1.2.1      2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs           0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                   \n withr           2.4.2      2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun            0.24       2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2            1.3.2      2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml            2.2.1      2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/stm-stemming/index.html",
    "href": "post/stm-stemming/index.html",
    "title": "Using stm to Investigate if Stemming is Appropriate",
    "section": "",
    "text": "It is known that topic modeling does not benefit from stemming ref. I propose a workflow to investigate if stemming is appropriate as a method for data reduction.\nIf grouped words are predicted to the same topic then we assume that stemming would not make much of a difference. If the words are predicted to be indifferent topics then we have a suspicion that the stemmed and unstemmed words have different uses and stemming would be ill-advised.\nFirst, we load the packages we will be using.\nAs a first test, we pick 3 English1 fairy tales by H.C. Andersens using the hcandersenr package. To create multiple “documents” for each fairy tale we start by tokenizing to sentences. Then we give each sentence a unique identifier.\nNow we unnest the tokens to words and create a new variable of the stemmed words\nWe can take a look at all the times where stemming we can look at all the times stemming yields a different token.\nIn this example, we have 759 different tokens. But since stemming can collapse multiple different tokens into one.\nWe can use the different data.frame and construct a list of words that would land in the same bucket after stemming.\nHere we see that “anxiou” and “anxious” would look the same after stemming, likewise will “apples”, “apple” and “appl”. The main point of this exercise is to see if the words in these groups of words end up in the topic when during topic modeling.\nIn this case, I fit the model to 3 topics because I knew that would be the right number since I picked the data. When doing this on your data you should run multiple models with a varying number of topics to find the best one. For more information please read Training, Evaluating, and Interpreting Topic Models by Julia Silge.\nNow that we have an stm model and a list of words, We can inspect the model object to check if multiple words are put in the same topic. Below is a function that will take a vector of characters and an stm model and return TRUE if all the words appear in the same topic and FALSE if they don’t.\nAs an example, if we pass the words “believed” and “believ”\nWe see that they did end up in the same bucket. If we instead pass in “dog” and “happy” they land in different topics.\nAll of this is not perfect, there is still some uncertainty but it is a good first step to evaluate if stemming is appropriate for your application.\nFirst, we’ll look at the distribution of TRUEs and FALSEs.\nSo it looks like most of the word groups were put into the same topic during modeling. This is a good sign. Please note that this category includes a lot of false positives. This is happening because stm_match() also returns true for a case where one of the words appears in the model and all other words don’t. So for the case of “accompanied” and “accompani”, the word “accompanied” was present in one of the topics, but the word “accompani” was not present in the original data and hence did not appear in any of the topics. In this case, the TRUE value we are getting is saying that the data doesn’t provide enough evidence to indicate that stemming would be bad. By looking at a sample of TRUE cases we see that a lot of them are happening because the stemmed word isn’t being used, like the words “aliv”, “alon” and “alwai”. On the other side, we have that the words “allowed” and “allow” are both real words AND they appeared in the same topic.\nTurning our head to the FALSE cases. These cases will not have any false positives as both words would have to appear in the original corpus for them to be put into different topics. These cases are still not going to be perfect, but will again be an indication.\nThis is the list I would advise you to look over carefully. Check to make sure that you are okay with the number and count of misgroupings you would get by applying stemming."
  },
  {
    "objectID": "post/stm-stemming/index.html#footnotes",
    "href": "post/stm-stemming/index.html#footnotes",
    "title": "Using stm to Investigate if Stemming is Appropriate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n#benderrule↩︎"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html",
    "title": "Recreate - Sunshine Report",
    "section": "",
    "text": "Note\n\n\n\nThis code has been lightly revised to make sure it works as of 2018-12-16.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data or using data of a similar style."
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#the-goal---an-annual-sunshine-record-report",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#the-goal---an-annual-sunshine-record-report",
    "title": "Recreate - Sunshine Report",
    "section": "The goal - An annual sunshine record report",
    "text": "The goal - An annual sunshine record report\nI have recently read The Visual Display of Quantitative Information by Edward R Tufte, which I highly recommend. In the book the following chart was displayed which showed the sunshine record for each day of the year.\n\nF.J. Monkhouse and H.R. Wilkinson, Maps and Diagrams (London, third edition 1971), 242-243.\nThe goal for the rest of this post is to create something similar. Since we don’t have direct access to the data, we will scrape some data for ourselves. All code will be shown together at the end of the post and this gist"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#r-packages",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#r-packages",
    "title": "Recreate - Sunshine Report",
    "section": "R packages",
    "text": "R packages\nFirst, we need some packages\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(glue)\nlibrary(ehlib) # devtools::install_github(\"EmilHvitfeldt/ehlib\")\nThe last package is my personal R package ehlib where I store some frequently used functions. If you do not wish to install/load this package just run the following code:\nstr_between &lt;- function(string, start, end) {\n  stringr::str_extract(string,\n                       stringr::str_c(start, '(.*?)', end, collapse = '')) %&gt;%\n    stringr::str_replace(start, \"\") %&gt;%\n    stringr::str_replace(end, \"\")\n}\n\nstr_before &lt;- function(string, pattern) {\n  stringr::str_extract(string, stringr::str_c(\".+?(?=\", pattern, \")\"))\n}"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#data-collection",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#data-collection",
    "title": "Recreate - Sunshine Report",
    "section": "Data collection",
    "text": "Data collection\nSo for this production, we need Weather information. But more specifically we need information about if the sun is shining for various times during the day, preferable for all days of the year. In addition, sunrise and sunset times are also needed.\nWe will be scraping weather history from wunderground. On the button of the page https://www.wunderground.com/history/airport/KCQT/2018/1/1/DailyHistory.html, we locate a table with “Time” and “Conditions”. Furthermore, both sunrise and sunset times are present on the page.\nFor the website, we need an airport code, year, month, and day. Airport codes will have to be found manually by browsing the website. For a vector of all the days in a given year, we use the following function that uses\nall_dates_in &lt;- function(year) {\n  if(ymd(glue::glue(\"{year}0101\")) &gt; as.Date(Sys.time())) {\n    stop(\"Please select a past or current year.\")\n  }\n  \n  start &lt;- ymd(glue::glue(\"{year}0101\"))\n  \n  if(as.Date(Sys.time()) &gt; ymd(glue::glue(\"{year}1231\"))) {\n    end &lt;- ymd(glue::glue(\"{year}1231\"))\n  } else {\n    end &lt;- as.Date(Sys.time())\n  }\n  \n  seq(start, end, by = \"day\")\n}\nthis function will work even if you pick a year that has not ended yet. As 2017 has just ended I thought it would be appropriate to look back on that year.\nyear &lt;- 2017\ndates &lt;- all_dates_in(year)\nhead(dates)\n## [1] \"2017-01-01\" \"2017-01-02\" \"2017-01-03\" \"2017-01-04\" \"2017-01-05\"\n## [6] \"2017-01-06\"\nnext, we have a little function that creates a URL from the airport code and the date. For safety, we will wrap that function in purrr::safely.\nweather_data_html &lt;- function(date, code) {\n  url &lt;- str_c(\"https://www.wunderground.com/history/airport/\", code, \"/\",\n               year(date), \"/\", month(date), \"/\", mday(date), \"/DailyHistory.html\")\n  \n  html_url &lt;- read_html(url)\n}\n\nweather_data_html &lt;- purrr::safely(weather_data_html)\nFor this code-though will be using airport code KCQT, which is placed in Los Angeles Downtown, CA.\nWe add some ‘crawl-delay’ of 5 seconds and let it run. Please remember that this will take over 30 minutes to run with a delay in place but we do it to be nice.\nairport_code &lt;- \"KCQT\"\n\nfull_data &lt;- map(dates, ~{\n  weather_data_html(.x, airport_code)\n  Sys.sleep(5)\n  cat(month(.x), \"/\", mday(.x), \"\\n\", sep = \"\")\n  })\nWe can check whether all of the links went through.\nmap_lgl(full_data, ~ is.null(.x$error))"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#data-wrangling",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#data-wrangling",
    "title": "Recreate - Sunshine Report",
    "section": "Data wrangling",
    "text": "Data wrangling\nSince we will be working with times quite a lot in the section we will use the lubridate package for quite some time. In addition to that package, I have devised the following function to turn something of the form “2:51 PM” into the number of minutes after midnight.\nampm_minutes &lt;- function(x) {\n  as.numeric(str_between(x, \":\", \" \")) +\n  as.numeric(str_replace(str_before(x, \":\"), \"12\", \"0\")) * 60 +\n  60 * 12 * str_detect(x, \"PM\")\n}\nNext, we have the main wrangling function that takes the input, extracts the sunrise, sunset times, and adds them to the table that is also extracted.\ndata_wrangling &lt;- function(html_url, date) {\n  \n  # Sun rise time\n    sun_rise &lt;- html_url %&gt;%\n    html_nodes('div[id=\"astronomy-mod\"] table') %&gt;%\n    html_text() %&gt;%\n    .[1] %&gt;%\n    str_between(\"Time\\n\\t\\t\", \"\\n\\t\\t\")\n  # Sun set time\n  sun_set &lt;- html_url %&gt;%\n    html_nodes('div[id=\"astronomy-mod\"] table') %&gt;%\n    html_text() %&gt;%\n    .[1] %&gt;%\n    str_between(\"\\n\\t\\t\", \"\\n\\t\\tCivil\")\n\n  # Table\n  table &lt;- html_url %&gt;%\n    html_nodes('table[id=\"obsTable\"]') %&gt;%\n    html_table() %&gt;% \n    .[[1]]\n  \n  # Time column standardization \n  is_daylight &lt;- any(\"Time (PDT)\" == names(table),\n                     \"Time (MDT)\" == names(table),\n                     \"Time (CDT)\" == names(table),\n                     \"Time (EDT)\" == names(table))\n  \n  time_names &lt;- str_c(\"Time\", c(\" (PDT)\", \" (MDT)\", \" (CDT)\", \" (EDT)\",\n                                \" (PST)\", \" (MST)\", \" (CST)\", \" (EST)\"))\n  \n  names(table) &lt;- if_else(names(table) %in% time_names,\n                          \"Time\",\n                          names(table))\n  \n  table %&gt;%\n    mutate(sun_set = sun_set,\n           sun_rise = sun_rise,\n           date = date,\n           yday = yday(date), \n           day_minutes = ampm_minutes(Time) - is_daylight * 60,\n           set_minutes = ampm_minutes(sun_set) - is_daylight * 60,\n           rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60,\n           sun_up = day_minutes &gt; (rise_minutes + 90) & \n                    day_minutes &lt; (set_minutes - 30))\n}\nIn this function, we arbitrarily decide that the sun is up if it is 90 minutes after sunrise and 30 minutes before sunset. This is done because our future visualization is being made with rectangles and the lag function, and to ensure that all the sunshine hours are within sunset and sunrise we have to put in some restrains.\nIt seems that the 30th of October doesn’t have hourly history data available so we will exclude it in the following:\nfull_data2 &lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %&gt;%\n                      data_wrangling(.y))\nAt this point, it would be wise to save our data.\nsave(full_data2, file = glue(\"{airport_code}-{year}.Rdata\"))"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#plotting-data",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#plotting-data",
    "title": "Recreate - Sunshine Report",
    "section": "Plotting data",
    "text": "Plotting data\nNow that we have all the data we need it is time to turn our heads to ggplot2. But before we do that let us create some axis breaks that we will need.\nx_axis &lt;- dates %&gt;% month() %&gt;% table() %&gt;% cumsum()\nnames(x_axis) &lt;- month.abb[1:12]\n\ny_axis &lt;- 1:24 * 60\nnames(y_axis) &lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), \n                       rep(c(\"AM\", \"PM\"), each = 12))\nSo we start by creating a new condition for “Clear”, creating a new day_minutes variable to act as the other side for our sunshine rectangles and lastly remove all the observations where the sun isn’t up. Using geom_rect() to create all the little rectangles and geom_line()’s to show the sun set and sun rise, we lastly fiddle a little with the theme giving us the final result:\nfull_data2 %&gt;%\n  mutate(con = Conditions == \"Clear\",\n         day_minutes2 = lag(day_minutes)) %&gt;%\n  filter(sun_up) %&gt;%\n  ggplot(aes(fill = con)) +\n  geom_rect(aes(xmin = yday, xmax = yday + 1,\n                ymin = day_minutes, ymax = day_minutes2)) +\n  geom_line(aes(yday, set_minutes)) +\n  geom_line(aes(yday, rise_minutes)) +\n  scale_fill_manual(values = c(\"grey40\", NA)) +\n  theme_minimal() +\n  guides(fill = \"none\") +\n  theme(\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(), \n  axis.text.x.bottom = element_text(hjust = 1.7)\n  ) +\n  scale_x_continuous(breaks = x_axis, position = \"right\") +\n  scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) +\n  labs(x = NULL, y = NULL, title = \"Sunshine report of Los Angeles 2017\")\n## Warning: Position guide is perpendicular to the intended axis. Did you mean to\n## specify a different guide `position`?"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#extra",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#extra",
    "title": "Recreate - Sunshine Report",
    "section": "Extra",
    "text": "Extra\n## Warning: Position guide is perpendicular to the intended axis. Did you mean to\n## specify a different guide `position`?"
  },
  {
    "objectID": "post/2018-01-01-recreate-sunshine-report/index.html#code",
    "href": "post/2018-01-01-recreate-sunshine-report/index.html#code",
    "title": "Recreate - Sunshine Report",
    "section": "Code",
    "text": "Code\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(glue)\n#library(ehlib) # devtools::install_github(\"EmilHvitfeldt/ehlib\")\n\nstr_between &lt;- function(string, start, end) {\n  stringr::str_extract(string,\n                       stringr::str_c(start, '(.*?)', end, collapse = '')) %&gt;%\n    stringr::str_replace(start, \"\") %&gt;%\n    stringr::str_replace(end, \"\")\n}\n\nstr_before &lt;- function(string, pattern) {\n  stringr::str_extract(string, stringr::str_c(\".+?(?=\", pattern, \")\"))\n}\n\nall_dates_in &lt;- function(year) {\n  if(ymd(glue::glue(\"{year}0101\")) &gt; as.Date(Sys.time())) {\n    stop(\"Please select a past or current year.\")\n  }\n  \n  start &lt;- ymd(glue::glue(\"{year}0101\"))\n  \n  if(as.Date(Sys.time()) &gt; ymd(glue::glue(\"{year}1231\"))) {\n    end &lt;- ymd(glue::glue(\"{year}1231\"))\n  } else {\n    end &lt;- as.Date(Sys.time())\n  }\n  \n  seq(start, end, by = \"day\")\n}\n\nairport_code &lt;- \"KCQT\"\n\nfull_data &lt;- map(dates, ~{\n  weather_data_html(.x, airport_code)\n  Sys.sleep(5)\n  cat(month(dates), \"/\", mday(dates), \"\\n\", sep = \"\")\n  })\n\nmap_lgl(full_data, ~ is.null(.x$error))\n\nampm_minutes &lt;- function(x) {\n  as.numeric(str_between(x, \":\", \" \")) +\n  as.numeric(str_replace(str_before(x, \":\"), \"12\", \"0\")) * 60 +\n  60 * 12 * str_detect(x, \"PM\")\n}\n\ndata_wrangling &lt;- function(html_url, date) {\n  \n  # Sun rise time\n    sun_rise &lt;- html_url %&gt;%\n    html_nodes('div[id=\"astronomy-mod\"] table') %&gt;%\n    html_text() %&gt;%\n    .[1] %&gt;%\n    str_between(\"Time\\n\\t\\t\", \"\\n\\t\\t\")\n  # Sun set time\n  sun_set &lt;- html_url %&gt;%\n    html_nodes('div[id=\"astronomy-mod\"] table') %&gt;%\n    html_text() %&gt;%\n    .[1] %&gt;%\n    str_between(\"\\n\\t\\t\", \"\\n\\t\\tCivil\")\n\n  # Table\n  table &lt;- html_url %&gt;%\n    html_nodes('table[id=\"obsTable\"]') %&gt;%\n    html_table() %&gt;% \n    .[[1]]\n  \n  # Time column standardization \n  is_daylight &lt;- any(\"Time (PDT)\" == names(table),\n                     \"Time (MDT)\" == names(table),\n                     \"Time (CDT)\" == names(table),\n                     \"Time (EDT)\" == names(table))\n  \n  time_names &lt;- str_c(\"Time\", c(\" (PDT)\", \" (MDT)\", \" (CDT)\", \" (EDT)\",\n                                \" (PST)\", \" (MST)\", \" (CST)\", \" (EST)\"))\n  \n  names(table) &lt;- if_else(names(table) %in% time_names,\n                          \"Time\",\n                          names(table))\n  \n  table %&gt;%\n    mutate(sun_set = sun_set,\n           sun_rise = sun_rise,\n           date = date,\n           yday = yday(date), \n           day_minutes = ampm_minutes(Time) - is_daylight * 60,\n           set_minutes = ampm_minutes(sun_set) - is_daylight * 60,\n           rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60,\n           sun_up = day_minutes &gt; (rise_minutes + 90) & \n                    day_minutes &lt; (set_minutes - 30))\n}\n\nfull_data2 &lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %&gt;%\n                      data_wrangling(.y))\n\nx_axis &lt;- dates %&gt;% month() %&gt;% table() %&gt;% cumsum()\nnames(x_axis) &lt;- month.abb[1:12]\n\ny_axis &lt;- 1:24 * 60\nnames(y_axis) &lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), \n                       rep(c(\"AM\", \"PM\"), each = 12))\n\nfull_data2 %&gt;%\n  mutate(con = Conditions == \"Clear\",\n         day_minutes2 = lag(day_minutes)) %&gt;%\n  filter(sun_up) %&gt;%\n  ggplot(aes(fill = con)) +\n  geom_rect(aes(xmin = yday, xmax = yday + 1,\n                ymin = day_minutes, ymax = day_minutes2)) +\n  geom_line(aes(yday, set_minutes)) +\n  geom_line(aes(yday, rise_minutes)) +\n  scale_fill_manual(values = c(\"grey40\", NA)) +\n  theme_minimal() +\n  guides(fill = \"none\") +\n  theme(\n  panel.grid.major.y = element_blank(),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(), \n  axis.text.x.bottom = element_text(hjust = 1.7)\n  ) +\n  scale_x_continuous(breaks = x_axis, position = \"right\") +\n  scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) +\n  labs(x = NULL, y = NULL, title = \"Sunshine report of Los Angeles 2017\")\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.5 (2021-03-31)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       Pacific/Honolulu            \n date     2021-07-05                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                              \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.0.0)                      \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.0.2)                      \n blogdown      1.3     2021-04-14 [1] CRAN (R 4.0.2)                      \n bookdown      0.22    2021-04-22 [1] CRAN (R 4.0.2)                      \n broom         0.7.6   2021-04-05 [1] CRAN (R 4.0.2)                      \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.0.2)                      \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.0.0)                      \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.0.2)                      \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.0.2)                      \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.0.2)                      \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.0.2)                      \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.0.2)                      \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.0.2)                      \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.0.2)                      \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.0.0)                      \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.0.2)                      \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.0.2)                      \n ehlib       * 0.2.7   2021-07-05 [1] Github (emilhvitfeldt/ehlib@8c40172)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.0.2)                      \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.0.0)                      \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.0.2)                      \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.0.2)                      \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.0.2)                      \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.0.2)                      \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.0.2)                      \n glue        * 1.4.2   2020-08-27 [1] CRAN (R 4.0.2)                      \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.0.0)                      \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.0.2)                      \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.0.2)                      \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.0.2)                      \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.0.2)                      \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.0.2)                      \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.0.2)                      \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.0.2)                      \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.0.2)                      \n lubridate   * 1.7.10  2021-02-26 [1] CRAN (R 4.0.2)                      \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.0.2)                      \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.0.0)                      \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.0.0)                      \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.0.2)                      \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.0.0)                      \n png           0.1-7   2013-12-03 [1] CRAN (R 4.0.0)                      \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.0.0)                      \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.0.2)                      \n Rcpp          1.0.6   2021-01-15 [1] CRAN (R 4.0.2)                      \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2)                      \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.0.2)                      \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.0.2)                      \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.0.2)                      \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.0.2)                      \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.0.2)                      \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.0.2)                      \n rvest       * 1.0.0   2021-03-09 [1] CRAN (R 4.0.2)                      \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.0.2)                      \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.0.0)                      \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.0.0)                      \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.0.2)                      \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.0.0)                      \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.0.2)                      \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.0.2)                      \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.0.2)                      \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.0.2)                      \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.0.2)                      \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.0.2)                      \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.0.2)                      \n xfun          0.23    2021-05-15 [1] CRAN (R 4.0.2)                      \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.0.0)                      \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.0.0)                      \n\n[1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library"
  },
  {
    "objectID": "post/2020-03-17-word-rank-slope-charts/index.html",
    "href": "post/2020-03-17-word-rank-slope-charts/index.html",
    "title": "Word Rank Slope Charts",
    "section": "",
    "text": "I have been working on visualizing how different kinds of words are used in texts and I finally found a good visualization style with the slope chart. More specifically I’m thinking of two groups of paired words."
  },
  {
    "objectID": "post/2020-03-17-word-rank-slope-charts/index.html#packages",
    "href": "post/2020-03-17-word-rank-slope-charts/index.html#packages",
    "title": "Word Rank Slope Charts",
    "section": "Packages 📦",
    "text": "Packages 📦\nlibrary(tidyverse)\nlibrary(hcandersenr)\nlibrary(tidytext)\nlibrary(paletteer)\nlibrary(ggrepel)"
  },
  {
    "objectID": "post/2020-03-17-word-rank-slope-charts/index.html#minimal-example-1",
    "href": "post/2020-03-17-word-rank-slope-charts/index.html#minimal-example-1",
    "title": "Word Rank Slope Charts",
    "section": "Minimal Example 1️⃣",
    "text": "Minimal Example 1️⃣\nFirst I’ll walk you through a minimal example of how the chart is created. Afterward, I have created a function to automate the whole procedure so we can quickly iterate. We start with an example of gendered words in fairy tales by H.C. Andersen using the hcandersenr package. We start by generating a data.frame of paired words. This is easily done using the tribble() function.\ngender_words &lt;- tribble(\n  ~men, ~women,\n  \"he\", \"she\",\n  \"his\", \"her\",\n  \"man\", \"woman\",\n  \"men\", \"women\",\n  \"boy\", \"girl\",\n  \"he's\", \"she's\",\n  \"he'd\", \"she'd\",\n  \"he'll\", \"she'll\",\n  \"himself\", \"herself\"\n)\nNext, we are going to tokenize and count the tokens in the corpus,\nordered_words &lt;- hcandersen_en %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  pull(word)\nNext, we are going to get the index for each word, which we will put on a log scale since it will be easier to visualize. Next, we will calculate a slope between the points and add the correct labels.\ngender_words_plot &lt;- gender_words %&gt;%\n  mutate(male_index = match(men, ordered_words),\n         female_index = match(women, ordered_words)) %&gt;%\n  mutate(slope = log10(male_index) - log10(female_index)) %&gt;%\n  pivot_longer(male_index:female_index) %&gt;%\n  mutate(value = log10(value),\n         label = ifelse(name == \"male_index\", men, women)) %&gt;%\n  mutate(name = factor(name, c(\"male_index\", \"female_index\"), c(\"men\", \"women\")))\nNext, we are going to manually calculate the limits to make sure a diverging color scale will have the colors done directly.\nlimit &lt;- max(abs(gender_words_plot$slope)) * c(-1, 1)\nLastly, we just put everything into ggplot2, and voila!!\ngender_words_plot %&gt;%\n  ggplot(aes(name, value, group = women, label = label)) +\n  geom_line(aes(color = slope)) +\n  scale_y_reverse(labels = function(x) 10 ^ x) +\n  geom_text() +\n  guides(color = \"none\") +\n  scale_color_distiller(type = \"div\", limit = limit) +\n  theme_minimal() +\n  theme(panel.border = element_blank(), panel.grid.major.x = element_blank()) +\n  labs(x = NULL, y = \"Word Rank\") +\n  labs(title = \"Masculine gendered words appeared more often in H.C. Andersen's fairy tales\")"
  },
  {
    "objectID": "post/2020-03-17-word-rank-slope-charts/index.html#make-it-into-a-function",
    "href": "post/2020-03-17-word-rank-slope-charts/index.html#make-it-into-a-function",
    "title": "Word Rank Slope Charts",
    "section": "Make it into a function ✨",
    "text": "Make it into a function ✨\nThis function is mostly the same as the code you saw earlier. Main difference is using .data from rlang to generalize. The function also includes other beautifications such as improved themes and theme support with paletteer.\nplot_fun &lt;- function(words, ref, palette = \"scico::roma\", ...) {\n  \n  names &lt;- colnames(ref)\n  \n  ordered_words &lt;- names(sort(table(words), decreasing = TRUE))\n\n  plot_data &lt;- ref %&gt;%\n    mutate(index1 = match(.data[[names[1]]], ordered_words),\n           index2 = match(.data[[names[2]]], ordered_words)) %&gt;%\n    mutate(slope = log10(index1) - log10(index2)) %&gt;%\n    pivot_longer(index1:index2) %&gt;%\n    mutate(value = log10(value),\n           label = ifelse(name == \"index1\", \n                          .data[[names[1]]], \n                          .data[[names[2]]]),\n           name = factor(name, c(\"index1\", \"index2\"), names))\n  \n  limit &lt;- max(abs(plot_data$slope)) * c(-1, 1)\n\n  plot_data %&gt;%\n    ggplot(aes(name, value, group = .data[[names[2]]], label = label)) +\n    geom_line(aes(color = slope), size = 1) +\n    scale_y_reverse(labels = function(x) round(10 ^ x)) +\n    geom_text_repel(data = subset(plot_data, name == names[1]),\n                    aes(segment.color = slope),\n                    nudge_x       = -0.1,\n                    segment.size  = 1,\n                    direction     = \"y\",\n                    hjust         = 1) + \n    geom_text_repel(data = subset(plot_data, name == names[2]),\n                    aes(segment.color = slope),\n                    nudge_x       = 0.1,\n                    segment.size  = 1,\n                    direction     = \"y\",\n                    hjust         = 0) + \n    scale_color_paletteer_c(palette, \n                            limit = limit,\n                            aesthetics = c(\"color\", \"segment.color\"), \n                            ...) +\n    guides(color = \"none\", segment.color = \"none\") +\n    theme_minimal() +\n    theme(panel.border = element_blank(), \n          panel.grid.major.x = element_blank(), axis.text.x = element_text(size = 15)) +\n    labs(x = NULL, y = \"Word Rank\")\n}\nNow we can recreate the previous chart with ease\nref &lt;- tribble(\n  ~Men, ~Women,\n  \"he\", \"she\",\n  \"his\", \"her\",\n  \"man\", \"woman\",\n  \"men\", \"women\",\n  \"boy\", \"girl\",\n  \"he's\", \"she's\",\n  \"he'd\", \"she'd\",\n  \"he'll\", \"she'll\",\n  \"himself\", \"herself\"\n)\n\nwords &lt;- hcandersen_en %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  pull(word)\n\nplot_fun(words, ref, direction = -1) +\n  labs(title = \"Masculine gendered words appeared more often in H.C. Andersen's fairy tales\")"
  },
  {
    "objectID": "post/2020-03-17-word-rank-slope-charts/index.html#gallery",
    "href": "post/2020-03-17-word-rank-slope-charts/index.html#gallery",
    "title": "Word Rank Slope Charts",
    "section": "Gallery 🖼",
    "text": "Gallery 🖼\nref &lt;- tribble(\n  ~Men, ~Women,\n  \"he\", \"she\",\n  \"his\", \"her\",\n  \"man\", \"woman\",\n  \"men\", \"women\",\n  \"boy\", \"girl\",\n  \"himself\", \"herself\"\n)\n\nwords &lt;- janeaustenr::austen_books() %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  pull(word)\n\nplot_fun(words, ref, direction = -1) +\n  labs(title = \"Masculine gendered words appeared less often in Jane Austen Novels\")\n\nMore examples using the tidygutenbergr package.\nref &lt;- tribble(\n  ~Men, ~Women,\n  \"he\", \"she\",\n  \"his\", \"her\",\n  \"man\", \"woman\",\n  \"men\", \"women\",\n  \"boy\", \"girl\",\n  \"he's\", \"she's\",\n  \"himself\", \"herself\"\n)\n\nwords &lt;- tidygutenbergr::phantom_of_the_opera() %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  pull(word)\n\nplot_fun(words, ref, \"scico::berlin\") +\n  labs(title = \"Masculine gendered words appeared more often in Phantom of the Opera\")\n\nref &lt;- tribble(\n  ~Positive, ~Negative,\n  \"good\", \"bad\",\n  \"pretty\", \"ugly\",\n  \"friendly\", \"hostile\"\n)\n\nwords &lt;- tidygutenbergr::dracula() %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  pull(word)\n\nplot_fun(words, ref, palette = \"scico::tokyo\") +\n  labs(title = \"Positive adjectives appeared more often in Dracula\")\n\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-16                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version    date       lib\n assertthat       0.2.1      2019-03-21 [1]\n backports        1.2.1      2020-12-09 [1]\n blogdown         1.3.2      2021-06-09 [1]\n bookdown         0.22       2021-04-22 [1]\n broom            0.7.8      2021-06-24 [1]\n bslib            0.2.5.1    2021-05-18 [1]\n cellranger       1.1.0      2016-07-27 [1]\n cli              3.0.0      2021-06-30 [1]\n clipr            0.7.1      2020-10-08 [1]\n codetools        0.2-18     2020-11-04 [1]\n colorspace       2.0-2      2021-06-24 [1]\n crayon           1.4.1      2021-02-08 [1]\n curl             4.3.2      2021-06-23 [1]\n DBI              1.1.1      2021-01-15 [1]\n dbplyr           2.1.1      2021-04-06 [1]\n desc             1.3.0      2021-03-05 [1]\n details        * 0.2.1      2020-01-12 [1]\n digest           0.6.27     2020-10-24 [1]\n dplyr          * 1.0.7      2021-06-18 [1]\n ellipsis         0.3.2      2021-04-29 [1]\n emo              0.0.0.9000 2021-07-17 [1]\n evaluate         0.14       2019-05-28 [1]\n fansi            0.5.0      2021-05-25 [1]\n farver           2.1.0      2021-02-28 [1]\n forcats        * 0.5.1      2021-01-27 [1]\n fs               1.5.0      2020-07-31 [1]\n generics         0.1.0      2020-10-31 [1]\n ggplot2        * 3.3.5      2021-06-25 [1]\n ggrepel        * 0.9.1      2021-01-15 [1]\n glue             1.4.2      2020-08-27 [1]\n gtable           0.3.0      2019-03-25 [1]\n gutenbergr       0.2.1      2021-06-01 [1]\n haven            2.4.1      2021-04-23 [1]\n hcandersenr    * 0.2.0      2019-01-19 [1]\n highr            0.9        2021-04-16 [1]\n hms              1.1.0      2021-05-17 [1]\n htmltools        0.5.1.1    2021-01-22 [1]\n httr             1.4.2      2020-07-20 [1]\n janeaustenr      0.1.5      2017-06-10 [1]\n jquerylib        0.1.4      2021-04-26 [1]\n jsonlite         1.7.2      2020-12-09 [1]\n knitr          * 1.33       2021-04-24 [1]\n labeling         0.4.2      2020-10-20 [1]\n lattice          0.20-44    2021-05-02 [1]\n lifecycle        1.0.0      2021-02-15 [1]\n lubridate        1.7.10     2021-02-26 [1]\n magrittr         2.0.1      2020-11-17 [1]\n Matrix           1.3-3      2021-05-04 [1]\n modelr           0.1.8      2020-05-19 [1]\n munsell          0.5.0      2018-06-12 [1]\n paletteer      * 1.3.0      2021-01-06 [1]\n pillar           1.6.1      2021-05-16 [1]\n pkgconfig        2.0.3      2019-09-22 [1]\n png              0.1-7      2013-12-03 [1]\n prismatic        1.0.0      2021-01-05 [1]\n purrr          * 0.3.4      2020-04-17 [1]\n R6               2.5.0      2020-10-28 [1]\n Rcpp             1.0.7      2021-07-07 [1]\n readr          * 1.4.0      2020-10-05 [1]\n readxl           1.3.1      2019-03-13 [1]\n rematch2         2.1.2      2020-05-01 [1]\n reprex           2.0.0      2021-04-02 [1]\n rlang            0.4.11     2021-04-30 [1]\n rmarkdown        2.9        2021-06-15 [1]\n rprojroot        2.0.2      2020-11-15 [1]\n rstudioapi       0.13       2020-11-12 [1]\n rvest            1.0.0      2021-03-09 [1]\n sass             0.4.0      2021-05-12 [1]\n scales           1.1.1      2020-05-11 [1]\n scico            1.2.0      2020-06-08 [1]\n sessioninfo      1.1.1      2018-11-05 [1]\n SnowballC        0.7.0      2020-04-01 [1]\n stringi          1.6.2      2021-05-17 [1]\n stringr        * 1.4.0      2019-02-10 [1]\n tibble         * 3.1.2      2021-05-16 [1]\n tidygutenbergr   0.0.0.9000 2021-07-17 [1]\n tidyr          * 1.1.3      2021-03-03 [1]\n tidyselect       1.1.1      2021-04-30 [1]\n tidytext       * 0.3.1      2021-04-10 [1]\n tidyverse      * 1.3.1      2021-04-15 [1]\n tokenizers       0.2.1      2018-03-29 [1]\n triebeard        0.3.0      2016-08-04 [1]\n urltools         1.7.3      2019-04-14 [1]\n utf8             1.2.1      2021-03-12 [1]\n vctrs            0.3.8      2021-04-29 [1]\n withr            2.4.2      2021-04-18 [1]\n xfun             0.24       2021-06-15 [1]\n xml2             1.3.2      2020-04-23 [1]\n yaml             2.2.1      2020-02-01 [1]\n source                                       \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n Github (rstudio/blogdown@00a2090)            \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n Github (hadley/emo@3f03b11)                  \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n Github (emilhvitfeldt/tidygutenbergr@89e4049)\n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n CRAN (R 4.1.0)                               \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html",
    "title": "Recreate - Sankey flow chart",
    "section": "",
    "text": "Warning\n\n\n\nThis blogpost uses the old api for gganimate and will not work with current version. No update of this blogpost is planned for this moment.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data, or using data of a similar style."
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#the-goal---a-flowing-sankey-chart-from-nytimes",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#the-goal---a-flowing-sankey-chart-from-nytimes",
    "title": "Recreate - Sankey flow chart",
    "section": "The goal - A flowing sankey chart from nytimes",
    "text": "The goal - A flowing sankey chart from nytimes\nIn this excellent article Extensive Data Shows Punishing Reach of Racism for Black Boys by NYTimes includes a lot of very nice charts, both in motion and still. The chart that got biggest reception is the following:\n\n(see article for moving picture) We see a animated flow chart that follow the style of the classical Sankey chart. This chart will be the goal in this blog post, with 2 changes for brevity. firstly will I use randomly simulated data for my visualization and secondly will I not include the counters on the right-hand side of the chart and only show the creation of the counter on the left-hand as they are created in much the same fashion."
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#r-packages",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#r-packages",
    "title": "Recreate - Sankey flow chart",
    "section": "R packages",
    "text": "R packages\nFirst we need some packages, but very few of those. Simply using tidyverse and gganimate for animation.\nlibrary(tidyverse)\nlibrary(gganimate)"
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#single-point",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#single-point",
    "title": "Recreate - Sankey flow chart",
    "section": "Single point",
    "text": "Single point\nWe will start with animating a single point first. The path of each point closely resembles a sigmoid curve. I have used those in past visualizations, namely Visualizing trigrams with the Tidyverse.\nand we steal the function I created in that post\nsigmoid &lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) {\n  x &lt;- seq(-scale, scale, length = n)\n  y &lt;- exp(x) / (exp(x) + 1)\n  tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from,\n         y = y * (y_to - y_from) + y_from)\n}\nAnd to get along with that we will have out data\nn_points &lt;- 400\ndata &lt;- tibble(from = rep(4, n_points),\n               to = sample(1:4, n_points, TRUE),\n               color = sample(c(\"A\", \"B\"), n_points, TRUE)) \nhere the data is fairly clean and tidy, with numerical values for from and to but this endpoint should be able to be achieved in most any other appropriate type of data.\nTo simulate the path of a single data point we will use the custom sigmoid on the data for a single row. This gives us this smooth curve of points that resembles the path taken by the points in the original visualization.\nsigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]), \n        n = 100, scale = 10) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point()\n\nTo set this in motion we will employ gganimate, for this we will add a time column to act as the frame.\np &lt;- sigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]),\n             n = 100, scale = 10) %&gt;%\n  mutate(time = row_number()) %&gt;%\n  ggplot(aes(x, y, frame = time)) +\n  geom_point()\n\ngganimate(p)\n\nWhich looks very nice so far. Next step is to have multiple points flowing towards different locations."
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#multiple-points",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#multiple-points",
    "title": "Recreate - Sankey flow chart",
    "section": "multiple points",
    "text": "multiple points\nTo account for the multiple points we will wrap everything from last section inside a map_df to iterate over the rows. To avoid over plotting we introduce some uniform noise to each point.\np &lt;- map_df(seq_len(nrow(data)), \n    ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %&gt;%\n      mutate(time = row_number() + .x,\n             y = y + runif(1, -0.25, 0.25))) %&gt;%\n  ggplot(aes(x, y, frame = time)) +\n  geom_point() \n\ngganimate(p)\n\nEverything looks good so far, however the points all look the same, so we will do a little bit of beautification now rather then later. In addition to that will we save the data for the different components in different objects.\nthe following point_data have the modification with bind_cols that binds the information from the data data.frame to the final object. We include the color and removing all themes and guides.\npoint_data &lt;- map_df(seq_len(nrow(data)), \n    ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %&gt;%\n      mutate(time = row_number() + .x,\n             y = y + runif(1, -0.25, 0.25),\n             id = .x) %&gt;%\n      bind_cols(bind_rows(replicate(100, data[.x, -(1:2)], simplify = FALSE))))\n\np &lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +\n  geom_point(shape = 15) +\n  theme_void() +\n  guides(color = \"none\")\n\ngganimate(p, title_frame = FALSE)\n\nWhich already looks way better. Next up to include animated counter on the left hand side that indicates how many points that have been introduced in the animation. This is simply done by counting how many have started their paths and afterwards padding to fill the length of the animation.\nstart_data_no_end &lt;- point_data %&gt;%\n  group_by(id) %&gt;%\n  summarize(time = min(time)) %&gt;%\n  count(time) %&gt;%\n  arrange(time) %&gt;%\n  mutate(n = cumsum(n),\n         x = 0.125, \n         y = 2,\n         n = str_c(\"Follow the lives of \", n, \" squares\"))\n  \n\n\n# duplicating last number to fill gif\nstart_data &lt;- start_data_no_end %&gt;%\n  bind_rows(\n    map_df(unique(point_data$time[point_data$time &gt; max(start_data_no_end$time)]),\n          ~ slice(start_data_no_end, nrow(start_data_no_end)) %&gt;%\n              mutate(time = .x))\n  )\nThis is added to our plot by the use of geom_text with a new data argument. We did some stringr magic to have a little annotation appear instead of the number itself. Important to have the hjust = 0 such that the annotation doesn’t move around too much.\np &lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +\n  geom_point(shape = 15) +\n  geom_text(data = start_data, hjust = 0,\n            aes(label = n, frame = time, x = x, y = y), color = \"black\") +\n  theme_void() +\n  guides(color = \"none\")\n\ngganimate(p, title_frame = FALSE)"
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#ending-boxes",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#ending-boxes",
    "title": "Recreate - Sankey flow chart",
    "section": "Ending boxes",
    "text": "Ending boxes\nLike the original illustration there are some boxes where the points “land” in. these are very easily replicated. This will be done a little more programmatic such that it adapts to multiple outputs.\nending_box &lt;- data %&gt;%\n  pull(to) %&gt;%\n  unique() %&gt;%\n  map_df(~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01),\n                      y = c(-0.3, 0.3, 0.3, -0.3, -0.3) + .x,\n                      id = .x))\nWe will add this in the same way as before, this time we will use geom_path to draw the box and frame = min(point_data$time) and cumulative = TRUE to have the boxes appear at the first frame and stay there forever.\np &lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +\n  geom_point() +\n  geom_text(data = start_data, \n            aes(label = n, frame = time, x = x, y = y), color = \"black\") +\n  geom_path(data = ending_box,\n            aes(x, y, group = id, frame = min(point_data$time),\n                cumulative = TRUE), color = \"grey70\") +\n  theme_void() +\n  coord_cartesian(xlim = c(-0.05, 1.15)) +\n  guides(color = \"none\")\n\ngganimate(p, title_frame = FALSE)"
  },
  {
    "objectID": "post/2018-03-20-recreate-sankey-flow-chart/index.html#filling-the-box",
    "href": "post/2018-03-20-recreate-sankey-flow-chart/index.html#filling-the-box",
    "title": "Recreate - Sankey flow chart",
    "section": "Filling the box",
    "text": "Filling the box\nLastly do we want to fill the boxes as the points approach them. This is done by first figuring out when they appear at the end of their paths, and them drawing boxes at those points, this is done by the end_points and end_lines respectively.\nend_points &lt;- point_data %&gt;% \n  group_by(id) %&gt;%\n  filter(time == max(time)) %&gt;%\n  ungroup()\n\nend_lines &lt;- map_df(end_points$id,\n    ~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01),\n                 y = c(-0.01, 0.01, 0.01, -0.01, -0.01) + as.numeric(end_points[.x, 2]),\n                 id = .x) %&gt;%\n      bind_cols(bind_rows(replicate(5, end_points[.x, -(1:2)], simplify = FALSE)))\n    )\nLike before we add the data in a new geom_, with cumulative = TRUE to let the “points” stay.\np &lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +\n  geom_point() +\n  geom_text(data = start_data, \n            aes(label = n, frame = time, x = x, y = y), color = \"black\") +\n  geom_path(data = ending_box,\n            aes(x, y, group = id, frame = min(point_data$time),\n                cumulative = TRUE), color = \"grey70\") +\n  geom_polygon(data = end_lines,\n               aes(x, y, fill = color, frame = time, group = id,\n                   cumulative = TRUE, color = color)) +\n  theme_void() +\n  coord_cartesian(xlim = c(-0.05, 1.15)) +\n  guides(color = \"none\",\n         fill = \"none\")\n\ngganimate(p, title_frame = FALSE)\n\nAnd this is what I have for you for now. Counters on the right hand side could be done in much the same way as we have seen here, but wouldn’t add much value to showcase that here.\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-13                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html",
    "title": "Custom Profiler in R",
    "section": "",
    "text": "This blog post is going to describe how to write a customizable profiling function. If you are not familiar with profiling read the Profiling section of Advanced R to familiarize yourself, I’ll wait.\n…\nWelcome back!"
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#packages",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#packages",
    "title": "Custom Profiler in R",
    "section": "Packages",
    "text": "Packages\nWhile these packages aren’t strictly needed since most of what we are doing is happening in base R, am I still loading in tidyverse to do some easier string manipulations and plotting.\nlibrary(tidyverse)"
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#profiling-basics",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#profiling-basics",
    "title": "Custom Profiler in R",
    "section": "Profiling basics",
    "text": "Profiling basics\nYou have properly used the profvis package. It is an amazing package and I use it on a daily basis. However, the amount of information you get can be overwhelming at times depending on your profiling goals.\nLet’s propose in this scenario that we take in some data, scale and center it, apply PCA while only keeping the components that explain 90% of the variance and lastly apply CLARA clustering and return the classification.\nThe code to do that is contained in the following chunk.\npca_threshold &lt;- function(x, threshold) {\n  data_pca &lt;- prcomp(x, scale. = TRUE)\n  total_var &lt;- sum(data_pca$sdev ^ 2)\n  num_comp &lt;- which.max(cumsum(data_pca$sdev ^ 2 / total_var) &gt;= threshold)\n  data_pca$x[, seq_len(num_comp)]\n}\n\npca_kmeans &lt;- function(x, threshold = 0.9, centers = 2) {\n  data_matrix &lt;- as.matrix(x)\n  data_pca &lt;- pca_threshold(data_matrix, threshold = threshold)\n  data_kmeans &lt;- cluster::clara(data_pca, k = centers)\n  data_kmeans$cluster\n}\nNow we create some data and run profvis on it\nlarge_data &lt;- diamonds %&gt;%\n  select_if(is.numeric) %&gt;%\n  sample_n(100000, replace = TRUE)\nprofvis::profvis({\n  pca_kmeans(large_data)\n})\nAnd we get the following information back."
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#the-problem",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#the-problem",
    "title": "Custom Profiler in R",
    "section": "The Problem",
    "text": "The Problem\nIt is very informative, but it is also giving a LOT of information. Let’s propose we want to know the percentage of the computation time is used to do the PCA calculations. In the profvis framework you would need to do the calculation manually. All while waiting for the HTML widget to load."
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#the-idea",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#the-idea",
    "title": "Custom Profiler in R",
    "section": "The Idea",
    "text": "The Idea\nprofvis uses the Rprof function internally to inspect what is happening. By using Rprof directly we can extract the profile and calculate our output/matrix.\nThe base profiling steps are\ntmp &lt;- tempfile()\nRprof(tmp)\n##################\n# Code goes here #\n##################\nRprof(NULL)\nprofile &lt;- readLines(tmp)\nThis chunk will set up a temporary file, start the profiler and set it to write to the temporary file, stop the profiler and read the result from the profiler.\nTrying it with our code we get\ntmp &lt;- tempfile()\nRprof(tmp)\nx &lt;- pca_kmeans(large_data)\nRprof(NULL)\nprofile &lt;- readLines(tmp)\n\nhead(profile)\n## [1] \"sample.interval=20000\"                                                                                                            \n## [2] \"\\\"aperm.default\\\" \\\"aperm\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"\n## [3] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [4] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [5] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [6] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"\nLet’s see what these lines mean. First, we notice that the first line is just denoting the sample interval, so we can ignore that for now. Let’s look at the next line\n## [1] \"\\\"aperm.default\\\" \\\"aperm\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"\nThis is a snapshot of the “call-stack”, and it reads inside-out. So we have that aperm.default is called inside aperm which is called inside apply which is called inside scale.default and so on and so forth all the way up to pca_kmeans.\nNow that we know how Rprof works, we can write some code that checks whether “pca_threshold” appears in the call stack and then find the percentage."
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#the-solution",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#the-solution",
    "title": "Custom Profiler in R",
    "section": "The Solution",
    "text": "The Solution\nWe can now create a function that will calculate the percentage of the time is being spent in a certain function.\nprof_procentage &lt;- function(expr, pattern) {\n  tmp &lt;- tempfile()\n  Rprof(tmp)\n  expr\n  Rprof(NULL)\n  profile &lt;- readLines(tmp)[-1]\n  \n  mean(grepl(pattern, profile))\n}\nThis function can now easily be used in our calculation.\nprof_procentage(\n  x &lt;- pca_kmeans(large_data),\n  pattern = \"pca_threshold\"\n)\n## [1] 0.875\nAnd this is how to create a custom profiler. Simply modify the last line in the skeleton function prof_procentage to change its behavior."
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#the-extensions",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#the-extensions",
    "title": "Custom Profiler in R",
    "section": "the Extensions",
    "text": "the Extensions\nThe sky’s the limit! you are only limited by your regex abilities. You can also change the output. In the last example, I returned a numeric of the percentage, But we can also have the output be a plot\nprof_procentage_plot &lt;- function(expr, pattern) {\n  tmp &lt;- tempfile()\n  Rprof(tmp)\n  expr\n  Rprof(NULL)\n  profile &lt;- readLines(tmp)[-1]\n  \n  data.frame(x = grepl(pattern, profile)) %&gt;%\n    ggplot(aes(x)) +\n    geom_bar()\n}\n\nprof_procentage_plot(\n  x &lt;- pca_kmeans(large_data),\n  pattern = \"pca_threshold\"\n)"
  },
  {
    "objectID": "post/2019-05-25-custom-profiler-in-r/index.html#the-follow-up",
    "href": "post/2019-05-25-custom-profiler-in-r/index.html#the-follow-up",
    "title": "Custom Profiler in R",
    "section": "The follow-up",
    "text": "The follow-up\nAfter my initial announcement of this post, I got a helpful tweet from Hadley Wickham about the profvis::parse_rprof(). In essence, it will help you parse the file you write with Rprof to help you get to your answer faster and safer.\nSo some output like\n## [1] \"sample.interval=20000\"                                                                                                            \n## [2] \"\\\"aperm.default\\\" \\\"aperm\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"\n## [3] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [4] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [5] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"          \n## [6] \"\\\"is.na\\\" \\\"FUN\\\" \\\"apply\\\" \\\"scale.default\\\" \\\"scale\\\" \\\"prcomp.default\\\" \\\"prcomp\\\" \\\"pca_threshold\\\" \\\"pca_kmeans\\\" \"\nWill be transformed to a nice data.frame with profvis::parse_rprof()\n##    time depth          label filenum linenum memalloc meminc filename\n## 1     1     9  aperm.default      NA      NA        0      0     &lt;NA&gt;\n## 2     1     8          aperm      NA      NA        0      0     &lt;NA&gt;\n## 3     1     7          apply      NA      NA        0      0     &lt;NA&gt;\n## 4     1     6  scale.default      NA      NA        0      0     &lt;NA&gt;\n## 5     1     5          scale      NA      NA        0      0     &lt;NA&gt;\n## 6     1     4 prcomp.default      NA      NA        0      0     &lt;NA&gt;\n## 7     1     3         prcomp      NA      NA        0      0     &lt;NA&gt;\n## 8     1     2  pca_threshold      NA      NA        0      0     &lt;NA&gt;\n## 9     1     1     pca_kmeans      NA      NA        0      0     &lt;NA&gt;\n## 10    2     9          is.na      NA      NA        0      0     &lt;NA&gt;\n## 11    2     8            FUN      NA      NA        0      0     &lt;NA&gt;\n## 12    2     7          apply      NA      NA        0      0     &lt;NA&gt;\n## 13    2     6  scale.default      NA      NA        0      0     &lt;NA&gt;\n## 14    2     5          scale      NA      NA        0      0     &lt;NA&gt;\n## 15    2     4 prcomp.default      NA      NA        0      0     &lt;NA&gt;\n## 16    2     3         prcomp      NA      NA        0      0     &lt;NA&gt;\n## 17    2     2  pca_threshold      NA      NA        0      0     &lt;NA&gt;\n## 18    2     1     pca_kmeans      NA      NA        0      0     &lt;NA&gt;\n## 19    3     9          is.na      NA      NA        0      0     &lt;NA&gt;\n## 20    3     8            FUN      NA      NA        0      0     &lt;NA&gt;\n## 21    3     7          apply      NA      NA        0      0     &lt;NA&gt;\n## 22    3     6  scale.default      NA      NA        0      0     &lt;NA&gt;\n## 23    3     5          scale      NA      NA        0      0     &lt;NA&gt;\n## 24    3     4 prcomp.default      NA      NA        0      0     &lt;NA&gt;\n## 25    3     3         prcomp      NA      NA        0      0     &lt;NA&gt;\n## 26    3     2  pca_threshold      NA      NA        0      0     &lt;NA&gt;\n## 27    3     1     pca_kmeans      NA      NA        0      0     &lt;NA&gt;\n## 28    4     9          is.na      NA      NA        0      0     &lt;NA&gt;\n## 29    4     8            FUN      NA      NA        0      0     &lt;NA&gt;\n## 30    4     7          apply      NA      NA        0      0     &lt;NA&gt;\n## 31    4     6  scale.default      NA      NA        0      0     &lt;NA&gt;\n## 32    4     5          scale      NA      NA        0      0     &lt;NA&gt;\n## 33    4     4 prcomp.default      NA      NA        0      0     &lt;NA&gt;\n## 34    4     3         prcomp      NA      NA        0      0     &lt;NA&gt;\n## 35    4     2  pca_threshold      NA      NA        0      0     &lt;NA&gt;\n## 36    4     1     pca_kmeans      NA      NA        0      0     &lt;NA&gt;\n## 37    5     9          is.na      NA      NA        0      0     &lt;NA&gt;\n## 38    5     8            FUN      NA      NA        0      0     &lt;NA&gt;\n## 39    5     7          apply      NA      NA        0      0     &lt;NA&gt;\n## 40    5     6  scale.default      NA      NA        0      0     &lt;NA&gt;\n## 41    5     5          scale      NA      NA        0      0     &lt;NA&gt;\n## 42    5     4 prcomp.default      NA      NA        0      0     &lt;NA&gt;\n## 43    5     3         prcomp      NA      NA        0      0     &lt;NA&gt;\n## 44    5     2  pca_threshold      NA      NA        0      0     &lt;NA&gt;\n## 45    5     1     pca_kmeans      NA      NA        0      0     &lt;NA&gt;\n\n\n session information \n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.1.0 (2021-05-18)\n os       macOS Big Sur 10.16         \n system   x86_64, darwin17.0          \n ui       X11                         \n language (EN)                        \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/Los_Angeles         \n date     2021-07-15                  \n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date       lib source                           \n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.1.0)                   \n backports     1.2.1   2020-12-09 [1] CRAN (R 4.1.0)                   \n blogdown      1.3.2   2021-06-09 [1] Github (rstudio/blogdown@00a2090)\n bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)                   \n broom         0.7.8   2021-06-24 [1] CRAN (R 4.1.0)                   \n bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)                   \n cellranger    1.1.0   2016-07-27 [1] CRAN (R 4.1.0)                   \n cli           3.0.0   2021-06-30 [1] CRAN (R 4.1.0)                   \n clipr         0.7.1   2020-10-08 [1] CRAN (R 4.1.0)                   \n cluster       2.1.2   2021-04-17 [1] CRAN (R 4.1.0)                   \n codetools     0.2-18  2020-11-04 [1] CRAN (R 4.1.0)                   \n colorspace    2.0-2   2021-06-24 [1] CRAN (R 4.1.0)                   \n crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)                   \n DBI           1.1.1   2021-01-15 [1] CRAN (R 4.1.0)                   \n dbplyr        2.1.1   2021-04-06 [1] CRAN (R 4.1.0)                   \n desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)                   \n details     * 0.2.1   2020-01-12 [1] CRAN (R 4.1.0)                   \n digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)                   \n dplyr       * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)                   \n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)                   \n evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)                   \n fansi         0.5.0   2021-05-25 [1] CRAN (R 4.1.0)                   \n farver        2.1.0   2021-02-28 [1] CRAN (R 4.1.0)                   \n forcats     * 0.5.1   2021-01-27 [1] CRAN (R 4.1.0)                   \n fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)                   \n generics      0.1.0   2020-10-31 [1] CRAN (R 4.1.0)                   \n ggplot2     * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)                   \n glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)                   \n gtable        0.3.0   2019-03-25 [1] CRAN (R 4.1.0)                   \n haven         2.4.1   2021-04-23 [1] CRAN (R 4.1.0)                   \n highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)                   \n hms           1.1.0   2021-05-17 [1] CRAN (R 4.1.0)                   \n htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)                   \n httr          1.4.2   2020-07-20 [1] CRAN (R 4.1.0)                   \n jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)                   \n jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)                   \n knitr       * 1.33    2021-04-24 [1] CRAN (R 4.1.0)                   \n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.1.0)                   \n lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)                   \n lubridate     1.7.10  2021-02-26 [1] CRAN (R 4.1.0)                   \n magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)                   \n modelr        0.1.8   2020-05-19 [1] CRAN (R 4.1.0)                   \n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.1.0)                   \n pillar        1.6.1   2021-05-16 [1] CRAN (R 4.1.0)                   \n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.1.0)                   \n png           0.1-7   2013-12-03 [1] CRAN (R 4.1.0)                   \n purrr       * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)                   \n R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)                   \n Rcpp          1.0.7   2021-07-07 [1] CRAN (R 4.1.0)                   \n readr       * 1.4.0   2020-10-05 [1] CRAN (R 4.1.0)                   \n readxl        1.3.1   2019-03-13 [1] CRAN (R 4.1.0)                   \n reprex        2.0.0   2021-04-02 [1] CRAN (R 4.1.0)                   \n rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)                   \n rmarkdown     2.9     2021-06-15 [1] CRAN (R 4.1.0)                   \n rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)                   \n rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)                   \n rvest         1.0.0   2021-03-09 [1] CRAN (R 4.1.0)                   \n sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)                   \n scales        1.1.1   2020-05-11 [1] CRAN (R 4.1.0)                   \n sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)                   \n stringi       1.6.2   2021-05-17 [1] CRAN (R 4.1.0)                   \n stringr     * 1.4.0   2019-02-10 [1] CRAN (R 4.1.0)                   \n tibble      * 3.1.2   2021-05-16 [1] CRAN (R 4.1.0)                   \n tidyr       * 1.1.3   2021-03-03 [1] CRAN (R 4.1.0)                   \n tidyselect    1.1.1   2021-04-30 [1] CRAN (R 4.1.0)                   \n tidyverse   * 1.3.1   2021-04-15 [1] CRAN (R 4.1.0)                   \n utf8          1.2.1   2021-03-12 [1] CRAN (R 4.1.0)                   \n vctrs         0.3.8   2021-04-29 [1] CRAN (R 4.1.0)                   \n withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)                   \n xfun          0.24    2021-06-15 [1] CRAN (R 4.1.0)                   \n xml2          1.3.2   2020-04-23 [1] CRAN (R 4.1.0)                   \n yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)                   \n\n[1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library"
  },
  {
    "objectID": "index.html#software-engineer-at-posit-pbc",
    "href": "index.html#software-engineer-at-posit-pbc",
    "title": "",
    "section": "Software Engineer at Posit PBC",
    "text": "Software Engineer at Posit PBC\nI’m a Software Engineer at Posit PBC. Proud co-author of Supervised Machine Learning for Text Analysis in R with Julia Silge. My interests include developing tools for natural language processing, machine learning using tidymodels, education, and the use of colors in data visualizations."
  },
  {
    "objectID": "project/ggpage/index.html",
    "href": "project/ggpage/index.html",
    "title": "ggpage",
    "section": "",
    "text": "Github    CRAN \n\n\nFacilitates the creation of page layout visualizations in which words are represented as rectangles with sizes relating to the length of the words. Which then is divided in lines and pages for easy overview of up to quite large texts."
  },
  {
    "objectID": "project/scotus/index.html",
    "href": "project/scotus/index.html",
    "title": "scotus",
    "section": "",
    "text": "github \n\n\nThe goal of scotus is to provide a access to Supreme Court of the United States opinions for use in text analysis."
  },
  {
    "objectID": "project/islr-tidymodels/index.html",
    "href": "project/islr-tidymodels/index.html",
    "title": "ISLR tidymodels labs",
    "section": "",
    "text": "github    Website \n\n\nThis book aims to be a complement to the 1st version An Introduction to Statistical Learning book with translations of the labs into using the tidymodels set of packages.\nThe labs will be mirrored quite closely to stay true to the original material."
  },
  {
    "objectID": "project/paletteer/index.html",
    "href": "project/paletteer/index.html",
    "title": "paletteer",
    "section": "",
    "text": "github    CRAN \n\n\npaletteer is a collection of most color palettes in R using a cummon interface.\nVersion 1.0.0 blogpost"
  },
  {
    "objectID": "project/tidygutenbergr/index.html",
    "href": "project/tidygutenbergr/index.html",
    "title": "tidygutenbergr",
    "section": "",
    "text": "github \n\n\nThe goal of tidygutenbergr is to provide simple cleaning functions for popular works collected from gutenberg.org."
  },
  {
    "objectID": "project/hcandersenr/index.html",
    "href": "project/hcandersenr/index.html",
    "title": "hcandersenr",
    "section": "",
    "text": "Github    CRAN \n\n\nThis package contains (most) the complete texts of 157 fairy tales of H.C. Andersen, in German, Danish, English, Spanish and French, formatted to be convenient for text analysis."
  },
  {
    "objectID": "project/r-text-data/index.html",
    "href": "project/r-text-data/index.html",
    "title": "R Text Data",
    "section": "",
    "text": "Website \n\n\nThis repository links to multiple data sources if you want to get into text mining with R and don’t want to spend time finding interesting data."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Slidecraft 101: theme variants\n\n\n\n\n\n\n\nslidecraft 101\n\n\nquarto\n\n\n\n\nAll about theme variants in slidecrafting\n\n\n\n\n\n\nMay 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSlidecraft 101: Code and Output\n\n\n\n\n\n\n\nslidecraft 101\n\n\nquarto\n\n\n\n\nAll about styling code and output in slidecrafting\n\n\n\n\n\n\nSep 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSlidecraft 101: Colors and Fonts\n\n\n\n\n\n\n\nslidecraft 101\n\n\nquarto\n\n\n\n\nAll about colors and fonts in slidecrafting\n\n\n\n\n\n\nAug 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSupervised Machine Learning for Text Analysis in R\n\n\n\n\n\n\n\nsmltar\n\n\nbook\n\n\n\n\nStory about “Supervised Machine Learning for Text Analysis in R” Book\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nxaringan first-letter\n\n\n\n\n\n\n\nxaringan\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nxaringancolor announcement\n\n\n\n\n\n\n\nxaringan\n\n\n\n\nAnnouncement post for the xaringancolor package.\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes Version 0.4.0\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes series: Pretrained Word Embedding\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntextrecipes series\n\n\n\n\nThis is the fifth blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes. This post looks at how to use pretrained word embeddings.\n\n\n\n\n\n\nAug 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSupervised Machine Learning for Text Analysis in R\n\n\n\n\n\n\n\nsmltar\n\n\nbook\n\n\n\n\nAnnouncement post for Supervised Machine Learning for Text Analysis in R book.\n\n\n\n\n\n\nJul 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes series: Feature Hashing\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntextrecipes series\n\n\n\n\nThis is the fourth blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes. This post looks at how to perform feature hashing.\n\n\n\n\n\n\nMay 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes series: TF-IDF\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntidytuesday\n\n\ntextrecipes series\n\n\n\n\nThis is the first blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes. This post looks at how\n\n\n\n\n\n\nMay 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes series: lexicons\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntidytuesday\n\n\ntextrecipes series\n\n\n\n\nThis is the second blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes. This post talks about how to use lexicons.\n\n\n\n\n\n\nMay 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\nTextrecipes series: Term Frequency\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntidytuesday\n\n\ntextrecipes series\n\n\n\n\nThis is the first blog post in a series I am starting to go over the various text preprocessing workflows you can do with textrecipes.\n\n\n\n\n\n\nMay 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\ntidytuesday: Part-of-Speech and textrecipes with The Office\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\ntidytuesday\n\n\n\n\nUsing Part-of-Speech features to enhance text classification.\n\n\n\n\n\n\nMar 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nWord Rank Slope Charts\n\n\n\n\n\n\n\ntidytext\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\nUsing stm to Investigate if Stemming is Appropriate\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nUsing stm to Investigate if Stemming is Appropriate\n\n\n\n\n\n\nMar 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\nUse prismatic with after_scale() for finer control of colors in ggplot2\n\n\n\n\n\n\n\ncolor palettes\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nDeploy your bookdown project to Netlify with Github Actions\n\n\n\n\n\nhis post will show you how you can set up a bookdown site with Netlify using Github Actions.\n\n\n\n\n\n\nJan 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nReal Emojis in ggplot2\n\n\n\n\n\n\n\nggplot2\n\n\nemoji\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPaletteer version 1.0.0\n\n\n\n\n\n\n\nggplot2\n\n\ncolor palettes\n\n\n\n\nAnnouncement of version 1.0.0 of the {paletteer} package that.\n\n\n\n\n\n\nDec 18, 2019\n\n\n\n\n\n\n  \n\n\n\n\nRefactoring Tests\n\n\n\n\n\n\n\npackage development\n\n\n\n\nThis shows an experimental take on how you can refactor tests into functions.\n\n\n\n\n\n\nNov 25, 2019\n\n\n\n\n\n\n  \n\n\n\n\nManipulating colors with {prismatic}\n\n\n\n\n\n\n\nggplot2\n\n\ncolor palettes\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2019\n\n\n\n\n\n\n  \n\n\n\n\nAuthorship classification with tidymodels and textrecipes\n\n\n\n\n\n\n\ntidymodels\n\n\ntextrecipes\n\n\n\n\nAn early attempt at using tidymodels to perform text classification.\n\n\n\n\n\n\nAug 9, 2019\n\n\n\n\n\n\n  \n\n\n\n\nCreating RStudio addin to modify selection\n\n\n\n\n\nCreating addins to be used in RStudio.\n\n\n\n\n\n\nJul 30, 2019\n\n\n\n\n\n\n  \n\n\n\n\nChanging Glyph in legend in ggplot2\n\n\n\n\n\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2019\n\n\n\n\n\n\n  \n\n\n\n\nCustom Profiler in R\n\n\n\n\n\n\n\npackage development\n\n\n\n\nThis blog post is going to describe how to write a customizable profiling function\n\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n  \n\n\n\n\nCenter continuous palettes in ggplot2\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nThis blog post will show you how to properly use a divergent color palette with a user-specified midpoint.\n\n\n\n\n\n\nMay 21, 2019\n\n\n\n\n\n\n  \n\n\n\n\nCircle Love - making hearts with circles\n\n\n\n\n\n\n\ngenerative art\n\n\n\n\nGenerative art, pretty hearts with circles.\n\n\n\n\n\n\nMay 8, 2019\n\n\n\n\n\n\n  \n\n\n\n\nText Classification with Tidymodels\n\n\n\n\n\n\n\ntidymodels\n\n\n\n\nAn early attempt at using tidymodels to perform text classification.\n\n\n\n\n\n\nDec 29, 2018\n\n\n\n\n\n\n  \n\n\n\n\nusethis workflow for package development\n\n\n\n\n\n\n\npackage development\n\n\n\n\nA quick overview on how to get started with creating an R package with the help of usethis.\n\n\n\n\n\n\nSep 2, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWhat are the reviews telling us?\n\n\n\n\n\n\n\nggplot2\n\n\ntidytext\n\n\n\n\nWe will scrape and analyze IMDb reviews. We will try using log odds to tell good and bad reviews apart.\n\n\n\n\n\n\nAug 17, 2018\n\n\n\n\n\n\n  \n\n\n\n\nggplot2 trial and error - US trade data\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nThis blog post will showcase an example of a workflow and its associated thought process when iterating through visualization styles working with ggplot2.\n\n\n\n\n\n\nJun 12, 2018\n\n\n\n\n\n\n  \n\n\n\n\nEmoji use on Twitter\n\n\n\n\n\n\n\nggplot2\n\n\ntidytext\n\n\nemoji\n\n\n\n\nThis post will be a short demonstration of how the occurrence of emojis on Twitter can be analyzed using tidytools.\n\n\n\n\n\n\nJun 4, 2018\n\n\n\n\n\n\n  \n\n\n\n\nUsing PCA for word embedding in R\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nA brief look at using PCA as a word embedding technique.\n\n\n\n\n\n\nMay 22, 2018\n\n\n\n\n\n\n  \n\n\n\n\nAnalysing ethnic diversity in Californian school\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nAn interesting study of a measure of diversity. This diversity measure was then applied to schools in California.\n\n\n\n\n\n\nMay 1, 2018\n\n\n\n\n\n\n  \n\n\n\n\nggpage version 0.2.0 showcase\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nhighlights of the ggpage package. It provides unique text-based visualizations.\n\n\n\n\n\n\nApr 7, 2018\n\n\n\n\n\n\n  \n\n\n\n\nBinary text classification with tidytext and caret\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nOne of my first attempts at text classification. This example uses tidytext and caret. There are mistakes here methodically and it should not be used as a guide.\n\n\n\n\n\n\nMar 31, 2018\n\n\n\n\n\n\n  \n\n\n\n\nRecreate - Sankey flow chart\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nThis entry in the recreate series explores how gganimate can be used to create Sankey flow charts.\n\n\n\n\n\n\nMar 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nTidy Text Summarization using TextRank\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nI’m taking a look at text summarization using the TextRank package.\n\n\n\n\n\n\nMar 15, 2018\n\n\n\n\n\n\n  \n\n\n\n\nCo Occurrence of Characters in Les Miserable\n\n\n\n\n\n\n\nggplot2\n\n\ntidytext\n\n\n\n\nI’ll be creating a new spin on the Les Miserable character co-occurrence graph by using Named Entity Recognition to detect the characters.\n\n\n\n\n\n\nFeb 23, 2018\n\n\n\n\n\n\n  \n\n\n\n\nRvision: A first look\n\n\n\n\n\nRecently I stumbled across the Rvision package, which frankly looks amazing so far. So I decided to take it for a spin and show you what I found.\n\n\n\n\n\n\nFeb 15, 2018\n\n\n\n\n\n\n  \n\n\n\n\nPredicting authorship in The Federalist Papers with tidytext\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nMy first attempt at performing supervised predictive modeling using text features.\n\n\n\n\n\n\nJan 30, 2018\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing trigrams with the Tidyverse\n\n\n\n\n\n\n\ntidytext\n\n\nggplot2\n\n\n\n\nI try my hand at a unique visualization of token trigrams.\n\n\n\n\n\n\nJan 23, 2018\n\n\n\n\n\n\n  \n\n\n\n\nPurrr - tips and tricks\n\n\n\n\n\nWith the advent of purrrresolution on Twitter I’ll throw my 2 cents in in form of my bag of tips and tricks.\n\n\n\n\n\n\nJan 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nRecreate - Sunshine Report\n\n\n\n\n\n\n\nggplot2\n\n\n\n\nThis mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\n  \n\n\n\n\nAnalysing useR!2017 schedule data\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nI enjoyed my first useR! conference so very much, so it is only natural thing for me was to explore the schedule to create some visualizations.\n\n\n\n\n\n\nJul 20, 2017\n\n\n\n\n\n\n  \n\n\n\n\nRepetition in musicals with tidytext\n\n\n\n\n\n\n\ntidytext\n\n\n\n\nA project looking at repetition in song lyrics. Using {rvest} to perform web scraping (now outdated) to fetch lyrics from genius.com.\n\n\n\n\n\n\nJun 5, 2017\n\n\n\n\n\n\n  \n\n\n\n\n2017 World Press Freedom Index with emojis\n\n\n\n\n\n\n\nggplot2\n\n\nemoji\n\n\n\n\nA first (now outclassed) attempt at using emojis in ggplot2. This post combines the hadley/emo package and data from the 2017 World Press Freedom Index.\n\n\n\n\n\n\nApr 26, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk/2021-08-27-rmedicine-pptxtemplate/index.html",
    "href": "talk/2021-08-27-rmedicine-pptxtemplate/index.html",
    "title": "Creating and Styling PPTX Slides with {rmarkdown}",
    "section": "",
    "text": "Slides Github\n\n\nSubmitted talk at R/Medicine\nThe goal of pptxtemplates is to provide rmarkdown templates for powerpoint presentatations that include reference document for styling. This talk shows how we can use this package to automatically style powerpoints."
  },
  {
    "objectID": "talk/2023-04-22-ocrug-hackathon/index.html",
    "href": "talk/2023-04-22-ocrug-hackathon/index.html",
    "title": "Tidy Text Tutorial",
    "section": "",
    "text": "Slides Github\n2 hour tutorial, 1 part text mining, 1 part modeling with text.\nhttps://github.com/socalrug/hackathon-2023-04"
  },
  {
    "objectID": "talk/2020-07-07-predictive-modeling-with-text/index.html",
    "href": "talk/2020-07-07-predictive-modeling-with-text/index.html",
    "title": "Predictive modeling with text using tidy data principles",
    "section": "",
    "text": "Slides Video Github\n\n\nJ. Silge and E. Hvitfeldt\nHave you ever encountered text data and suspected there was useful insight latent within it but felt frustrated about how to find that insight? Are you familiar with dplyr and ggplot2, and ready to learn how unstructured text data can be used for prediction within the tidyverse and tidymodels ecosystems? Do you need a flexible framework for handling text data that allows you to engage in tasks from exploratory data analysis to supervised predictive modeling? This tutorial is geared toward an R user with intermediate familiarity with R, RStudio, the basics of regression and classification modeling, and tidyverse packages such as dplyr and ggplot2.\nThis person is comfortable with the main functions from dplyr and ggplot2 and is now ready to learn how to analyze and model text using tidy data principles. This R user has some experience with statistical modeling (such as using lm() and glm()) for prediction and classification and wants to learn how to build models with text."
  },
  {
    "objectID": "talk/2019-11-19-textrecipes-ecosystem/index.html",
    "href": "talk/2019-11-19-textrecipes-ecosystem/index.html",
    "title": "Building a package that fits into an evolving ecosystem",
    "section": "",
    "text": "Slides Github\n\n\nWith an ever-increasing amount of textual data is available to us, having a well-thought-out toolchain for modelling is crucial. tidymodels is a recent effort to create a modelling framework that shares the underlying design philosophy, grammar, and data structures of the tidyverse. textrecipes joined the roster a year ago and provided an exciting bridge to text preprocessing. This talk tells the tale of the package textrecipes; starting with the Github issue that sparked the idea for the package, go over the trials and challenges associated with building a package that heavily integrates with other packages all the way to the release on CRAN."
  },
  {
    "objectID": "talk/2019-01-29-working-with-tidymodels/index.html",
    "href": "talk/2019-01-29-working-with-tidymodels/index.html",
    "title": "Working with tidymodels",
    "section": "",
    "text": "Slides Github\n\n\nTidymodels is a “meta-package” in the same way as tidyverse, but with a focus on modeling and statistical analysis. This talk will go through how to use tidymodels to do modeling in a tidy fashion."
  },
  {
    "objectID": "talk/2019-11-09-hackathon-dataviz/index.html",
    "href": "talk/2019-11-09-hackathon-dataviz/index.html",
    "title": "Data visualization with ggplot2",
    "section": "",
    "text": "Slides Github\n\n\nHastly put together slides for Data-viz prep for Hackathon."
  },
  {
    "objectID": "talk/2018-10-29-best-practices-in-r/index.html",
    "href": "talk/2018-10-29-best-practices-in-r/index.html",
    "title": "Best Practices in R",
    "section": "",
    "text": "Slides Github\n\n\nThis presentation will let you though a lot of different aspects of what you can do in R to make yourself happy, make sure that future you is happy, and avoid getting mad at past you. This presentation will have a quite small focus on the actual R code you will produce by having the focus be on how you interact with the R code itself, touching on topics such as style and reproducibility."
  },
  {
    "objectID": "talk/2019-02-19-happyscientist-debugging-and-profiling/index.html",
    "href": "talk/2019-02-19-happyscientist-debugging-and-profiling/index.html",
    "title": "Debugging and Profiling in R",
    "section": "",
    "text": "Slides Github\n\n\nHitting an error or a speed-bump while working in R can be a frustration. This seminar will cover strategies and techniques for performing debugging and code profiling in R. We will look at some different ways to identify bugs, how to fix them and how to prevent them from coming back again. We will also look at a couple of typical patterns seen in slow code and at what can be done to fix it."
  },
  {
    "objectID": "talk/2021-06-03-aalborg-tidymodels/index.html",
    "href": "talk/2021-06-03-aalborg-tidymodels/index.html",
    "title": "Tidymodels - An Overview",
    "section": "",
    "text": "Slides Github\n\n\nInvited talk at Aalborg RUG\nThe tidymodels framework is a collection of R packages for modeling and machine learning using tidyverse principles. A large number of packages from preprocessing to evaluation come together to form a coherent framework for doing modeling: statistically, and machine learning-wise. This talk will walk through the landscape of packages and their individual place, helping you get a bird’s eye view."
  },
  {
    "objectID": "talk/2023-06-07-astrazeneca-recipes/index.html",
    "href": "talk/2023-06-07-astrazeneca-recipes/index.html",
    "title": "Flexible feature engineering using {recipes}",
    "section": "",
    "text": "Slides Github\n\n\nThis was a talk given for internal AstraZeneca conference."
  },
  {
    "objectID": "talk/2022-07-27-rstudioconf-tidyclust/index.html",
    "href": "talk/2022-07-27-rstudioconf-tidyclust/index.html",
    "title": "tidyclust - expanding tidymodels to clustering",
    "section": "",
    "text": "Slides Video Github\n\n\nAccepted talk at rstudio::conf(2022)\nThis talk marks the grand introduction of tidyclust, a new package that provides a tidy unified interface to clustering model within the tidymodels framework.\nWhile tidymodels has been a leap forward in making machine learning methods accessible to a general audience in R, it is currently limited to the realm of supervised learning. tidyclust, by Emil Hvitfeldt and Kelly Bodwin, builds upon the interfaces familiar to tidymodels users to make unsupervised clustering models equally approachable."
  },
  {
    "objectID": "talk/2023-02-21-ocrug-purrr/index.html",
    "href": "talk/2023-02-21-ocrug-purrr/index.html",
    "title": "Having a purrr-tastic time",
    "section": "",
    "text": "Slides Video Github\n\n\nThis was a small talk about purrr in general and some of the new things that was included in the 1.0.0 release."
  },
  {
    "objectID": "talk/2023-07-13-nyr-slidecraft/index.html",
    "href": "talk/2023-07-13-nyr-slidecraft/index.html",
    "title": "Slidecraft - the art of creating pretty slides",
    "section": "",
    "text": "Slides Github\n\n\nThis was a talk given at NY R Conference"
  },
  {
    "objectID": "talk/2023-05-05-timerecipes/index.html",
    "href": "talk/2023-05-05-timerecipes/index.html",
    "title": "time aware recipes",
    "section": "",
    "text": "Slides Github\n\n\nThis talk was orginally giving internally at Posit."
  }
]