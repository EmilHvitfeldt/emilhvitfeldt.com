{
  "hash": "0c90e8e436e460e8e8e48399e70cad58",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Dummy variables, sparse vs dense\nsummary: |\n  Analysis and visualizations of differences between sparse and dense dummy variables. \ndate: '2025-04-28'\ncategories:\n - tidymodels\nimage: \"index_files/figure-html/fig-length-v-runtime-1.png\"\n---\n\n\n\n\nOver the last year, I have worked on [improving sparsity support in tidymodels](https://www.tidyverse.org/blog/2025/03/tidymodels-sparsity/).\nWith that work, I spend time with various levels of abstraction,\none of the low levels includes the creation of dummy variables.\n\n[Dummy Encoding](https://feaz-book.com/categorical-dummy.html) is what happens when you take a factor variable and return a matrix of indicators,\none for each level in the factor.\nDummy encoding is a large source of sparse data for many modeling tasks,\nwhich is why I'm going to compare and contrasts its dense representation with its sparse representation.\nThis blog post will mostly be a number of visualizations based on benchmarks.\nTrying to pry out some insight into why and when you wanna use sparse encoding of your data.\n\n## Benchmarking\n\nFirst, I need to create some factors with specific lengths and a specific number of levels. `make_factor()` does that job for us.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_factor <- function(len, n_lvl) {\n lvls <- paste0(\"f\", seq_len(n_lvl))\n out <- sample(lvls, len, TRUE)\n  factor(out, lvls)\n}\n```\n:::\n\n\n\n\nWe notice that it works even when the number of levels is larger than the number of elements in the factor.\nThis can happen in real life as well and is important that we capture it in our benchmarking.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_factor(10, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] f1 f1 f2 f2 f1 f1 f1 f2 f2 f2\nLevels: f1 f2\n```\n\n\n:::\n\n```{.r .cell-code}\nmake_factor(10, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] f14 f15 f3  f7  f10 f15 f19 f5  f13 f4 \n20 Levels: f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15 f16 f17 ... f20\n```\n\n\n:::\n:::\n\n\n\n\nTo do the actual benchmarking I will be using the wonderful [bench](https://bench.r-lib.org/) package.\nUsing `bench::press()` we are able to perform the benchmarkings for a grid of difference values of length `len` and number of levels `n_lvl`.\nTo compare sparse and dense creation I will be using `sparsevctrs::sparse_dummy()` to create sparse dummy variables and `hardhat::fct_encode_one_hot()`.\nBoth of these functions do roughly the same thing,\nnamely take a factor as input and return dummy variables.\nWhile `hardhat::fct_encode_one_hot()` returns a matrix and `sparsevctrs::sparse_dummy()` returns a list of vectors,\nthey contain the same information so I won't bother with converting to a common format and will thus set `check = FALSE` to stop `bench::mark()` from complaining.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench_res <- bench::press(\n len = 10 * 2^(0:11),\n n_lvl = 10 * 2^(0:11),\n {\n fac <- make_factor(len, n_lvl)\n bench::mark(\n check = FALSE,\n min_iterations = 10,\n dense = hardhat::fct_encode_one_hot(fac),\n sparse = sparsevctrs::sparse_dummy(fac)\n )\n }\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nThis data set contains what we need. \nSpecifically, we want the following:\n\n- `expression` - sparse or dense\n- `len` - length of factor\n- `n_lvl` - number of levels in factor\n- `median` - the sample median of execution time\n- `mem_alloc` - the total amount of memory allocated by R while running the expression.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(bench)\n\nbench_res <- bench_res |>\n  select(expression, len, n_lvl, median, mem_alloc)\n```\n:::\n\n\n\n\n## Visualizations for time\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nWhat we are trying to uncover is some relationships between our inputs `len` and `n_lvl` and the quantities we care about the time taken `median` and memory allocation `mem_alloc`.\nAll together with how they differ between sparse and dense representation.\nThat is a lot of information to add to one chart so we will start small and build up.\nWe will also need multiple charts to explore different facets.\n\nFirst, we just look at vector length compared to runtime for one value of `n_lvl`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  filter(n_lvl == 40) |>\n  mutate(expression = as.character(expression)) |>\n  mutate(median = as.numeric(median)) |>\n  ggplot(aes(len, median, color = expression)) +\n  geom_point() +\n  labs(\n title = \"When number of levels is 40\",\n x = \"Vector length\",\n y = \"Median runtime\",\n color = NULL\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-runtime-40-1.png){#fig-length-v-runtime-40 width=672}\n:::\n:::\n\n\n\n\nHowever, this chart is going to work better on a log-log scale.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  filter(n_lvl == 40) |>\n  mutate(expression = as.character(expression)) |>\n  ggplot(aes(len, median, color = expression)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(\n title = \"When number of levels is 40\",\n x = \"Vector length\",\n y = \"Median runtime\",\n color = NULL\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-runtime-40-loglog-1.png){#fig-length-v-runtime-40-loglog width=672}\n:::\n:::\n\n\n\n\nWhat we see now is that dense vectors are faster to generate for smaller vectors and sparse vectors are faster to generate for larger vectors.\n\nLet us try a facet chart to explore all values of `n_lvl` at the same time.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  mutate(expression = as.character(expression)) |>\n  ggplot(aes(len, median, color = expression)) +\n  geom_point() +\n  facet_wrap(~n_lvl, scales = \"free_y\") +\n  scale_x_log10() +\n  labs(\n x = \"Vector length\",\n y = \"Median runtime\",\n color = NULL,\n title = \"Trends change as the number of levels in factor increases\",\n subtitle = \"Both sparse and dense straightens out with different slopes\"\n ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-runtime-1.png){#fig-length-v-runtime width=672}\n:::\n:::\n\n\n\n\nWe now notice a couple of things.\nThe first thing I notice is that the intersection goes up as we increase the number of levels.\nFor low-level counts sparse always wins.\nas we increase in `n_lvls` the two curves each converge to a linear trend (in our log-log chart),\nThe difference is the slopes they have.\nThe slope of the dense times suggests that the relationship between vector length and computation time,\nif the vector is 10 times as long it takes 10 times as long.\nThe sparse time is almost constant over the vector length.\nIt takes roughly the same time regardless of how long the vector is.\n\nThe above results make sense.\nWhen looking at the sparse calculations there are 2 things that take time to do,\nsetting up the output which is a list of `n_lvl` vectors,\nand filling in the non-zero values.\nSetting up the output takes the same amount of time regardless of how long the vectors are,\nand we can see that values are reflected on the left-hand side of the chart.\nFilling in the values does depend on the length,\nas it needs to add a value for each element in the input,\nwe see this as an increase in time.\nAnd that time is more or less independent of the number of levels. \nHence why the difference between the left and right values for sparse calculations appear to be the same in all facets.\n\nLet us now look at the relationship between how long it takes to run the dense version compared to the sparse version.\nDoing it for `n_lvl = 640` for simplicity first. \nThe line here is set to represent when the dense time is higher than the sparse time.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  filter(n_lvl == 640) |>\n  select(expression, len, n_lvl, median) |>\n  mutate(median = as.numeric(median)) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = dense / sparse) |>\n  ggplot(aes(len, ratio)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 1) +\n  scale_x_log10() +\n  labs(\n title = \"When number of levels is 640\",\n x = \"Vector length\",\n y = \"dense runtime / sparse runtime\",\n color = NULL\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-ratio-640-1.png){#fig-length-v-ratio-640 width=672}\n:::\n:::\n\n\n\n\nWe notice this shape and see what will happen if we throw a `log()` on the y-axis as well.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  filter(n_lvl == 640) |>\n  select(expression, len, n_lvl, median) |>\n  mutate(median = as.numeric(median)) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = log10(dense / sparse)) |>\n  ggplot(aes(len, ratio)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  scale_x_log10() +\n  labs(\n title = \"When number of levels is 640\",\n x = \"Vector length\",\n y = \"log(dense runtime / sparse runtime)\",\n color = NULL\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-ratio-640-loglog-1.png){#fig-length-v-ratio-640-loglog width=672}\n:::\n:::\n\n\n\n\nAnd look at that!\nA linear relationship.\nWe can use this linear relationship to make some statements about what happens.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nWe can fit a line on this log-log chart and it would give us a slope of ratio_estimate_640`. \nTaking this estimate to the power of 10 gives us 5.34 which is how much the relationship between the dense runtime and the sparse runtime increases each time the length gets 10 times longer.\n\nWe can expand this to look at all the different levels.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  select(expression, len, n_lvl, median) |>\n  mutate(median = as.numeric(median)) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = log(dense / sparse)) |>\n  mutate(n_lvl = as.factor(n_lvl)) |>\n  ggplot(aes(len, ratio, color = n_lvl)) +\n  geom_point() +\n  geom_abline(slope = 0, intercept = 0) +\n  scale_color_manual(values = diy_rainbow) +\n  scale_x_log10() +\n  labs(\n x = \"Vector length\",\n y = \"log(dense runtime / sparse runtime)\",\n color = \"number of levels\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-ratio-loglog-1.png){#fig-length-v-ratio-loglog width=672}\n:::\n:::\n\n\n\n\nSince they all appear to be linear in log-log I'll recreate this chart using fitted lines instead.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  select(expression, len, n_lvl, median) |>\n  mutate(median = as.numeric(median)) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = log(dense / sparse)) |>\n  mutate(n_lvl = as.factor(n_lvl)) |>\n  ggplot(aes(len, ratio, color = n_lvl)) +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n  geom_abline(slope = 0, intercept = 0) +\n  scale_x_log10() +\n  scale_color_manual(values = diy_rainbow) +\n  labs(\n x = \"Vector length\",\n y = \"log(dense runtime / sparse runtime)\",\n color = \"number of levels\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-ratio-loglog-lines-1.png){#fig-length-v-ratio-loglog-lines width=672}\n:::\n:::\n\n\n\n\nThe results are not too surprising considering what we have seen so far.\nThe slopes for each of these curves and their associated effect is seen below.\nNotice how big of a difference we get as the number of levels increases.\nThis makes sense as sparse dummies take up roughly the same amount of memory regardless of how many levels the factor had.\n\n\n\n\n::: {#tbl-ratio_lm_estimates .cell}\n::: {.cell-output-display}\n\n\n| n_lvl| estimate| effect|\n|-----:|--------:|------:|\n|    10|    0.056|  1.137|\n|    20|    0.136|  1.368|\n|    40|    0.239|  1.734|\n|    80|    0.357|  2.273|\n|   160|    0.478|  3.005|\n|   320|    0.614|  4.114|\n|   640|    0.728|  5.344|\n|  1280|    0.818|  6.570|\n|  2560|    0.892|  7.797|\n|  5120|    0.942|  8.757|\n| 10240|    0.995|  9.892|\n| 20480|    1.018| 10.414|\n\n\n:::\n:::\n\n\n\n\nIf we instead wanted to look at whether the dense and sparse method is fastest for a given vector length and number of levels.\nThe below chart tries to answer that question.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  mutate(expression = as.character(expression)) |>\n  mutate(median = as.numeric(median)) |>\n  mutate(len = factor(len), n_lvl = factor(n_lvl)) |>\n  select(expression, len, n_lvl, median) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = log10(dense / sparse)) |>\n  ggplot(aes(len, n_lvl)) +\n  geom_tile(aes(fill = ratio)) +\n  scale_fill_gradient2(low = \"#A06928\", mid = \"#EEEBC3\", high = \"#2887A1\") +\n    labs(\n x = \"Vector length\",\n y = \"number of levels\",\n fill = \"ratio\",\n subtitle = \"ratio = log(dense runtime / sparse runtime)\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-n_lvls-1.png){#fig-length-v-n_lvls width=672}\n:::\n:::\n\n\n\n\nThe top left represents the types of factor vectors where it is faster to generate dense versions.\nEverything else represents scenarios where generating sparse versions is faster.\nThe intensity of the colors shows how much faster it is, remember it is on a log scale so `1` means that it is 10 times faster,\nand `-1` means it is 0.1 times faster or a 10th of the time.\n\nBelow we highlighted the top left area a little bit more.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nres_ratio <- bench_res |>\n  mutate(expression = as.character(expression)) |>\n  mutate(median = as.numeric(median)) |>\n  mutate(len = factor(len), n_lvl = factor(n_lvl)) |>\n  select(expression, len, n_lvl, median) |>\n  pivot_wider(names_from = expression, values_from = median) |>\n  mutate(ratio = log10(dense / sparse))\n\nres_ratio |>\n  ggplot(aes(len, n_lvl)) +\n  geom_tile(aes(fill = ratio)) +\n  geom_tile(\n data = filter(res_ratio, ratio < 0),\n color = \"#A06928\",\n fill = \"#00000000\"\n ) +\n  scale_fill_gradient2(low = \"#A06928\", mid = \"#EEEBC3\", high = \"#2887A1\") +\n  labs(\n x = \"Vector length\",\n y = \"number of levels\",\n fill = \"ratio\",\n subtitle = \"ratio = log(dense runtime / sparse runtime)\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-n_lvls-highlights-1.png){#fig-length-v-n_lvls-highlights width=672}\n:::\n:::\n\n\n\n\n## Visualisations for memory\n\nEach of the charts we developed in the previous section can be redone to show memory allocation in place of runtime.\nWe will only show the full charts since we have already built these charts up step by step.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  mutate(expression = as.character(expression)) |>\n  ggplot(aes(len, mem_alloc, color = expression)) +\n  geom_point() +\n  facet_wrap(~n_lvl, scales = \"free_y\") +\n  scale_x_log10() +\n  labs(\n x = \"Vector length\",\n y = \"Memory Allocation\",\n color = NULL\n ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-memory-1.png){#fig-length-v-memory width=672}\n:::\n:::\n\n\n\n\nthe biggest difference here is that the question isn't whether sparse will take up less than dense,\ninstead, we can look at how much less memory it will take.\nThere are technically a few cases both of those happen for very low counts that are hard to see from this chart.\n\nWe are seeing a much more interesting chart here than last time.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  select(expression, len, n_lvl, mem_alloc) |>\n  mutate(mem_alloc = as.numeric(mem_alloc)) |>\n  pivot_wider(names_from = expression, values_from = mem_alloc) |>\n  mutate(ratio = log(dense / sparse)) |>\n  mutate(n_lvl = as.factor(n_lvl)) |>\n  ggplot(aes(len, ratio, color = n_lvl)) +\n  geom_line() +\n  geom_abline(slope = 0, intercept = 0) +\n  scale_color_manual(values = diy_rainbow) +\n  scale_x_log10() +\n  labs(\n x = \"Vector length\",\n y = \"log(dense memory / sparse memory)\",\n color = \"number of levels\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-length-v-ratio-memory-loglog-lines-1.png){#fig-length-v-ratio-memory-loglog-lines width=672}\n:::\n:::\n\n\n\n\nWe don't look at when sparse will take up less space than dense,\nwe look at how much and what trends we are seeing.\nThe main takeaway is that the sparse representation gets better when the vector is longer and when it has more levels.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbench_res |>\n  mutate(expression = as.character(expression)) |>\n  mutate(mem_alloc = as.numeric(mem_alloc)) |>\n  mutate(len = factor(len), n_lvl = factor(n_lvl)) |>\n  select(expression, len, n_lvl, mem_alloc) |>\n  pivot_wider(names_from = expression, values_from = mem_alloc) |>\n  mutate(ratio = log(dense / sparse)) |>\n  ggplot(aes(len, n_lvl)) +\n  geom_tile(aes(fill = ratio)) +\n  scale_fill_viridis_c(begin = 0.05, end = 0.95) +\n  labs(\n x = \"Vector length\",\n y = \"number of levels\",\n fill = \"ratio\",\n subtitle = \"ratio = log(dense memory / sparse memory)\"\n )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/length-v-n_lvls-memory-1.png){width=672}\n:::\n:::\n\n\n\n\n## Conclusions\n\nsparse representation \"wins\" unless the vectors are short (less than 1000),\nwhich would indicate that using it is a good idea all the time.\nHowever the answer is not quite that simple,\nthe time it takes to perform dummy variables is rarely the dominant factor when using a ML system.\nThe main deciding factor will often be fitting the model,\nthe whole process should be done with and without sparsity if you truly wanna know which is faster.\nThere is also a penalty for creating the wrong type as conversion back and forth takes longer than either of the creations.\n\nAlso, these charts are not expected to generalize when the number of levels and vector lengths increase.\nWhen we find linear trends we don't expect them to go on forever.\nEspecially since we are dealing with memory,\nas you eventually  hit the garbage collector which will happen to the dense version much earlier than for the sparse data.\nThere are also factors that you wouldn't be able to create dummies from densely as the resulting object wouldn't fit in memory,\nwhere the corresponding sparse representation would happen much easier.\n\nThe purpose of this post was mainly to explore and contrast the creation of dummies as sparse and dense vectors.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}