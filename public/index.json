[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["tidymodels","textrecipes","tidytuesday"],"content":" I’m ready for my second #tidytuesday and as a massive The Office fan this dataset is right up my alley. In this post, you will read how to\n Use the R wrapper spacyr of spacy to extract part of speech tags Use a custom tokenizer in conjunction with textrecipes package Do hyperparameter tuning with the tune package Try to predict the author of each line in the show  I’ll put a little more effort into the explorative charts then I usually do. I’ll not be explaining each line of code for those, but you are encouraged to play around with them yourself.\nPackages 📦 library(schrute) library(tidytext) library(tidymodels) library(tokenizers) library(textrecipes) library(spacyr) library(paletteer) We will be using the schrute package which includes the dataset for the week. tidytext and tokenizers to do data exploration for the text. spacyr to access the spacy to perform part of speech tagging. tidymodels and textrecipes to do to the preprocessing and modeling. And lastly, we use paletteer to get pretty color palettes.\n Exploring the data ⛏ The data comes with a lot of different variables. We will be focusing on character and text. First, let us take a look at how many lines each character has\ntheoffice %\u0026gt;% count(character, sort = TRUE) Micheal, Dwight, Jim, and Pam are dominating the charts. This is unsurprising since they are some of the main characters having a central role in the episodes they appear in. This will be too many classes for the scope of this post so I’ll limit it to the top 5 characters with the most lines since the number drops off more after the first 5.\nsmall_office \u0026lt;- theoffice %\u0026gt;% select(character, text) %\u0026gt;% filter(character %in% c(\u0026quot;Michael\u0026quot;, \u0026quot;Dwight\u0026quot;, \u0026quot;Jim\u0026quot;, \u0026quot;Pam\u0026quot;, \u0026quot;Andy\u0026quot;)) Let us take a lot at how many words each line in the script is. This is going to be a problem for us later on as predicting with shorter text is harder than longer text as there is less information in it.\nsmall_office %\u0026gt;% mutate(n_words = count_words(text)) %\u0026gt;% ggplot(aes(n_words, color = character)) + geom_density(binwidth = 1, key_glyph = draw_key_timeseries) + xlim(c(0, 50)) + scale_color_paletteer_d(\u0026quot;nord::aurora\u0026quot;) + labs(x = \u0026quot;Number of words\u0026quot;, y = \u0026quot;Density\u0026quot;, color = NULL, title = \u0026quot;Distribution of line length in The Office\u0026quot;) + theme_minimal() + theme(legend.position = \u0026quot;top\u0026quot;, plot.title.position = \u0026quot;plot\u0026quot;)  These lines are thankfully pretty similar, which will make it easier for us to make a good predictive model. However, we can still see some differences. Pam and Jim both have shorter lines than the rest, and Michael and Andy both have fewer shorter lines in exchange for more long lines.\nWe will be also be exploring part of speech tagging and for that, we will be using the spacyr package. It isn’t always needed but I’m going to explicitly initialize the spacy model\nspacy_initialize(model = \u0026quot;en_core_web_sm\u0026quot;) the spacyr package outputs in this nice format with doc_id, sentence_id, token_id, token and pos.\nspacy_parse(small_office$text[1], entity = FALSE, lemma = FALSE) Normally I would just analyze the data in this format. But since I have to create a custom wrapper for textrecipes anyway I’ll do the remaining of the text mining in tidytext. textrecipes requires that the tokenizer returns the tokens in a list format similar to the tokenizers in tokenizers. The following function takes a character vector and returns the part of speech tags in a list format.\nspacy_pos \u0026lt;- function(x) { tokens \u0026lt;- spacy_parse(x, entity = FALSE, lemma = FALSE) token_list \u0026lt;- split(tokens$pos, tokens$doc_id) names(token_list) \u0026lt;- gsub(\u0026quot;text\u0026quot;, \u0026quot;\u0026quot;, names(token_list)) res \u0026lt;- unname(token_list[as.character(seq_along(x))]) empty \u0026lt;- lengths(res) == 0 res[empty] \u0026lt;- lapply(seq_len(sum(empty)), function(x) character(0)) res } Little example to showcase the function\nexample_string \u0026lt;- c(\u0026quot;Hello there pig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;One more pig here\u0026quot;) spacy_pos(x = example_string) We can use a custom tokenizer by simply passing it to the token argument. This is going to take a little longer than normal since POS tagging takes longer than simply tokenizing.\nsmall_office_tokens \u0026lt;- small_office %\u0026gt;% unnest_tokens(text, text, token = spacy_pos, to_lower = FALSE) Below is a chart of the number of each part of speech tags. The meaning of the acronyms can be found here if you click on the Universal Part-of-speech Tags button.\ncolors \u0026lt;- rep(paletteer_d(\u0026quot;rcartocolor::Pastel\u0026quot;), length.out = 16) small_office_tokens %\u0026gt;% count(text) %\u0026gt;% ggplot(aes(n, reorder(text, n), fill = reorder(text, n))) + geom_col() + labs(x = NULL, y = NULL, title = \u0026quot;Part of Speech tags in The Office\u0026quot;) + scale_fill_manual(values = colors) + guides(fill = \u0026quot;none\u0026quot;) + theme_minimal() + theme(plot.title.position = \u0026quot;plot\u0026quot;)  I found it initially surprising that punctuation (PUNCT) was leading the chart. But after thinking about it a little bit it, I can imagine it has something to do with all the lines being very short and having to end in some kind of punctuation.\nWe can facet this by the character to see who uses what part of speech.\nsmall_office_tokens %\u0026gt;% count(character, text) %\u0026gt;% group_by(character) %\u0026gt;% mutate(prop = n / sum(n)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(forcats::fct_rev(reorder(text, n)), prop, fill = character)) + geom_col(position = \u0026quot;dodge\u0026quot;) + scale_fill_paletteer_d(\u0026quot;nord::aurora\u0026quot;) + labs(x = NULL, y = NULL, fill = NULL, title = \u0026quot;Part of speech tags by main character in The Office\u0026quot;) + theme_minimal() + theme(legend.position = \u0026quot;top\u0026quot;, plot.title.position = \u0026quot;plot\u0026quot;)  I don’t immediately see anything popping out at me, but it is a very pretty chart otherwise. I feel like I have seen enough, lets get to modeling!\n Modeling ⚙️ Not that we have gotten a look at the data lets get to modeling. First we need to do a test/train split which we can do with yardstick.\nset.seed(1234) office_split \u0026lt;- initial_split(small_office, strata = character) office_test \u0026lt;- testing(office_split) office_train \u0026lt;- training(office_split) Next we are going to prepare the preprocessing steps. We will be using the custom part of speech tokenizer we defined earlier to include part of speech tag counts as features in our model. Since this data is going to a little sparse will we also include bi-grams of the data. To this, we first create a copy of the text variable and apply the tokenizers to each copy. Lastly will be also be doing some downsampling of the data to handle the imbalance in the data. This calculation will once again take a little while since the part of speech calculations takes a minute or two.\nrec \u0026lt;- recipe(character ~ text, data = small_office) %\u0026gt;% # Deal with imbalance step_downsample(character) %\u0026gt;% # Create copy of text variable step_mutate(text_copy = text) %\u0026gt;% # Tokenize the two text columns step_tokenize(text, token = \u0026quot;ngrams\u0026quot;, options = list(n = 2)) %\u0026gt;% step_tokenize(text_copy, custom_token = spacy_pos) %\u0026gt;% # Filter to only keep the most 100 frequent n-grams step_tokenfilter(text, max_tokens = 100) %\u0026gt;% # Calculate tf-idf for both sets of tokens step_tfidf(text, text_copy) %\u0026gt;% prep() We can now extract the processed data\noffice_test_prepped \u0026lt;- bake(rec, office_test) office_train_prepped \u0026lt;- juice(rec) To do the actual modeling we will be using multinom_reg() with \"glmnet\" as the engine. This model has two hyperparameters, which we will be doing a grid search over to find optimal values. We specify that we want to tune these parameters by passing tune() to them.\ntune_spec \u0026lt;- multinom_reg(penalty = tune(), mixture = tune()) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) tune_spec Next we set up a bootstrap sampler and grid to optimize over.\nset.seed(12345) office_boot \u0026lt;- bootstraps(office_train_prepped, strata = character, times = 10) hyper_grid \u0026lt;- grid_regular(penalty(), mixture(), levels = 10) Now we pass all the objects to tune_grid(). It is also possible to combine our recipe and model object into a workflow object to pass to tune_grid instead. However, since the preprocessing step took so long and we didn’t vary anything it makes more sense time-wise to use tune_grid() with a formula instead. I also set control = control_grid(verbose = TRUE) so I get a live update of how far the calculations are going.\nset.seed(123456) fitted_grid \u0026lt;- tune_grid( formula = character ~ ., model = tune_spec, resamples = office_boot, grid = hyper_grid, control = control_grid(verbose = TRUE) ) We can now look at the best performing models with show_best()\nfitted_grid %\u0026gt;% show_best(\u0026quot;roc_auc\u0026quot;) And we can use the values from the best performing model to fit our final model.\nfinal_model \u0026lt;- tune_spec %\u0026gt;% update(penalty = 0.005994843, mixture = 1 / 3) %\u0026gt;% fit(character ~ ., data = office_train_prepped)  Evaluation 📐 Now that we have our final model we can predict on our test set and look at the confusion matrix to see how well we did.\nbind_cols( predict(final_model, office_test_prepped), office_test_prepped ) %\u0026gt;% conf_mat(truth = character, estimate = .pred_class) %\u0026gt;% autoplot(type = \u0026quot;heatmap\u0026quot;) These are not going too well. It is doing best at predicting Michael correctly, and it seems to confuse Dwight and Michael a little bit.\nLet us investigate the cases that didn’t go too well. We can get the individual class probabilities by setting type = \"prob\" in predict()\nclass_predictions \u0026lt;- predict(final_model, office_test_prepped, type = \u0026quot;prob\u0026quot;) class_predictions We can do some wrangling to get the 5 worst predicted texts for each character:\nbind_cols( class_predictions, office_test ) %\u0026gt;% pivot_longer(starts_with(\u0026quot;.pred_\u0026quot;)) %\u0026gt;% filter(gsub(\u0026quot;.pred_\u0026quot;, \u0026quot;\u0026quot;, name) == character) %\u0026gt;% group_by(character) %\u0026gt;% arrange(value) %\u0026gt;% slice(1:5) %\u0026gt;% ungroup() %\u0026gt;% select(-name, -value) %\u0026gt;% reactable::reactable() So the first striking thing here is that many of the lines are quite short, with most of Pam’s being 5 words or less. On the other hand, all the wrongly predicted lines for Michael are quite a bit longer than the rest.\nWe can also get the best predicted lines for each character by flipping the sign with desc()\nbind_cols( class_predictions, office_test ) %\u0026gt;% pivot_longer(starts_with(\u0026quot;.pred_\u0026quot;)) %\u0026gt;% filter(gsub(\u0026quot;.pred_\u0026quot;, \u0026quot;\u0026quot;, name) == character) %\u0026gt;% group_by(character) %\u0026gt;% arrange(desc(value)) %\u0026gt;% slice(1:5) %\u0026gt;% ungroup() %\u0026gt;% select(-name, -value) %\u0026gt;% reactable::reactable() One thing I noticed is that many of Pam’s lines start with “Oh my” and that might have been a unique character trait that got picked up in the bi-grams.\n ","date":1584576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584576000,"objectID":"59ecf4835f0c1c18fdc9b0c751d64204","permalink":"/2020/03/19/tidytuesday-part-of-speech-and-textrecipes-with-the-office/","publishdate":"2020-03-19T00:00:00Z","relpermalink":"/2020/03/19/tidytuesday-part-of-speech-and-textrecipes-with-the-office/","section":"post","summary":"I’m ready for my second #tidytuesday and as a massive The Office fan this dataset is right up my alley. In this post, you will read how to\n Use the R wrapper spacyr of spacy to extract part of speech tags Use a custom tokenizer in conjunction with textrecipes package Do hyperparameter tuning with the tune package Try to predict the author of each line in the show  I’ll put a little more effort into the explorative charts then I usually do.","tags":null,"title":"tidytuesday: Part-of-Speech and textrecipes with The Office","type":"post"},{"authors":null,"categories":["tidytext","ggplot2"],"content":" I have been working on visualizing how different kinds of words are used in texts and I finally found a good visualization style with the slope chart. More specifically I’m thinking of two groups of paired words.\nPackages 📦 library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(hcandersenr) library(tidytext) library(paletteer) library(ggrepel)  Minimal Example 1️⃣ First I’ll walk you through a minimal example of how the chart is created. Afterward, I have created a function to automate the whole procedure so we can quickly iterate. We start by an example of gendered words in fairly tales by H.C. Andersen using the hcandersenr package. We start by generating a data.frame of paired words. This is easily done using the tribble() function.\ngender_words \u0026lt;- tribble( ~men, ~women, \u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;, \u0026quot;his\u0026quot;, \u0026quot;her\u0026quot;, \u0026quot;man\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;men\u0026quot;, \u0026quot;women\u0026quot;, \u0026quot;boy\u0026quot;, \u0026quot;girl\u0026quot;, \u0026quot;he\u0026#39;s\u0026quot;, \u0026quot;she\u0026#39;s\u0026quot;, \u0026quot;he\u0026#39;d\u0026quot;, \u0026quot;she\u0026#39;d\u0026quot;, \u0026quot;he\u0026#39;ll\u0026quot;, \u0026quot;she\u0026#39;ll\u0026quot;, \u0026quot;himself\u0026quot;, \u0026quot;herself\u0026quot; ) Next, we are going to tokenize and count the tokens in the corpus,\nordered_words \u0026lt;- hcandersen_en %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% pull(word) Next, we are going to get the index for each word, which we will put on a log scale since it will be easier to visualize. Next, we will calculate a slope between the points and add the correct labels.\ngender_words_plot \u0026lt;- gender_words %\u0026gt;% mutate(male_index = match(men, ordered_words), female_index = match(women, ordered_words)) %\u0026gt;% mutate(slope = log10(male_index) - log10(female_index)) %\u0026gt;% pivot_longer(male_index:female_index) %\u0026gt;% mutate(value = log10(value), label = ifelse(name == \u0026quot;male_index\u0026quot;, men, women)) %\u0026gt;% mutate(name = factor(name, c(\u0026quot;male_index\u0026quot;, \u0026quot;female_index\u0026quot;), c(\u0026quot;men\u0026quot;, \u0026quot;women\u0026quot;))) Next, we are going to manually calculate the limits to make sure a diverging color scale will have the colors done directly.\nlimit \u0026lt;- max(abs(gender_words_plot$slope)) * c(-1, 1) Lastly, we just put everything into ggplot2 and voila!!\ngender_words_plot %\u0026gt;% ggplot(aes(name, value, group = women, label = label)) + geom_line(aes(color = slope)) + scale_y_reverse(labels = function(x) 10 ^ x) + geom_text() + guides(color = \u0026quot;none\u0026quot;) + scale_color_distiller(type = \u0026quot;div\u0026quot;, limit = limit) + theme_minimal() + theme(panel.border = element_blank(), panel.grid.major.x = element_blank()) + labs(x = NULL, y = \u0026quot;Word Rank\u0026quot;) + labs(title = \u0026quot;Masculine gendered words appeared more often in H.C. Andersen\u0026#39;s fairy tales\u0026quot;)  Make it into a function ✨ This function is mostly the same as the code you saw earlier. Main difference is using .data from rlang to generalize. The function also includes other beautifications such as improved themes and theme support with paletteer.\nplot_fun \u0026lt;- function(words, ref, palette = \u0026quot;scico::roma\u0026quot;, ...) { names \u0026lt;- colnames(ref) ordered_words \u0026lt;- names(sort(table(words), decreasing = TRUE)) plot_data \u0026lt;- ref %\u0026gt;% mutate(index1 = match(.data[[names[1]]], ordered_words), index2 = match(.data[[names[2]]], ordered_words)) %\u0026gt;% mutate(slope = log10(index1) - log10(index2)) %\u0026gt;% pivot_longer(index1:index2) %\u0026gt;% mutate(value = log10(value), label = ifelse(name == \u0026quot;index1\u0026quot;, .data[[names[1]]], .data[[names[2]]]), name = factor(name, c(\u0026quot;index1\u0026quot;, \u0026quot;index2\u0026quot;), names)) limit \u0026lt;- max(abs(plot_data$slope)) * c(-1, 1) plot_data %\u0026gt;% ggplot(aes(name, value, group = .data[[names[2]]], label = label)) + geom_line(aes(color = slope), size = 1) + scale_y_reverse(labels = function(x) round(10 ^ x)) + geom_text_repel(data = subset(plot_data, name == names[1]), aes(segment.color = slope), nudge_x = -0.1, segment.size = 1, direction = \u0026quot;y\u0026quot;, hjust = 1) + geom_text_repel(data = subset(plot_data, name == names[2]), aes(segment.color = slope), nudge_x = 0.1, segment.size = 1, direction = \u0026quot;y\u0026quot;, hjust = 0) + scale_color_paletteer_c(palette, limit = limit, aesthetics = c(\u0026quot;color\u0026quot;, \u0026quot;segment.color\u0026quot;), ...) + guides(color = \u0026quot;none\u0026quot;, segment.color = \u0026quot;none\u0026quot;) + theme_minimal() + theme(panel.border = element_blank(), panel.grid.major.x = element_blank(), axis.text.x = element_text(size = 15)) + labs(x = NULL, y = \u0026quot;Word Rank\u0026quot;) } Now we can recreate the previous chart with ease\nref \u0026lt;- tribble( ~Men, ~Women, \u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;, \u0026quot;his\u0026quot;, \u0026quot;her\u0026quot;, \u0026quot;man\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;men\u0026quot;, \u0026quot;women\u0026quot;, \u0026quot;boy\u0026quot;, \u0026quot;girl\u0026quot;, \u0026quot;he\u0026#39;s\u0026quot;, \u0026quot;she\u0026#39;s\u0026quot;, \u0026quot;he\u0026#39;d\u0026quot;, \u0026quot;she\u0026#39;d\u0026quot;, \u0026quot;he\u0026#39;ll\u0026quot;, \u0026quot;she\u0026#39;ll\u0026quot;, \u0026quot;himself\u0026quot;, \u0026quot;herself\u0026quot; ) words \u0026lt;- hcandersen_en %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% pull(word) plot_fun(words, ref, direction = -1) + labs(title = \u0026quot;Masculine gendered words appeared more often in H.C. Andersen\u0026#39;s fairy tales\u0026quot;) ## Warning: Ignoring unknown aesthetics: segment.colour ## Warning: Ignoring unknown aesthetics: segment.colour  Gallery 🖼 ref \u0026lt;- tribble( ~Men, ~Women, \u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;, \u0026quot;his\u0026quot;, \u0026quot;her\u0026quot;, \u0026quot;man\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;men\u0026quot;, \u0026quot;women\u0026quot;, \u0026quot;boy\u0026quot;, \u0026quot;girl\u0026quot;, \u0026quot;himself\u0026quot;, \u0026quot;herself\u0026quot; ) words \u0026lt;- janeaustenr::austen_books() %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% pull(word) plot_fun(words, ref, direction = -1) + labs(title = \u0026quot;Masculine gendered words appeared less often in Jane Austen Novels\u0026quot;) ## Warning: Ignoring unknown aesthetics: segment.colour ## Warning: Ignoring unknown aesthetics: segment.colour More examples using the tidygutenbergr package.\nref \u0026lt;- tribble( ~Men, ~Women, \u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;, \u0026quot;his\u0026quot;, \u0026quot;her\u0026quot;, \u0026quot;man\u0026quot;, \u0026quot;woman\u0026quot;, \u0026quot;men\u0026quot;, \u0026quot;women\u0026quot;, \u0026quot;boy\u0026quot;, \u0026quot;girl\u0026quot;, \u0026quot;he\u0026#39;s\u0026quot;, \u0026quot;she\u0026#39;s\u0026quot;, \u0026quot;himself\u0026quot;, \u0026quot;herself\u0026quot; ) words \u0026lt;- tidygutenbergr::phantom_of_the_opera() %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% pull(word) plot_fun(words, ref, \u0026quot;scico::berlin\u0026quot;) + labs(title = \u0026quot;Masculine gendered words appeared more often in Phantom of the Opera\u0026quot;) ## Warning: Ignoring unknown aesthetics: segment.colour ## Warning: Ignoring unknown aesthetics: segment.colour ref \u0026lt;- tribble( ~Positive, ~Negative, \u0026quot;good\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;pretty\u0026quot;, \u0026quot;ugly\u0026quot;, \u0026quot;friendly\u0026quot;, \u0026quot;hostile\u0026quot; ) words \u0026lt;- tidygutenbergr::dracula() %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% pull(word) plot_fun(words, ref, palette = \u0026quot;scico::tokyo\u0026quot;) + labs(title = \u0026quot;Positive adjectives appeared more often in Dracula\u0026quot;) ## Warning: Ignoring unknown aesthetics: segment.colour ## Warning: Ignoring unknown aesthetics: segment.colour  ","date":1584403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584403200,"objectID":"8b49ff2244272d5505b4aa27d64c3eb1","permalink":"/2020/03/17/word-rank-slope-charts/","publishdate":"2020-03-17T00:00:00Z","relpermalink":"/2020/03/17/word-rank-slope-charts/","section":"post","summary":"I have been working on visualizing how different kinds of words are used in texts and I finally found a good visualization style with the slope chart. More specifically I’m thinking of two groups of paired words.","tags":null,"title":"Word Rank Slope Charts","type":"post"},{"authors":null,"categories":["tidytext"],"content":" Photo by Plush Design Studio on Unsplash\nIt is known that topic modeling does not benefit from stemming ref. I propose a workflow to investigate if stemming is appropriate as a method for data reduction.\nTake all the tokens and apply the stemming algorithm you would like to test Construct a list of words that should be equal under stemming Apply a topic model to your original data Predict the topic for each word created in 2.  If grouped words are predicted to the same topic then we assume that stemming would not make much of a difference. If the words are predicted to be indifferent topics then we have a suspicion that the stemmed and unstemmed words have different uses and stemming would be ill-advised.\nFirst, we load the packages we will be using.\nlibrary(tidytext) library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(stm) library(hcandersenr) library(SnowballC) ## Warning: package \u0026#39;SnowballC\u0026#39; was built under R version 3.6.2 As a first test, we pick 3 fairy tales by H.C. Andersens using the hcandersenr package. To create multiple “documents” for each fairy tale we start by tokenizing to sentences. Then we give each sentence a unique identifier.\nfairy_tales \u0026lt;- hcandersen_en %\u0026gt;% filter(book %in% c(\u0026quot;The fir tree\u0026quot;, \u0026quot;The tinder-box\u0026quot;, \u0026quot;Thumbelina\u0026quot;)) %\u0026gt;% unnest_tokens(token, text, token = \u0026quot;sentences\u0026quot;) %\u0026gt;% group_by(book) %\u0026gt;% mutate(sentence = row_number()) %\u0026gt;% ungroup() %\u0026gt;% unite(document, book, sentence) fairy_tales ## # A tibble: 501 x 2 ## document token ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 The fir tree_1 \u0026quot;far down in the forest, where the warm sun and the fresh air… ## 2 The fir tree_2 \u0026quot;the sun shone, and the soft air fluttered its leaves, and th… ## 3 The fir tree_3 \u0026quot;sometimes the children would bring a large basket of raspber… ## 4 The fir tree_4 \u0026quot;which made it feel more unhappy than before.\u0026quot; ## 5 The fir tree_5 \u0026quot;and yet all this while the tree grew a notch or joint taller… ## 6 The fir tree_6 \u0026quot;still, as it grew, it complained.\u0026quot; ## 7 The fir tree_7 \u0026quot;\\\u0026quot;oh!\u0026quot; ## 8 The fir tree_8 \u0026quot;how i wish i were as tall as the other trees, then i would s… ## 9 The fir tree_9 \u0026quot;i should have the birds building their nests on my boughs, a… ## 10 The fir tree_… \u0026quot;the tree was so discontented, that it took no pleasure in th… ## # … with 491 more rows Now we unnest the tokens to words and create a new variable of the stemmed words\nfairy_tales_tokens \u0026lt;- fairy_tales %\u0026gt;% unnest_tokens(token, token) %\u0026gt;% mutate(token_stem = wordStem(token)) fairy_tales_tokens ## # A tibble: 10,577 x 3 ## document token token_stem ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 The fir tree_1 far far ## 2 The fir tree_1 down down ## 3 The fir tree_1 in in ## 4 The fir tree_1 the the ## 5 The fir tree_1 forest forest ## 6 The fir tree_1 where where ## 7 The fir tree_1 the the ## 8 The fir tree_1 warm warm ## 9 The fir tree_1 sun sun ## 10 The fir tree_1 and and ## # … with 10,567 more rows We can take a look at all the times where stemming we can look at all the times stemming yields a different token.\ndifferent \u0026lt;- fairy_tales_tokens %\u0026gt;% select(token, token_stem) %\u0026gt;% filter(token != token_stem) %\u0026gt;% unique() different ## # A tibble: 759 x 2 ## token token_stem ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 resting rest ## 2 pretty pretti ## 3 little littl ## 4 was wa ## 5 happy happi ## 6 wished wish ## 7 its it ## 8 companions companion ## 9 pines pine ## 10 firs fir ## # … with 749 more rows In this example, we have 759 different tokens. But since stemming can collapse multiple different tokens into one.\ndifferent %\u0026gt;% count(token_stem, sort = TRUE) ## # A tibble: 672 x 2 ## token_stem n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 seiz 4 ## 2 leav 3 ## 3 live 3 ## 4 look 3 ## 5 place 3 ## 6 plai 3 ## 7 pleas 3 ## 8 sai 3 ## 9 trembl 3 ## 10 appear 2 ## # … with 662 more rows We can use the different data.frame and construct a list of words that would land in the same bucket after stemming.\nstem_buckets \u0026lt;- split(different$token, different$token_stem) %\u0026gt;% imap(~ c(.x, .y)) stem_buckets[21:25] ## $anxiou ## [1] \u0026quot;anxious\u0026quot; \u0026quot;anxiou\u0026quot; ## ## $anyth ## [1] \u0026quot;anything\u0026quot; \u0026quot;anyth\u0026quot; ## ## $apart ## [1] \u0026quot;apartment\u0026quot; \u0026quot;apart\u0026quot; ## ## $appear ## [1] \u0026quot;appearance\u0026quot; \u0026quot;appeared\u0026quot; \u0026quot;appear\u0026quot; ## ## $appl ## [1] \u0026quot;apples\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;appl\u0026quot; Here we see that “anxiou” and “anxious” would look the same after stemming, likewise will “apples”, “apple” and “appl”. The main point of this exercise is to see if the words in these groups of words end up in the topic when during topic modeling.\nstm_model \u0026lt;- fairy_tales_tokens %\u0026gt;% count(document, token) %\u0026gt;% cast_sparse(document, token, n) %\u0026gt;% stm(K = 3, verbose = FALSE) stm_model ## A topic model with 3 topics, 501 documents and a 1518 word dictionary. In this case, I fit the model to 3 topics because I knew that would be the right number since I picked the data. When doing this on your data you should run multiple models with a varying number of topics to find the best one. For more information please read Training, Evaluating, and Interpreting Topic Models by Julia Silge.\nNow that we have a stm model and a list of words, We can inspect the model object to check if multiple words are put in the same topic. Below is a function that will take a vector of characters and a stm model and return TRUE if all the words appear in the same topic and FALSE if they don’t.\nstm_match \u0026lt;- function(x, model) { topics \u0026lt;- tidy(model) %\u0026gt;% filter(term %in% x) %\u0026gt;% group_by(term) %\u0026gt;% top_n(1, beta) %\u0026gt;% ungroup() %\u0026gt;% select(topic) %\u0026gt;% n_distinct() topics == 1 } As an example, if we pass the words “believed” and “believ”\nstm_match(c(\u0026quot;believed\u0026quot;, \u0026quot;believ\u0026quot;), stm_model) ## [1] TRUE We see that they did end up in the same bucket. If we instead pass in “dog” and “happy” they land in different topics.\nstm_match(c(\u0026quot;dog\u0026quot;, \u0026quot;happy\u0026quot;), stm_model) ## [1] FALSE All of this is not perfect, there is still some uncertainty but it is a good first step to evaluate if stemming is appropriate for your application.\ntested \u0026lt;- tibble(terms = stem_buckets, stem = names(stem_buckets)) %\u0026gt;% mutate(match = map_lgl(terms, stm_match, stm_model)) tested ## # A tibble: 672 x 3 ## terms stem match ## \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;lgl\u0026gt; ## 1 \u0026lt;chr [2]\u0026gt; a FALSE ## 2 \u0026lt;chr [2]\u0026gt; abl TRUE ## 3 \u0026lt;chr [2]\u0026gt; abov TRUE ## 4 \u0026lt;chr [2]\u0026gt; accompani TRUE ## 5 \u0026lt;chr [2]\u0026gt; ach TRUE ## 6 \u0026lt;chr [2]\u0026gt; admir TRUE ## 7 \u0026lt;chr [2]\u0026gt; adorn TRUE ## 8 \u0026lt;chr [2]\u0026gt; ag TRUE ## 9 \u0026lt;chr [2]\u0026gt; ala TRUE ## 10 \u0026lt;chr [2]\u0026gt; alarm TRUE ## # … with 662 more rows First, we’ll look at the distribution of TRUEs and FALSEs.\ntested %\u0026gt;% ggplot(aes(match)) + geom_bar() So it looks like most of the word groups were put into the same topic during modeling. This is a good sign. Please note that this category includes a lot of false positives. This is happening because stm_match() also returns true for a case where one of the words appears in the model and all other words don’t. So for the case of “accompanied” and “accompani”, the word “accompanied” was present in one of the topics, but the word “accompani” was not present in the original data and hence did not appear in any of the topics. In this case, the TRUE value we are getting is saying that the data doesn’t provide enough evidence to indicate that stemming would be bad. By looking at a sample of TRUE cases we see that a lot of them are happening because the stemmed word isn’t being used, like the words “aliv”, “alon” and “alwai”. On the other side, we have that the words “allowed” and “allow” are both real words AND they appeared in the same topic.\ntested %\u0026gt;% filter(match) %\u0026gt;% slice(10:15) %\u0026gt;% pull(terms) ## [[1]] ## [1] \u0026quot;alighted\u0026quot; \u0026quot;alight\u0026quot; ## ## [[2]] ## [1] \u0026quot;alive\u0026quot; \u0026quot;aliv\u0026quot; ## ## [[3]] ## [1] \u0026quot;allowed\u0026quot; \u0026quot;allow\u0026quot; ## ## [[4]] ## [1] \u0026quot;alone\u0026quot; \u0026quot;alon\u0026quot; ## ## [[5]] ## [1] \u0026quot;already\u0026quot; \u0026quot;alreadi\u0026quot; ## ## [[6]] ## [1] \u0026quot;always\u0026quot; \u0026quot;alwai\u0026quot; Turning our head to the FALSE cases. These cases will not have any false positives as both words would have to appear in the original corpus for them to put into different topics. These cases are still not going to be perfect, but will again be an indication.\ntested %\u0026gt;% filter(!match) %\u0026gt;% pull(terms) %\u0026gt;% head() ## [[1]] ## [1] \u0026quot;as\u0026quot; \u0026quot;a\u0026quot; ## ## [[2]] ## [1] \u0026quot;appearance\u0026quot; \u0026quot;appeared\u0026quot; \u0026quot;appear\u0026quot; ## ## [[3]] ## [1] \u0026quot;backs\u0026quot; \u0026quot;back\u0026quot; ## ## [[4]] ## [1] \u0026quot;beginning\u0026quot; \u0026quot;begin\u0026quot; ## ## [[5]] ## [1] \u0026quot;beside\u0026quot; \u0026quot;besides\u0026quot; \u0026quot;besid\u0026quot; ## ## [[6]] ## [1] \u0026quot;birds\u0026quot; \u0026quot;bird\u0026quot; This is the list I would advise you to look over carefully. Check to make sure that you are okay with the number and count of misgroupings you would get by applying stemming.\n current session info  \n ─ Session info ─────────────────────────────────────────────────────────────── setting value version R version 3.6.0 (2019-04-26) os macOS Mojave 10.14.6 system x86_64, darwin15.6.0 ui X11 language (EN) collate en_US.UTF-8 ctype en_US.UTF-8 tz America/Los_Angeles date 2020-04-21 ─ Packages ─────────────────────────────────────────────────────────────────── package * version date lib source assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.0) blogdown 0.18 2020-03-04 [1] CRAN (R 3.6.0) bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) broom 0.5.5 2020-02-29 [1] CRAN (R 3.6.0) cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) clipr 0.7.0 2019-07-23 [1] CRAN (R 3.6.0) codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.0) colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) data.table 1.12.8 2019-12-09 [1] CRAN (R 3.6.0) DBI 1.1.0 2019-12-15 [1] CRAN (R 3.6.0) dbplyr 1.4.2 2019-06-17 [1] CRAN (R 3.6.0) desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.0) details * 0.2.1 2020-01-12 [1] CRAN (R 3.6.0) digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) dplyr * 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) farver 2.0.3 2020-01-16 [1] CRAN (R 3.6.0) forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.0) fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.0) generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.0) glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.0) gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) haven 2.2.0 2019-11-08 [1] CRAN (R 3.6.0) hcandersenr * 0.2.0 2019-01-19 [1] CRAN (R 3.6.0) hms 0.5.3 2020-01-08 [1] CRAN (R 3.6.0) htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) janeaustenr 0.1.5 2017-06-10 [1] CRAN (R 3.6.0) jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.0) knitr * 1.28 2020-02-06 [1] CRAN (R 3.6.0) labeling 0.3 2014-08-23 [1] CRAN (R 3.6.0) lattice 0.20-41 2020-04-02 [1] CRAN (R 3.6.0) lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) lubridate 1.7.8 2020-04-06 [1] CRAN (R 3.6.0) magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) Matrix 1.2-18 2019-11-27 [1] CRAN (R 3.6.0) modelr 0.1.6 2020-02-22 [1] CRAN (R 3.6.0) munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) nlme 3.1-145 2020-03-04 [1] CRAN (R 3.6.0) pillar 1.4.3 2019-12-20 [1] CRAN (R 3.6.0) pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) plyr 1.8.6 2020-03-03 [1] CRAN (R 3.6.0) png 0.1-7 2013-12-03 [1] CRAN (R 3.6.0) purrr * 0.3.3 2019-10-18 [1] CRAN (R 3.6.0) R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.0) Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.0) readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) reshape2 1.4.4 2020-04-09 [1] CRAN (R 3.6.2) rlang 0.4.5 2020-03-01 [1] CRAN (R 3.6.0) rmarkdown 2.1 2020-01-20 [1] CRAN (R 3.6.0) rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.6.0) rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) rvest 0.3.5 2019-11-08 [1] CRAN (R 3.6.0) scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) SnowballC * 0.7.0 2020-04-01 [1] CRAN (R 3.6.2) stm * 1.3.5 2019-12-17 [1] CRAN (R 3.6.0) stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) tibble * 3.0.1 2020-04-20 [1] CRAN (R 3.6.2) tidyr * 1.0.2 2020-01-24 [1] CRAN (R 3.6.0) tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) tidytext * 0.2.3 2020-03-04 [1] CRAN (R 3.6.0) tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.0) tokenizers 0.2.1 2018-03-29 [1] CRAN (R 3.6.0) utf8 1.1.4 2018-05-24 [1] CRAN (R 3.6.0) vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) xml2 1.3.0 2020-04-01 [1] CRAN (R 3.6.2) yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library   \n","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"e871f5ab0105c00a0afedb4174402b83","permalink":"/2020/03/16/using-stm-to-investigate-if-stemming-is-appropriate/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/2020/03/16/using-stm-to-investigate-if-stemming-is-appropriate/","section":"post","summary":"Photo by Plush Design Studio on Unsplash\nIt is known that topic modeling does not benefit from stemming ref. I propose a workflow to investigate if stemming is appropriate as a method for data reduction.","tags":null,"title":"Using stm to Investigate if Stemming is Appropriate","type":"post"},{"authors":null,"categories":["color","ggplot2"],"content":" With the release of version 3.3.0 of ggplot2 came the ability to have more control over the aesthetic evaluation. This allows us to modify the colors of the mapped palettes with prismatic now easier than ever.\nPackages 📦 We load the essential packages to wrangle, collect data (we will use tweets), scrape websites and handle emojis.\nlibrary(ggplot2) library(prismatic)  Examples Suppose you have a simple bar chart and you have added colors to each bar.\nggplot(diamonds, aes(cut)) + geom_bar(aes(fill = cut)) Next, suppose you would like to add a border around each bar. Traditionally you could add a single color like black but it isn’t that satisfying as it doesn’t have any relation to the mapped colors.\nggplot(diamonds, aes(cut)) + geom_bar(aes(fill = cut), color = \u0026quot;black\u0026quot;) now that after_scale() is available for us we can base the color based on the mapped fill colors. Below I have used clr_darken() to create a border that is just slightly darker than the fill color.\nggplot(diamonds, aes(cut)) + geom_bar(aes(fill = cut, color = after_scale(clr_darken(fill, 0.3)))) this could also have been done in reverse by supplying the color and modifying the fill after. Notice how we are able to chain multiple color modifications together. Here we hare taking the color, then desaturating it followed by some lighting.\nggplot(diamonds, aes(cut)) + geom_bar(aes(color = cut, fill = after_scale(clr_lighten(clr_desaturate(color), space = \u0026quot;combined\u0026quot;)))) If you only need to specify one color directly you can use the stage() function.\nggplot(diamonds, aes(cut)) + geom_bar(aes(fill = stage(start = cut, after_scale = clr_lighten(fill, space = \u0026quot;combined\u0026quot;))))  current session info  \n ─ Session info ─────────────────────────────────────────────────────────────── setting value version R version 3.6.0 (2019-04-26) os macOS Mojave 10.14.6 system x86_64, darwin15.6.0 ui X11 language (EN) collate en_US.UTF-8 ctype en_US.UTF-8 tz America/Los_Angeles date 2020-04-20 ─ Packages ─────────────────────────────────────────────────────────────────── package * version date lib source assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.0) blogdown 0.18 2020-03-04 [1] CRAN (R 3.6.0) bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) clipr 0.7.0 2019-07-23 [1] CRAN (R 3.6.0) codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.0) colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.0) details * 0.2.1 2020-01-12 [1] CRAN (R 3.6.0) digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) dplyr 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) emo 0.0.0.9000 2019-12-18 [1] Github (hadley/emo@3f03b11) evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) farver 2.0.3 2020-01-16 [1] CRAN (R 3.6.0) generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.0) glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.0) gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) knitr * 1.28 2020-02-06 [1] CRAN (R 3.6.0) labeling 0.3 2014-08-23 [1] CRAN (R 3.6.0) lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) lubridate 1.7.8 2020-04-06 [1] CRAN (R 3.6.0) magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) pillar 1.4.3 2019-12-20 [1] CRAN (R 3.6.0) pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) png 0.1-7 2013-12-03 [1] CRAN (R 3.6.0) prismatic * 0.2.0.9000 2020-03-15 [1] local purrr 0.3.3 2019-10-18 [1] CRAN (R 3.6.0) R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.0) Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.0) rlang 0.4.5 2020-03-01 [1] CRAN (R 3.6.0) rmarkdown 2.1 2020-01-20 [1] CRAN (R 3.6.0) rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.6.0) rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) stringr 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) tibble 3.0.0 2020-03-30 [1] CRAN (R 3.6.2) tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) viridisLite 0.3.0 2018-02-01 [1] CRAN (R 3.6.0) withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) xml2 1.3.0 2020-04-01 [1] CRAN (R 3.6.2) yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library   \n ","date":1582588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582588800,"objectID":"603a183d0adffb8213f857193f9852ee","permalink":"/2020/02/25/use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/","publishdate":"2020-02-25T00:00:00Z","relpermalink":"/2020/02/25/use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/","section":"post","summary":"With the release of version 3.3.0 of ggplot2 came the ability to have more control over the aesthetic evaluation. This allows us to modify the colors of the mapped palettes with prismatic now easier than ever.","tags":null,"title":"Use prismatic with after_scale() for finer control of colors in ggplot2","type":"post"},{"authors":null,"categories":[],"content":" With the new Github Actions comes many possibilities. Some new and some old. One of the benefits is that you don’t have to use third-party applications to do continuous integration.\nThis post will show you how you can set up a bookdown site with Netlify using Github Actions. This was previously and still is possible to do with Travis-CI.\nThis post wouldn’t have been possible without Jim Hester’s work on Github Actions.\nIf you are transferring a book from Travis-CI build look at the notes at the end of this post.\nCreate Repository First, you need to create a bookdown repository. For this, I suggest you follow the Getting Started chapter from the Bookdown Book and download the GitHub repository https://github.com/rstudio/bookdown-demo as a Zip file, then unzip it locally. I recommend that you change the name of the .Rproj file so isn’t the default value.\nThe next step isn’t necessary but is still highly recommended. Go fill in the information in the DESCRIPTION file. Most importantly the Package and Title fields. The Package field will be used as the name of the repository and the Title field will be the description of the repository once it hits Github.\n Connect to Github Now we want to connect our repository to Github. For this, I will use the usethis package which is wonderful for things like this. If you haven’t used usethis before please go do the usethis setup before moving forward.\nSimply add Git to the repository by running usethis::use_git() and connect it to Github with usethis::use_github(). This should open up a webpage with the newly linked repository.\n Create Netlify account If you haven’t already got a Netlify account, go to netlify.com/ to create one for free. I have it set up with Github for easier interaction.\n Create Netlify site Once you have logged into Netlify go to your team page and to create a “New site from Git”\nSelect Github for Continuous Deployment\nNow we need to select the GitHub repository. Depending on how many repositories you have you can find it in the list or search for it with the search bar. Once you have found it click the little arrow to the right of it.\nDon’t touch any of the settings.\nAnd voila! Here is your new site, it is currently empty. Now click on the “Site settings” button\ncopy the API ID and save it, you will need it in a little bit. If you lose it you can always come back here and copy it again.\nYou might have noticed that the website is completely random. If you click on the “Change site name” button you can set a new prefix name.\nScroll down to get the Status Badge, you can copy this too and put it in the top of your README.md file if you want.\n Get a Netlify personal access token Scroll all the way up and click on your icon in the top right corner. Then go to “User settings”\ngo to “Applications”\nAnd click on “New access token” to create a personal access token\nThe description of your token isn’t important but try to make it related to your book so you remember. Click “Generate token” when you are done.\nHere is your authentication token. Copy it and don’t lose it! Once you leave this site you can get it back.\n Store Your Secrets Now that you have the API ID and personal access token go back to your Github repository and go to “Settings”\ngo to “Secrets”\nClick on “Add a new secret”\nYou need to do this twice.\n one named “NETLIFY_AUTH_TOKEN” where you put the personal access token as the value and, one named “NETLIFY_SITE_ID” where you put the API ID as the value.   Create Github workflow Now add the GitHub workflow file.\nFor this you will need version 1.5.1.9000 or higher of usethis for it to work. You can get the newest version of usethis from github with\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;r-lib/usethis\u0026quot;) then you run use_github_action(\"bookdown.yaml\") which will create the .yaml file in the right directory for you.\n Run renv::snapshot Install the renv package and run renv::snapshot(). This will ensure the package versions remain consistent across builds.\nOnce you need more packages, add them to the description like you normally would with an R package.\n Push changes And that is everything you need to do, just commit the workflow file and the renv files you created and the website should build for you.\nMy example can be found here with the repository.\n Notes the line\nnetlify deploy --prod --dir _book in the workflow file it the one that deploys the built book to Netlify. It defaults to the _book folder. In the _bookdown.yml file you can change the output folder. So if you have set it to output_dir: \"docs\" then you need to change the deploy option to\nnetlify deploy --prod --dir docs  ","date":1579478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579478400,"objectID":"aa73df7e61ca0086e9823ca666ba246d","permalink":"/2020/01/20/deploy-your-bookdown-project-to-netlify-with-github-actions/","publishdate":"2020-01-20T00:00:00Z","relpermalink":"/2020/01/20/deploy-your-bookdown-project-to-netlify-with-github-actions/","section":"post","summary":"With the new Github Actions comes many possibilities. Some new and some old. One of the benefits is that you don’t have to use third-party applications to do continuous integration.","tags":null,"title":"Deploy your bookdown project to Netlify with Github Actions","type":"post"},{"authors":null,"categories":["ggplot2"],"content":" I have been trying to use Emojis for a long time. It was actually part of my very first post on this blog. Others have made progress such as with emojifont, but it is not using the classical Apple Color Emoji font which is the most commonly recognized. I made a breakthrough when I was writing the packagecalander entry on ggtext. While the method is the best I have found it does have some cons.\nPros:\n Actually works Doesn’t require the use of SVG Previews nicely  Cons:\n Relies on experimental package ggtext Needs web scraping Required access to the internet to render Size can’t be adjusted using the size aesthetic  All in all, it is a fair trade for my needs.\nPackages 📦 We load the essential packages to wrangle, collect data (we will use tweets), scrape websites and handle emojis.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(rtweet) library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 # devtools::install_github(\u0026quot;clauswilke/ggtext\u0026quot;) library(ggtext) library(emo)  Getting the tweets 🐦 For a simple dataset where we find emojis I’m going to get some tweets with the word “happy”.\nhappy \u0026lt;- search_tweets(\u0026quot;happy\u0026quot;, include_rts = FALSE, n = 1000) we can use the ji_extract_all() function from the emo package. This will give us a list of emojis so we can use the unnest() function to get back to a tidy format. I’m going to do a simple count() of the emojis for the following visualizations.\nhappy_emojis \u0026lt;- happy %\u0026gt;% mutate(emoji = emo::ji_extract_all(text)) %\u0026gt;% unnest(cols = c(emoji)) %\u0026gt;% count(emoji, sort = TRUE) Next is where the magic happens. We don’t have a way to displays emojis in ggplot2, but we can use ggtext to embed images into the text using HTML. Now we just need to get an image of each emoji. The following function will accept an emoji as a string and return the URL to a .png of that emoji.\nemoji_to_link \u0026lt;- function(x) { paste0(\u0026quot;https://emojipedia.org/emoji/\u0026quot;,x) %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;tr td a\u0026quot;) %\u0026gt;% .[1] %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) %\u0026gt;% paste0(\u0026quot;https://emojipedia.org/\u0026quot;, .) %\u0026gt;% read_html() %\u0026gt;% html_node(\u0026#39;div[class=\u0026quot;vendor-image\u0026quot;] img\u0026#39;) %\u0026gt;% html_attr(\u0026quot;src\u0026quot;) } Then this function will take that URL and construct the necessary HTML code to show the emoji PNGs.\nlink_to_img \u0026lt;- function(x, size = 25) { paste0(\u0026quot;\u0026lt;img src=\u0026#39;\u0026quot;, x, \u0026quot;\u0026#39; width=\u0026#39;\u0026quot;, size, \u0026quot;\u0026#39;/\u0026gt;\u0026quot;) } To be courteous we are only going to scrape the emojis we are actually going to use. So we will slice() the 10 most frequent emojis. We will also be adding a 5 second delay using slowly() and rate_delay() from purrr.\ntop_happy \u0026lt;- happy_emojis %\u0026gt;% slice(1:10) %\u0026gt;% mutate(url = map_chr(emoji, slowly(~emoji_to_link(.x), rate_delay(1))), label = link_to_img(url))  emoji-scatter plot 📈 Now we can use the geom_richtext() function from ggtext to create a emoji scatter chart.\ntop_happy %\u0026gt;% ggplot(aes(emoji, n, label = label)) + geom_richtext(aes(y = n), fill = NA, label.color = NA, # remove background and outline label.padding = grid::unit(rep(0, 4), \u0026quot;pt\u0026quot;) # remove padding ) + theme_minimal() This is a little off, so lets other these by counts and put them over a bar chart. I’m also going to the x-axis ticks and text.\noffset \u0026lt;- max(top_happy$n) / 20 top_happy %\u0026gt;% ggplot(aes(fct_reorder(emoji, n, .desc = TRUE), n, label = label)) + geom_col() + geom_richtext(aes(y = n + offset), fill = NA, label.color = NA, label.padding = grid::unit(rep(0, 4), \u0026quot;pt\u0026quot;) ) + theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) + labs(x = NULL) + theme_minimal()  Emojis in labels and text 📊 We are not only limited to using emojis in the geoms. We can set the text element using emojis to element_markdown(). Below we have the same bar chart as above but with the emoji as labels below instead of on top.\ntop_happy %\u0026gt;% ggplot(aes(fct_reorder(label, n, .desc = TRUE), n)) + geom_col() + theme_minimal() + theme(axis.text.x = element_markdown()) + labs(x = NULL)  Adding a splash of color 🌈 We can employ a little more scraping and color calculations to had colors to the bars according to the colors of the emoji. The following function takes a URL to a .png file and returns the most common color that isn’t pure black or pure white.\nmean_emoji_color \u0026lt;- function(x) { data \u0026lt;- png::readPNG(RCurl::getURLContent(x)) color_freq \u0026lt;- names(sort(table(rgb(data[,,1], data[,,2], data[,,3])), decreasing = TRUE)) setdiff(color_freq, c(\u0026quot;#FFFFFF\u0026quot;, \u0026quot;#000000\u0026quot;))[1] } We apply this to all the emoji URLs and color the bars accordingly.\nplot_data \u0026lt;- top_happy %\u0026gt;% mutate(color = map_chr(url, slowly(~mean_emoji_color(.x), rate_delay(1)))) plot_data %\u0026gt;% ggplot(aes(fct_reorder(label, n, .desc = TRUE), color = color, fill = unclass(prismatic::clr_lighten(color, 0.4)), n)) + geom_col() + scale_fill_identity() + scale_color_identity() + theme_minimal() + theme(axis.text.x = element_markdown()) + labs(x = NULL, y = \u0026quot;Count\u0026quot;, title = \u0026quot;Emojis used in (small sample) of \u0026#39;happy\u0026#39; tweets\u0026quot;, subtitle = \u0026quot;Displayed in ggplot2!!!\u0026quot;, caption = \u0026quot;@Emil_Hvitfeldt\u0026quot;)  Final note 🗒 If you want to use emojis in the text you need to call theme_*() before theme() such that element_markdown() isn’t being overwritten.\n current session info  \n ─ Session info ─────────────────────────────────────────────────────────────── setting value version R version 3.6.0 (2019-04-26) os macOS Mojave 10.14.6 system x86_64, darwin15.6.0 ui X11 language (EN) collate en_US.UTF-8 ctype en_US.UTF-8 tz America/Los_Angeles date 2020-04-21 ─ Packages ─────────────────────────────────────────────────────────────────── package * version date lib source askpass 1.1 2019-01-13 [1] CRAN (R 3.6.0) assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.0) bitops 1.0-6 2013-08-17 [1] CRAN (R 3.6.0) blogdown 0.18 2020-03-04 [1] CRAN (R 3.6.0) bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) broom 0.5.5 2020-02-29 [1] CRAN (R 3.6.0) cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) clipr 0.7.0 2019-07-23 [1] CRAN (R 3.6.0) codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.0) colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) curl 4.3 2019-12-02 [1] CRAN (R 3.6.0) DBI 1.1.0 2019-12-15 [1] CRAN (R 3.6.0) dbplyr 1.4.2 2019-06-17 [1] CRAN (R 3.6.0) desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.0) details * 0.2.1 2020-01-12 [1] CRAN (R 3.6.0) digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) dplyr * 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) emo * 0.0.0.9000 2019-12-18 [1] Github (hadley/emo@3f03b11) evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) farver 2.0.3 2020-01-16 [1] CRAN (R 3.6.0) forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.0) fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.0) generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.0) ggtext * 0.1.0 2019-12-13 [1] Github (clauswilke/ggtext@cc8ea0c) glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.0) gridtext 0.1.1 2020-02-24 [1] CRAN (R 3.6.0) gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) haven 2.2.0 2019-11-08 [1] CRAN (R 3.6.0) hms 0.5.3 2020-01-08 [1] CRAN (R 3.6.0) htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.0) knitr * 1.28 2020-02-06 [1] CRAN (R 3.6.0) labeling 0.3 2014-08-23 [1] CRAN (R 3.6.0) lattice 0.20-41 2020-04-02 [1] CRAN (R 3.6.0) lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) lubridate 1.7.8 2020-04-06 [1] CRAN (R 3.6.0) magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) markdown 1.1 2019-08-07 [1] CRAN (R 3.6.0) modelr 0.1.6 2020-02-22 [1] CRAN (R 3.6.0) munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) nlme 3.1-145 2020-03-04 [1] CRAN (R 3.6.0) openssl 1.4.1 2019-07-18 [1] CRAN (R 3.6.0) pillar 1.4.3 2019-12-20 [1] CRAN (R 3.6.0) pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) png 0.1-7 2013-12-03 [1] CRAN (R 3.6.0) prettyunits 1.1.1 2020-01-24 [1] CRAN (R 3.6.0) prismatic 0.2.0.9000 2020-03-15 [1] local progress 1.2.2 2019-05-16 [1] CRAN (R 3.6.0) purrr * 0.3.3 2019-10-18 [1] CRAN (R 3.6.0) R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.0) Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.0) RCurl 1.98-1.1 2020-01-19 [1] CRAN (R 3.6.0) readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) rlang 0.4.5 2020-03-01 [1] CRAN (R 3.6.0) rmarkdown 2.1 2020-01-20 [1] CRAN (R 3.6.0) rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.6.0) rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) rtweet * 0.7.0 2020-01-08 [1] CRAN (R 3.6.0) rvest * 0.3.5 2019-11-08 [1] CRAN (R 3.6.0) scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) selectr 0.4-2 2019-11-20 [1] CRAN (R 3.6.0) sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) tibble * 3.0.1 2020-04-20 [1] CRAN (R 3.6.2) tidyr * 1.0.2 2020-01-24 [1] CRAN (R 3.6.0) tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.0) vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) xml2 * 1.3.0 2020-04-01 [1] CRAN (R 3.6.2) yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library   \n ","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"6ec42d1a25a8599495a1b63a560621e6","permalink":"/2020/01/02/real-emojis-in-ggplot2/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/2020/01/02/real-emojis-in-ggplot2/","section":"post","summary":"I have been trying to use Emojis for a long time. It was actually part of my very first post on this blog. Others have made progress such as with emojifont, but it is not using the classical Apple Color Emoji font which is the most commonly recognized.","tags":null,"title":"Real Emojis in ggplot2","type":"post"},{"authors":null,"categories":["color"],"content":" I’m over-the-moon excited to announce the release of version 1.0.0 of paletteer. This version comes with breaking changes and major quality of life improvements. I will unironically name this the “first useable version” for reasons that will be obvious later in this post.\nBreaking Changes 💥 There has been a significant change in syntax for this version. For versions \u0026lt;= 0.2.1 the way to specify a palette was done using the arguments package and palette. Both could be taken as both string or unquoted strings.\n# versions \u0026lt;= 0.2.1 paletteer_c(\u0026quot;gameofthrones\u0026quot;, \u0026quot;baratheon\u0026quot;, 10) paletteer_d(nord, halifax_harbor) While convinient and cool to use NSE, tt was not very useful and I had several people complaining. I realized that using NSE wasn’t a good fit at all for this package. This means that from version 1.0.0 and moving forward only strings will be used to specify palettes.\nSecondly, I have eliminated the package argument and from now on all specification is done on the form package::palette\n# versions \u0026gt;= 1.0.0 paletteer_c(\u0026quot;gameofthrones::baratheon\u0026quot;, 10) paletteer_d(\u0026quot;nord::halifax_harbor\u0026quot;) The above change is the most likely to break your earlier code.\n Autocomplete 🎉 The biggest downside to the original version of paletteer and later version was the lack of discoverability. Unless you knew the palette you wanted and the EXACT spelling you couldn’t really use paletteer. Sure you could browse palettes_c_names and palettes_d_names like some caveman, but to be honest the package felt more like a novelty project than a useful tool.\nAll of this changes with version 1.0.0 🎉! Simply starting by typing paletteer_d() or any of the other related functions and simply hit tab. This will prompt all the names of available palettes which you then can search through using fuzzy search.\nThis change is the single biggest improvement to this package.\n Discoverability ✅ No more missspellings ✅ Total awesomeness ✅  And yes, it also work with the scale_*_paletteer() functions 🙌\n Prismatic integration 💎 You can see from the first gif that the output is a little more colorful then what you are used to. This all comes from the prismatic package I released earlier this year. The prismatic colors objects that are returned from all paletteer functions will be printed with colorful backgrounds provided that the crayon package is available, otherwise, it will just print normally. This is great for when you want to take a quick look at the colors you are about to use. Please note that the background can only take 256 different colors. Some palettes will fit nicely inside these 256 values and will display nicely (viridis::magma) below, while other palettes with a lot of value will show weird jumps in colors (gameofthrones::greyjoy)\nIf you want more accurate color depictions you can simply plot() the output to see the real colors\nplot(paletteer_c(\u0026quot;viridis::magma\u0026quot;, 10)) plot(paletteer_c(\u0026quot;gameofthrones::greyjoy\u0026quot;, 100))  More color palettes 🌈 It wouldn’t be a paletteer release without more palettes. And this release is no different! This update brings us 654 new palettes!!! from 19 different packages bringing out total up to 1759. I did a little live-tweeting while implementing these packages so you can take a look at the newly included palettes here:\nI'll be adding a whole bunch of new palettes to {paletteer} tonight! 🌈\nRead this thread if you want to see the new colorful goodies coming your way!\n❤️💙💚🧡💛💜#rstats pic.twitter.com/c0qK27nc4N — Emil Hvitfeldt (@Emil_Hvitfeldt) December 8, 2019   That is all I have for you this time around if you create or find more palette packages please go over and file an issue so they can be included as well. Thank you!\n ","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"4f3f99eda06f0e589e56a68f4de235cd","permalink":"/2019/12/18/paletteer-version-1.0.0/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/2019/12/18/paletteer-version-1.0.0/","section":"post","summary":"I’m over-the-moon excited to announce the release of version 1.0.0 of paletteer. This version comes with breaking changes and major quality of life improvements. I will unironically name this the “first useable version” for reasons that will be obvious later in this post.","tags":null,"title":"Paletteer version 1.0.0","type":"post"},{"authors":null,"categories":[],"content":" We all know the saying\n When you’ve written the same code 3 times, write a function\n However, I would like to expend that to\n When you’re written the same test 3 times, write a test function\n During my lasting packages such as prismatic, I found myself copy-pasting tests around whenever I needed to test a new function. I realized that the refactoring practices I try to apply in my general code writing, wasn’t carried over to the way I was writing my tests. I would frequency copy-paste hundreds of lines of tests only to replace the function name. In this post will I go over a refactoring scenario I am working on at the moment.\nThe copy-pasted test The prismatic package includes almost a dozen different functions that work mostly the same way. They all take the same type of arguments, return the returns in the same fashion and so on. This leads me to have a great overlap between what tests I’m performing for each function.\nTaking a look at the following code chuck we see a test that makes sure that the function clr_alpha() complain when given the wrong type of the first argument.\ntest_that(\u0026quot;complain when `col` type is wrong.\u0026quot;, { expect_error(clr_alpha(\u0026quot;not a color\u0026quot;)) expect_error(clr_alpha(list(pal = \u0026quot;#000000\u0026quot;))) expect_error(clr_alpha(character())) }) When looking at the same test for clr_mix() we see that it is a carbon copy except for the function name.\ntest_that(\u0026quot;it complains when col type is wrong.\u0026quot;, { expect_error(clr_mix(\u0026quot;not a color\u0026quot;)) expect_error(clr_mix(list(pal = \u0026quot;#000000\u0026quot;))) expect_error(clr_mix(character())) }) I’m going to propose 2 different styles of refactoring, with the main difference being how RStudio returns the error when tests are not met.\nFix #1 - Plays well with error messages The first solution is to wrap the inside of your test into a function. The above test would create the refactored testing-function\ntest_wrong_input \u0026lt;- function(clr_) { expect_error(clr_(\u0026quot;not a color\u0026quot;)) expect_error(clr_(list(pal = \u0026quot;#000000\u0026quot;))) expect_error(clr_(character())) } and the test would be changed to\ntest_that(\u0026quot;it complains when col type is wrong.\u0026quot;, { test_wrong_input(clr_alpha) }) this change will perform the tests, and adding tests for the new function would only need 1 change in the test instead of 3.\ntest_that(\u0026quot;it complains when col type is wrong.\u0026quot;, { test_wrong_input(clr_mix) }) More importantly, let’s imagine that we want to extend the types of wrong inputs we what to screen. Now we simply just need to add it once and it propagates to all the functions.\nThe main benefit of this refactoring style is that when an error appears, It will denote the line where the test broke.\n Fix #2 - Less typing, worse error message The second solution is to wrap the entire testing statement inside a function. For this example, the function would look like this\ntest_wrong_input \u0026lt;- function(clr_) { test_that(\u0026quot;it complains when col type is wrong.\u0026quot;, { expect_error(clr_(\u0026quot;not a color\u0026quot;)) expect_error(clr_(list(pal = \u0026quot;#000000\u0026quot;))) expect_error(clr_(character())) }) } and the testing would look like\ntest_wrong_input(clr_mix) This reduces the number of lines needed for each test from 3 down to 1. However, it comes with a downside. When an error appears testthat will give the location of the definition of the test-function, not the location from where it was called.\nWe can still see that the error happens inside the “alpha” Context, but it is slightly harder to track down.\n Fix #2.1 - ugly hack to give me the location The second solution can be made slightly better by making the description of the test more informative.\ntest_wrong_input \u0026lt;- function(clr_) { test_that(paste0(\u0026quot;test_wrong_input: \u0026quot;, deparse(substitute(clr_)), \u0026quot;() complains when col type is wrong.\u0026quot;), { expect_error(clr_(\u0026quot;not a color\u0026quot;)) expect_error(clr_(list(pal = \u0026quot;#000000\u0026quot;))) expect_error(clr_(\u0026quot;pink\u0026quot;)) }) } It takes more work upfront when writing the test functions. But it gives a compromise between the brevity of test files and the clarity of the debugging page.\nThanks for reading along! I hope you found it useful!\n  ","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"36885b6193765e325dbed8a0e1ff53d3","permalink":"/2019/11/25/refactoring-tests/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/2019/11/25/refactoring-tests/","section":"post","summary":"We all know the saying\n When you’ve written the same code 3 times, write a function\n However, I would like to expend that to\n When you’re written the same test 3 times, write a test function","tags":null,"title":"Refactoring Tests","type":"post"},{"authors":null,"categories":["color","prismatic"],"content":" I’m happy to announce my newest package prismatic which facilitates simple manipulations of colors. I had been working on this package online and offline for some time, but the promise of easy manipulation of mapped data in ggplot2 forced me to get some work done to get this package out before ggplot2 version 3.3.0. (as of time of writing.)\nThis post will go over some of the finer details with lots of pretty pictures!\nLoading Packages The prismatic package is fairly low dependency with only 1 import being farver for lightning fast conversion between color spaces. I have also loaded the colorspace package, from which some of the following functions have been inspired. I will use colorspace to enable plotting of multiple color palettes side by side, but I will not showcase the code each time. Go to the end of the post for example code for comparison plots.\nlibrary(prismatic) library(colorspace) # for plotting functions library(magrittr) # for the glorious pipe  Let me see the colors!! If you have seen my work, you will properly know that I like colors alot! But being also to quickly inspect some colors have always been a little too much work. Now all you have to do it pass your colors to color() (or colour() for our friends across the pond) to get a  object which has a nice plot() method\nrainbow(10) %\u0026gt;% color() %\u0026gt;% plot() hcl.colors(25) %\u0026gt;% color() %\u0026gt;% plot() scico::scico(256, palette = \u0026quot;buda\u0026quot;) %\u0026gt;% color() %\u0026gt;% plot() Which I would like to think is one of the main features of the package. If you happens to have crayon available you will see a approximation of the colors with a filled in background (this limited to 256 colors so you milage might very, when in doubt use plot())\nThis is the extent of what the color object can do.\n Manipulations The second star of the package is the collection of functions to manipulate the colors. All these functions have a couple of things in common.\n They all start with clr_ for easy auto completion in your favorite IDE. They all take a vector of colors as the first argument and results a colors object of the same length.  these two facts make the function super pipe friendly.\nSaturation The two functions clr_saturate() and clr_desaturate() both modifies the saturation of a color. It takes a single additional argument to specifying the degree of which the (de)saturation should occur. These values should be between 0(nothing happens) and 1(full on power!).\nnotice how you don’t have to call color() on the output of clr_desaturate() as it already returns a colors object.\nhcl.colors(10, \u0026quot;plasma\u0026quot;) %\u0026gt;% clr_desaturate(0.8) %\u0026gt;% plot() Example done with Mango palette from LaCroixColoR package.\n Seeing life in black and white Turns out there is a lot of different ways to turn colors into grayscale. Prismatic has implemented a handful of these. Notice how the viridis palette is still working once you have it transformed to black and white.\nhcl.colors(10) %\u0026gt;% clr_greyscale() %\u0026gt;% plot() Be advised that not all of these methods are meant to be perceptually uniform.\n Negate Negation of a color is pretty simple. it will just pick the opposite color in RGB space.\nterrain.colors(10) %\u0026gt;% clr_negate() %\u0026gt;% plot()  Mixing Mixing is just adding colors together. Thus my mixing a color with red would make a color more red.\nrainbow(10) %\u0026gt;% clr_mix(\u0026quot;red\u0026quot;) %\u0026gt;% plot()  Rotation the clr_rotate() function will take a color and rotate its hue, which is a way walk around the rainbow.\nterrain.colors(10) %\u0026gt;% clr_rotate(90) %\u0026gt;% plot()  Color blindness also includes 3 functions (clr_protan(), clr_deutan() and clr_tritan()) to simulate colorblindness. These functions has a severity argument to control the strength of the deficiency.\nhcl.colors(10) %\u0026gt;% clr_deutan() %\u0026gt;% plot()  Light and darkness Lastly we have functions to simulate lightness and darkness. This is surprisingly hard to do and no one way works great all the time. Please refer to the excellent colorspace paper for more information. These functions (clr_lighten() and clr_darken()) also include a space argument to determine the space in which to perform the transformation. Please try each of these to find the optimal method for your use case.\nrainbow(10) %\u0026gt;% clr_darken() %\u0026gt;% plot()   Comparison Code swatchplot( list( saturate = rbind(\u0026quot;0\u0026quot; = clr_rotate(terrain.colors(10), 0), \u0026quot;60\u0026quot; = clr_rotate(terrain.colors(10), 60), \u0026quot;120\u0026quot; = clr_rotate(terrain.colors(10), 120), \u0026quot;180\u0026quot; = clr_rotate(terrain.colors(10), 180), \u0026quot;240\u0026quot; = clr_rotate(terrain.colors(10), 240), \u0026quot;300\u0026quot; = clr_rotate(terrain.colors(10), 300)), desaturate = rbind(\u0026quot;0\u0026quot; = clr_rotate(hcl.colors(10), 0), \u0026quot;60\u0026quot; = clr_rotate(hcl.colors(10), 60), \u0026quot;120\u0026quot; = clr_rotate(hcl.colors(10), 120), \u0026quot;180\u0026quot; = clr_rotate(hcl.colors(10), 180), \u0026quot;240\u0026quot; = clr_rotate(hcl.colors(10), 240), \u0026quot;300\u0026quot; = clr_rotate(hcl.colors(10), 300)) ), nrow = 7, line = 2.5 )  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"61032e8336ba25244f2139a0892c8372","permalink":"/2019/10/01/manipulating-colors-with-prismatic/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/2019/10/01/manipulating-colors-with-prismatic/","section":"post","summary":"I’m happy to announce my newest package prismatic which facilitates simple manipulations of colors. I had been working on this package online and offline for some time, but the promise of easy manipulation of mapped data in ggplot2 forced me to get some work done to get this package out before ggplot2 version 3.","tags":null,"title":"Manipulating colors with {prismatic}","type":"post"},{"authors":null,"categories":["tidymodels","textrecipes"],"content":" In this post we will revisit on of my earlier blogposts where I tried to use tidytext and glmnet to predict the authorship of the anonymous Federalist Papers. If you want more information regarding the data, please read the old post. In the post we will try to achieve the same goal, but use the tidymodels framework.\nLoading Packages library(tidymodels) # Modeling framework library(textrecipes) # extension to preprocessing engine to handle text library(stringr) # String modification library(gutenbergr) # Portal to download the Federalist Papers library(tokenizers) # Tokenization engine library(furrr) # to be able to fit the models in parallel  Fetching the Data The text is provided from the Gutenberg Project. A simple search reveals that the Federalist Papers have the id of 1404.\npapers \u0026lt;- gutenberg_download(1404) papers  shaping data This is the first we will deviate from the original post in that we will divide the text into paragraphs instead of sentences as we did in the last post. Hopefully this will strike a good balance between size of each observation and the number of observations.\nIn the following pipe we: - pull() out the text vector - paste together the strings with \\n to denote line-breaks - tokenize into paragraphs - put it in a tibble - create a variable no to denote which paper the paragraph is in - add author variable to denote author - remove preamble text\n# attribution numbers hamilton \u0026lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) madison \u0026lt;- c(10, 14, 18:20, 37:48) jay \u0026lt;- c(2:5, 64) unknown \u0026lt;- c(49:58, 62:63) papers_paragraphs \u0026lt;- papers %\u0026gt;% pull(text) %\u0026gt;% str_c(collapse = \u0026quot;\\n\u0026quot;) %\u0026gt;% tokenize_paragraphs() %\u0026gt;% unlist() %\u0026gt;% tibble(text = .) %\u0026gt;% mutate(no = cumsum(str_detect(text, regex(\u0026quot;FEDERALIST No\u0026quot;, ignore_case = TRUE)))) %\u0026gt;% mutate(author = case_when(no %in% hamilton ~ \u0026quot;hamilton\u0026quot;, no %in% madison ~ \u0026quot;madison\u0026quot;, no %in% jay ~ \u0026quot;jay\u0026quot;, no %in% unknown ~ \u0026quot;unknown\u0026quot;)) %\u0026gt;% filter(no \u0026gt; 0)  Class Balance There is quite a bit inbalence between the classes. For the remaining of the analysis will we exclude all the papers written by Jay, partly because it is a small class, but more importantly because he isn’t suspected to be the mystery author.\npapers_paragraphs %\u0026gt;% count(author) %\u0026gt;% ggplot(aes(author, n)) + geom_col() It is wroth remembering that we don’t have the true answer, much more like in real world problems.\n Splitting the Data Here we will use the rsample package to split the data into a testing, validation and training dataset. We will let the testing dataset be all the paragraphs where author == \"unknown\" and the training and validation datasets being the paragraphs written by Hamilton and Madison. intial_split() will insure that each dataset with have the same proportions with respect to the author column.\ndata_split \u0026lt;- papers_paragraphs %\u0026gt;% filter(author %in% c(\u0026quot;hamilton\u0026quot;, \u0026quot;madison\u0026quot;)) %\u0026gt;% initial_split(strata = author) training_data \u0026lt;- training(data_split) validation_data \u0026lt;- testing(data_split) testing_data \u0026lt;- papers_paragraphs %\u0026gt;% filter(author == \u0026quot;unknown\u0026quot;)  specifying data preprocessing We will go with a rather simple preprocessing. start by specifying a recipe where author is to be predicted, and we only want to use the text data. Here we make sure to use the training dataset. We then\n tokenize according to (n-grams)[https://www.tidytextmining.com/ngrams.html] only keep the 250 most frequent tokens calculate the (term frequency–inverse document frequency)[https://en.wikipedia.org/wiki/Tf%E2%80%93idf] up-sample the observation to achieve class balance  and finally prep the recipe.\nrec \u0026lt;- recipe(author ~ text, data = training_data) %\u0026gt;% step_tokenize(text, token = \u0026quot;ngrams\u0026quot;, options = list(n = 3)) %\u0026gt;% step_tokenfilter(text, max_tokens = 250) %\u0026gt;% step_tfidf(text) %\u0026gt;% step_upsample(author) %\u0026gt;% prep()  Apply Preprocessing Now we apply the prepped recipe to get back the processed datasets. Note that I have used shorter names for processed datasets (train_data vs training_data).\ntrain_data \u0026lt;- juice(rec) val_data \u0026lt;- bake(rec, new_data = validation_data) test_data \u0026lt;- bake(rec, new_data = testing_data)  Fitting the Models This time I’m going to try to run some (random forests)[https://en.wikipedia.org/wiki/Random_forest]. And that would be fairly easy to use. First we specify the the model type (rand_forest) the type (classification) and the engine (randomForest). Next we fit the model to the training dataset, predict it on the validation datasets, add the true value and calculate the accuracy\nrand_forest(\u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;randomForest\u0026quot;) %\u0026gt;% fit(author ~ ., data = train_data) %\u0026gt;% predict(new_data = val_data) %\u0026gt;% mutate(truth = val_data$author) %\u0026gt;% accuracy(truth, .pred_class) However we want to try some different hyper-parameter values to make sure we are using the best we can. The dials allows us to do hyper-parameter searching in a fairly easy way. First we will create a parameter_grid, where we will vary the number of trees in our forest (trees()) and the number of predictors to be randomly sampled. We give it some reasonable ranges, and say that we want 5 levels for each parameter, resulting in 5 * 5 = 25 parameter pairs.\nparam_grid \u0026lt;- grid_regular(range_set(trees(), c(50, 250)), range_set(mtry(), c(1, 15)), levels = 5) Next we create a model specification where we use varying() to denote that these parameters are to be varying. Then we merge() the model specification into the parameter grid such that we have a tibble of model specifications\nrf_spec \u0026lt;- rand_forest(\u0026quot;classification\u0026quot;, mtry = varying(), trees = varying()) %\u0026gt;% set_engine(\u0026quot;randomForest\u0026quot;) param_grid \u0026lt;- param_grid %\u0026gt;% mutate(specs = merge(., rf_spec)) param_grid Next we want to iterate through the model specification. We will here create a function that will take a model specification, fit it to the training data, predict according to the validation data, calculate the accuracy and return it as a single number. Create this function makes it so we can use map() over all the model specifications.\nfit_one_spec \u0026lt;- function(model) { model %\u0026gt;% fit(author ~ ., data = train_data) %\u0026gt;% predict(new_data = val_data) %\u0026gt;% mutate(truth = val_data$author) %\u0026gt;% accuracy(truth, .pred_class) %\u0026gt;% pull(.estimate) } While this is a fairly small dataset, I’ll showcase how we can parallize the calculations. Since we have a framework where are we map()’ing over the specification it is a obvious case for the furrr package. (if you don’t want or isn’t able to to run your models on multiple cores, simply delete plan(multicore) and turn future_map_dbl() to map_dbl()).\nplan(multicore) final \u0026lt;- param_grid %\u0026gt;% mutate(accuracy = future_map_dbl(specs, fit_one_spec)) Now we can try to visualize the optimal hyper-parameters\nfinal %\u0026gt;% mutate_at(vars(trees:mtry), factor) %\u0026gt;% ggplot(aes(mtry, trees, fill = accuracy)) + geom_tile() + scale_fill_viridis_c() and we clearly see that only having 1 predictor to split with it sub-optimal, but otherwise having a low number of predictors are to be preferred. We can use arrange() to look at the top parameter pairs\narrange(final, desc(accuracy)) %\u0026gt;% slice(1:5) and we pick trees == 100 and mtry == 4 as our hyper-parameters. And we use these to fit our final model\nfinal_model \u0026lt;- rf_spec %\u0026gt;% update(trees = 100, mtry = 4) %\u0026gt;% fit(author ~ ., data = train_data)  Predicting the unknown papers Lastly we predict on the unknown papers.\nfinal_predict \u0026lt;- testing_data %\u0026gt;% bind_cols(predict(final_model, new_data = test_data))  We can’t calculate an accuracy or any other metric, as we don’t know the true value. However we can see how the the different paragraphs have been classified within each paper.\nfinal_predict %\u0026gt;% count(no, .pred_class) %\u0026gt;% mutate(no = factor(no)) %\u0026gt;% group_by(no) %\u0026gt;% mutate(highest = n == max(n)) Now we can visualize the results, and it looks like from this limited analysis that Hamilton is the author of mysterious papers.\nfinal_predict %\u0026gt;% count(no, .pred_class) %\u0026gt;% mutate(no = factor(no), .pred_class = factor(.pred_class, levels = c(\u0026quot;hamilton\u0026quot;, \u0026quot;madison\u0026quot;), labels = c(\u0026quot;Hamilton\u0026quot;, \u0026quot;Madison\u0026quot;))) %\u0026gt;% group_by(no) %\u0026gt;% mutate(highest = n == max(n)) %\u0026gt;% ggplot(aes(no, n, fill = .pred_class, alpha = highest)) + scale_alpha_ordinal(range = c(0.5, 1)) + geom_col(position = \u0026quot;dodge\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_minimal() + scale_fill_manual(values = c(\u0026quot;#304890\u0026quot;, \u0026quot;#6A7E50\u0026quot;)) + guides(alpha = \u0026quot;none\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) + labs(x = \u0026quot;Federalist Paper Number\u0026quot;, y = \u0026quot;Number of paragraphs\u0026quot;, fill = \u0026quot;Author\u0026quot;, title = \u0026quot;Hamilton were predicted more often to be the author then\\nMadison in all but 1 Paper\u0026quot;)   Conclusion In this post we have touched a lot of different topics; tidymodels, text preprocessing, parallel computing etc. Since we have covered so many topics have left each section not covered it a lot of detail. In a more proper analysis you would want to try some different models and different ways to do the preprocessing.\n ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"b8b57411e6dff5e04d5aca0fb02182c6","permalink":"/2019/08/09/authorship-classification-with-tidymodels-and-textrecipes/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/2019/08/09/authorship-classification-with-tidymodels-and-textrecipes/","section":"post","summary":"In this post we will revisit on of my earlier blogposts where I tried to use tidytext and glmnet to predict the authorship of the anonymous Federalist Papers. If you want more information regarding the data, please read the old post.","tags":null,"title":"Authorship classification with tidymodels and textrecipes","type":"post"},{"authors":null,"categories":["rstudio addin"],"content":" The problem Lately there have been some well deservered buzz around addins in RStudio, datapasta being one and hrbraddins being another highly liked ones.\nI find datapasta helpful for creating little tibbles for teaching. I'll find some interesting data online and just copy and paste the table directly into the correct format. You can also set up keyboard shortcuts, because who doesn't love a keyboard shortcut. Thanks @MilesMcBain pic.twitter.com/deaZVVYYDu — We are R-Ladies (@WeAreRLadies) July 22, 2019   My keyboard shortcut for this lil' function gets quite the workout…\n📺 “hrbraddins::bare_combine()” by @hrbrmstr https://t.co/8dwqNEso0B #rstats pic.twitter.com/gyqz2mUE0Y — Mara Averick (@dataandme) July 29, 2019   All of this is done with RStudio Addins using the rstudioapi r package.\nA lot of the popular addins follows the same simple formula\n extract highlighted text modify extracted text replace highlighted text with modified text.  if your problem can be solved with the above steps, then this post is for you.\n The solution Once you have found the name of your addin, go to your package directory, or create a new package. Then we use usethis to create a .R file for the function and to create the necessary infrastructure to let RStudio know it is a Addin.\nuse_r(\u0026quot;name_of_your_addin\u0026quot;) use_addin(\u0026quot;name_of_your_addin\u0026quot;) The inst/rstudio/addins.dcf file will be populated to make a binding between your function to the addins menu. From here you will in Name to change the text of the button in the drop-down menu and change the description to change the hover text.\nName: New Addin Name Description: New Addin Description Binding: name_of_your_addin Interactive: false now you can go back to the .R to write your function. Below is the minimal code needed. Just replace any_function with a function that takes a string and returns a modified string. build the package and you are done!\nexample \u0026lt;- function() { # Gets The active Documeent ctx \u0026lt;- rstudioapi::getActiveDocumentContext() # Checks that a document is active if (!is.null(ctx)) { # Extracts selection as a string selected_text \u0026lt;- ctx$selection[[1]]$text # modify string selected_text \u0026lt;- any_function(selected_text) # replaces selection with string rstudioapi::modifyRange(ctx$selection[[1]]$range, selected_text) } }  Examples - slugify While I was writing this post I created an addin to turn the title of the blog post into a slug i could use. I replaced\nselected_text \u0026lt;- any_function(selected_text) with\nselected_text \u0026lt;- stringr::str_to_lower(selected_text) selected_text \u0026lt;- stringr::str_replace_all(selected_text, \u0026quot; \u0026quot;, \u0026quot;-\u0026quot;) Which gave me this little gem of a addin!\n ","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"43ac6ce5c86a8b7f1ce9cd132a02ca70","permalink":"/2019/07/30/creating-rstudio-addin-to-modify-selection/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/2019/07/30/creating-rstudio-addin-to-modify-selection/","section":"post","summary":"The problem Lately there have been some well deservered buzz around addins in RStudio, datapasta being one and hrbraddins being another highly liked ones.\nI find datapasta helpful for creating little tibbles for teaching.","tags":null,"title":"Creating RStudio addin to modify selection","type":"post"},{"authors":null,"categories":["ggplot2"],"content":" Introduction The newest version of ggplot2 3.2.0 gave us the ability to change the glyph in the legend like so\nlibrary(ggplot2) ggplot(economics_long, aes(date, value01, colour = variable)) + geom_line(key_glyph = \u0026quot;timeseries\u0026quot;) And they can likewise be specified with the draw_key_* functions as well\nggplot(economics_long, aes(date, value01, colour = variable)) + geom_line(key_glyph = draw_key_timeseries)  Showcase The following is all the available draw_key_* functions in ggplot2. Notice that the dark gray color in dotplot and polygon is a result of a unspecified fill aesthetic. Code to generate these figures can be found at the end of this post.\n## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union  Custom glyph key Since the draw_key_* function just return a grob, you can use spend some time and create your own custom glyphs! Taking inspiration from draw_key_boxplot\ndraw_key_boxplot ## function (data, params, size) ## { ## grobTree(linesGrob(0.5, c(0.1, 0.25)), linesGrob(0.5, c(0.75, ## 0.9)), rectGrob(height = 0.5, width = 0.75), linesGrob(c(0.125, ## 0.875), 0.5), gp = gpar(col = data$colour %||% \u0026quot;grey20\u0026quot;, ## fill = alpha(data$fill %||% \u0026quot;white\u0026quot;, data$alpha), lwd = (data$size %||% ## 0.5) * .pt, lty = data$linetype %||% 1)) ## } ## \u0026lt;bytecode: 0x7ff0874a8c08\u0026gt; ## \u0026lt;environment: namespace:ggplot2\u0026gt; will I try to make a glyph by myself using both points and lines.\nlibrary(grid) library(rlang) draw_key_smile \u0026lt;- function(data, params, size) { grobTree( pointsGrob(0.25, 0.75, size = unit(.25, \u0026quot;npc\u0026quot;), pch = 16), pointsGrob(0.75, 0.75, size = unit(.25, \u0026quot;npc\u0026quot;), pch = 16), linesGrob(c(0.9, 0.87, 0.78, 0.65, 0.5, 0.35, 0.22, 0.13, 0.1), c(0.5, 0.35, 0.22, 0.13, 0.1, 0.13, 0.22, 0.35, 0.5)), gp = gpar( col = data$colour %||% \u0026quot;grey20\u0026quot;, fill = alpha(data$fill %||% \u0026quot;white\u0026quot;, data$alpha), lwd = (data$size %||% 0.5) * .pt, lty = data$linetype %||% 1 ) ) } ggplot(economics_long, aes(date, value01, colour = variable)) + geom_line(key_glyph = draw_key_smile) And it looks so happy!\n Appendix library(dplyr) library(magrittr) library(ggplot2) library(grid) draws \u0026lt;- ls(getNamespace(\u0026quot;ggplot2\u0026quot;), pattern = \u0026quot;^draw_key_\u0026quot;) legend_fun \u0026lt;- function(x) { ggg \u0026lt;- economics_long %\u0026gt;% mutate(variable = factor(variable, labels = paste(\u0026quot;Option\u0026quot;, LETTERS[1:5]))) %\u0026gt;% ggplot(aes(date, value01, colour = variable)) + geom_line(key_glyph = get(x)) + labs(color = x) legend \u0026lt;- cowplot::get_legend(ggg) grid.newpage() grid.draw(legend) } purrr::walk(draws[1:12], legend_fun) p \u0026lt;- ggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) + geom_text(aes(colour = factor(ceiling(seq_len(nrow(mtcars)) %% 5), labels = paste(\u0026quot;Option\u0026quot;, LETTERS[1:5])))) + labs(color = \u0026quot;draw_key_text\u0026quot;) legend \u0026lt;- cowplot::get_legend(p) grid.newpage() grid.draw(legend) purrr::walk(draws[14:16], legend_fun)  ","date":1560729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560729600,"objectID":"4978dba5e696a934f0524b906911f259","permalink":"/2019/06/17/changing-glyph-in-legend-in-ggplot2/","publishdate":"2019-06-17T00:00:00Z","relpermalink":"/2019/06/17/changing-glyph-in-legend-in-ggplot2/","section":"post","summary":"Introduction The newest version of ggplot2 3.2.0 gave us the ability to change the glyph in the legend like so\nlibrary(ggplot2) ggplot(economics_long, aes(date, value01, colour = variable)) + geom_line(key_glyph = \u0026quot;timeseries\u0026quot;) And they can likewise be specified with the draw_key_* functions as well","tags":null,"title":"Changing Glyph in legend in ggplot2","type":"post"},{"authors":null,"categories":[],"content":" This blogpost is going to describe how to write a customizable profiling function. If you are not familiar with profiling read the Profiling section of Advanced R to familiarize yourself, I’ll wait.\n…\nWelcome back!\nPackages While these packages aren’t strictly needed since most of what we are doing is happening in base R, am I still loading in tidyverse to do some easier string manipulations and plotting.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2  Profiling basics You have properly used the profvis package. It is an amazing package and I use it on a daily basis. However, the amount of information you get can be overwhelming at times depending on your profiling goals.\nLets propose in this scenario that we take in some data, scale and center it, apply PCA while only keeping the components that explain 90% of the variance and lastly apply CLARA clustering and return the classification.\nThe code to do that is contained in the following chunk.\npca_threshold \u0026lt;- function(x, threshold) { data_pca \u0026lt;- prcomp(x, scale. = TRUE) total_var \u0026lt;- sum(data_pca$sdev ^ 2) num_comp \u0026lt;- which.max(cumsum(data_pca$sdev ^ 2 / total_var) \u0026gt;= threshold) data_pca$x[, seq_len(num_comp)] } pca_kmeans \u0026lt;- function(x, threshold = 0.9, centers = 2) { data_matrix \u0026lt;- as.matrix(x) data_pca \u0026lt;- pca_threshold(data_matrix, threshold = threshold) data_kmeans \u0026lt;- cluster::clara(data_pca, k = centers) data_kmeans$cluster } Now we create some data, and run profvis on it\nlarge_data \u0026lt;- diamonds %\u0026gt;% select_if(is.numeric) %\u0026gt;% sample_n(100000, replace = TRUE) profvis::profvis({ pca_kmeans(large_data) }) And we get the following information back.\n The Problem It is very infomrative, but it is also giving a LOT of information. Lets propose we want to know the percentage of the computation time is used to do the PCA calculations. In the profvis framework you would need to do the calculation manually. All while waiting for the html widget to load.\n The Idea profvis uses the Rprof function internally to inspect what is happening. By using Rprof directly we can extract the profile and calculate out out output/metrix.\nThe base profiling steps are\ntmp \u0026lt;- tempfile() Rprof(tmp) ################## # Code goes here # ################## Rprof(NULL) profile \u0026lt;- readLines(tmp) This chunk will set up a temporary file, start the profiler and set it to write to the temporary file, stop the profiler and read the result from the profiler.\nTrying it with our code we get\ntmp \u0026lt;- tempfile() Rprof(tmp) x \u0026lt;- pca_kmeans(large_data) Rprof(NULL) profile \u0026lt;- readLines(tmp) head(profile) ## [1] \u0026quot;sample.interval=20000\u0026quot; ## [2] \u0026quot;\\\u0026quot;aperm.default\\\u0026quot; \\\u0026quot;aperm\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [3] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [4] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [5] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [6] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; Lets see what these lines mean. first we notice that the first line is just denoting the sample interval, so we can ignore that for now. Lets look at the next line\n## [1] \u0026quot;\\\u0026quot;aperm.default\\\u0026quot; \\\u0026quot;aperm\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; This is a snapshot of the “call-stack”, and it reads inside-out. So we have that aperm.default is called inside aperm which is called inside apply which is called inside scale.default and so on and so forth all the way up to pca_kmeans.\nNow that we know how Rprof works, we can write some code that checks whether “pca_threshold” appear in the call-stack and then find the percentage.\n The Solution We can now create a function that will calculate the percentage of the time is being spend in a certain function.\nprof_procentage \u0026lt;- function(expr, pattern) { tmp \u0026lt;- tempfile() Rprof(tmp) expr Rprof(NULL) profile \u0026lt;- readLines(tmp)[-1] mean(grepl(pattern, profile)) } This function can now easily be used on our calculation.\nprof_procentage( x \u0026lt;- pca_kmeans(large_data), pattern = \u0026quot;pca_threshold\u0026quot; ) ## [1] 0.75 And this is how to create a custom profiler. Simple modify the last line in the skeleton function prof_procentage to change its behavior.\n the Extensions The sky’s the limit! you are only limited by your regex abilities. You can also change the output. In the last example I returned a numeric of the percentage, But we can also have the output be a plot\nprof_procentage_plot \u0026lt;- function(expr, pattern) { tmp \u0026lt;- tempfile() Rprof(tmp) expr Rprof(NULL) profile \u0026lt;- readLines(tmp)[-1] data.frame(x = grepl(pattern, profile)) %\u0026gt;% ggplot(aes(x)) + geom_bar() } prof_procentage_plot( x \u0026lt;- pca_kmeans(large_data), pattern = \u0026quot;pca_threshold\u0026quot; )  The follow-up After my initial announcement of this post I got a helpful tweet from Hadley Wickham about the profvis::parse_rprof(). In essence it will help you parse the file you write with Rprof to help you get to your answer faster and safer.\nSo some output like\n## [1] \u0026quot;sample.interval=20000\u0026quot; ## [2] \u0026quot;\\\u0026quot;aperm.default\\\u0026quot; \\\u0026quot;aperm\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [3] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [4] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [5] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; ## [6] \u0026quot;\\\u0026quot;is.na\\\u0026quot; \\\u0026quot;FUN\\\u0026quot; \\\u0026quot;apply\\\u0026quot; \\\u0026quot;scale.default\\\u0026quot; \\\u0026quot;scale\\\u0026quot; \\\u0026quot;prcomp.default\\\u0026quot; \\\u0026quot;prcomp\\\u0026quot; \\\u0026quot;pca_threshold\\\u0026quot; \\\u0026quot;pca_kmeans\\\u0026quot; \u0026quot; Will be transformed to a nice data.frame with profvis::parse_rprof()\n## time depth label filenum linenum memalloc meminc filename ## 1 1 9 aperm.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 2 1 8 aperm NA NA 0 0 \u0026lt;NA\u0026gt; ## 3 1 7 apply NA NA 0 0 \u0026lt;NA\u0026gt; ## 4 1 6 scale.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 5 1 5 scale NA NA 0 0 \u0026lt;NA\u0026gt; ## 6 1 4 prcomp.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 7 1 3 prcomp NA NA 0 0 \u0026lt;NA\u0026gt; ## 8 1 2 pca_threshold NA NA 0 0 \u0026lt;NA\u0026gt; ## 9 1 1 pca_kmeans NA NA 0 0 \u0026lt;NA\u0026gt; ## 10 2 9 is.na NA NA 0 0 \u0026lt;NA\u0026gt; ## 11 2 8 FUN NA NA 0 0 \u0026lt;NA\u0026gt; ## 12 2 7 apply NA NA 0 0 \u0026lt;NA\u0026gt; ## 13 2 6 scale.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 14 2 5 scale NA NA 0 0 \u0026lt;NA\u0026gt; ## 15 2 4 prcomp.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 16 2 3 prcomp NA NA 0 0 \u0026lt;NA\u0026gt; ## 17 2 2 pca_threshold NA NA 0 0 \u0026lt;NA\u0026gt; ## 18 2 1 pca_kmeans NA NA 0 0 \u0026lt;NA\u0026gt; ## 19 3 9 is.na NA NA 0 0 \u0026lt;NA\u0026gt; ## 20 3 8 FUN NA NA 0 0 \u0026lt;NA\u0026gt; ## 21 3 7 apply NA NA 0 0 \u0026lt;NA\u0026gt; ## 22 3 6 scale.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 23 3 5 scale NA NA 0 0 \u0026lt;NA\u0026gt; ## 24 3 4 prcomp.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 25 3 3 prcomp NA NA 0 0 \u0026lt;NA\u0026gt; ## 26 3 2 pca_threshold NA NA 0 0 \u0026lt;NA\u0026gt; ## 27 3 1 pca_kmeans NA NA 0 0 \u0026lt;NA\u0026gt; ## 28 4 9 is.na NA NA 0 0 \u0026lt;NA\u0026gt; ## 29 4 8 FUN NA NA 0 0 \u0026lt;NA\u0026gt; ## 30 4 7 apply NA NA 0 0 \u0026lt;NA\u0026gt; ## 31 4 6 scale.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 32 4 5 scale NA NA 0 0 \u0026lt;NA\u0026gt; ## 33 4 4 prcomp.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 34 4 3 prcomp NA NA 0 0 \u0026lt;NA\u0026gt; ## 35 4 2 pca_threshold NA NA 0 0 \u0026lt;NA\u0026gt; ## 36 4 1 pca_kmeans NA NA 0 0 \u0026lt;NA\u0026gt; ## 37 5 9 is.na NA NA 0 0 \u0026lt;NA\u0026gt; ## 38 5 8 FUN NA NA 0 0 \u0026lt;NA\u0026gt; ## 39 5 7 apply NA NA 0 0 \u0026lt;NA\u0026gt; ## 40 5 6 scale.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 41 5 5 scale NA NA 0 0 \u0026lt;NA\u0026gt; ## 42 5 4 prcomp.default NA NA 0 0 \u0026lt;NA\u0026gt; ## 43 5 3 prcomp NA NA 0 0 \u0026lt;NA\u0026gt; ## 44 5 2 pca_threshold NA NA 0 0 \u0026lt;NA\u0026gt; ## 45 5 1 pca_kmeans NA NA 0 0 \u0026lt;NA\u0026gt;  ","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558742400,"objectID":"1b092e5e01a531b7d11d0432b3d3fd21","permalink":"/2019/05/25/custom-profiler-in-r/","publishdate":"2019-05-25T00:00:00Z","relpermalink":"/2019/05/25/custom-profiler-in-r/","section":"post","summary":"This blogpost is going to describe how to write a customizable profiling function. If you are not familiar with profiling read the Profiling section of Advanced R to familiarize yourself, I’ll wait.","tags":null,"title":"Custom Profiler in R","type":"post"},{"authors":null,"categories":["ggplot2"],"content":" Using a divergent color palette can be beneficial when you want to draw attention to some values compared to a fixed point. Like temperature around freezing, monetary values around zero and so on. However it can be hard to align 0 to the middle of a continuous color scale. This post will explain how to do this correctly for scale_colour_distiller and scale_fill_distiller, and this will also work for extension packages such as scico.\nPackages and data library(ggplot2) library(scico) theme_set(theme_minimal()) example_data \u0026lt;- data.frame(name = letters[1:10], value = -2:7 + 0.5)  The problem First lets construct a simple chart, we have a bar chart where some of the bars go up, and some of the bars go down.\nggplot(example_data, aes(name, value)) + geom_col() Next lets add some color by assigning the value to the fill aesthetic.\nggplot(example_data, aes(name, value, fill = value)) + geom_col() Using a sequential palette for a chart like this doesn’t give us much insight. Lets add a divergent scale with scale_fill_gradient2(). While it is doing its job, you still have to define the colors yourself.\nggplot(example_data, aes(name, value, fill = value)) + geom_col() + scale_fill_gradient2() Lets instead use the scale_fill_distiller() function to access the continuous versions of the brewer scales.\nggplot(example_data, aes(name, value, fill = value)) + geom_col() + scale_fill_distiller(type = \u0026quot;div\u0026quot;) But look! some of the upwards facing bars are colored green instead of orange.\n The solution The solution is to manually specify the the limits of the color palette such that the center of the palette appears in the middle of the range. This is simply done by finding the absolute maximum of the range of the variable to are mapping to the color. We then set the limits to go from negative max to positive max, thus making zero appear in the middle.\nlimit \u0026lt;- max(abs(example_data$value)) * c(-1, 1) ggplot(example_data, aes(name, value, fill = value)) + geom_col() + scale_fill_distiller(type = \u0026quot;div\u0026quot;, limit = limit) This approach also works with the scico package.\nlimit \u0026lt;- max(abs(example_data$value)) * c(-1, 1) ggplot(example_data, aes(name, value, fill = value)) + geom_col() + scale_fill_scico(palette = \u0026quot;cork\u0026quot;, limit = limit)   ","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"73011d59a2844e7b0edcca7a7ca74e6b","permalink":"/2019/05/21/center-continuous-palettes-in-ggplot2/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/2019/05/21/center-continuous-palettes-in-ggplot2/","section":"post","summary":"Using a divergent color palette can be beneficial when you want to draw attention to some values compared to a fixed point. Like temperature around freezing, monetary values around zero and so on.","tags":null,"title":"Center continuous palettes in ggplot2","type":"post"},{"authors":null,"categories":["generative  art"],"content":" Why are we here? Some days ago I say this little cute pen and it sparked something inside me.\nSee the Pen Heart is Home by Chris Gannon (@chrisgannon) on CodePen.  I throw together some lines of code and took my first splash into using simple Features. This post is not meant as an introduction to sf, a great introduction to the sf objects is made by Jesse Sadler.\n Loading packages library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(sf) library(patchwork)  First run First we create the center shape. I have gone for heart shape, for which I found a parametric expression, I have wrapped all of this in a little function such that I can specify the number of points the polygon has.\nheart_fun \u0026lt;- function(n) { t \u0026lt;- c(seq(0, 2 * pi, length.out = n), 0) out \u0026lt;- data.frame( x = c(16 * sin(t) ^ 3), y = 13 * cos(t) - 5 * cos(2 * t) - 2 * cos(3 * t) - cos(4 * t) ) out \u0026lt;- as.matrix(out) out \u0026lt;- list(out) st_polygon(out) } Lets check that the function works\nheart_fun(100) ## POLYGON ((0 5, 0.004082058 5.082247, 0.03245962 5.325084, 0.1084517 5.716992, 0.2534598 6.239393, 0.4860975 6.867539, 0.8214215 7.571701, 1.270293 8.31857, 1.838891 9.072817, 2.528404 9.798711, 3.334892 10.46172, 4.24935 11.03003, 5.25795 11.47583, 6.342465 11.77642, 7.480851 11.915, 8.647981 11.88112, 9.816481 11.67082, 10.95766 11.28641, 12.04251 10.736, 13.04268 10.03268, 13.93146 9.193568, 14.68474 8.238708, 15.28179 7.189845, 15.70606 6.069255, 15.94569 4.898625, 15.99396 3.698075, 15.8495 2.485356, 15.51639 1.275288, 15.00393 0.07943237, 14.32642 -1.093982, 13.50257 -2.239884, 12.5549 -3.355982, 11.50893 -4.442201, 10.3923 -5.5, 9.233833 -6.531618, 8.062492 -7.539309, 6.906432 -8.524629, 5.792014 -9.487815, 4.742924 -10.42731, 3.77938 -11.33948, 2.917472 -12.21848, 2.168659 -13.05638, 1.539432 -13.84345, 1.031163 -14.56857, 0.6401401 -15.21987, 0.3577924 -15.78537, 0.1710904 -16.25367, 0.06311066 -16.61466, 0.01374229 -16.86016, 0.0005110288 -16.9844, -0.0005110288 -16.9844, -0.01374229 -16.86016, -0.06311066 -16.61466, -0.1710904 -16.25367, -0.3577924 -15.78537, -0.6401401 -15.21987, -1.031163 -14.56857, -1.539432 -13.84345, -2.168659 -13.05638, -2.917472 -12.21848, -3.77938 -11.33948, -4.742924 -10.42731, -5.792014 -9.487815, -6.906432 -8.524629, -8.062492 -7.539309, -9.233833 -6.531618, -10.3923 -5.5, -11.50893 -4.442201, -12.5549 -3.355982, -13.50257 -2.239884, -14.32642 -1.093982, -15.00393 0.07943237, -15.51639 1.275288, -15.8495 2.485356, -15.99396 3.698075, -15.94569 4.898625, -15.70606 6.069255, -15.28179 7.189845, -14.68474 8.238708, -13.93146 9.193568, -13.04268 10.03268, -12.04251 10.736, -10.95766 11.28641, -9.816481 11.67082, -8.647981 11.88112, -7.480851 11.915, -6.342465 11.77642, -5.25795 11.47583, -4.24935 11.03003, -3.334892 10.46172, -2.528404 9.798711, -1.838891 9.072817, -1.270293 8.31857, -0.8214215 7.571701, -0.4860975 6.867539, -0.2534598 6.239393, -0.1084517 5.716992, -0.03245962 5.325084, -0.004082058 5.082247, -2.350945e-46 5, 0 5)) and that it plots correctly.\nplot(heart_fun(100)) We also create a helper function to create a unit circle.\ncircle_fun \u0026lt;- function(n) { t \u0026lt;- c(seq(0, 2 * pi, length.out = n), 0) out \u0026lt;- data.frame( x = sin(t), y = cos(t) ) out \u0026lt;- as.matrix(out) out \u0026lt;- list(out) st_polygon(out) } plot(circle_fun(100)) So we have a heart shape, lets check the boundaries of that shape.\nst_bbox(heart_fun(100)) ## xmin ymin xmax ymax ## -15.99396 -16.98440 15.99396 11.91500 Lets generate a sf polygon of both the heart and circle polygon.\ncircle \u0026lt;- circle_fun(100) heart \u0026lt;- heart_fun(100) Next we want to generate a list of candidate points where we try to place circles. for now we will just randomly sample between -25 and 25 on the x axis and -20 and 20 on the y axis. then we will save them as a sf object.\npoints \u0026lt;- data.frame(x = runif(250, -25, 25), y = runif(250, -20, 20)) %\u0026gt;% sf::st_as_sf(coords = c(1, 2)) plot(points) Next we will filter the points such that we only consider points that are outside the heart shape.\npoints \u0026lt;- points[!lengths(st_intersects(points, heart)), ] plot(points) Next we will loop through every single point and calculate the distance (using st_distance) from the point to the heart. then we will place a circle on that point and scale it such that is has a radius equal to the distance we calculated. That way the heart shape should show given enough points.\nall_polygons \u0026lt;- map(points[[1]], ~ (circle * st_distance(heart, .x, by_element = TRUE)) + .x) %\u0026gt;% st_sfc() plot(all_polygons) And we get something nice! however some of the circle become quite big. Lets bound the radius and give it some variation.\nbound \u0026lt;- function(x, limit) { ifelse(x \u0026gt; limit, runif(1, limit / 4, limit), x) } all_polygons \u0026lt;- map(points[[1]], ~ (circle * bound(st_distance(heart, .x, by_element = TRUE), 4)) + .x) %\u0026gt;% st_sfc() plot(all_polygons) Now lets turn this into a data.frame and extract the x and y coordinate so we can use them for coloring.\nplotting_data \u0026lt;- data.frame(all_polygons) %\u0026gt;% mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]), y = map_dbl(geometry, ~st_centroid(.x)[[2]]))  Now that we have everything we need we will turn to ggplot2 to pretty everything up.\nplotting_data %\u0026gt;% ggplot() + geom_sf(aes(color = y, geometry = geometry), alpha = 0.2, fill = NA) + coord_sf(datum = NA) + theme_void() + guides(color = \u0026quot;none\u0026quot;) And we are done! It looks nice and pretty, now there is a bunch of things we can change.\n color scales coloring patterns circle arrangement (rectangle, circle, buffer)   One function plotting Everything from before is not wrapper up nice and tight in this function.\ncircle_heart \u0026lt;- function(n, center_sf, outside_sf, outside_filter = \u0026quot;None\u0026quot;, plotting_margin = 5, ...) { bound \u0026lt;- function(x, limit) { ifelse(x \u0026gt; limit, runif(1, limit / 4, limit), x) } range \u0026lt;- st_bbox(center_sf) points \u0026lt;- data.frame(x = runif(n, range[[\u0026quot;xmin\u0026quot;]] - plotting_margin, range[[\u0026quot;xmax\u0026quot;]] + plotting_margin), y = runif(n, range[[\u0026quot;ymin\u0026quot;]] - plotting_margin, range[[\u0026quot;ymax\u0026quot;]] + plotting_margin)) %\u0026gt;% sf::st_as_sf(coords = c(1, 2)) if (outside_filter == \u0026quot;buffer\u0026quot;) { points \u0026lt;- st_intersection(points, st_buffer(center_sf, plotting_margin)) } points \u0026lt;- points[!lengths(st_intersects(points, center_sf)), ] all_polygons \u0026lt;- map(points[[1]], ~ (outside_sf * bound(st_distance(center_sf, .x, by_element = TRUE), 4)) + .x) %\u0026gt;% st_sfc() plotting_data \u0026lt;- data.frame(all_polygons) %\u0026gt;% mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]), y = map_dbl(geometry, ~st_centroid(.x)[[2]])) plotting_data %\u0026gt;% ggplot() + geom_sf(..., mapping = aes(geometry = geometry)) + coord_sf(datum = NA) + theme_void() } It returns a simple ggplot2 object that we then can further modify to our visual liking.\ncircle_heart(300, heart_fun(100), circle_fun(100)) A handful of examples\np1 \u0026lt;- circle_heart(300, heart_fun(100), circle_fun(100), plotting_margin = 10, fill = NA) + aes(color = sin(x / y)) + scale_color_viridis_c() + guides(color = \u0026quot;none\u0026quot;) p2 \u0026lt;- circle_heart(300, heart_fun(100), circle_fun(100), outside_filter = \u0026quot;buffer\u0026quot;, plotting_margin = 10, color = NA, alpha = 0.4) + aes(fill = cos(x / y)) + scale_fill_viridis_c(option = \u0026quot;A\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) p3 \u0026lt;- circle_heart(300, heart_fun(100), circle_fun(5), outside_filter = \u0026quot;buffer\u0026quot;, plotting_margin = 10, color = NA, alpha = 0.4) + aes(fill = x + y) + scale_fill_gradient(low = \u0026quot;pink\u0026quot;, high = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) p4 \u0026lt;- circle_heart(500, heart_fun(100), circle_fun(4), outside_filter = \u0026quot;buffer\u0026quot;, plotting_margin = 10, color = NA, alpha = 0.4) + aes(fill = atan2(y, x)) + scale_fill_gradientn(colours = rainbow(256)) + guides(fill = \u0026quot;none\u0026quot;) p5 \u0026lt;- circle_heart(300, heart_fun(100), circle_fun(10), outside_filter = \u0026quot;buffer\u0026quot;, plotting_margin = 10, color = NA, alpha = 0.4) + aes(fill = factor(floor(x * y) %% 8)) + scale_fill_brewer(palette = \u0026quot;Set1\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) p6 \u0026lt;- circle_heart(500, heart_fun(100), heart_fun(100) / 20, outside_filter = \u0026quot;buffer\u0026quot;, plotting_margin = 10, color = \u0026quot;grey70\u0026quot;, alpha = 0.4) + aes(fill = (y %% 4) * (x %% 1)) + scale_fill_gradientn(colours = cm.colors(256)) + guides(fill = \u0026quot;none\u0026quot;) p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)  ","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"902aed1f8d44ec5a92d755d7adfda91f","permalink":"/2019/05/08/circle-love-making-hearts-with-circles/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/2019/05/08/circle-love-making-hearts-with-circles/","section":"post","summary":"Why are we here? Some days ago I say this little cute pen and it sparked something inside me.\nSee the Pen Heart is Home by Chris Gannon (@chrisgannon) on CodePen.","tags":null,"title":"Circle Love - making hearts with circles","type":"post"},{"authors":["Nelson Bighetti"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["tidymodels","textrecipes"],"content":" Introduction I have previously used this blog to talk about text classification a couple of times. tidymodels have since then seen quite a bit of progress. I did in addition get the textrecipes package on CRAN, which provides extra steps to recipes package from tidymodels.\nSeeing the always wonderful post by Julia Silge on text classification with tidy data principles encouraged me to show how the same workflow also can be accomplished in tidymodels.\nTo give this post a little spice will we only be using stop words. Yes, you read that right, we will only keep stop words. Words you are often encouraged to exclude as they don’t provide much information. We will challenge that assumption in this post! To have a baseline for our stop word model will I be using the same data as Julia used in her post.\n Data The data we will be using is the text from Pride and Prejudice and text from The War of the Worlds. These texts can we get from Project Gutenberg using the gutenbergr package.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(gutenbergr) titles \u0026lt;- c( \u0026quot;The War of the Worlds\u0026quot;, \u0026quot;Pride and Prejudice\u0026quot; ) books \u0026lt;- gutenberg_works(title %in% titles) %\u0026gt;% gutenberg_download(meta_fields = \u0026quot;title\u0026quot;) %\u0026gt;% mutate(title = as.factor(title)) %\u0026gt;% select(-gutenberg_id) books ## # A tibble: 19,504 x 2 ## text title ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 \u0026quot;The War of the Worlds\u0026quot; The War of the Wor… ## 2 \u0026quot;\u0026quot; The War of the Wor… ## 3 \u0026quot;by H. G. Wells [1898]\u0026quot; The War of the Wor… ## 4 \u0026quot;\u0026quot; The War of the Wor… ## 5 \u0026quot;\u0026quot; The War of the Wor… ## 6 \u0026quot; But who shall dwell in these worlds if they be\u0026quot; The War of the Wor… ## 7 \u0026quot; inhabited? . . . Are we or they Lords of the\u0026quot; The War of the Wor… ## 8 \u0026quot; World? . . . And how are all things made for ma… The War of the Wor… ## 9 \u0026quot; KEPLER (quoted in The Anatomy of Melancholy)\u0026quot; The War of the Wor… ## 10 \u0026quot;\u0026quot; The War of the Wor… ## # … with 19,494 more rows (deviating from Julia, will we drop the gutenberg_id variable as it is redundant, remove the document variable as it isn’t needed in the tidymodels framework and set the title variable as a factor as it works better with tidymodels used later on.)\nI’m going to quote Julia to explain the modeling problem we are facing;\n We have the text data now, and let’s frame the kind of prediction problem we are going to work on. Imagine that we take each book and cut it up into lines, like strips of paper (✨ confetti ✨) with an individual line on each paper. Let’s train a model that can take an individual line and give us a probability that this book comes from Pride and Prejudice vs. from The War of the Worlds.\n So that is fairly straight-forward task, we already have the data as we want in books. Before we go on lets investigate the class imbalance.\nbooks %\u0026gt;% ggplot(aes(title)) + geom_bar() + theme_minimal() + labs(x = NULL, y = \u0026quot;Count\u0026quot;, title = \u0026quot;Number of Strips in \u0026#39;Pride and Prejudice\u0026#39; and \u0026#39;The War of the Worlds\u0026#39;\u0026quot;) It is a little uneven, but we will carry on.\n Stop words Lets first have a talk about stop words. These are the words that are needed for the sentences to be structurally sound, but doesn’t add any information. however such a concept as “non-informational” is quite abstract and is bound to be highly domain specific. We will be using the English snowball stop word lists provided by the stopwords package (because that is what textrecipes naively uses).\nlibrary(stopwords) stopwords(language = \u0026quot;en\u0026quot;, source = \u0026quot;snowball\u0026quot;) %\u0026gt;% sort() ## [1] \u0026quot;a\u0026quot; \u0026quot;about\u0026quot; \u0026quot;above\u0026quot; \u0026quot;after\u0026quot; \u0026quot;again\u0026quot; ## [6] \u0026quot;against\u0026quot; \u0026quot;all\u0026quot; \u0026quot;am\u0026quot; \u0026quot;an\u0026quot; \u0026quot;and\u0026quot; ## [11] \u0026quot;any\u0026quot; \u0026quot;are\u0026quot; \u0026quot;aren\u0026#39;t\u0026quot; \u0026quot;as\u0026quot; \u0026quot;at\u0026quot; ## [16] \u0026quot;be\u0026quot; \u0026quot;because\u0026quot; \u0026quot;been\u0026quot; \u0026quot;before\u0026quot; \u0026quot;being\u0026quot; ## [21] \u0026quot;below\u0026quot; \u0026quot;between\u0026quot; \u0026quot;both\u0026quot; \u0026quot;but\u0026quot; \u0026quot;by\u0026quot; ## [26] \u0026quot;can\u0026#39;t\u0026quot; \u0026quot;cannot\u0026quot; \u0026quot;could\u0026quot; \u0026quot;couldn\u0026#39;t\u0026quot; \u0026quot;did\u0026quot; ## [31] \u0026quot;didn\u0026#39;t\u0026quot; \u0026quot;do\u0026quot; \u0026quot;does\u0026quot; \u0026quot;doesn\u0026#39;t\u0026quot; \u0026quot;doing\u0026quot; ## [36] \u0026quot;don\u0026#39;t\u0026quot; \u0026quot;down\u0026quot; \u0026quot;during\u0026quot; \u0026quot;each\u0026quot; \u0026quot;few\u0026quot; ## [41] \u0026quot;for\u0026quot; \u0026quot;from\u0026quot; \u0026quot;further\u0026quot; \u0026quot;had\u0026quot; \u0026quot;hadn\u0026#39;t\u0026quot; ## [46] \u0026quot;has\u0026quot; \u0026quot;hasn\u0026#39;t\u0026quot; \u0026quot;have\u0026quot; \u0026quot;haven\u0026#39;t\u0026quot; \u0026quot;having\u0026quot; ## [51] \u0026quot;he\u0026quot; \u0026quot;he\u0026#39;d\u0026quot; \u0026quot;he\u0026#39;ll\u0026quot; \u0026quot;he\u0026#39;s\u0026quot; \u0026quot;her\u0026quot; ## [56] \u0026quot;here\u0026quot; \u0026quot;here\u0026#39;s\u0026quot; \u0026quot;hers\u0026quot; \u0026quot;herself\u0026quot; \u0026quot;him\u0026quot; ## [61] \u0026quot;himself\u0026quot; \u0026quot;his\u0026quot; \u0026quot;how\u0026quot; \u0026quot;how\u0026#39;s\u0026quot; \u0026quot;i\u0026quot; ## [66] \u0026quot;i\u0026#39;d\u0026quot; \u0026quot;i\u0026#39;ll\u0026quot; \u0026quot;i\u0026#39;m\u0026quot; \u0026quot;i\u0026#39;ve\u0026quot; \u0026quot;if\u0026quot; ## [71] \u0026quot;in\u0026quot; \u0026quot;into\u0026quot; \u0026quot;is\u0026quot; \u0026quot;isn\u0026#39;t\u0026quot; \u0026quot;it\u0026quot; ## [76] \u0026quot;it\u0026#39;s\u0026quot; \u0026quot;its\u0026quot; \u0026quot;itself\u0026quot; \u0026quot;let\u0026#39;s\u0026quot; \u0026quot;me\u0026quot; ## [81] \u0026quot;more\u0026quot; \u0026quot;most\u0026quot; \u0026quot;mustn\u0026#39;t\u0026quot; \u0026quot;my\u0026quot; \u0026quot;myself\u0026quot; ## [86] \u0026quot;no\u0026quot; \u0026quot;nor\u0026quot; \u0026quot;not\u0026quot; \u0026quot;of\u0026quot; \u0026quot;off\u0026quot; ## [91] \u0026quot;on\u0026quot; \u0026quot;once\u0026quot; \u0026quot;only\u0026quot; \u0026quot;or\u0026quot; \u0026quot;other\u0026quot; ## [96] \u0026quot;ought\u0026quot; \u0026quot;our\u0026quot; \u0026quot;ours\u0026quot; \u0026quot;ourselves\u0026quot; \u0026quot;out\u0026quot; ## [101] \u0026quot;over\u0026quot; \u0026quot;own\u0026quot; \u0026quot;same\u0026quot; \u0026quot;shan\u0026#39;t\u0026quot; \u0026quot;she\u0026quot; ## [106] \u0026quot;she\u0026#39;d\u0026quot; \u0026quot;she\u0026#39;ll\u0026quot; \u0026quot;she\u0026#39;s\u0026quot; \u0026quot;should\u0026quot; \u0026quot;shouldn\u0026#39;t\u0026quot; ## [111] \u0026quot;so\u0026quot; \u0026quot;some\u0026quot; \u0026quot;such\u0026quot; \u0026quot;than\u0026quot; \u0026quot;that\u0026quot; ## [116] \u0026quot;that\u0026#39;s\u0026quot; \u0026quot;the\u0026quot; \u0026quot;their\u0026quot; \u0026quot;theirs\u0026quot; \u0026quot;them\u0026quot; ## [121] \u0026quot;themselves\u0026quot; \u0026quot;then\u0026quot; \u0026quot;there\u0026quot; \u0026quot;there\u0026#39;s\u0026quot; \u0026quot;these\u0026quot; ## [126] \u0026quot;they\u0026quot; \u0026quot;they\u0026#39;d\u0026quot; \u0026quot;they\u0026#39;ll\u0026quot; \u0026quot;they\u0026#39;re\u0026quot; \u0026quot;they\u0026#39;ve\u0026quot; ## [131] \u0026quot;this\u0026quot; \u0026quot;those\u0026quot; \u0026quot;through\u0026quot; \u0026quot;to\u0026quot; \u0026quot;too\u0026quot; ## [136] \u0026quot;under\u0026quot; \u0026quot;until\u0026quot; \u0026quot;up\u0026quot; \u0026quot;very\u0026quot; \u0026quot;was\u0026quot; ## [141] \u0026quot;wasn\u0026#39;t\u0026quot; \u0026quot;we\u0026quot; \u0026quot;we\u0026#39;d\u0026quot; \u0026quot;we\u0026#39;ll\u0026quot; \u0026quot;we\u0026#39;re\u0026quot; ## [146] \u0026quot;we\u0026#39;ve\u0026quot; \u0026quot;were\u0026quot; \u0026quot;weren\u0026#39;t\u0026quot; \u0026quot;what\u0026quot; \u0026quot;what\u0026#39;s\u0026quot; ## [151] \u0026quot;when\u0026quot; \u0026quot;when\u0026#39;s\u0026quot; \u0026quot;where\u0026quot; \u0026quot;where\u0026#39;s\u0026quot; \u0026quot;which\u0026quot; ## [156] \u0026quot;while\u0026quot; \u0026quot;who\u0026quot; \u0026quot;who\u0026#39;s\u0026quot; \u0026quot;whom\u0026quot; \u0026quot;why\u0026quot; ## [161] \u0026quot;why\u0026#39;s\u0026quot; \u0026quot;will\u0026quot; \u0026quot;with\u0026quot; \u0026quot;won\u0026#39;t\u0026quot; \u0026quot;would\u0026quot; ## [166] \u0026quot;wouldn\u0026#39;t\u0026quot; \u0026quot;you\u0026quot; \u0026quot;you\u0026#39;d\u0026quot; \u0026quot;you\u0026#39;ll\u0026quot; \u0026quot;you\u0026#39;re\u0026quot; ## [171] \u0026quot;you\u0026#39;ve\u0026quot; \u0026quot;your\u0026quot; \u0026quot;yours\u0026quot; \u0026quot;yourself\u0026quot; \u0026quot;yourselves\u0026quot; this list contains 175 words. Many of these words will at first glance pass the “non-informational” test. However if you look at it more you will realize that many of these can have meaning in certain contexts. The word “i” for example will be used more in blog posts then legal documents. Secondly there appear to be quite a lot of negation words, “wouldn’t”, “don’t”, “doesn’t” and “mustn’t” just to list a few. This is another reminder that constructing your own stop word list can be highly beneficial for your project as the default list might not work in your field.\nWhile these words are assumed to have little information, the distribution of them and the relational information contained with how the stop word are used compared to each other might give us some information anyways. One author might use negations more often then another, maybe someon really like to use the word “nor”. These kind of features can be extracted as the distributional information, or in other words “counts”. We will count how often each stop word appear and hope that some of the words can divide the authors. Next we have the order of which words appear in. This is related to writing style, some authors might write “… will you please…” while others might use “… you will handle…”. The way each word combination is used might be worth a little bit of information. We will capture the relational information with ngrams.\nWe will briefly showcase how this works with an example.\nsentence \u0026lt;- \u0026quot;This an example sentence that is used to explain the concept of ngrams.\u0026quot; to extract the ngrams we will use the tokenizers package (also default in textrecipes). Here we can get all the trigrams (ngrams of length 3).\nlibrary(tokenizers) tokenize_ngrams(sentence, n = 3) ## [[1]] ## [1] \u0026quot;this an example\u0026quot; \u0026quot;an example sentence\u0026quot; \u0026quot;example sentence that\u0026quot; ## [4] \u0026quot;sentence that is\u0026quot; \u0026quot;that is used\u0026quot; \u0026quot;is used to\u0026quot; ## [7] \u0026quot;used to explain\u0026quot; \u0026quot;to explain the\u0026quot; \u0026quot;explain the concept\u0026quot; ## [10] \u0026quot;the concept of\u0026quot; \u0026quot;concept of ngrams\u0026quot; however we would also like to the singular word counts (unigrams) and bigrams (ngrams of length 2). This can easily be done by setting the n_min argument.\ntokenize_ngrams(sentence, n = 3, n_min = 1) ## [[1]] ## [1] \u0026quot;this\u0026quot; \u0026quot;this an\u0026quot; \u0026quot;this an example\u0026quot; ## [4] \u0026quot;an\u0026quot; \u0026quot;an example\u0026quot; \u0026quot;an example sentence\u0026quot; ## [7] \u0026quot;example\u0026quot; \u0026quot;example sentence\u0026quot; \u0026quot;example sentence that\u0026quot; ## [10] \u0026quot;sentence\u0026quot; \u0026quot;sentence that\u0026quot; \u0026quot;sentence that is\u0026quot; ## [13] \u0026quot;that\u0026quot; \u0026quot;that is\u0026quot; \u0026quot;that is used\u0026quot; ## [16] \u0026quot;is\u0026quot; \u0026quot;is used\u0026quot; \u0026quot;is used to\u0026quot; ## [19] \u0026quot;used\u0026quot; \u0026quot;used to\u0026quot; \u0026quot;used to explain\u0026quot; ## [22] \u0026quot;to\u0026quot; \u0026quot;to explain\u0026quot; \u0026quot;to explain the\u0026quot; ## [25] \u0026quot;explain\u0026quot; \u0026quot;explain the\u0026quot; \u0026quot;explain the concept\u0026quot; ## [28] \u0026quot;the\u0026quot; \u0026quot;the concept\u0026quot; \u0026quot;the concept of\u0026quot; ## [31] \u0026quot;concept\u0026quot; \u0026quot;concept of\u0026quot; \u0026quot;concept of ngrams\u0026quot; ## [34] \u0026quot;of\u0026quot; \u0026quot;of ngrams\u0026quot; \u0026quot;ngrams\u0026quot; Now we get unigrams, bigrams and trigrams in one. But wait, we wanted to limit our focus to stop words. Here is how the end result will look once we exclude all non-stop words and perform the ngram operation.\ntokenize_words(sentence) %\u0026gt;% unlist() %\u0026gt;% intersect(stopwords(language = \u0026quot;en\u0026quot;, source = \u0026quot;snowball\u0026quot;)) %\u0026gt;% paste(collapse = \u0026quot; \u0026quot;) %\u0026gt;% print() %\u0026gt;% tokenize_ngrams(n = 3, n_min = 1) ## [1] \u0026quot;this an that is to the of\u0026quot; ## [[1]] ## [1] \u0026quot;this\u0026quot; \u0026quot;this an\u0026quot; \u0026quot;this an that\u0026quot; \u0026quot;an\u0026quot; \u0026quot;an that\u0026quot; ## [6] \u0026quot;an that is\u0026quot; \u0026quot;that\u0026quot; \u0026quot;that is\u0026quot; \u0026quot;that is to\u0026quot; \u0026quot;is\u0026quot; ## [11] \u0026quot;is to\u0026quot; \u0026quot;is to the\u0026quot; \u0026quot;to\u0026quot; \u0026quot;to the\u0026quot; \u0026quot;to the of\u0026quot; ## [16] \u0026quot;the\u0026quot; \u0026quot;the of\u0026quot; \u0026quot;of\u0026quot; We have quite a reduction in ngrams then the full sentence, but hopefully there is some information within.\n Training \u0026amp; testing split Before we start modeling we need to split our data into a testing and training set. This is easily done using the rsample package from tidymodels.\nlibrary(tidymodels) ## Warning: package \u0026#39;rsample\u0026#39; was built under R version 3.6.2 set.seed(1234) books_split \u0026lt;- initial_split(books, strata = \u0026quot;title\u0026quot;, p = 0.75) train_data \u0026lt;- training(books_split) test_data \u0026lt;- testing(books_split)  Preprocessing Next step is to do the preprocessing. For this will we use the recipes from tidymodels. This allows us to specify a preprocessing design that can be train on the training data and applied to the training and testing data alike. I created textrecipes as recipes doesn’t naively support text preprocessing.\nI’m are going to replicate Julia’s preprocessing here to make comparisons easier for myself. Notice the step_filter() call, the original text have quite a lot of empty lines and these don’t contain any textual information at all so we will filter away these observations. Note also that we could have used all_predictors() instead of text at it is the only predictor we have.\nlibrary(textrecipes) julia_rec \u0026lt;- recipe(title ~ ., data = train_data) %\u0026gt;% step_filter(text != \u0026quot;\u0026quot;) %\u0026gt;% step_tokenize(text) %\u0026gt;% step_tokenfilter(text, min_times = 11) %\u0026gt;% step_tf(text) %\u0026gt;% prep(training = train_data) julia_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Training data contained 14629 data points and no missing data. ## ## Operations: ## ## Row filtering [trained] ## Tokenization for text [trained] ## Text filtering for text [trained] ## Term frequency with text [trained] This recipe will remove empty texts, tokenize to words (default in step_tokenize()), keeping words that appear 10 times or more in the training set and then count how many times each word appears. The processed data looks like this\njulia_train_data \u0026lt;- juice(julia_rec) julia_test_data \u0026lt;- bake(julia_rec, test_data) str(julia_train_data, list.len = 10) ## tibble [12,138 × 101] (S3: tbl_df/tbl/data.frame) ## $ title : Factor w/ 2 levels \u0026quot;Pride and Prejudice\u0026quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ tf_text_a : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_about : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_after : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_again : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_all : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ... ## $ tf_text_am : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_an : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_and : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ... ## $ tf_text_any : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## [list output truncated] The reason we get 101 features and Julia got 1652 is because she did her filtering on the full dataset where we only did the filtering on the training set and that Julia didn’t explicitly remove empty oberservations.\nBack to stop words!! In this case we need a slightly more complicated recipe\nstopword_rec \u0026lt;- recipe(title ~ ., data = train_data) %\u0026gt;% step_filter(text != \u0026quot;\u0026quot;) %\u0026gt;% step_tokenize(text) %\u0026gt;% step_stopwords(text, keep = TRUE) %\u0026gt;% step_untokenize(text) %\u0026gt;% step_tokenize(text, token = \u0026quot;ngrams\u0026quot;, options = list(n = 3, n_min = 1)) %\u0026gt;% step_tokenfilter(text, min_times = 10) %\u0026gt;% step_tf(text) %\u0026gt;% prep(training = train_data) stopword_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Training data contained 14629 data points and no missing data. ## ## Operations: ## ## Row filtering [trained] ## Tokenization for text [trained] ## Stop word removal for text [trained] ## Untokenization for text [trained] ## Tokenization for text [trained] ## Text filtering for text [trained] ## Term frequency with text [trained] First we tokenize to words, remove all non-stop words, untokenize (which is basically just paste() with a fancy name), tokenize to ngrams, remove ngrams that appear less then 10 times and lastly we count how often each ngram appear.\n# Processed data stopword_train_data \u0026lt;- juice(stopword_rec) stopword_test_data \u0026lt;- bake(stopword_rec, test_data) str(stopword_train_data, list.len = 10) ## tibble [12,138 × 101] (S3: tbl_df/tbl/data.frame) ## $ title : Factor w/ 2 levels \u0026quot;Pride and Prejudice\u0026quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ tf_text_a : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_a and : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_a of : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_about : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_after : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_again : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_all : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ... ## $ tf_text_am : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## $ tf_text_an : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ... ## [list output truncated] And we are left with 101 features.\n Modeling For modeling we will be using the parsnip package from tidymodels. First we start by defining a model specification. This defines the intent of our model, what we want to do, not what we want to do it on. Meaning we don’t include the data yet, just the kind of model, its hyperparameters and the engine (the package that will do the work). We will be be using glmnet package here so we will specify a logistic regression model\nglmnet_model \u0026lt;- logistic_reg(mixture = 0, penalty = 0.1) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) glmnet_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.1 ## mixture = 0 ## ## Computational engine: glmnet Here we will fit the models using both our training data, first using the stop words, then using the simple would count approach.\nstopword_model \u0026lt;- glmnet_model %\u0026gt;% fit(title ~ ., data = stopword_train_data) julia_model \u0026lt;- glmnet_model %\u0026gt;% fit(title ~ ., data = julia_train_data) This is the part of the workflow where one should do hyperparameter optimization and explore different models to find the best model for the task. For the interest of the length of this post will this step be excluded, possible to be explored in a future post 😉.\n Evaluation Now that we have fitted the data based on the training data we can evaluate based on the testing data set. Here we will use the parsnip functions predict_class() and predict_classprob() to give us the predicted class and predicted probabilities for the two models. Neatly collecting the whole thing in one tibble.\neval_tibble \u0026lt;- stopword_test_data %\u0026gt;% select(title) %\u0026gt;% mutate( class_stopword = parsnip:::predict_class(stopword_model, stopword_test_data), class_julia = parsnip:::predict_class(julia_model, julia_test_data), prop_stopword = parsnip:::predict_classprob(stopword_model, stopword_test_data) %\u0026gt;% pull(`The War of the Worlds`), prop_julia = parsnip:::predict_classprob(julia_model, julia_test_data) %\u0026gt;% pull(`The War of the Worlds`) ) eval_tibble ## # A tibble: 4,027 x 5 ## title class_stopword class_julia prop_stopword prop_julia ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 The War of the… Pride and Prejudi… The War of the W… 0.475 0.508 ## 2 The War of the… Pride and Prejudi… Pride and Prejud… 0.498 0.388 ## 3 The War of the… Pride and Prejudi… Pride and Prejud… 0.335 0.315 ## 4 The War of the… The War of the Wo… The War of the W… 0.690 0.710 ## 5 The War of the… The War of the Wo… The War of the W… 0.650 0.607 ## 6 The War of the… Pride and Prejudi… Pride and Prejud… 0.241 0.264 ## 7 The War of the… Pride and Prejudi… Pride and Prejud… 0.369 0.351 ## 8 The War of the… Pride and Prejudi… The War of the W… 0.403 0.568 ## 9 The War of the… The War of the Wo… The War of the W… 0.520 0.631 ## 10 The War of the… The War of the Wo… The War of the W… 0.511 0.545 ## # … with 4,017 more rows Tidymodels includes the yardstick package which makes evaluation calculations much easier and tidy. It can allow us to calculate the accuracy by calling the accuracy() function\naccuracy(eval_tibble, truth = title, estimate = class_stopword) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy binary 0.778 accuracy(eval_tibble, truth = title, estimate = class_julia) ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy binary 0.801 And we see that the stop words model beats the naive model (one that always picks the majority class), while lacking behind the word count model.\ntest_data %\u0026gt;% filter(text != \u0026quot;\u0026quot;) %\u0026gt;% summarise(mean(title == \u0026quot;Pride and Prejudice\u0026quot;)) ## # A tibble: 1 x 1 ## `mean(title == \u0026quot;Pride and Prejudice\u0026quot;)` ## \u0026lt;dbl\u0026gt; ## 1 0.662 We are also able to plot the ROC curve using roc_curve()(notice how we are using the predicted probabilities instead of class) and autoplot()\neval_tibble %\u0026gt;% roc_curve(title, prop_stopword) %\u0026gt;% autoplot() To superimpose both ROC curve we are going to tidyr our data a little bit.\neval_tibble %\u0026gt;% rename(`Word Count` = prop_julia, `Stopwords` = prop_stopword) %\u0026gt;% gather(\u0026quot;Stopwords\u0026quot;, \u0026quot;Word Count\u0026quot;, key = \u0026quot;Model\u0026quot;, value = \u0026quot;Prop\u0026quot;) %\u0026gt;% group_by(Model) %\u0026gt;% roc_curve(title, Prop) %\u0026gt;% autoplot() + labs(title = \u0026quot;ROC curve for text classification using word count or stopwords\u0026quot;, subtitle = \u0026quot;Predicting whether text was written by Jane Austen or H.G. Wells\u0026quot;) + paletteer::scale_color_paletteer_d(\u0026quot;ggsci::category10_d3\u0026quot;)  Conclusion I’m not going to tell you that you should run a “all stop words only” model every-time you want to do text classification. But I hope this exercise shows you that stop words which are assumed to have no information does indeed have some degree on information. Please always look at your stop word list, check if you even need to remove them, some studies shows that removal of stop words might not provide the benefit you thought.\nFurthermore I hope to have showed the power of tidymodels. Tidymodels is still growing, so if you have any feedback/bug reports/suggests please go to the respective repositories, we would highly appreciate it!\n Comments This plot was suggested in the comments, Thanks Isaiah!\nstopword_model$fit %\u0026gt;% tidy() %\u0026gt;% mutate(term = str_replace(term, \u0026quot;tf_text_\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% group_by(estimate \u0026gt; 0) %\u0026gt;% top_n(10, abs(estimate)) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate \u0026gt; 0)) + geom_col(alpha = 0.8, show.legend = FALSE) + coord_flip() + theme_minimal() + labs(x = NULL, title = \u0026quot;Coefficients that increase/decrease probability the most\u0026quot;, subtitle = \u0026quot;Stopwords only\u0026quot;) And Isaiah notes that\n Whereas Julia’s analysis using non stop words showed that Elizabeth is the opposite of a Martian, stop words shows that Pride and Prejudice talks of men and women, and War of the Worlds makes declarations about existence.\n Which I would like to say looks pretty spot on.\n ","date":1546041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546041600,"objectID":"7724ee4052da6d77f8a50a22c327613f","permalink":"/2018/12/29/text-classification-with-tidymodels/","publishdate":"2018-12-29T00:00:00Z","relpermalink":"/2018/12/29/text-classification-with-tidymodels/","section":"post","summary":"Introduction I have previously used this blog to talk about text classification a couple of times. tidymodels have since then seen quite a bit of progress. I did in addition get the textrecipes package on CRAN, which provides extra steps to recipes package from tidymodels.","tags":null,"title":"Text Classification with Tidymodels","type":"post"},{"authors":null,"categories":["usethis"],"content":" This code have been lightly revised to make sure it works as of 2018-12-20.\nThere have been a lot of progress in the aid of package development in R in recent times. The classic blogpost by Hilary Parker Writing an R Package From Scratch and its younger sister Writing an R package from scratch by Tomas Westlake are both great sources of information to create a package. Fo more general documentation on package development you would be right to look at Hadley Wickhams book R packages. The devtools package have always been instrumental for good package development, but some of these features and additional ones are now to be found in the usethis package. The usethis promises to\n … it automates repetitive tasks that arise during project setup and development, both for R packages and non-package projects.\n In this blogpost I’ll outline the basis workflow you can acquire using the tools in usethis. More specifically I’ll outline a workflow of a R package development. The course of any R package development can be broken down into these steps:\n Before creation Creating minimal functional package One time modifications Multiple time modifications Before every commit Before every release  Before we start, I assume that you will be using Rstudio for this tutorial.\nBefore creation Before we get started we need to make sure we have the essential packages installed to create a R package development workflow\n#install.packages(c(\u0026quot;devtools\u0026quot;, \u0026quot;roxygen2\u0026quot;, \u0026quot;usethis\u0026quot;)) library(devtools) library(roxygen2) library(usethis) Side-note, if you are to create a R package, you need a name. It needs to be unique, especially if you plan on getting your package on CRAN. The available package can help you evaluate possible names to make sure they don’t clash with other names and that they don’t mean something rude. For this example I’m going to make a horrible name by shortening the phrases “usethis workflow”\nlibrary(available) available(\u0026quot;utwf\u0026quot;) the only acronym it finds is “Umauma Triple Water Falls” so we are good to go. Next we need to make sure that you have setup usethis, for this section I’ll refer to the original documentation usethis setup as it explains these steps better then I could.\n Creating minimal functional package Now that you have followed the setup guide you are ready to create a minimal functional package.\nFor creation we will use the create_package() function to create a R package.\ncreate_package(\u0026quot;~/Desktop/utwf\u0026quot;) use_git() use_github() And we are done! We now have a minimal R package, complete with Github repository. With these files included:\nRight now it doesn’t have much, in fact it doesn’t even have a single function in it. We can check that the package works by pressing “Install and Restart” in the “Build” panel. Alternatively you can use the keyboard shortcut Cmd+Shift+B (Ctrl+Shift+B for Windows).\n One time modifications Now that we are up and running there is a bunch of things we should do before we start writing code. Firstly we will go over all the actions that only have to be done once and get those out of the way.\nFirstly we will go into the DESCRIPTION file and make sure that the Authors@R is populated correctly and modify the Title and Description fields.\nNext we will license the package. This can be done using one of the following functions (we will use MIT for this example)\nuse_mit_license() use_gpl3_license() use_apl2_license() use_cc0_license() Choice of which license you neeed is beyond the scope of this post. Please refer to the R Packages license section or https://choosealicense.com/ for further assistance.\nNow we add the readme files, this is done using the\nuse_readme_rmd() This will create a readme.Rmd file that you can edit and knit as you normally would.\nNext we will setup some continuous integration. I’ll recommend trying to do all of the 3 following:\nuse_travis() use_appveyor() use_coverage(type = c(\u0026quot;codecov\u0026quot;)) These calls won’t do all the work for you, so you would have to follow the directions (following red circles) and turn on the services on the Travis and AppVeyor websites respectively, copy badges to the readme (typically placed right under the main title “# utwf”) and copy the code snippet to the .travis.yml file.\nYou will most likely also want to include unit testing, this can be achieved using the testthat package, to include the testing capasity of testthat in your package simply run the following\nuse_testthat() you will need to add at least one test to avoid failed builds on Travis-ci and Appveyor. More information on how to do testing can be found at the Testing chapter in the R packages book.\nNext we will add spell checking to our workflow, this is done with\nuse_spell_check() Make sure that the spelling package is installed before running.\nIf you are going to include data in your package, you would want to include a data-raw folder where the data is created/formatted.\nuse_data_raw() Lastly if you plan on doing a little larger project a NEWS file is very handy to keep track on what is happening in your package.\nuse_news_md()  Multiple time modifications Now that we have setup all the basics, the general development can begin.\nYou typical workflow will be repeating the following steps in the order that suits your flow\n Write some code Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows) Build and Reload Cmd+Shift+B (Ctrl+Shift+B for Windows) Test Package Cmd+Shift+T (Ctrl+Shift+T for Windows) Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows) Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows)  Writing code most likely includes writing functions, this is helped by the use_r() function by adding and opening a .R file that you write your function in\nuse_r(\u0026quot;function_name\u0026quot;) This function is very important and you will using it a lot, not only will it create the files you save your functions in, but it will also open the files if they are already created, this makes navigating your R files much easier. Once you have created your function it is time to add some tests! This is done using the use_test() function, and it works much the same way as the use_r().\nuse_test(\u0026quot;function_name\u0026quot;) In the creating of your functions, you might need to depend on another package, to add a function to the imports field in the DESCRIPTION file you can use the use_package() function\nuse_package(\u0026quot;dplyr\u0026quot;)  Special cases function includes use_rcpp(), use_pipe() and use_tibble().\nAn vignette provides a nice piece of documentation once you have added a bunch of capabilities to your package.\nuse_vignette(\u0026quot;How to do this cool analysis\u0026quot;)  Before every commit Before you commit, run the following commands one more time to make sure you didn’t break anything.\n Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows) Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows) Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows)   Before every release You have worked and have created something wonderful. You want to showcase the work. First go knit the readme.Rmd file and then run these commands again to check that everything is working.\n Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows) Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows) Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows)  update the version number with the use of\nuse_version() And you are good to go!\n Conclusion This is the end of this post, and there are many more functions in usethis that I haven’t covered here, both for development and otherwise. One set of functions I would like to highlight in particular is the Helpers for tidyverse development which helps you follow tidyverse conventions which are generally a little stricter than the defaults. If you have any questions or additions you would like to have added please don’t refrain from contacting me!\n ","date":1535846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535846400,"objectID":"98759be01ba6a4b3a613a4b6097e90a0","permalink":"/2018/09/02/usethis-workflow-for-package-development/","publishdate":"2018-09-02T00:00:00Z","relpermalink":"/2018/09/02/usethis-workflow-for-package-development/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-20.\nThere have been a lot of progress in the aid of package development in R in recent times.","tags":null,"title":"usethis workflow for package development","type":"post"},{"authors":null,"categories":["tidytext","ggplot2","plotly"],"content":"       This code have been lightly revised to make sure it works as of 2018-12-20.\nIn this post we will look at a handful of movies reviews from imdb which I have scraped and placed in this repository movie reviews. I took a look at the best and worst rated movies with their best and worst reviews respectively. From that we will try to see if we are able to see how positive reviews on good movies are different then positive reviews on bad movies and so on.\nWe will use fairly standard packages with the inclusion of paletteer for the sole reason of self promotion. (yay!!!)\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(plotly) library(paletteer) we will read in the data using readr\nreviews_raw \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/EmilHvitfeldt/movie-reviews/master/reviews_v1.csv\u0026quot;) Lets take a look at the data I prepared for us:\nglimpse(reviews_raw) ## Rows: 9,764 ## Columns: 7 ## $ text \u0026lt;chr\u0026gt; \u0026quot;It is a very boring and weird movie. Watch it only if … ## $ id \u0026lt;chr\u0026gt; \u0026quot;tt0012349\u0026quot;, \u0026quot;tt0012349\u0026quot;, \u0026quot;tt0012349\u0026quot;, \u0026quot;tt0012349\u0026quot;, \u0026quot;tt… ## $ review_rating \u0026lt;chr\u0026gt; \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;, \u0026quot;bad\u0026quot;,… ## $ title \u0026lt;chr\u0026gt; \u0026quot;The Kid\u0026quot;, \u0026quot;The Kid\u0026quot;, \u0026quot;The Kid\u0026quot;, \u0026quot;The Kid\u0026quot;, \u0026quot;The Kid\u0026quot;, … ## $ rating \u0026lt;dbl\u0026gt; 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, … ## $ url \u0026lt;chr\u0026gt; \u0026quot;https://www.imdb.com/title/tt0012349/\u0026quot;, \u0026quot;https://www.i… ## $ movie_rating \u0026lt;chr\u0026gt; \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;, \u0026quot;good\u0026quot;,… It include 7 different variables. There is some redundancy, the url variable contains the url of the movie, and id and title are just the extracts from the url variable. The rating variable is the average rating of the movie and will not be used in this analysis. Lastly we have the review_rating and movie_rating which will denote if the review is positive or negative and if the movie being reviewed is good or bad respectively.\nLets start by unnesting the words and get the counts. We also don’t want to look at all the stopwords and words that contains numbers, this it likely not a great number of words but we will exclude them for now anyways.\ncounted_words \u0026lt;- unnest_tokens(reviews_raw, word, text) %\u0026gt;% count(word, movie_rating, review_rating) %\u0026gt;% anti_join(stop_words, by = \u0026quot;word\u0026quot;) %\u0026gt;% filter(!str_detect(word, \u0026quot;\\\\d\u0026quot;)) And lets have a quick looks at the result:\ncounted_words %\u0026gt;% arrange(desc(n)) %\u0026gt;% head(n = 15) ## # A tibble: 15 x 4 ## word movie_rating review_rating n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 movie bad good 7504 ## 2 movie bad bad 7426 ## 3 movie good bad 5692 ## 4 movie good good 5507 ## 5 film good good 4701 ## 6 film good bad 3926 ## 7 film bad bad 3243 ## 8 film bad good 3023 ## 9 bad bad bad 2080 ## 10 time good good 1757 ## 11 story good good 1496 ## 12 people bad good 1409 ## 13 time good bad 1387 ## 14 people good bad 1292 ## 15 time bad bad 1263 And we notice that the word movie have been used quite a lot more in reviews of bad movies then in good movies.\nLog odds We have a bunch of counts here and we would like to find a worthwhile transformation of them. Since we have the number of reviews for good movies and bad movies we would be able to find the percentage of words appearing in good movies. This would give us a number between 0 and 1, where the interesting words would be when the percentage is close to 0 and 1 as it would show that the word is being used more in one than another.\nBy doing this transformation to both the review scores and movie scores will give us the following plot:\ncounted_words %\u0026gt;% mutate(rating = str_c(movie_rating, \u0026quot;_\u0026quot;, review_rating)) %\u0026gt;% select(-movie_rating, -review_rating) %\u0026gt;% spread(rating, n) %\u0026gt;% drop_na() %\u0026gt;% mutate(review_lo = (bad_good + good_good) / (bad_bad + good_bad + bad_good + good_good), movie_lo = (good_bad + good_good) / (bad_bad + bad_good + good_bad + good_good)) %\u0026gt;% ggplot() + aes(movie_lo, review_lo) + geom_text(aes(label = word)) Another way to do this is to take the log of the odds of one event happening over the other event. We will create this little helper function for us.\nlog_odds \u0026lt;- function(x, y) { total \u0026lt;- x + y p \u0026lt;- x / total log(p / (1 - p)) } applying this transformation instead expands the the limit from 0 to 1 to the whole number range where the midpoint is 0, this has some nice properties from a visualization perspective, it will also compact the center points a little more allowing outliers to be more prominent.\nplot_data \u0026lt;- counted_words %\u0026gt;% mutate(rating = str_c(movie_rating, \u0026quot;_\u0026quot;, review_rating)) %\u0026gt;% select(-movie_rating, -review_rating) %\u0026gt;% spread(rating, n) %\u0026gt;% drop_na() %\u0026gt;% mutate(review_lo = log_odds(bad_good + good_good, bad_bad + good_bad), movie_lo = log_odds(good_bad + good_good, bad_bad + bad_good)) plot_data %\u0026gt;% ggplot() + aes(movie_lo, review_lo, label = word) + geom_text() We have a good degree of over plotting in this plot, but part of that might be because of the text, a quick look at the scatterplot still reveals a good deal of overplotting. We will try to counter that later on.\nplot_data %\u0026gt;% ggplot() + aes(movie_lo, review_lo) + geom_point(alpha = 0.5) Lets stay in the in the scatterplot. Lets tighten up the theme and include guidelines at y = 0 and x = 0. We will also find the range of the data to make sure we include all the points.\nplot_data %\u0026gt;% select(movie_lo, review_lo) %\u0026gt;% range() ## [1] -4.574711 3.970292 plot_data %\u0026gt;% ggplot() + aes(movie_lo, review_lo) + geom_vline(xintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_hline(yintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_point(alpha = 0.5) + theme_minimal() + coord_cartesian(ylim = c(-4.6, 4.6), xlim = c(-4.6, 4.6)) + labs(x = \u0026quot;← Bad Movies - Good Movies →\u0026quot;, y = \u0026quot;← Bad Reviews - Good Reviews →\u0026quot;) We still have quite a bit of over plotting, I’m going to sample the points based on importance. The importance matrix I’m going to work with is the distance from the middle. In addition we are going to display the number of times a word is used by the size of the points.\nset.seed(13) plot_data_v2 \u0026lt;- plot_data %\u0026gt;% mutate(distance = review_lo ^ 2 + movie_lo ^ 2, n = bad_bad + bad_good + good_bad + good_good) %\u0026gt;% sample_frac(0.1, weight = distance) plot_data_v2 %\u0026gt;% ggplot() + aes(movie_lo, review_lo, size = n) + geom_vline(xintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_hline(yintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_point(alpha = 0.5) + theme_minimal() + coord_cartesian(ylim = c(-4.6, 4.6), xlim = c(-4.6, 4.6)) + labs(x = \u0026quot;← Bad Movies - Good Movies →\u0026quot;, y = \u0026quot;← Bad Reviews - Good Reviews →\u0026quot;) Lastly we will make the whole thing interactive with plotly to allow hover text. We include some color to indicate distance to the center.\np \u0026lt;- plot_data_v2 %\u0026gt;% ggplot() + aes(movie_lo, review_lo, size = n, color = distance, text = word) + geom_vline(xintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_hline(yintercept = 0, color = \u0026quot;grey\u0026quot;) + geom_point(alpha = 0.5) + theme_minimal() + coord_cartesian(ylim = c(-4.6, 4.6), xlim = c(-4.6, 4.6)) + labs(x = \u0026quot;← Bad Movies - Good Movies →\u0026quot;, y = \u0026quot;← Bad Reviews - Good Reviews →\u0026quot;, title = \u0026quot;What are people saying about the best and worst movies on IMDB?\u0026quot;) + scale_color_paletteer_c(\u0026quot;viridis::viridis\u0026quot;) + guides(color = \u0026quot;none\u0026quot;, size = \u0026quot;none\u0026quot;) ggplotly(p, width = 700, height = 700, displayModeBar = FALSE, tooltip = \u0026quot;text\u0026quot;) %\u0026gt;% config(displayModeBar = F)  {\"x\":{\"data\":[{\"x\":[0,0],\"y\":[-5.06,5.06],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(190,190,190,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[-5.06,5.06],\"y\":[0,0],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(190,190,190,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[0.0540672212702756,2.01490302054226,1.20397280432594,2.42353770341171,0.405465108108164,3.70306408910589,-0.78845736036427,0.857450231851222,0.628608659422374,3.22457384598284,-0.154150679827258,-0.405465108108164,0.336472236621213,0.328504066972036,-1.3054422644908,1.65822807660353,1.70474809223843,1.17865499634165,-0.0606246218164347,0.348306694268216,-2.484906649788,-0.200670695462151,-0.847297860387204,1.25276296849537,2.44234703536921,0.773189888233482,3.17805383034794,-1.09861228866811,-0.680243775724037,-0.500775287912489,1.38629436111989,1.6094379124341,0.0741079721537218,0.916290731874155,-1.04982212449868,-0.474457979595116,1.20397280432594,-3.09104245335832,-1.02450431651439,-2.81839825827108,0.916290731874155,-2.11021320034659,0.510825623765991,-0.916290731874155,-1.24078677744865,-0.0392207131532813,-2.33537491581704,1.57553636075842,1.09861228866811,0.897941593205959,1.3415584672785,-0.693147180559945,-2.90872089656436,-0.182321556793955,2.69848075008606,-0.713214743610755,-2.38122822031316,1.79175946922805,1.76358859226136,1.67397643357167,1.17865499634165,-1.32175583998232,0.916290731874155,0.22314355131421,-1.70474809223843,1.36948724280351,-1.6094379124341,-2.75153531304195,1.32175583998232,-1.6094379124341,1.38629436111989,1.90954250488444,-1.70474809223843,1.50407739677627,0.336472236621213,0.650587566141149,2.01490302054226,0.117783035656383,-2.20727491318972,-1.19392246847243,-1.38629436111989,-1.55814461804655,0.613104472886409,0.657587878523458,1.07535542650384,-1.29928298413026,0.394654192003949,0.993251773010283,1.79175946922805,0.133531392624523,1.20397280432594,-0.63907995928967,1.25276296849537,-0.479573080261886,3.3787245258101,1.38629436111989,1.20397280432594,0.287682072451781,-0.693147180559945,-0.792238083204176,0.470003629245736,-0.916290731874155,-0.957839734787027,-0.22314355131421,-0.0786431273191131,-4.57471097850338,1.36919992776059,2.484906649788,0.693147180559945,-1.09861228866811,-1.68639895357023,-1.17007125265025,-2.07944154167984,1.50737229267313,-0.0689928714869513,2.14006616349627,-0.470003629245736,1.09861228866811,-0.965080896043587,0.693147180559945,-1.87180217690159,1.6094379124341,-1.25276296849537,-0.287682072451781,-1.53623450841081,3.44468249360189,1.99809590222588,-2.91568956588045,0.8754687373539,0.405465108108164,-2.28577797467766,-0.826678573184468,1.87180217690159,2.2512917986065,-0.125163142954006,-0.523248143764548,0.693147180559945,-0.451985123743057,-0.653926467406664,2.10006082888257,1.74296930505862,-2.54553127160444,-1.49065437644413,0.78845736036427,-1.6094379124341,-0.367724780125317,-2.4423470353692,-0.331357135954442,0,0.575364144903562,-2.4423470353692,-1.54044504094715,-0.405465108108164,0.916290731874155,1.67397643357167,1.5114575040739,0.117783035656383,0,0.955511445027436,0.725937003382936,1.46633706879343,0.757685701697516,-1.25276296849537,0.893817876022097,2.56494935746154,0.693147180559945,3.3499040872746,0.78845736036427,0,0.113328685307003,1.50407739677627,-0.916290731874155,1.09861228866811,1.82253112789481,-1.46633706879343,-4.28358656186063,-0.0444517625708338,0.510825623765991,0.8754687373539,-0.451985123743057,-0.510825623765991,-3.25809653802148,-0.268263986594679,0.405465108108164,-2.484906649788,0.916290731874155,2.72457950305342,-1.25276296849537,2.75153531304195,-0.305381649551182,1.6094379124341,1.09861228866811,-1.36524095192206,-3.23867845216438,0.693147180559945,-0.510825623765991,0.798507696217772,0.613104472886409,-0.287682072451781,1.68639895357023,0.60077386042893,0.405465108108164,0.318453731118535,0.916290731874155,-2.9704144655697,0.980829253011726,-1.22377543162212,0,-1.62225860086316,0.0923733201310153,-1.40477917579399,0.798507696217772,-0.710846757659346,3.25809653802148,1.1314021114911,-2.59026716544583,-1.38629436111989,1.33850369728354,0.405465108108164,1.38629436111989,-1.94591014905531,-0.525266307920785,0.517943091534855,-0.693147180559945,2.32727770558442,1.09861228866811,1.94591014905531,-0.606135803570315,0,1.38629436111989,1.07451473708905,1.75785791755237,0.916290731874155,0.217723483844871,-0.693147180559945,-0.352821374622742,1.09861228866811,1.79175946922805,-0.955511445027436,-1.69773051957978,0.712949807856125,1.87180217690159,1.05314991459135,3.10608033072286,1.46633706879343,1.48807705542983,-1.04145387482816,0.318453731118535,2.16905370036952,1.22377543162212,-1.42138568093116,2.69462718077007,1.6094379124341,-0.798507696217772,0.72054615474806,0.857450231851222,-1.7730673362159,3.58351893845611,-2.12823170584927,0.82098055206983,0.916290731874155,1.11861295537478,1.25276296849537,-0.336472236621213,1.16315080980568,2.13162729485041,-0.802346472524937,0.336472236621213,-0.502091943797236,2.96183072187831,-1.09861228866811,1.1239300966524,1.50407739677627,-1.46633706879343,-0.543615446588982,0.903970247486115,0,-0.99212880826566,-2.60268968544438,-0.693147180559945,-1.04982212449868,3.25809653802148,-2.35137525716348,-0.559615787935423,2.34180580614733,1.70474809223843,2.75684036527164,1.75401914124521,1.09861228866811,2.52572864430826,-0.405465108108164,0.980829253011726,-2.89037175789616,1.09861228866811,-1.67397643357167,3.97029191355212,2.80336038090653,0.105360515657826,2.16905370036952,0,3.73766961828337,2.81626378574244,0.405465108108164,0.980829253011726,2.44234703536921,-1.29928298413026,0.412532275331257,2.11021320034659,0.810930216216329,-3.09516617054218,1.87180217690159,1.25276296849537,0.22314355131421,-2.56494935746154,3.66356164612965,1.25276296849537,-1.42222637034595,0.133531392624523,0.362335747211132,-0.693147180559945,1.33500106673234,-0.22314355131421,-0.339867825622351,-0.287682072451781,1.7227665977411,2.65675690671466,0.133531392624523,1.15267950993839,0.8754687373539,1.46633706879343,1.46343009000212,1.04145387482816,0.182321556793954,0.955511445027436,-1.38629436111989,1.19770319131234,-0.916290731874155,1.50407739677627,2.39789527279837,-0.405465108108164,-0.252152651297294,0.510825623765991,1.54044504094715,-1.94591014905531,-0.16799173123913,-1.87180217690159,-0.182321556793955,2.60268968544438,0.287682072451781,1.68639895357023,0,1.29928298413026,-0.510825623765991,0.693147180559945,-2.04769284336526,-0.182321556793955,-0.287682072451781,0.281851152140987,-1.70474809223843,-0.385662480811985,1.50407739677627,2.83321334405622,0.635988766719997,1.50407739677627,3.2188758248682,-2.58399755243223,1.38629436111989,1.20397280432594,0.470003629245736,-0.251314428280906,-1.29928298413026,0,1.27686052007443,3.40119738166216,1.6094379124341,0.271933715483642,2.19722457733622,1.54044504094715,0.287682072451781,0.653926467406664,0.825074723602493,3.1267605359604,1.09861228866811,1.32175583998232,0.980829253011726,0.980829253011726,1.74919985480926,0,1.01160091167848,-1.20397280432594,1.09861228866811,0.133531392624523,0.693147180559945,1.18199389760716,0.22314355131421,-1.70474809223843,1.09861228866811,-0.810930216216329,-0.405465108108164,2.09714111877924,-0.587786664902119,-2.11453286149111,0.955511445027436,0.916290731874155,1.33500106673234,0,0.559615787935423,1.09861228866811,0.693147180559945,-0.631271776841858,1.65292302437384,-1.29532258291416,1.22377543162212,1.84582669049833,0.405465108108164,1.29928298413026,2.39789527279837,3.6603513704994,-2.07944154167984,0,-2.92673940206704,2.35137525716348,1.28519824424852,0.606135803570316,-1.43508452528932,-0.908855753386637,0.980829253011726,2.42774823594805,2.8233610476132,1.22866541691631,-0.510825623765991,0.281639757995818,2.14006616349627,-0.267957567104002,0.520534437892952,-0.38193461069797,1.44691898293633,-0.154150679827258,1.50407739677627,0.63907995928967,-1.8294997972109,2.00148000021012,-0.916290731874155,-1.92896059074154,0.82098055206983,0.955511445027436,0.693147180559945,0,-1.09861228866811,1.29392104098888,-1.04145387482816,-1.07755887947028,1.6094379124341,1.48538526376412,-0.916290731874155,-0.693147180559945,0.559615787935423,1.74919985480926,0,0,-1.38629436111989,1.79175946922805,2.49869997192034,1.58412010444981,1.32175583998232,0.780158557549575,0.693147180559945,0.847297860387203,-2.51230562397611,0.356674943938732,1.46633706879343,0.510825623765991,-1.64865862558738,-0.251314428280906,-2.35137525716348,1.22377543162212,1.6094379124341,0.847297860387203,-0.59783700075562,0.606135803570316,1.25276296849537,0.0800427076735366,0.773189888233482,-0.8754687373539,3.34403896782221,-1.64412347042199,-2.2512917986065,-0.810930216216329,-1.72474875894509,2.67414864942653,1.09861228866811,0.559615787935423,1.67397643357167,1.40534255609058,-1.22377543162212,-0.287682072451781,3.10608033072286,1.50407739677627,1.38629436111989,-2.39789527279837,-0.133531392624523,-1.49165487677772,0.916290731874155,0.405465108108164,1.20397280432594,2.07944154167984,0.133531392624523,-2.30258509299405,-0.22314355131421,-1.44691898293633,0.676886659688165,-0.693147180559945,1.29928298413026,-0.905708622543618,-0.356674943938732,-0.352821374622742,3.00403107636869,2.01490302054226,2.23804657185647,-0.880358722648092,0.296020417368935,0.59783700075562,2.42036812865043,0.693147180559945,3.58351893845611,1.34117392583942,-0.22314355131421,1.29928298413026,0.641853886172395,-0.693147180559945,0.78845736036427,0.798507696217772,0.587786664902119,2.15176220325946,-0.980829253011726,-0.916290731874155,-0.528969875889394,1.6094379124341,0.693147180559945,0.896088024556636,-1.84582669049833,0.44558510189759,2.30258509299405,0.975131231897089,1.29928298413026,1.09861228866811,1.6094379124341,0.980829253011726,-2.79320800944252,1.6094379124341,2.54206506362795,-1.41369333530801,0.430782916092454,1.42996942462255,0.916290731874155,0.356674943938732,-1.99243016469021,0.933288308242726,0.498991166118988,-0.998528830111127,-1.44036158239017,0.916290731874155,0.1633250561033,-0.287682072451781,-0.22314355131421,-0.916290731874155,1.30992138233532,-0.372809145134112,-1.38629436111989,1.46633706879343,-0.287682072451781,2.32060359849672,2.01490302054226,0.336472236621213,0.510825623765991,0.823200308808143,0.980829253011726,-1.019831410815,-0.916290731874155,1.02961941718116,0,1.28093384546206,3.33220451017521,1.43508452528932,0.234281295724666,-1.50407739677627,-1.20397280432594,2.00372971994414,1.23214368129263,-0.510825623765991,2.15948424935337,0.831297519040763,-0.78845736036427,-0.893817876022096,-2.67797274586493,1.09861228866811,2.33537491581704,0.693147180559945,-1.25276296849537,1.06087196068526,2.2512917986065,0.538996500732687,-0.570544858467613,0.693147180559945,-0.575364144903562,0.55206858230004,0.1633250561033,-2.19722457733622,-2.11021320034659,-1.42138568093116,1.17865499634165,-2.484906649788,-0.959775843813894,2.39789527279837,0.176930708159078,-2.30258509299405,-1.73460105538811,1.09861228866811,2.33939906611676,1.67397643357167,0.78845736036427,0.635988766719997,0.635988766719997,-0.916290731874155,1.20397280432594,0.405465108108164,0.693147180559945,1.6094379124341,3.16406758837321,2.41591377830105,-0.405465108108164,-0.405465108108164,1.99243016469021,-1.09861228866811,-1.09861228866811,-3.15700042115011,-0.405465108108164,1.09861228866811,1.16315080980568,2.86931834869833,1.019831410815,0.693147180559945,0.693147180559945,0.700473220652018,-0.926762031741451,1.29037423924115,1.34992671694902,-0.628608659422374,1.84582669049833,-2.96183072187831,-0.693147180559945,1.76358859226136,0.382992252256106,1.38629436111989,1.70474809223843,1.38629436111989,0.54654370636807,1.01160091167848,0.105360515657826,1.32175583998232,0.693147180559945,-1.64865862558738,0.847297860387203,1.17865499634165,-0.428995605518359,-0.117783035656384,0.693147180559945,-0.133531392624523,1.09861228866811,-2.52572864430826,0.182321556793954,1.145132304303,2.86220088092947,0.405465108108164,-0.129211731480006,-1.09861228866811,-0.451985123743057,2.70805020110221,-0.916290731874155,-1.38629436111989,3.29583686600433,-2.14006616349627,1.56861591791385,-1.0055218656021,1.65292302437384,-0.510825623765991,1.98100146886658,2.085998942226,-0.470003629245736,0.510825623765991,1.25276296849537,2.35137525716348,0.587786664902119,0.664976303593249,2.44234703536921,0,0.593774706746742,0.405465108108164,-0.0540672212702757,-3.41224721784874,-0.287682072451781,1.6094379124341,-0.405465108108164,0.356674943938732,1.09861228866811,-1.50407739677627,-0.251314428280906,0.00836824967051658,1.38629436111989,0.916290731874155,2.07944154167984,2.65926003693278,0.133531392624523,-0.336472236621213,0.510825623765991,2.87919845729804,0.587786664902119,-1.6094379124341,-1.17865499634165,0.8754687373539,0.887303195000903,0.693147180559945,0.200670695462151,0.929535958624175,0.451985123743057,0.606135803570316,-0.470003629245736,-1.09861228866811,0.606135803570316,-0.693147180559945,2.83321334405622,0.332133835022615,1.25276296849537,2.91777073208428,-2.00976162104185,-0.980829253011726,0.980829253011726,0.356674943938732,0.887303195000903,0,1.00458333901983,-0.356674943938732,-1.79175946922805,-1.01160091167848,-0.810930216216329,1.21639532432449,-0.405465108108164,-1.36330484289519,0,-0.117783035656384,0.405465108108164,-0.451985123743057,1.25276296849537,-1.38629436111989,0.693147180559945,1.09861228866811,1.70474809223843,-1.09861228866811,1.25276296849537,0.22314355131421,0.139761942375158,0.693147180559945,0.628608659422374,-1.25276296849537,-1.38629436111989,1.50407739677627,1.55973824388183,2.41358186896607,1.21924027645672,-1.09861228866811,1.6094379124341,0.772636331618184,1.09861228866811,3.03495298670727,0.0165293019512105,1.20896034583697,-1.20397280432594,-1.32954154452744,-1.24782468685479,0.916290731874155,-1.04982212449868,0.587786664902119,0.538996500732687,0,-0.847297860387204,-0.614366302706831,1.13497993283898,1.29928298413026,0.8754687373539,-0.510825623765991,0.405465108108164,0.510825623765991,1.84582669049833,-0.22314355131421,-0.693147180559945,0.847297860387203,-1.02165124753198,-0.693147180559945,-1.79175946922805,1.76958561173373,0.510825623765991,0.78845736036427,-0.356674943938732,-0.508322493547872,-0.693147180559945,0.847297860387203,1.38629436111989,-1.85389125033506,1.49009115480153],\"y\":[1.13497993283898,0.479573080261886,-1.20397280432594,1.40089316054104,-0.405465108108164,0.49801666547341,1.09861228866811,0.567984037605939,0.534082485930258,0.319633672258383,-1.20397280432594,0.693147180559945,0.693147180559945,-1.32913594727994,0.153509859555409,0.575364144903562,0.470003629245736,-0.606135803570315,0.832909122935104,-1.145132304303,-0.470003629245736,-1.38629436111989,0.847297860387203,-0.693147180559945,1.38629436111989,1.02961941718116,0.944461608840851,1.09861228866811,-1.69459572077441,-1.15820438587036,0.847297860387203,0.405465108108164,0.693147180559945,0.287682072451781,-0.955511445027436,2.61006979274201,0.998528830111127,0.262364264467491,-0.189241999638528,0.169418151958047,-0.916290731874155,-0.0540672212702757,-0.200670695462151,-0.916290731874155,-0.0748012130826984,-0.693147180559945,-0.737598943130779,0.916290731874155,0,-0.773189888233482,0.324049716622656,-1.51982575374441,-0.27763173659828,-0.182321556793955,-2.20727491318972,-1.38629436111989,-0.602175402354219,0,1.1314021114911,2.14006616349627,0.8754687373539,-0.246860077931526,0.287682072451781,-1.25276296849537,0.470003629245736,0.7339691750802,-1.01160091167848,0.241162056816888,1.02961941718116,0,-1.38629436111989,0.0645385211375712,-0.310154928303839,1.84582669049833,1.6094379124341,-0.0571584138399485,1.02165124753198,-1.54044504094715,-0.266691796559945,-0.233614851181505,-0.171850256926659,-0.0870113769896297,-0.0540672212702757,1.4743052384426,0.318453731118535,0,-0.448950220047903,1.28785428830664,0.435318071257845,-0.133531392624523,-1.70474809223843,0.0727593542824283,-2.07944154167984,0.479573080261886,0.516690743218389,1.18958406687384,-0.470003629245736,-0.916290731874155,0,0.394654192003949,1.20397280432594,-0.916290731874155,0.253448900809539,1.25276296849537,-1.74663903394759,-0.329479201130242,1.32720544474988,-0.470003629245736,-1.09861228866811,-0.302280871872934,0,-2.4567357728213,-0.693147180559945,0.176398538490832,-0.965080896043587,0.773189888233482,-0.470003629245736,1.09861228866811,-0.348306694268216,0,1.87180217690159,1.90423745265474,-0.22314355131421,1.29928298413026,0.336472236621213,0.616774201775371,1.07880966137193,-0.102857385439708,-0.117783035656384,0.405465108108164,0.0923733201310153,-0.0870113769896297,0.405465108108164,0.485507815781701,-0.78845736036427,-0.328504066972036,0.405465108108164,-1.25276296849537,-1.48807705542983,0.481838086892738,1.74296930505862,0.255933374137201,-0.117783035656384,-0.78845736036427,0.955511445027436,0.679541528504167,2.19722457733622,-1.15923691048454,-1.38629436111989,1.01160091167848,-0.75377180237638,-0.8754687373539,-0.405465108108164,0.916290731874155,0.538996500732687,0.569094531889966,1.17865499634165,1.09861228866811,-0.693147180559945,-0.262364264467491,0.379489621704904,-0.387765531008763,0.22314355131421,-0.325422400434628,0.143100843640673,-0.693147180559945,0.630233355149376,-0.510825623765991,-0.693147180559945,0.664976303593249,0.182321556793954,-0.916290731874155,-0.510825623765991,0.706219262127298,-1.94591014905531,-0.259957524436926,0.594707107746693,-1.38629436111989,-0.117783035656384,1.6094379124341,1.09861228866811,0,-1.87180217690159,-0.405465108108164,-0.635988766719997,-0.287682072451781,0.0718257345712558,-0.22314355131421,0,-0.0606246218164347,0.22314355131421,1.09861228866811,-2.17853244432407,-0.189241999638528,1.09861228866811,0.510825623765991,1.145132304303,-1.13497993283898,-0.916290731874155,-0.510825623765991,-1.6094379124341,-0.241162056816888,-1.02961941718116,0.916290731874155,-0.245122458032985,-0.559615787935423,-0.559615787935423,-0.916290731874155,-0.170625517030763,-0.341749293722057,-1.1623789070271,-1.56861591791385,-0.260283098263666,1.6094379124341,-0.34484048629173,-0.728238500371215,0.405465108108164,0.43324467221524,0.405465108108164,0.405465108108164,-0.887303195000903,-2.86220088092947,0.268263986594679,0.693147180559945,-0.22314355131421,0.510825623765991,0.78845736036427,1.54044504094715,-1.46633706879343,1.38629436111989,-1.07451473708905,0.356674943938732,0.287682072451781,0.958850346292951,-0.693147180559945,-0.765467842139572,-0.510825623765991,1.16315080980568,-1.25276296849537,0.385662480811985,-0.105360515657827,0.769133087537867,0.492476485097794,-0.714653385780909,0.251314428280906,0.897941593205959,1.04145387482816,-0.693147180559945,1.51982575374441,-0.559615787935423,-0.693147180559945,0.228841572428847,0,0.641853886172395,-1.27629346590556,1.39518330853714,-1.53393035992596,0.162518929497775,0.127833371509885,1.25276296849537,0.287682072451781,0.393042588109607,0.498991166118988,0.693147180559945,-0.287682072451781,-0.121360857004267,-0.485507815781701,1.09861228866811,0.230523658611832,0.870828357797398,-1.09861228866811,0.750305594399894,0.832909122935104,0,-0.818310323513951,0.710846757659346,0.916290731874155,-0.432864082296279,-0.0689928714869513,0,-0.610909082322973,-0.0741079721537221,-0.628608659422374,-0.980829253011726,0.390866308687012,0.154150679827258,1.7404661748405,1.03407376753054,-0.405465108108164,0.610909082322973,-0.405465108108164,0.367724780125318,-0.158224005214894,-1.09861228866811,-0.105360515657827,0.864997437486605,-0.916290731874155,-0.693147180559945,1.06471073699243,-0.693147180559945,0.891998039305111,-0.562785362696702,-0.405465108108164,-0.182321556793955,0.405465108108164,0,-1.58923520511658,0.0540672212702756,1.20397280432594,0.498246841559131,0.847297860387203,0.693147180559945,1.25276296849537,0.287682072451781,0.63740519755171,-0.693147180559945,-0.0157981168765913,-0.693147180559945,0.0932574934865829,-2.2512917986065,0.336472236621213,1.25276296849537,-0.723000143709626,-0.916290731874155,0.0606246218164348,1.40691364832263,1.01160091167848,0.75377180237638,0.606135803570316,0.251314428280906,1.66139765136481,0,1.22377543162212,0.22314355131421,-0.133531392624523,2.18122423598978,0.287682072451781,0.559615787935423,0.78845736036427,0.405465108108164,-0.0924750857648483,0,0.606135803570316,1.46633706879343,-0.839329690738027,-0.405465108108164,-0.182321556793955,0.492476485097794,0.916290731874155,0.125163142954006,0.251314428280906,0.287682072451781,-1.09861228866811,-1.09861228866811,-0.526093095896779,-0.980829253011726,-0.916290731874155,0.447312218043665,-0.470003629245736,-1.25276296849537,0.980829253011726,0.955511445027436,0.810930216216329,2.30258509299405,0.154150679827258,-0.576887374444083,0.693147180559945,0.998528830111127,1.20397280432594,0.78845736036427,-2.12026353620009,-1.46633706879343,0.10648348040245,0.816761136527122,0.111225635110224,-0.271933715483642,0.475423696715075,0.356674943938732,-0.916290731874155,0.897941593205959,-0.219628609206765,0.322083499169114,-0.336472236621213,0.318453731118535,-0.559615787935423,-0.980829253011726,1.48160454092422,-0.619039208406224,0.268263986594679,-1.20397280432594,0.847297860387203,1.38629436111989,0.693147180559945,-0.0408219945202552,0.693147180559945,0.154150679827258,-1.6094379124341,-0.693147180559945,-1.38629436111989,1.46633706879343,-0.587786664902119,-0.739667196194838,0,-0.287682072451781,1.09861228866811,-0.78845736036427,-0.559615787935423,0,0.22314355131421,-0.22314355131421,0.83034830207343,-0.168622712435793,2.30258509299405,-1.22377543162212,-0.405465108108164,0,0.693147180559945,0.479182684004732,-0.0444517625708338,1.38629436111989,-0.307484699747961,-0.628608659422374,0.0666913744986724,0.117783035656383,-1.43508452528932,0.432133355190326,0.559615787935423,0.613104472886409,0.397682967666109,0.189241999638528,-0.510825623765991,-0.84483176789201,0.773189888233482,-0.194744076792512,1.12300374179227,-0.460815203191329,0.287682072451781,-0.810930216216329,2.30258509299405,0.330241686870577,-0.0851578083403066,-0.485507815781701,1.16315080980568,-0.164549387048157,1.42138568093116,-0.451985123743057,-0.693147180559945,-0.955511445027436,-1.29928298413026,0.572069249006709,-0.262364264467491,-0.553385238184787,-0.693147180559945,-0.216223108469636,-0.916290731874155,-0.336472236621213,-0.182321556793955,-0.0741079721537221,1.09861228866811,0.510825623765991,0.0571584138399484,0.0659579677917976,0.952008814476234,-0.0851578083403066,0.773189888233482,-0.526093095896779,-0.693147180559945,1.38629436111989,-1.55059741241117,0.8754687373539,0.510825623765991,-0.887303195000903,-0.59783700075562,0.78845736036427,0.333491608483075,-0.414943852062708,1.25276296849537,-1.73460105538811,-1.64865862558738,-1.17865499634165,-0.22314355131421,0.944461608840851,1.02961941718116,-1.17865499634165,0.710241613919245,0,-0.313657558855041,-0.470003629245736,-0.569533224592769,0.741937344729377,0,-1.22377543162212,0.105360515657826,0,-1.05860695405441,-0.916290731874155,0.405465108108164,1.50407739677627,-0.575364144903562,0,-1.01160091167848,-1.79175946922805,0.693147180559945,-1.09861228866811,-1.20397280432594,0.955511445027436,-1.87180217690159,-0.27443684570176,-1.25276296849537,-0.095310179804325,-0.0434851119397389,-1.6094379124341,0.287682072451781,0.0303053494953288,-0.117783035656384,-0.693147180559945,0.966440515559627,0.606135803570316,-0.466089729924599,-2.25606507735915,-0.966843011036987,1.05605267424931,1.01856958099457,-0.693147180559945,0.126293725324292,0.344840486291729,0.22314355131421,-1.52605630349505,-0.965080896043587,0,1.09861228866811,0.591364486250003,1.79175946922805,1.21302263984585,-0.182321556793955,-0.916290731874155,-0.528969875889394,0.451985123743057,1.42138568093116,0.50310357767208,-0.367724780125317,0.592051063688577,0.559615787935423,0.643754425230369,0,0.693147180559945,0.451985123743057,0.182321556793954,-0.310154928303839,0.268263986594679,-0.163453072489572,-0.262364264467491,-0.559615787935423,1.26165191591261,0.287682072451781,-1.17865499634165,-0.405465108108164,0.129211731480006,-0.405465108108164,-0.998528830111127,-0.961411167154625,0.287682072451781,0.328857986635967,-0.916290731874155,0.955511445027436,-0.287682072451781,0.619039208406224,-0.931821673905032,-1.01160091167848,1.09861228866811,1.29928298413026,0.962275845115979,-1.54044504094715,1.42138568093116,-0.510825623765991,-0.238411023444998,-0.559615787935423,-1.5114575040739,-0.916290731874155,1.02961941718116,-1.29928298413026,0.896088024556636,0.27763173659828,0.154150679827258,0.0636256958802116,-0.980829253011726,-0.154150679827258,1.67006253425054,-0.325422400434628,0.510825623765991,-0.965080896043587,-1.59601489210196,-1.46633706879343,-0.45953232937844,-0.143100843640673,0.75377180237638,-0.117783035656384,-0.693147180559945,0.693147180559945,0.405465108108164,0.693147180559945,-0.105360515657827,-2.07944154167984,1.46040233327361,-1.15267950993839,0.998528830111127,-1.16899308542991,0.847297860387203,-0.0180185055026783,0,0.356674943938732,0.470003629245736,-0.602175402354219,0.887303195000903,-0.606135803570315,-0.559615787935423,-1.38629436111989,-1.09861228866811,0.611801541105993,1.02961941718116,0,0.154150679827258,-0.470003629245736,-0.287682072451781,-0.810930216216329,0.405465108108164,-0.336472236621213,1.09861228866811,0.4678082386823,0.644357016390513,0.405465108108164,-0.650587566141149,0.75377180237638,-1.09861228866811,0,-0.287682072451781,-1.09861228866811,1.09861228866811,0.485507815781701,-0.564892845036267,0.569094531889966,1.25276296849537,-0.693147180559945,0.44628710262842,-1.7404661748405,1.71410853499799,-0.23638877806423,0.826678573184468,0.274436845701761,0.342095494175575,1.25276296849537,-2.53897387105828,-0.860201265223111,-1.38629436111989,-0.998528830111127,1.18958406687384,0.405465108108164,0.693147180559945,1.02961941718116,0.773189888233482,0,-0.194156014440957,0,-1.17865499634165,-0.800777844752311,1.17865499634165,0.693147180559945,-1.38629436111989,-0.167054084663166,0.374693449441411,0.559615787935423,-0.641853886172395,1.13497993283898,0.405465108108164,0.893817876022097,-1.09861228866811,-0.451985123743057,0.444685821261446,0.916290731874155,0,0.324239668185579,0.595983432106298,0.492476485097794,-0.287682072451781,1.19625075823203,-0.887303195000903,0.0606246218164348,0.624154309072994,0.470003629245736,-0.336472236621213,-0.22314355131421,-0.351397886837888,0.916290731874155,0.581921545449721,0.405465108108164,0.693147180559945,0.0384662808277959,-0.847297860387204,-1.28785428830664,-0.227932068046007,-0.287682072451781,1.01160091167848,-0.405465108108164,-1.17865499634165,0.693147180559945,-0.559615787935423,-1.46633706879343,0.451007128555081,0.405465108108164,-0.916290731874155,0.53062825106217,0.637577329405134,0,-1.09861228866811,-0.510825623765991,0.432133355190326,-0.587786664902119,0.42744401482694,-0.356674943938732,0.117783035656383,0.887303195000903,-0.693147180559945,-1.09861228866811,0.0377403279828471,0,-2.01490302054226,-0.470003629245736,-0.78845736036427,1.17865499634165,-0.336472236621213,0,0.02531780798429,0.22314355131421,0.2578291093021,0.145711811181394,-0.182321556793955,-0.559615787935423,1.17865499634165,-0.336472236621213,-0.847297860387204,0.527354925717201,-0.8754687373539,-0.485507815781701,0.133531392624523,-1.20397280432594,0.916290731874155,-0.405465108108164,-1.14862270924277,0.619039208406224,0.8754687373539,-0.650587566141149,-0.693147180559945,0.22314355131421,-1.87180217690159,-0.693147180559945,1.66500776358891,-0.154150679827258,-0.693147180559945,-0.22314355131421,0.864997437486605,-0.836248024200619,0.693147180559945,1.04145387482816,0.22314355131421,-0.268263986594679,1.50407739677627,0.677398823591806,0.287682072451781,0.615185639090233,1.09861228866811,1.6094379124341,-1.49645699273658,-0.336472236621213,0.62509371731493,0.559615787935423,0.641853886172395,-0.810930216216329,-0.0125061755052297,0.123232640423948,0.693147180559945,-2.52572864430826,0,-1.02961941718116,-0.916290731874155,0,0,-0.993251773010283,-0.916290731874155,0.782759339249632,-1.09861228866811,0.405465108108164,-0.510825623765991,0.367724780125318,1.25276296849537,-0.22314355131421,-0.619039208406224,0.8754687373539,-0.336472236621213,-0.287682072451781,0.316799471022508,-1.09861228866811,-0.510825623765991,-1.17865499634165,-1.28935241592766,0.693147180559945,0,1.38629436111989,-0.251314428280906,0.852777326151829],\"text\":[\"patrick\",\"debbie\",\"ploy\",\"travis\",\"elevating\",\"hitchcock\",\"pumping\",\"strength\",\"lessons\",\"luke\",\"associate\",\"uncut\",\"sheets\",\"flashback\",\"low\",\"remarkably\",\"gimli\",\"committing\",\"recognition\",\"challenged\",\"underwater\",\"monotone\",\"colourful\",\"overdose\",\"pursuit\",\"adored\",\"marion\",\"tasty\",\"avoid\",\"propaganda\",\"seedy\",\"historically\",\"agrees\",\"restrictions\",\"spending\",\"fabulous\",\"prequels\",\"mortal\",\"bus\",\"daniel\",\"jokers\",\"beer\",\"spends\",\"taped\",\"cartoon\",\"pass\",\"jon\",\"polish\",\"endings\",\"justify\",\"modern\",\"ugh\",\"cuba\",\"geeks\",\"overrated\",\"poorly\",\"turkey\",\"wits\",\"astonishing\",\"annette\",\"stirring\",\"rental\",\"it.if\",\"nah\",\"ethan\",\"touching\",\"stale\",\"julia\",\"dean\",\"carey's\",\"offset\",\"platoon\",\"hilarity\",\"gandalf\",\"collected\",\"bone\",\"overlook\",\"unlikable\",\"jokes\",\"steel\",\"balls\",\"imitation\",\"humans\",\"excellent\",\"terminator\",\"lil\",\"display\",\"edition\",\"perkins\",\"tool\",\"alleged\",\"entertained\",\"illogical\",\"absurdity\",\"pitt\",\"doubts\",\"filling\",\"clarify\",\"knights\",\"humorous\",\"flowing\",\"ann\",\"dance\",\"biblical\",\"pointless\",\"shark\",\"powerful\",\"murdering\",\"ai\",\"cable\",\"dolls\",\"stinker\",\"chuckle\",\"war\",\"insults\",\"samuel\",\"swinging\",\"catalyst\",\"omg\",\"observer\",\"funky\",\"flawless\",\"roars\",\"recreate\",\"jamie\",\"coppola\",\"machines\",\"jaws\",\"invented\",\"relish\",\"wire\",\"trap\",\"assembly\",\"jackson's\",\"passes\",\"hint\",\"symbols\",\"fools\",\"unbearable\",\"neo\",\"superbly\",\"jessica\",\"jason\",\"shout\",\"nonstop\",\"surprisingly\",\"haters\",\"bothered\",\"rescuing\",\"experienced\",\"dungeons\",\"breasts\",\"objectification\",\"rounded\",\"scar\",\"photography\",\"realizes\",\"damien\",\"pig\",\"surface\",\"uplifting\",\"fought\",\"thirsty\",\"fed\",\"camps\",\"photographed\",\"ford\",\"portrayals\",\"booze\",\"highlight\",\"anticipating\",\"violated\",\"stumbling\",\"shining\",\"carpenter\",\"hilton\",\"absence\",\"senseless\",\"worship\",\"exceeded\",\"expanded\",\"barney\",\"repeating\",\"festivals\",\"shitty\",\"herring\",\"toys\",\"fashionable\",\"suspects\",\"parent\",\"defense\",\"tasks\",\"abomination\",\"golf\",\"feared\",\"heaven's\",\"heartwarming\",\"rave\",\"mercy\",\"depressed\",\"shallow\",\"accurately\",\"debate\",\"honour\",\"ron\",\"trigger\",\"awkward\",\"inform\",\"spy\",\"appears\",\"joke\",\"soap\",\"corny\",\"welles\",\"globe\",\"nicholas\",\"pillow\",\"fiction\",\"supremely\",\"loathing\",\"lava\",\"redeeming\",\"leading\",\"intends\",\"roman\",\"gratification\",\"artwork\",\"possess\",\"injury\",\"luc\",\"drags\",\"explore\",\"characters.the\",\"cried\",\"misfortune\",\"unbelievably\",\"lurid\",\"miguel\",\"vomit\",\"jr\",\"examples\",\"odyssey\",\"united\",\"sentimental\",\"interactions\",\"insight\",\"wicked\",\"bore\",\"impeccable\",\"subjects\",\"cox\",\"jedi\",\"denial\",\"communication\",\"overdone\",\"spectacular\",\"crappy\",\"scorsese\",\"vegas\",\"darkness\",\"certainty\",\"pace\",\"slave\",\"wounds\",\"puppet\",\"murderer\",\"dick\",\"hur\",\"infamous\",\"lang\",\"depraved\",\"survival\",\"hardy\",\"harold\",\"swear\",\"twists\",\"elegant\",\"ass\",\"june\",\"cornball\",\"channel\",\"bogart\",\"shields\",\"coma\",\"lucas\",\"redux\",\"masterful\",\"innocence\",\"electric\",\"robinson\",\"proposal\",\"convincingly\",\"vampires\",\"novelty\",\"demonic\",\"hitchcock's\",\"communist\",\"accidentally\",\"influential\",\"nudge\",\"anthony\",\"nazis\",\"one.this\",\"carol\",\"straightforward\",\"mischief\",\"mediocre\",\"conditions\",\"illness\",\"superman\",\"circle\",\"thrilled\",\"triggered\",\"collins\",\"nolan\",\"falsely\",\"scary\",\"sinking\",\"shot\",\"placement\",\"compassion\",\"plunge\",\"media\",\"peril\",\"spectacle\",\"leonard\",\"surrealism\",\"reed\",\"handled\",\"glover\",\"superb\",\"philosophical\",\"nailed\",\"dawn\",\"cheat\",\"breathtaking\",\"gospel\",\"restored\",\"leonardo\",\"hears\",\"rating\",\"mexicans\",\"assigned\",\"facility\",\"fake\",\"gadgets\",\"swedish\",\"inglourious\",\"disturbs\",\"complexity\",\"goal\",\"vicious\",\"nausea\",\"struggled\",\"boobs\",\"neon\",\"claw\",\"scale\",\"tooth\",\"utter\",\"apt\",\"strangelove\",\"arrival\",\"excellence\",\"poverty\",\"troll\",\"strongest\",\"layers\",\"mixing\",\"grief\",\"monstrosity\",\"nutshell\",\"psycho\",\"reservoir\",\"directorial\",\"trek\",\"charles\",\"rooted\",\"ticked\",\"treasure\",\"bruce\",\"quentin\",\"praises\",\"unit\",\"address\",\"sections\",\"engrossing\",\"laden\",\"novels\",\"tropical\",\"paranoid\",\"smoothly\",\"tremendously\",\"lived\",\"solves\",\"hostage\",\"glossy\",\"crack\",\"moderately\",\"andrew\",\"would've\",\"fart\",\"unstable\",\"tens\",\"unfolds\",\"duck\",\"instincts\",\"women's\",\"robbing\",\"throwing\",\"joseph\",\"tommy\",\"captures\",\"november\",\"outdo\",\"chandler\",\"raymond\",\"tarantino\",\"zombie\",\"well.the\",\"austin\",\"morally\",\"window\",\"spiral\",\"pitiful\",\"moon\",\"switches\",\"gregory\",\"wilder\",\"japan\",\"coarse\",\"weak\",\"profoundly\",\"acting\",\"relate\",\"giant\",\"progression\",\"tin\",\"haunting\",\"extended\",\"island\",\"lawyer\",\"bible\",\"parody\",\"phenomenal\",\"cliched\",\"raving\",\"adolescent\",\"resemble\",\"humanity\",\"corpse\",\"continuity\",\"limitations\",\"factory\",\"dracula\",\"advertised\",\"sophistication\",\"arc\",\"premier\",\"classroom\",\"activity\",\"boot\",\"extraordinary\",\"period\",\"nominations\",\"similarly\",\"cowardly\",\"decoration\",\"unfunny\",\"distinction\",\"deemed\",\"suspend\",\"moore\",\"germany's\",\"fighter\",\"trick\",\"del\",\"unpleasant\",\"incoherent\",\"exaggeration\",\"crafting\",\"capturing\",\"exploration\",\"basement\",\"brad\",\"girls\",\"games\",\"stereo\",\"pg\",\"eli\",\"boxing\",\"despicable\",\"daycare\",\"depiction\",\"sh\",\"horrifically\",\"norton\",\"brilliantly\",\"institution\",\"bros\",\"submit\",\"embarrassed\",\"faced\",\"monkeys\",\"timeline\",\"transported\",\"catastrophe\",\"shyamalan\",\"grate\",\"juno\",\"pacing\",\"tossed\",\"exposure\",\"bear\",\"upcoming\",\"load\",\"samurai\",\"authorities\",\"technical\",\"pile\",\"hour\",\"differences\",\"bates\",\"versa\",\"jews\",\"filmmaking\",\"slipped\",\"blacks\",\"shouting\",\"triple\",\"bounty\",\"steven\",\"vasquez\",\"nomination\",\"ordeal\",\"categorize\",\"looked\",\"primarily\",\"scariest\",\"expression\",\"youtube\",\"created\",\"jew\",\"george\",\"enduring\",\"savvy\",\"definitive\",\"conveying\",\"dennis\",\"crop\",\"silent\",\"previews\",\"lastly\",\"journey\",\"indictment\",\"circles\",\"independence\",\"classics\",\"trapped\",\"signed\",\"bucks\",\"abandons\",\"fi\",\"confirm\",\"hysterical\",\"fruit\",\"tragic\",\"ridiculous\",\"hmm\",\"transcends\",\"messiah\",\"henry\",\"calculated\",\"unexpected\",\"agenda\",\"behavior\",\"traumatized\",\"atrocious\",\"pacifist\",\"imaginative\",\"spared\",\"crafted\",\"noir\",\"crucial\",\"follow\",\"um\",\"transformers\",\"unforgettable\",\"pages\",\"portal\",\"accolades\",\"contrived\",\"semblance\",\"replace\",\"mike\",\"struggle\",\"france\",\"formal\",\"day's\",\"mary\",\"raises\",\"resistance\",\"disappear\",\"wonderfully\",\"subjected\",\"roller\",\"cliché\",\"pirates\",\"grey\",\"cannon\",\"alec\",\"wives\",\"teeth\",\"gangs\",\"false\",\"wicker\",\"rubber\",\"attitudes\",\"freeman\",\"achieves\",\"expressing\",\"broken\",\"sounding\",\"barrels\",\"obsolete\",\"deviates\",\"patch\",\"caan\",\"joker\",\"towers\",\"it.this\",\"mentioning\",\"doc\",\"piper\",\"vengeful\",\"leatherface\",\"crystal\",\"resorting\",\"environmental\",\"holocaust\",\"engaging\",\"restrained\",\"jail\",\"relationship\",\"costs\",\"outstanding\",\"surrounded\",\"freaky\",\"inception\",\"wayans\",\"wizards\",\"affairs\",\"endure\",\"manhattan\",\"scottish\",\"provoking\",\"translation\",\"keen\",\"retired\",\"spawned\",\"brink\",\"mall\",\"inch\",\"historic\",\"sign\",\"stray\",\"plummer\",\"juvenile\",\"heroic\",\"stack\",\"tables\",\"england\",\"eve\",\"improbable\",\"lovely\",\"jake\",\"spit\",\"basterds\",\"makeover\",\"closet\",\"hanks\",\"exorcist\",\"compelling\",\"flying\",\"touched\",\"redeem\",\"requiem\",\"lawrence\",\"formed\",\"demanding\",\"listens\",\"submarine\",\"duel\",\"learns\",\"norman\",\"transforming\",\"deliver\",\"simpson\",\"severe\",\"spoof\",\"requirement\",\"blend\",\"travesties\",\"stabbed\",\"improving\",\"overweight\",\"fairness\",\"books\",\"raging\",\"sweating\",\"akira\",\"vader\",\"shortly\",\"bend\",\"omnipresent\",\"forrest\",\"uttered\",\"laughed\",\"nerds\",\"planes\",\"blown\",\"elicited\",\"incapable\",\"sadistic\",\"mother's\",\"unexplained\",\"buys\",\"gosh\",\"visions\",\"superheroes\",\"eastwood\",\"killer\",\"heightened\",\"nest\",\"ben\",\"shelby\",\"allied\",\"joining\",\"predicted\",\"allusions\",\"officer\",\"flashes\",\"hopper\",\"directions\",\"stalking\",\"influenced\",\"elm\",\"videos\",\"attract\",\"bent\",\"positively\",\"bearing\",\"relentlessly\",\"stinkers\",\"unknowingly\",\"calm\",\"oppressed\",\"modified\",\"adequately\",\"duncan\",\"paint\",\"commercially\",\"arthur\",\"wile\",\"rap\",\"tender\",\"knight\",\"toy\",\"obsession\",\"preconceived\",\"harrowing\",\"boring\",\"interact\",\"woody\",\"ride\",\"apartment\",\"beard\",\"funny\",\"wood\",\"widow\",\"moronic\",\"delve\",\"reasoning\",\"tests\",\"snobbish\",\"weekend\",\"atrocities\",\"emotionless\",\"technique\",\"dives\",\"sacrilegious\",\"styled\",\"ambiguous\",\"psychosis\",\"editors\",\"healthy\",\"hilariously\",\"stumble\",\"clark\",\"godfather\",\"assumption\",\"psychic\",\"stooges\",\"badly\",\"scroll\",\"implied\",\"studying\",\"geniuses\",\"kane\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":[\"rgba(72,23,105,1)\",\"rgba(64,69,136,1)\",\"rgba(70,48,126,1)\",\"rgba(44,113,142,1)\",\"rgba(70,7,90,1)\",\"rgba(52,182,121,1)\",\"rgba(72,32,113,1)\",\"rgba(72,20,102,1)\",\"rgba(71,13,96,1)\",\"rgba(33,144,141,1)\",\"rgba(72,27,109,1)\",\"rgba(71,12,95,1)\",\"rgba(70,11,94,1)\",\"rgba(72,33,114,1)\",\"rgba(72,30,112,1)\",\"rgba(70,51,127,1)\",\"rgba(70,52,128,1)\",\"rgba(72,31,112,1)\",\"rgba(71,13,96,1)\",\"rgba(72,26,108,1)\",\"rgba(52,96,141,1)\",\"rgba(72,34,116,1)\",\"rgba(72,26,108,1)\",\"rgba(72,36,117,1)\",\"rgba(44,113,142,1)\",\"rgba(72,29,111,1)\",\"rgba(31,149,139,1)\",\"rgba(72,41,121,1)\",\"rgba(69,55,129,1)\",\"rgba(72,28,110,1)\",\"rgba(71,45,123,1)\",\"rgba(71,46,124,1)\",\"rgba(70,9,93,1)\",\"rgba(71,17,100,1)\",\"rgba(72,35,116,1)\",\"rgba(49,104,142,1)\",\"rgba(72,41,121,1)\",\"rgba(36,134,142,1)\",\"rgba(72,20,103,1)\",\"rgba(44,115,142,1)\",\"rgba(72,29,111,1)\",\"rgba(63,71,136,1)\",\"rgba(69,6,89,1)\",\"rgba(72,29,111,1)\",\"rgba(72,28,110,1)\",\"rgba(70,9,93,1)\",\"rgba(54,92,141,1)\",\"rgba(69,55,129,1)\",\"rgba(72,22,104,1)\",\"rgba(72,26,108,1)\",\"rgba(72,33,115,1)\",\"rgba(71,47,125,1)\",\"rgba(41,121,142,1)\",\"rgba(68,2,85,1)\",\"rgba(31,162,135,1)\",\"rgba(72,41,121,1)\",\"rgba(54,92,141,1)\",\"rgba(69,53,129,1)\",\"rgba(64,70,136,1)\",\"rgba(47,108,142,1)\",\"rgba(72,37,118,1)\",\"rgba(72,32,113,1)\",\"rgba(71,17,100,1)\",\"rgba(72,28,110,1)\",\"rgba(70,52,128,1)\",\"rgba(72,41,121,1)\",\"rgba(68,59,132,1)\",\"rgba(46,111,142,1)\",\"rgba(71,47,125,1)\",\"rgba(71,44,122,1)\",\"rgba(67,62,133,1)\",\"rgba(68,59,132,1)\",\"rgba(70,50,126,1)\",\"rgba(56,87,140,1)\",\"rgba(71,46,124,1)\",\"rgba(70,8,92,1)\",\"rgba(60,80,139,1)\",\"rgba(72,41,121,1)\",\"rgba(61,78,138,1)\",\"rgba(72,27,109,1)\",\"rgba(72,34,115,1)\",\"rgba(72,41,121,1)\",\"rgba(70,7,91,1)\",\"rgba(71,44,122,1)\",\"rgba(72,23,105,1)\",\"rgba(72,30,111,1)\",\"rgba(70,7,90,1)\",\"rgba(71,45,123,1)\",\"rgba(69,56,130,1)\",\"rgba(68,1,84,1)\",\"rgba(64,70,136,1)\",\"rgba(70,8,92,1)\",\"rgba(55,90,140,1)\",\"rgba(70,9,92,1)\",\"rgba(31,158,137,1)\",\"rgba(69,55,129,1)\",\"rgba(72,29,111,1)\",\"rgba(71,17,100,1)\",\"rgba(70,9,93,1)\",\"rgba(71,15,98,1)\",\"rgba(72,29,111,1)\",\"rgba(72,29,111,1)\",\"rgba(71,18,101,1)\",\"rgba(72,28,110,1)\",\"rgba(70,51,127,1)\",\"rgba(253,231,37,1)\",\"rgba(68,59,132,1)\",\"rgba(52,96,141,1)\",\"rgba(72,30,111,1)\",\"rgba(72,24,106,1)\",\"rgba(71,47,125,1)\",\"rgba(46,109,142,1)\",\"rgba(62,76,138,1)\",\"rgba(72,39,120,1)\",\"rgba(71,17,100,1)\",\"rgba(59,81,139,1)\",\"rgba(70,8,92,1)\",\"rgba(72,41,121,1)\",\"rgba(72,20,102,1)\",\"rgba(70,9,93,1)\",\"rgba(49,104,142,1)\",\"rgba(53,94,141,1)\",\"rgba(72,28,110,1)\",\"rgba(72,31,112,1)\",\"rgba(71,42,122,1)\",\"rgba(32,163,134,1)\",\"rgba(60,80,139,1)\",\"rgba(41,121,142,1)\",\"rgba(71,15,98,1)\",\"rgba(70,7,90,1)\",\"rgba(59,81,139,1)\",\"rgba(71,13,96,1)\",\"rgba(68,60,132,1)\",\"rgba(59,82,139,1)\",\"rgba(71,12,95,1)\",\"rgba(70,7,91,1)\",\"rgba(71,12,95,1)\",\"rgba(72,31,112,1)\",\"rgba(71,45,123,1)\",\"rgba(62,73,137,1)\",\"rgba(54,92,141,1)\",\"rgba(51,98,141,1)\",\"rgba(72,38,119,1)\",\"rgba(72,23,105,1)\",\"rgba(68,57,131,1)\",\"rgba(70,11,94,1)\",\"rgba(32,147,140,1)\",\"rgba(72,26,108,1)\",\"rgba(72,33,115,1)\",\"rgba(72,24,106,1)\",\"rgba(51,98,141,1)\",\"rgba(70,52,128,1)\",\"rgba(70,7,90,1)\",\"rgba(72,29,111,1)\",\"rgba(70,51,127,1)\",\"rgba(71,44,122,1)\",\"rgba(72,26,108,1)\",\"rgba(72,22,104,1)\",\"rgba(72,25,107,1)\",\"rgba(70,11,94,1)\",\"rgba(72,39,120,1)\",\"rgba(71,14,97,1)\",\"rgba(72,28,110,1)\",\"rgba(71,17,100,1)\",\"rgba(51,99,141,1)\",\"rgba(71,18,100,1)\",\"rgba(30,157,137,1)\",\"rgba(71,16,99,1)\",\"rgba(70,9,93,1)\",\"rgba(70,9,92,1)\",\"rgba(72,39,120,1)\",\"rgba(72,29,111,1)\",\"rgba(72,27,109,1)\",\"rgba(67,62,133,1)\",\"rgba(55,91,141,1)\",\"rgba(171,220,50,1)\",\"rgba(70,7,90,1)\",\"rgba(72,37,118,1)\",\"rgba(71,15,98,1)\",\"rgba(71,47,125,1)\",\"rgba(72,27,109,1)\",\"rgba(32,146,140,1)\",\"rgba(68,58,131,1)\",\"rgba(70,7,90,1)\",\"rgba(51,99,141,1)\",\"rgba(71,17,100,1)\",\"rgba(46,109,142,1)\",\"rgba(72,28,110,1)\",\"rgba(46,111,142,1)\",\"rgba(68,2,86,1)\",\"rgba(71,45,123,1)\",\"rgba(72,41,121,1)\",\"rgba(51,99,141,1)\",\"rgba(33,144,141,1)\",\"rgba(72,30,111,1)\",\"rgba(70,10,93,1)\",\"rgba(72,34,115,1)\",\"rgba(72,29,111,1)\",\"rgba(71,17,100,1)\",\"rgba(70,51,127,1)\",\"rgba(70,49,126,1)\",\"rgba(69,4,88,1)\",\"rgba(72,22,104,1)\",\"rgba(72,29,111,1)\",\"rgba(39,126,142,1)\",\"rgba(72,23,105,1)\",\"rgba(72,32,113,1)\",\"rgba(71,16,99,1)\",\"rgba(71,45,123,1)\",\"rgba(68,3,86,1)\",\"rgba(69,55,129,1)\",\"rgba(70,51,127,1)\",\"rgba(70,11,94,1)\",\"rgba(40,174,128,1)\",\"rgba(72,26,108,1)\",\"rgba(47,107,142,1)\",\"rgba(72,36,117,1)\",\"rgba(72,35,116,1)\",\"rgba(70,7,90,1)\",\"rgba(72,36,117,1)\",\"rgba(63,72,137,1)\",\"rgba(41,121,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,18,100,1)\",\"rgba(58,84,140,1)\",\"rgba(72,27,109,1)\",\"rgba(64,70,136,1)\",\"rgba(71,46,124,1)\",\"rgba(72,37,118,1)\",\"rgba(67,62,133,1)\",\"rgba(72,40,120,1)\",\"rgba(69,53,129,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,101,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(72,27,109,1)\",\"rgba(63,72,137,1)\",\"rgba(71,42,122,1)\",\"rgba(70,51,127,1)\",\"rgba(70,10,93,1)\",\"rgba(66,65,134,1)\",\"rgba(72,24,106,1)\",\"rgba(34,140,141,1)\",\"rgba(72,38,119,1)\",\"rgba(70,50,126,1)\",\"rgba(72,37,118,1)\",\"rgba(70,11,94,1)\",\"rgba(49,104,142,1)\",\"rgba(72,32,113,1)\",\"rgba(71,42,122,1)\",\"rgba(47,108,142,1)\",\"rgba(71,44,122,1)\",\"rgba(72,20,102,1)\",\"rgba(72,37,118,1)\",\"rgba(71,45,123,1)\",\"rgba(58,84,140,1)\",\"rgba(37,171,130,1)\",\"rgba(63,72,137,1)\",\"rgba(72,38,119,1)\",\"rgba(71,17,100,1)\",\"rgba(72,26,108,1)\",\"rgba(72,32,113,1)\",\"rgba(70,11,94,1)\",\"rgba(72,26,108,1)\",\"rgba(63,72,137,1)\",\"rgba(71,16,99,1)\",\"rgba(72,24,106,1)\",\"rgba(69,6,89,1)\",\"rgba(37,132,142,1)\",\"rgba(72,41,121,1)\",\"rgba(72,32,113,1)\",\"rgba(70,49,126,1)\",\"rgba(72,37,118,1)\",\"rgba(71,18,100,1)\",\"rgba(72,24,106,1)\",\"rgba(71,16,99,1)\",\"rgba(72,22,104,1)\",\"rgba(50,101,142,1)\",\"rgba(70,9,93,1)\",\"rgba(72,27,109,1)\",\"rgba(32,146,140,1)\",\"rgba(55,91,141,1)\",\"rgba(72,23,105,1)\",\"rgba(57,86,140,1)\",\"rgba(70,49,126,1)\",\"rgba(32,146,140,1)\",\"rgba(65,66,135,1)\",\"rgba(72,25,107,1)\",\"rgba(50,101,142,1)\",\"rgba(70,7,90,1)\",\"rgba(72,20,103,1)\",\"rgba(42,119,142,1)\",\"rgba(72,41,121,1)\",\"rgba(71,47,125,1)\",\"rgba(112,207,87,1)\",\"rgba(41,123,142,1)\",\"rgba(70,9,93,1)\",\"rgba(55,90,140,1)\",\"rgba(70,9,93,1)\",\"rgba(68,191,112,1)\",\"rgba(42,118,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,19,101,1)\",\"rgba(54,93,141,1)\",\"rgba(72,30,111,1)\",\"rgba(71,45,123,1)\",\"rgba(63,71,136,1)\",\"rgba(72,36,117,1)\",\"rgba(35,136,142,1)\",\"rgba(65,68,135,1)\",\"rgba(72,36,117,1)\",\"rgba(72,28,110,1)\",\"rgba(50,100,142,1)\",\"rgba(50,182,122,1)\",\"rgba(72,36,117,1)\",\"rgba(72,35,116,1)\",\"rgba(70,10,93,1)\",\"rgba(68,3,86,1)\",\"rgba(57,85,140,1)\",\"rgba(72,33,115,1)\",\"rgba(72,28,110,1)\",\"rgba(71,12,95,1)\",\"rgba(71,17,100,1)\",\"rgba(70,50,126,1)\",\"rgba(39,127,142,1)\",\"rgba(71,19,102,1)\",\"rgba(72,33,115,1)\",\"rgba(72,21,104,1)\",\"rgba(72,38,119,1)\",\"rgba(61,77,138,1)\",\"rgba(72,20,103,1)\",\"rgba(72,27,109,1)\",\"rgba(71,18,100,1)\",\"rgba(72,34,115,1)\",\"rgba(53,94,141,1)\",\"rgba(71,17,100,1)\",\"rgba(71,44,122,1)\",\"rgba(52,96,141,1)\",\"rgba(70,7,90,1)\",\"rgba(68,2,85,1)\",\"rgba(69,5,89,1)\",\"rgba(71,46,124,1)\",\"rgba(55,91,141,1)\",\"rgba(71,14,97,1)\",\"rgba(68,60,132,1)\",\"rgba(68,2,85,1)\",\"rgba(49,104,142,1)\",\"rgba(71,17,100,1)\",\"rgba(71,47,125,1)\",\"rgba(68,2,85,1)\",\"rgba(72,31,112,1)\",\"rgba(72,27,109,1)\",\"rgba(72,30,111,1)\",\"rgba(63,71,136,1)\",\"rgba(71,19,101,1)\",\"rgba(71,17,100,1)\",\"rgba(69,5,89,1)\",\"rgba(70,52,128,1)\",\"rgba(72,30,112,1)\",\"rgba(69,53,129,1)\",\"rgba(39,126,142,1)\",\"rgba(72,20,102,1)\",\"rgba(46,111,142,1)\",\"rgba(33,143,141,1)\",\"rgba(49,104,142,1)\",\"rgba(72,41,121,1)\",\"rgba(72,41,121,1)\",\"rgba(72,29,111,1)\",\"rgba(71,13,96,1)\",\"rgba(53,94,141,1)\",\"rgba(72,37,118,1)\",\"rgba(72,29,111,1)\",\"rgba(32,163,134,1)\",\"rgba(71,44,122,1)\",\"rgba(69,3,87,1)\",\"rgba(60,79,138,1)\",\"rgba(71,42,122,1)\",\"rgba(71,17,100,1)\",\"rgba(72,23,105,1)\",\"rgba(71,14,97,1)\",\"rgba(35,137,142,1)\",\"rgba(72,24,106,1)\",\"rgba(72,32,113,1)\",\"rgba(72,23,105,1)\",\"rgba(72,33,115,1)\",\"rgba(59,82,139,1)\",\"rgba(70,7,91,1)\",\"rgba(72,20,103,1)\",\"rgba(70,48,126,1)\",\"rgba(72,33,115,1)\",\"rgba(72,34,115,1)\",\"rgba(71,18,100,1)\",\"rgba(72,26,108,1)\",\"rgba(70,10,93,1)\",\"rgba(70,49,126,1)\",\"rgba(67,62,133,1)\",\"rgba(72,21,104,1)\",\"rgba(72,36,117,1)\",\"rgba(51,98,141,1)\",\"rgba(71,13,96,1)\",\"rgba(60,79,138,1)\",\"rgba(71,17,100,1)\",\"rgba(71,17,100,1)\",\"rgba(70,50,126,1)\",\"rgba(70,12,95,1)\",\"rgba(70,12,95,1)\",\"rgba(72,22,104,1)\",\"rgba(70,10,93,1)\",\"rgba(70,8,92,1)\",\"rgba(69,56,130,1)\",\"rgba(72,30,111,1)\",\"rgba(50,101,142,1)\",\"rgba(61,77,138,1)\",\"rgba(70,7,90,1)\",\"rgba(72,30,111,1)\",\"rgba(53,94,141,1)\",\"rgba(46,179,124,1)\",\"rgba(64,69,136,1)\",\"rgba(72,33,115,1)\",\"rgba(41,123,142,1)\",\"rgba(55,91,141,1)\",\"rgba(72,29,111,1)\",\"rgba(70,7,91,1)\",\"rgba(65,66,135,1)\",\"rgba(71,19,101,1)\",\"rgba(72,23,105,1)\",\"rgba(53,95,141,1)\",\"rgba(43,116,142,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(71,15,98,1)\",\"rgba(59,81,139,1)\",\"rgba(68,2,86,1)\",\"rgba(72,27,109,1)\",\"rgba(70,7,90,1)\",\"rgba(72,37,118,1)\",\"rgba(71,13,96,1)\",\"rgba(46,111,142,1)\",\"rgba(70,10,93,1)\",\"rgba(69,55,129,1)\",\"rgba(65,68,135,1)\",\"rgba(72,37,118,1)\",\"rgba(67,61,132,1)\",\"rgba(71,45,123,1)\",\"rgba(72,21,103,1)\",\"rgba(71,18,100,1)\",\"rgba(71,17,100,1)\",\"rgba(70,48,126,1)\",\"rgba(72,35,116,1)\",\"rgba(72,22,104,1)\",\"rgba(72,27,109,1)\",\"rgba(70,51,127,1)\",\"rgba(72,38,119,1)\",\"rgba(72,29,111,1)\",\"rgba(70,11,94,1)\",\"rgba(70,7,90,1)\",\"rgba(70,51,127,1)\",\"rgba(72,22,104,1)\",\"rgba(69,5,89,1)\",\"rgba(72,33,115,1)\",\"rgba(69,53,129,1)\",\"rgba(48,106,142,1)\",\"rgba(71,43,122,1)\",\"rgba(72,40,120,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,100,1)\",\"rgba(71,45,123,1)\",\"rgba(40,124,142,1)\",\"rgba(71,17,100,1)\",\"rgba(72,41,121,1)\",\"rgba(72,20,102,1)\",\"rgba(70,51,127,1)\",\"rgba(71,13,96,1)\",\"rgba(57,86,140,1)\",\"rgba(72,29,111,1)\",\"rgba(65,67,135,1)\",\"rgba(67,61,132,1)\",\"rgba(70,51,127,1)\",\"rgba(72,31,112,1)\",\"rgba(72,28,110,1)\",\"rgba(71,17,100,1)\",\"rgba(72,29,111,1)\",\"rgba(72,37,118,1)\",\"rgba(31,158,137,1)\",\"rgba(71,46,124,1)\",\"rgba(60,80,139,1)\",\"rgba(71,16,99,1)\",\"rgba(69,55,129,1)\",\"rgba(45,112,142,1)\",\"rgba(72,22,104,1)\",\"rgba(72,32,113,1)\",\"rgba(71,47,125,1)\",\"rgba(72,34,116,1)\",\"rgba(71,45,123,1)\",\"rgba(71,17,100,1)\",\"rgba(35,136,142,1)\",\"rgba(63,72,137,1)\",\"rgba(72,38,119,1)\",\"rgba(56,89,140,1)\",\"rgba(71,19,102,1)\",\"rgba(58,84,140,1)\",\"rgba(72,24,106,1)\",\"rgba(72,25,107,1)\",\"rgba(70,48,126,1)\",\"rgba(59,81,139,1)\",\"rgba(68,58,131,1)\",\"rgba(58,83,139,1)\",\"rgba(72,28,110,1)\",\"rgba(72,36,117,1)\",\"rgba(70,9,92,1)\",\"rgba(70,51,127,1)\",\"rgba(72,31,112,1)\",\"rgba(71,16,98,1)\",\"rgba(68,3,86,1)\",\"rgba(70,11,94,1)\",\"rgba(35,138,141,1)\",\"rgba(64,70,136,1)\",\"rgba(59,81,139,1)\",\"rgba(55,90,140,1)\",\"rgba(71,19,101,1)\",\"rgba(72,27,109,1)\",\"rgba(49,102,142,1)\",\"rgba(71,18,100,1)\",\"rgba(37,171,130,1)\",\"rgba(72,33,115,1)\",\"rgba(68,2,86,1)\",\"rgba(66,65,134,1)\",\"rgba(72,24,106,1)\",\"rgba(70,9,93,1)\",\"rgba(72,32,113,1)\",\"rgba(71,19,101,1)\",\"rgba(68,58,131,1)\",\"rgba(54,93,141,1)\",\"rgba(71,19,101,1)\",\"rgba(72,29,111,1)\",\"rgba(70,11,94,1)\",\"rgba(71,47,125,1)\",\"rgba(71,42,122,1)\",\"rgba(72,20,102,1)\",\"rgba(68,58,131,1)\",\"rgba(70,10,93,1)\",\"rgba(57,86,140,1)\",\"rgba(72,25,107,1)\",\"rgba(72,30,111,1)\",\"rgba(72,30,111,1)\",\"rgba(71,47,125,1)\",\"rgba(71,19,101,1)\",\"rgba(44,114,142,1)\",\"rgba(71,45,123,1)\",\"rgba(51,98,141,1)\",\"rgba(72,36,117,1)\",\"rgba(70,10,93,1)\",\"rgba(68,59,132,1)\",\"rgba(71,17,100,1)\",\"rgba(72,27,109,1)\",\"rgba(65,66,135,1)\",\"rgba(71,17,100,1)\",\"rgba(70,8,92,1)\",\"rgba(72,35,116,1)\",\"rgba(70,50,126,1)\",\"rgba(71,17,100,1)\",\"rgba(68,3,86,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,100,1)\",\"rgba(71,17,100,1)\",\"rgba(72,36,117,1)\",\"rgba(71,19,101,1)\",\"rgba(70,49,126,1)\",\"rgba(69,56,130,1)\",\"rgba(72,31,112,1)\",\"rgba(53,95,141,1)\",\"rgba(52,97,141,1)\",\"rgba(72,37,118,1)\",\"rgba(70,10,93,1)\",\"rgba(71,14,97,1)\",\"rgba(72,23,105,1)\",\"rgba(69,55,129,1)\",\"rgba(72,29,111,1)\",\"rgba(72,37,118,1)\",\"rgba(72,30,111,1)\",\"rgba(72,41,121,1)\",\"rgba(31,151,139,1)\",\"rgba(72,36,117,1)\",\"rgba(68,1,85,1)\",\"rgba(69,53,129,1)\",\"rgba(72,27,109,1)\",\"rgba(50,101,142,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(57,86,140,1)\",\"rgba(69,53,129,1)\",\"rgba(71,46,124,1)\",\"rgba(71,19,101,1)\",\"rgba(48,106,142,1)\",\"rgba(72,31,112,1)\",\"rgba(58,84,140,1)\",\"rgba(71,18,100,1)\",\"rgba(72,36,117,1)\",\"rgba(72,23,105,1)\",\"rgba(57,85,140,1)\",\"rgba(69,6,89,1)\",\"rgba(62,73,137,1)\",\"rgba(71,44,122,1)\",\"rgba(72,29,111,1)\",\"rgba(72,24,106,1)\",\"rgba(72,25,107,1)\",\"rgba(57,85,140,1)\",\"rgba(63,71,136,1)\",\"rgba(72,35,116,1)\",\"rgba(72,27,109,1)\",\"rgba(52,96,141,1)\",\"rgba(72,23,105,1)\",\"rgba(51,98,141,1)\",\"rgba(70,8,91,1)\",\"rgba(57,86,140,1)\",\"rgba(61,78,138,1)\",\"rgba(72,41,121,1)\",\"rgba(55,90,140,1)\",\"rgba(66,63,133,1)\",\"rgba(70,12,95,1)\",\"rgba(70,8,92,1)\",\"rgba(70,12,95,1)\",\"rgba(71,17,100,1)\",\"rgba(72,36,117,1)\",\"rgba(70,7,90,1)\",\"rgba(70,11,94,1)\",\"rgba(67,62,133,1)\",\"rgba(34,141,141,1)\",\"rgba(53,95,141,1)\",\"rgba(70,7,90,1)\",\"rgba(70,11,94,1)\",\"rgba(63,72,137,1)\",\"rgba(72,41,121,1)\",\"rgba(72,22,104,1)\",\"rgba(34,139,141,1)\",\"rgba(72,25,107,1)\",\"rgba(72,41,121,1)\",\"rgba(72,28,110,1)\",\"rgba(41,122,142,1)\",\"rgba(72,25,107,1)\",\"rgba(72,36,117,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(66,63,133,1)\",\"rgba(62,73,137,1)\",\"rgba(72,33,114,1)\",\"rgba(72,20,103,1)\",\"rgba(68,57,131,1)\",\"rgba(39,126,142,1)\",\"rgba(72,36,117,1)\",\"rgba(37,133,142,1)\",\"rgba(71,17,100,1)\",\"rgba(67,62,133,1)\",\"rgba(66,63,133,1)\",\"rgba(69,55,129,1)\",\"rgba(70,9,92,1)\",\"rgba(72,27,109,1)\",\"rgba(72,20,103,1)\",\"rgba(72,40,120,1)\",\"rgba(70,9,93,1)\",\"rgba(71,46,124,1)\",\"rgba(71,13,96,1)\",\"rgba(71,46,124,1)\",\"rgba(71,16,99,1)\",\"rgba(72,26,108,1)\",\"rgba(71,18,100,1)\",\"rgba(72,34,115,1)\",\"rgba(72,23,105,1)\",\"rgba(51,98,141,1)\",\"rgba(70,7,90,1)\",\"rgba(72,30,112,1)\",\"rgba(37,132,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,15,98,1)\",\"rgba(72,41,121,1)\",\"rgba(70,8,91,1)\",\"rgba(46,110,142,1)\",\"rgba(72,29,111,1)\",\"rgba(72,33,115,1)\",\"rgba(31,149,139,1)\",\"rgba(61,78,138,1)\",\"rgba(71,46,124,1)\",\"rgba(72,20,103,1)\",\"rgba(65,67,135,1)\",\"rgba(72,20,102,1)\",\"rgba(66,63,133,1)\",\"rgba(62,75,137,1)\",\"rgba(70,8,92,1)\",\"rgba(70,7,91,1)\",\"rgba(72,28,110,1)\",\"rgba(57,87,140,1)\",\"rgba(72,22,104,1)\",\"rgba(71,15,98,1)\",\"rgba(54,93,141,1)\",\"rgba(70,9,93,1)\",\"rgba(70,7,90,1)\",\"rgba(71,16,99,1)\",\"rgba(72,29,111,1)\",\"rgba(31,158,137,1)\",\"rgba(69,4,87,1)\",\"rgba(68,59,132,1)\",\"rgba(70,7,90,1)\",\"rgba(72,27,109,1)\",\"rgba(72,30,111,1)\",\"rgba(71,44,122,1)\",\"rgba(72,38,119,1)\",\"rgba(69,4,88,1)\",\"rgba(72,36,117,1)\",\"rgba(72,29,111,1)\",\"rgba(62,73,137,1)\",\"rgba(46,110,142,1)\",\"rgba(68,1,84,1)\",\"rgba(72,24,106,1)\",\"rgba(70,10,93,1)\",\"rgba(41,121,142,1)\",\"rgba(71,13,96,1)\",\"rgba(71,46,124,1)\",\"rgba(72,27,109,1)\",\"rgba(71,15,98,1)\",\"rgba(72,28,110,1)\",\"rgba(71,18,100,1)\",\"rgba(72,23,105,1)\",\"rgba(71,16,99,1)\",\"rgba(69,4,88,1)\",\"rgba(64,70,136,1)\",\"rgba(70,8,92,1)\",\"rgba(72,32,113,1)\",\"rgba(72,31,112,1)\",\"rgba(70,11,94,1)\",\"rgba(44,115,142,1)\",\"rgba(68,2,86,1)\",\"rgba(72,28,110,1)\",\"rgba(41,122,142,1)\",\"rgba(66,65,134,1)\",\"rgba(71,19,101,1)\",\"rgba(72,23,105,1)\",\"rgba(72,27,109,1)\",\"rgba(71,17,100,1)\",\"rgba(71,13,96,1)\",\"rgba(72,23,105,1)\",\"rgba(71,17,100,1)\",\"rgba(68,57,131,1)\",\"rgba(71,19,102,1)\",\"rgba(72,36,117,1)\",\"rgba(72,40,120,1)\",\"rgba(70,7,90,1)\",\"rgba(70,52,128,1)\",\"rgba(70,7,91,1)\",\"rgba(71,15,98,1)\",\"rgba(70,11,94,1)\",\"rgba(71,13,96,1)\",\"rgba(72,28,110,1)\",\"rgba(58,84,140,1)\",\"rgba(71,18,100,1)\",\"rgba(66,64,134,1)\",\"rgba(70,49,126,1)\",\"rgba(72,30,111,1)\",\"rgba(72,28,110,1)\",\"rgba(71,15,98,1)\",\"rgba(71,14,97,1)\",\"rgba(71,18,100,1)\",\"rgba(72,27,109,1)\",\"rgba(72,28,110,1)\",\"rgba(72,35,116,1)\",\"rgba(63,72,137,1)\",\"rgba(70,48,126,1)\",\"rgba(55,90,140,1)\",\"rgba(72,32,114,1)\",\"rgba(72,41,121,1)\",\"rgba(59,81,139,1)\",\"rgba(71,47,125,1)\",\"rgba(72,24,106,1)\",\"rgba(37,133,142,1)\",\"rgba(70,6,90,1)\",\"rgba(72,33,114,1)\",\"rgba(72,36,117,1)\",\"rgba(72,31,112,1)\",\"rgba(72,28,110,1)\",\"rgba(72,24,106,1)\",\"rgba(46,110,142,1)\",\"rgba(70,7,90,1)\",\"rgba(72,24,106,1)\",\"rgba(71,16,99,1)\",\"rgba(71,13,96,1)\",\"rgba(70,7,91,1)\",\"rgba(72,39,119,1)\",\"rgba(71,43,122,1)\",\"rgba(72,25,107,1)\",\"rgba(72,27,109,1)\",\"rgba(70,7,90,1)\",\"rgba(70,10,93,1)\",\"rgba(68,58,131,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(72,20,103,1)\",\"rgba(72,32,113,1)\",\"rgba(70,11,94,1)\",\"rgba(69,54,129,1)\",\"rgba(69,53,129,1)\",\"rgba(72,27,109,1)\",\"rgba(71,16,99,1)\",\"rgba(72,27,109,1)\",\"rgba(72,33,115,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(67,62,133,1)\",\"rgba(68,57,131,1)\",\"rgba(70,49,126,1)\"],\"opacity\":0.5,\"size\":[5.85498525186645,5.75530441896398,4.81725640546078,7.08156052852364,3.77952755905512,9.9842436220335,4.99637249295625,6.1572630538017,6.1287861523447,8.67448473764607,4.81725640546078,4.93974368044287,4.75023400996066,6.04120514519478,11.800976881346,5.42032093320559,4.81725640546078,5.05048064158557,5.7209404608662,5.57692664550957,5.46084045125838,5.20049630343061,4.59992424613035,4.51331266338146,5.42032093320559,5.15231378700878,6.24071762028082,4.41500410032034,9.30732123076958,8.33254439454472,4.59992424613035,5.61399031987096,5.50040616788309,4.29839198225795,6.3477754241973,6.80500105069749,5.46084045125838,7.20167346483616,6.32143372411601,8.07389478160306,4.29839198225795,5.85498525186645,5.95009316947746,4.29839198225795,9.08363165588957,7.39300013384823,5.75530441896398,5.7890808290592,5.5390824263879,5.88716480950467,10.9409844765614,5.91886038129558,6.45054565636157,4.67822710228234,9.18419234317619,9.22142353961344,7.88151099443129,4.88020521554463,5.98088287203413,5.15231378700878,5.05048064158557,6.42522737787542,4.29839198225795,4.51331266338146,4.81725640546078,8.16692436252423,5.61399031987096,6.24071762028082,5.15231378700878,4.75023400996066,4.59992424613035,5.65031984192698,5.46084045125838,5.2922643048763,4.75023400996066,5.7890808290592,5.75530441896398,5.05048064158557,13.9735960212014,6.04120514519478,5.7890808290592,5.33612082866362,8.16692436252423,13.2479147743441,7.60999825947506,4.88020521554463,6.89271409827211,5.85498525186645,5.5390824263879,5.61399031987096,4.81725640546078,7.53905747746838,5.10237746846527,5.75530441896398,7.18194911665511,5.61399031987096,4.81725640546078,4.29839198225795,4.14642011121829,6.89271409827211,4.81725640546078,4.29839198225795,10.663041994367,5.10237746846527,9.27067247086978,10.016700945829,9.9842436220335,5.46084045125838,4.75023400996066,5.95009316947746,5.68595718285079,5.88716480950467,5.50040616788309,15.9976555097511,5.57692664550957,5.88716480950467,4.81725640546078,4.41500410032034,5.57692664550957,4.14642011121829,4.93974368044287,7.50308028699906,4.51331266338146,4.88020521554463,7.27945944121871,7.29863729372068,6.6684424037042,9.55735724835328,5.75530441896398,3.77952755905512,6.6214650478061,5.33612082866362,4.93974368044287,5.2470977677078,5.68595718285079,6.04120514519478,4.93974368044287,5.10237746846527,5.88716480950467,6.37384967506928,6.1572630538017,6.37384967506928,7.97880475556136,4.99637249295625,5.10237746846527,7.53905747746838,6.24071762028082,6.6684424037042,4.59992424613035,6.84917088333504,5.42032093320559,5.75530441896398,3.77952755905512,4.88020521554463,5.88716480950467,7.01983484339694,6.26791391617673,4.99637249295625,5.10237746846527,6.1287861523447,5.68595718285079,6.1572630538017,4.51331266338146,5.65031984192698,5.5390824263879,4.51331266338146,7.67964888615596,4.99637249295625,4.14642011121829,6.32143372411601,4.67822710228234,4.88020521554463,4.41500410032034,7.62752910941972,4.99637249295625,8.15155690093495,6.09995980183062,5.95009316947746,5.05048064158557,5.10237746846527,4.41500410032034,6.3477754241973,5.61399031987096,3.77952755905512,5.46084045125838,4.29839198225795,8.83679238485105,4.51331266338146,6.24071762028082,5.7209404608662,5.82229883597697,4.75023400996066,6.47562618873679,6.32143372411601,5.3787751170721,4.41500410032034,5.57692664550957,5.85498525186645,5.2470977677078,5.68595718285079,7.27945944121871,5.42032093320559,5.15231378700878,4.29839198225795,5.98088287203413,4.67822710228234,7.12207778622041,4.88020521554463,8.74275800951811,8.83679238485105,10.4947638556414,5.57692664550957,7.06111430735605,6.3477754241973,5.98088287203413,6.04120514519478,4.59992424613035,10.702042388254,4.59992424613035,4.59992424613035,5.3787751170721,8.16692436252423,8.19749892618241,5.10237746846527,6.09995980183062,4.41500410032034,4.99637249295625,5.05048064158557,4.99637249295625,4.59992424613035,6.37384967506928,5.75530441896398,4.29839198225795,7.01983484339694,5.50040616788309,6.57369799056115,4.41500410032034,5.2470977677078,5.82229883597697,7.04053989294647,6.8710191542449,6.50047554933428,6.45054565636157,6.73750988052566,4.99637249295625,5.88716480950467,5.33612082866362,6.42522737787542,5.91886038129558,5.2922643048763,5.82229883597697,6.9356566308181,4.75023400996066,5.57692664550957,6.37384967506928,8.05819320353604,7.7818527876071,7.55691757760989,6.1572630538017,6.78267252788327,4.29839198225795,7.94662591203789,6.09995980183062,4.75023400996066,5.2470977677078,6.64504999568007,6.01124782799873,4.75023400996066,6.52510001496245,6.52510001496245,4.75023400996066,6.32143372411601,5.7209404608662,4.99637249295625,6.21321742685738,8.49234772297105,4.88020521554463,7.74807728728558,6.45054565636157,4.14642011121829,6.3477754241973,6.3477754241973,5.33612082866362,4.67822710228234,7.60999825947506,5.46084045125838,6.6684424037042,6.52510001496245,5.20049630343061,6.3477754241973,3.77952755905512,5.2922643048763,6.8710191542449,4.75023400996066,5.15231378700878,7.50308028699906,5.7890808290592,6.42522737787542,5.91886038129558,4.14642011121829,7.08156052852364,7.7818527876071,3.77952755905512,4.67822710228234,5.42032093320559,4.88020521554463,7.67964888615596,5.85498525186645,4.81725640546078,9.56899437935629,5.61399031987096,5.10237746846527,4.51331266338146,5.5390824263879,9.40388388095622,4.51331266338146,12.9738285632099,4.93974368044287,12.407355216846,5.2470977677078,5.3787751170721,4.51331266338146,7.37432573196402,4.88020521554463,5.7209404608662,6.52510001496245,4.93974368044287,5.42032093320559,6.26791391617673,4.99637249295625,9.88583660124146,6.1287861523447,5.2922643048763,5.10237746846527,4.93974368044287,6.71466797636047,4.29839198225795,5.2922643048763,6.18540291488937,4.93974368044287,12.7740090363008,4.41500410032034,5.05048064158557,4.99637249295625,8.61917271143022,4.93974368044287,4.67822710228234,5.57692664550957,4.29839198225795,5.68595718285079,6.59768272602791,5.5390824263879,4.41500410032034,4.75023400996066,5.7890808290592,4.67822710228234,4.29839198225795,7.35555381163162,4.81725640546078,7.81534563284998,4.67822710228234,6.78267252788327,5.46084045125838,5.7209404608662,6.29481617214543,7.60999825947506,5.61399031987096,5.46084045125838,4.81725640546078,4.99637249295625,5.5390824263879,4.99637249295625,8.74275800951811,6.54950558342098,5.82229883597697,5.85498525186645,7.71401185255184,5.05048064158557,4.29839198225795,5.88716480950467,7.84856280275047,7.69686804858876,4.75023400996066,5.15231378700878,5.2922643048763,4.67822710228234,5.50040616788309,5.20049630343061,5.61399031987096,4.81725640546078,5.20049630343061,4.93974368044287,4.93974368044287,7.31771119892611,4.51331266338146,4.81725640546078,4.75023400996066,5.91886038129558,4.59992424613035,6.59768272602791,7.31771119892611,6.6214650478061,5.10237746846527,4.29839198225795,5.3787751170721,4.99637249295625,4.67822710228234,4.41500410032034,4.51331266338146,6.78267252788327,6.39966446129382,7.48496070147531,6.07077081294139,5.2922643048763,3.77952755905512,4.88020521554463,5.3787751170721,10.2808801473632,6.09995980183062,4.59992424613035,6.47562618873679,5.33612082866362,8.63305981358281,5.05048064158557,5.46084045125838,7.24078497365455,5.2922643048763,5.85498525186645,7.48496070147531,6.32143372411601,4.41500410032034,8.81010478523574,5.15231378700878,22.6771653543307,7.53905747746838,7.46675207461058,5.2470977677078,4.81725640546078,6.64504999568007,6.37384967506928,7.24078497365455,6.01124782799873,5.2470977677078,9.72952487762825,6.78267252788327,5.10237746846527,4.93974368044287,5.10237746846527,5.5390824263879,8.31773779651867,5.33612082866362,6.57369799056115,5.10237746846527,6.6214650478061,5.2470977677078,4.75023400996066,4.67822710228234,5.50040616788309,4.75023400996066,4.41500410032034,5.7890808290592,7.18194911665511,6.9356566308181,8.74275800951811,5.88716480950467,5.7890808290592,4.75023400996066,4.59992424613035,8.34730299673642,5.05048064158557,4.99637249295625,5.3787751170721,5.65031984192698,4.99637249295625,7.62752910941972,7.12207778622041,5.10237746846527,5.95009316947746,5.65031984192698,5.05048064158557,4.51331266338146,5.42032093320559,5.15231378700878,5.05048064158557,7.12207778622041,11.2536970852041,10.2393375328987,4.81725640546078,7.69686804858876,5.65031984192698,5.3787751170721,5.2922643048763,5.15231378700878,6.64504999568007,6.64504999568007,4.29839198225795,6.73750988052566,7.53905747746838,5.42032093320559,6.50047554933428,4.93974368044287,6.21321742685738,6.01124782799873,5.20049630343061,4.81725640546078,5.10237746846527,4.93974368044287,6.07077081294139,4.51331266338146,5.2470977677078,7.20167346483616,4.75023400996066,4.88020521554463,7.91419663816726,5.05048064158557,6.57369799056115,7.83198825221453,5.05048064158557,7.01983484339694,7.64498045314673,11.8594978711237,5.65031984192698,6.21321742685738,4.51331266338146,7.55691757760989,6.99899663828633,4.51331266338146,5.5390824263879,5.57692664550957,4.75023400996066,5.68595718285079,7.10188092320679,4.88020521554463,6.18540291488937,4.67822710228234,4.29839198225795,10.9127340432949,5.82229883597697,5.82229883597697,6.71466797636047,6.64504999568007,9.99508156228079,5.7209404608662,9.4158378930007,4.88020521554463,4.75023400996066,5.10237746846527,4.67822710228234,6.29481617214543,5.61399031987096,9.31948360778685,6.1287861523447,5.7209404608662,8.12065877989981,4.29839198225795,5.05048064158557,5.42032093320559,7.7818527876071,6.09995980183062,5.46084045125838,6.1572630538017,4.29839198225795,9.7521055937645,4.29839198225795,5.82229883597697,4.29839198225795,6.95691026538124,11.1357003562906,4.93974368044287,4.99637249295625,4.88020521554463,7.76500081705985,5.05048064158557,6.78267252788327,4.99637249295625,6.47562618873679,4.67822710228234,7.01983484339694,4.29839198225795,5.88716480950467,4.88020521554463,6.71466797636047,7.64498045314673,5.46084045125838,9.89684888979454,5.2922643048763,5.46084045125838,7.37432573196402,5.65031984192698,4.41500410032034,5.57692664550957,7.14215334346163,4.99637249295625,5.65031984192698,8.04243379218159,7.35555381163162,5.75530441896398,4.14642011121829,4.51331266338146,5.7890808290592,5.2470977677078,5.15231378700878,5.82229883597697,6.71466797636047,5.42032093320559,6.29481617214543,7.96274627533861,5.20049630343061,7.55691757760989,5.82229883597697,5.75530441896398,5.46084045125838,6.6214650478061,5.3787751170721,6.69164691177173,5.2922643048763,5.20049630343061,4.75023400996066,7.18194911665511,5.15231378700878,4.99637249295625,6.91425889886634,5.46084045125838,4.29839198225795,4.81725640546078,3.77952755905512,4.75023400996066,4.75023400996066,8.16692436252423,6.52510001496245,5.20049630343061,5.7890808290592,5.42032093320559,4.41500410032034,4.41500410032034,6.21321742685738,5.20049630343061,4.41500410032034,5.2470977677078,8.18223818501315,7.01983484339694,4.51331266338146,6.26791391617673,8.96817179108344,6.6684424037042,8.76980578717714,5.75530441896398,5.33612082866362,7.12207778622041,8.67448473764607,4.51331266338146,5.98088287203413,5.85498525186645,4.59992424613035,5.46084045125838,6.50047554933428,5.61399031987096,4.93974368044287,5.15231378700878,5.15231378700878,4.14642011121829,5.65031984192698,4.59992424613035,5.05048064158557,6.76017674320361,5.05048064158557,4.51331266338146,6.09995980183062,6.18540291488937,5.50040616788309,4.67822710228234,5.57692664550957,6.82716598798061,4.59992424613035,6.54950558342098,4.99637249295625,5.10237746846527,6.59768272602791,4.29839198225795,4.59992424613035,7.57469362882203,6.8710191542449,7.10188092320679,7.57469362882203,6.39966446129382,5.3787751170721,5.7209404608662,8.52082423687209,4.81725640546078,5.3787751170721,4.51331266338146,6.1287861523447,4.88020521554463,6.32143372411601,6.24071762028082,4.14642011121829,7.4300623607585,4.59992424613035,5.85498525186645,9.88583660124146,4.29839198225795,5.61399031987096,3.77952755905512,5.05048064158557,4.75023400996066,4.67822710228234,4.99637249295625,9.39190440767069,5.42032093320559,4.29839198225795,5.50040616788309,7.48496070147531,5.61399031987096,4.75023400996066,4.41500410032034,7.24078497365455,4.88020521554463,9.2584017406471,5.05048064158557,5.05048064158557,7.27945944121871,4.14642011121829,5.20049630343061,6.32143372411601,5.10237746846527,5.05048064158557,4.81725640546078,4.99637249295625,5.05048064158557,4.75023400996066,6.3477754241973,10.2497481923104,4.51331266338146,5.91886038129558,7.53905747746838,4.67822710228234,4.67822710228234,5.05048064158557,5.3787751170721,4.59992424613035,7.29863729372068,5.05048064158557,5.2470977677078,4.93974368044287,4.81725640546078,5.7890808290592,4.59992424613035,6.3477754241973,5.20049630343061,5.05048064158557,5.7890808290592,5.10237746846527,4.51331266338146,4.93974368044287,4.14642011121829,6.07077081294139,4.81725640546078,4.75023400996066,4.51331266338146,5.50040616788309,6.04120514519478,4.51331266338146,5.33612082866362,4.51331266338146,5.61399031987096,4.67822710228234,8.76980578717714,9.86375222856076,6.42522737787542,4.41500410032034,5.10237746846527,14.6323556111669,4.75023400996066,7.52111212479883,7.73108127887284,7.10188092320679,4.81725640546078,20.4882510406151,7.88151099443129,5.2470977677078,5.50040616788309,4.88020521554463,5.15231378700878,4.88020521554463,4.59992424613035,7.24078497365455,5.85498525186645,4.88020521554463,6.26791391617673,4.41500410032034,3.77952755905512,4.41500410032034,5.2922643048763,4.51331266338146,4.51331266338146,5.20049630343061,5.75530441896398,4.75023400996066,5.7890808290592,9.64980839366583,4.41500410032034,4.99637249295625,5.05048064158557,9.07092719669573,4.14642011121829,5.20049630343061,4.93974368044287,7.27945944121871,8.54913073717632],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":[\"rgba(72,23,105,1)\",\"rgba(64,69,136,1)\",\"rgba(70,48,126,1)\",\"rgba(44,113,142,1)\",\"rgba(70,7,90,1)\",\"rgba(52,182,121,1)\",\"rgba(72,32,113,1)\",\"rgba(72,20,102,1)\",\"rgba(71,13,96,1)\",\"rgba(33,144,141,1)\",\"rgba(72,27,109,1)\",\"rgba(71,12,95,1)\",\"rgba(70,11,94,1)\",\"rgba(72,33,114,1)\",\"rgba(72,30,112,1)\",\"rgba(70,51,127,1)\",\"rgba(70,52,128,1)\",\"rgba(72,31,112,1)\",\"rgba(71,13,96,1)\",\"rgba(72,26,108,1)\",\"rgba(52,96,141,1)\",\"rgba(72,34,116,1)\",\"rgba(72,26,108,1)\",\"rgba(72,36,117,1)\",\"rgba(44,113,142,1)\",\"rgba(72,29,111,1)\",\"rgba(31,149,139,1)\",\"rgba(72,41,121,1)\",\"rgba(69,55,129,1)\",\"rgba(72,28,110,1)\",\"rgba(71,45,123,1)\",\"rgba(71,46,124,1)\",\"rgba(70,9,93,1)\",\"rgba(71,17,100,1)\",\"rgba(72,35,116,1)\",\"rgba(49,104,142,1)\",\"rgba(72,41,121,1)\",\"rgba(36,134,142,1)\",\"rgba(72,20,103,1)\",\"rgba(44,115,142,1)\",\"rgba(72,29,111,1)\",\"rgba(63,71,136,1)\",\"rgba(69,6,89,1)\",\"rgba(72,29,111,1)\",\"rgba(72,28,110,1)\",\"rgba(70,9,93,1)\",\"rgba(54,92,141,1)\",\"rgba(69,55,129,1)\",\"rgba(72,22,104,1)\",\"rgba(72,26,108,1)\",\"rgba(72,33,115,1)\",\"rgba(71,47,125,1)\",\"rgba(41,121,142,1)\",\"rgba(68,2,85,1)\",\"rgba(31,162,135,1)\",\"rgba(72,41,121,1)\",\"rgba(54,92,141,1)\",\"rgba(69,53,129,1)\",\"rgba(64,70,136,1)\",\"rgba(47,108,142,1)\",\"rgba(72,37,118,1)\",\"rgba(72,32,113,1)\",\"rgba(71,17,100,1)\",\"rgba(72,28,110,1)\",\"rgba(70,52,128,1)\",\"rgba(72,41,121,1)\",\"rgba(68,59,132,1)\",\"rgba(46,111,142,1)\",\"rgba(71,47,125,1)\",\"rgba(71,44,122,1)\",\"rgba(67,62,133,1)\",\"rgba(68,59,132,1)\",\"rgba(70,50,126,1)\",\"rgba(56,87,140,1)\",\"rgba(71,46,124,1)\",\"rgba(70,8,92,1)\",\"rgba(60,80,139,1)\",\"rgba(72,41,121,1)\",\"rgba(61,78,138,1)\",\"rgba(72,27,109,1)\",\"rgba(72,34,115,1)\",\"rgba(72,41,121,1)\",\"rgba(70,7,91,1)\",\"rgba(71,44,122,1)\",\"rgba(72,23,105,1)\",\"rgba(72,30,111,1)\",\"rgba(70,7,90,1)\",\"rgba(71,45,123,1)\",\"rgba(69,56,130,1)\",\"rgba(68,1,84,1)\",\"rgba(64,70,136,1)\",\"rgba(70,8,92,1)\",\"rgba(55,90,140,1)\",\"rgba(70,9,92,1)\",\"rgba(31,158,137,1)\",\"rgba(69,55,129,1)\",\"rgba(72,29,111,1)\",\"rgba(71,17,100,1)\",\"rgba(70,9,93,1)\",\"rgba(71,15,98,1)\",\"rgba(72,29,111,1)\",\"rgba(72,29,111,1)\",\"rgba(71,18,101,1)\",\"rgba(72,28,110,1)\",\"rgba(70,51,127,1)\",\"rgba(253,231,37,1)\",\"rgba(68,59,132,1)\",\"rgba(52,96,141,1)\",\"rgba(72,30,111,1)\",\"rgba(72,24,106,1)\",\"rgba(71,47,125,1)\",\"rgba(46,109,142,1)\",\"rgba(62,76,138,1)\",\"rgba(72,39,120,1)\",\"rgba(71,17,100,1)\",\"rgba(59,81,139,1)\",\"rgba(70,8,92,1)\",\"rgba(72,41,121,1)\",\"rgba(72,20,102,1)\",\"rgba(70,9,93,1)\",\"rgba(49,104,142,1)\",\"rgba(53,94,141,1)\",\"rgba(72,28,110,1)\",\"rgba(72,31,112,1)\",\"rgba(71,42,122,1)\",\"rgba(32,163,134,1)\",\"rgba(60,80,139,1)\",\"rgba(41,121,142,1)\",\"rgba(71,15,98,1)\",\"rgba(70,7,90,1)\",\"rgba(59,81,139,1)\",\"rgba(71,13,96,1)\",\"rgba(68,60,132,1)\",\"rgba(59,82,139,1)\",\"rgba(71,12,95,1)\",\"rgba(70,7,91,1)\",\"rgba(71,12,95,1)\",\"rgba(72,31,112,1)\",\"rgba(71,45,123,1)\",\"rgba(62,73,137,1)\",\"rgba(54,92,141,1)\",\"rgba(51,98,141,1)\",\"rgba(72,38,119,1)\",\"rgba(72,23,105,1)\",\"rgba(68,57,131,1)\",\"rgba(70,11,94,1)\",\"rgba(32,147,140,1)\",\"rgba(72,26,108,1)\",\"rgba(72,33,115,1)\",\"rgba(72,24,106,1)\",\"rgba(51,98,141,1)\",\"rgba(70,52,128,1)\",\"rgba(70,7,90,1)\",\"rgba(72,29,111,1)\",\"rgba(70,51,127,1)\",\"rgba(71,44,122,1)\",\"rgba(72,26,108,1)\",\"rgba(72,22,104,1)\",\"rgba(72,25,107,1)\",\"rgba(70,11,94,1)\",\"rgba(72,39,120,1)\",\"rgba(71,14,97,1)\",\"rgba(72,28,110,1)\",\"rgba(71,17,100,1)\",\"rgba(51,99,141,1)\",\"rgba(71,18,100,1)\",\"rgba(30,157,137,1)\",\"rgba(71,16,99,1)\",\"rgba(70,9,93,1)\",\"rgba(70,9,92,1)\",\"rgba(72,39,120,1)\",\"rgba(72,29,111,1)\",\"rgba(72,27,109,1)\",\"rgba(67,62,133,1)\",\"rgba(55,91,141,1)\",\"rgba(171,220,50,1)\",\"rgba(70,7,90,1)\",\"rgba(72,37,118,1)\",\"rgba(71,15,98,1)\",\"rgba(71,47,125,1)\",\"rgba(72,27,109,1)\",\"rgba(32,146,140,1)\",\"rgba(68,58,131,1)\",\"rgba(70,7,90,1)\",\"rgba(51,99,141,1)\",\"rgba(71,17,100,1)\",\"rgba(46,109,142,1)\",\"rgba(72,28,110,1)\",\"rgba(46,111,142,1)\",\"rgba(68,2,86,1)\",\"rgba(71,45,123,1)\",\"rgba(72,41,121,1)\",\"rgba(51,99,141,1)\",\"rgba(33,144,141,1)\",\"rgba(72,30,111,1)\",\"rgba(70,10,93,1)\",\"rgba(72,34,115,1)\",\"rgba(72,29,111,1)\",\"rgba(71,17,100,1)\",\"rgba(70,51,127,1)\",\"rgba(70,49,126,1)\",\"rgba(69,4,88,1)\",\"rgba(72,22,104,1)\",\"rgba(72,29,111,1)\",\"rgba(39,126,142,1)\",\"rgba(72,23,105,1)\",\"rgba(72,32,113,1)\",\"rgba(71,16,99,1)\",\"rgba(71,45,123,1)\",\"rgba(68,3,86,1)\",\"rgba(69,55,129,1)\",\"rgba(70,51,127,1)\",\"rgba(70,11,94,1)\",\"rgba(40,174,128,1)\",\"rgba(72,26,108,1)\",\"rgba(47,107,142,1)\",\"rgba(72,36,117,1)\",\"rgba(72,35,116,1)\",\"rgba(70,7,90,1)\",\"rgba(72,36,117,1)\",\"rgba(63,72,137,1)\",\"rgba(41,121,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,18,100,1)\",\"rgba(58,84,140,1)\",\"rgba(72,27,109,1)\",\"rgba(64,70,136,1)\",\"rgba(71,46,124,1)\",\"rgba(72,37,118,1)\",\"rgba(67,62,133,1)\",\"rgba(72,40,120,1)\",\"rgba(69,53,129,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,101,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(72,27,109,1)\",\"rgba(63,72,137,1)\",\"rgba(71,42,122,1)\",\"rgba(70,51,127,1)\",\"rgba(70,10,93,1)\",\"rgba(66,65,134,1)\",\"rgba(72,24,106,1)\",\"rgba(34,140,141,1)\",\"rgba(72,38,119,1)\",\"rgba(70,50,126,1)\",\"rgba(72,37,118,1)\",\"rgba(70,11,94,1)\",\"rgba(49,104,142,1)\",\"rgba(72,32,113,1)\",\"rgba(71,42,122,1)\",\"rgba(47,108,142,1)\",\"rgba(71,44,122,1)\",\"rgba(72,20,102,1)\",\"rgba(72,37,118,1)\",\"rgba(71,45,123,1)\",\"rgba(58,84,140,1)\",\"rgba(37,171,130,1)\",\"rgba(63,72,137,1)\",\"rgba(72,38,119,1)\",\"rgba(71,17,100,1)\",\"rgba(72,26,108,1)\",\"rgba(72,32,113,1)\",\"rgba(70,11,94,1)\",\"rgba(72,26,108,1)\",\"rgba(63,72,137,1)\",\"rgba(71,16,99,1)\",\"rgba(72,24,106,1)\",\"rgba(69,6,89,1)\",\"rgba(37,132,142,1)\",\"rgba(72,41,121,1)\",\"rgba(72,32,113,1)\",\"rgba(70,49,126,1)\",\"rgba(72,37,118,1)\",\"rgba(71,18,100,1)\",\"rgba(72,24,106,1)\",\"rgba(71,16,99,1)\",\"rgba(72,22,104,1)\",\"rgba(50,101,142,1)\",\"rgba(70,9,93,1)\",\"rgba(72,27,109,1)\",\"rgba(32,146,140,1)\",\"rgba(55,91,141,1)\",\"rgba(72,23,105,1)\",\"rgba(57,86,140,1)\",\"rgba(70,49,126,1)\",\"rgba(32,146,140,1)\",\"rgba(65,66,135,1)\",\"rgba(72,25,107,1)\",\"rgba(50,101,142,1)\",\"rgba(70,7,90,1)\",\"rgba(72,20,103,1)\",\"rgba(42,119,142,1)\",\"rgba(72,41,121,1)\",\"rgba(71,47,125,1)\",\"rgba(112,207,87,1)\",\"rgba(41,123,142,1)\",\"rgba(70,9,93,1)\",\"rgba(55,90,140,1)\",\"rgba(70,9,93,1)\",\"rgba(68,191,112,1)\",\"rgba(42,118,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,19,101,1)\",\"rgba(54,93,141,1)\",\"rgba(72,30,111,1)\",\"rgba(71,45,123,1)\",\"rgba(63,71,136,1)\",\"rgba(72,36,117,1)\",\"rgba(35,136,142,1)\",\"rgba(65,68,135,1)\",\"rgba(72,36,117,1)\",\"rgba(72,28,110,1)\",\"rgba(50,100,142,1)\",\"rgba(50,182,122,1)\",\"rgba(72,36,117,1)\",\"rgba(72,35,116,1)\",\"rgba(70,10,93,1)\",\"rgba(68,3,86,1)\",\"rgba(57,85,140,1)\",\"rgba(72,33,115,1)\",\"rgba(72,28,110,1)\",\"rgba(71,12,95,1)\",\"rgba(71,17,100,1)\",\"rgba(70,50,126,1)\",\"rgba(39,127,142,1)\",\"rgba(71,19,102,1)\",\"rgba(72,33,115,1)\",\"rgba(72,21,104,1)\",\"rgba(72,38,119,1)\",\"rgba(61,77,138,1)\",\"rgba(72,20,103,1)\",\"rgba(72,27,109,1)\",\"rgba(71,18,100,1)\",\"rgba(72,34,115,1)\",\"rgba(53,94,141,1)\",\"rgba(71,17,100,1)\",\"rgba(71,44,122,1)\",\"rgba(52,96,141,1)\",\"rgba(70,7,90,1)\",\"rgba(68,2,85,1)\",\"rgba(69,5,89,1)\",\"rgba(71,46,124,1)\",\"rgba(55,91,141,1)\",\"rgba(71,14,97,1)\",\"rgba(68,60,132,1)\",\"rgba(68,2,85,1)\",\"rgba(49,104,142,1)\",\"rgba(71,17,100,1)\",\"rgba(71,47,125,1)\",\"rgba(68,2,85,1)\",\"rgba(72,31,112,1)\",\"rgba(72,27,109,1)\",\"rgba(72,30,111,1)\",\"rgba(63,71,136,1)\",\"rgba(71,19,101,1)\",\"rgba(71,17,100,1)\",\"rgba(69,5,89,1)\",\"rgba(70,52,128,1)\",\"rgba(72,30,112,1)\",\"rgba(69,53,129,1)\",\"rgba(39,126,142,1)\",\"rgba(72,20,102,1)\",\"rgba(46,111,142,1)\",\"rgba(33,143,141,1)\",\"rgba(49,104,142,1)\",\"rgba(72,41,121,1)\",\"rgba(72,41,121,1)\",\"rgba(72,29,111,1)\",\"rgba(71,13,96,1)\",\"rgba(53,94,141,1)\",\"rgba(72,37,118,1)\",\"rgba(72,29,111,1)\",\"rgba(32,163,134,1)\",\"rgba(71,44,122,1)\",\"rgba(69,3,87,1)\",\"rgba(60,79,138,1)\",\"rgba(71,42,122,1)\",\"rgba(71,17,100,1)\",\"rgba(72,23,105,1)\",\"rgba(71,14,97,1)\",\"rgba(35,137,142,1)\",\"rgba(72,24,106,1)\",\"rgba(72,32,113,1)\",\"rgba(72,23,105,1)\",\"rgba(72,33,115,1)\",\"rgba(59,82,139,1)\",\"rgba(70,7,91,1)\",\"rgba(72,20,103,1)\",\"rgba(70,48,126,1)\",\"rgba(72,33,115,1)\",\"rgba(72,34,115,1)\",\"rgba(71,18,100,1)\",\"rgba(72,26,108,1)\",\"rgba(70,10,93,1)\",\"rgba(70,49,126,1)\",\"rgba(67,62,133,1)\",\"rgba(72,21,104,1)\",\"rgba(72,36,117,1)\",\"rgba(51,98,141,1)\",\"rgba(71,13,96,1)\",\"rgba(60,79,138,1)\",\"rgba(71,17,100,1)\",\"rgba(71,17,100,1)\",\"rgba(70,50,126,1)\",\"rgba(70,12,95,1)\",\"rgba(70,12,95,1)\",\"rgba(72,22,104,1)\",\"rgba(70,10,93,1)\",\"rgba(70,8,92,1)\",\"rgba(69,56,130,1)\",\"rgba(72,30,111,1)\",\"rgba(50,101,142,1)\",\"rgba(61,77,138,1)\",\"rgba(70,7,90,1)\",\"rgba(72,30,111,1)\",\"rgba(53,94,141,1)\",\"rgba(46,179,124,1)\",\"rgba(64,69,136,1)\",\"rgba(72,33,115,1)\",\"rgba(41,123,142,1)\",\"rgba(55,91,141,1)\",\"rgba(72,29,111,1)\",\"rgba(70,7,91,1)\",\"rgba(65,66,135,1)\",\"rgba(71,19,101,1)\",\"rgba(72,23,105,1)\",\"rgba(53,95,141,1)\",\"rgba(43,116,142,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(71,15,98,1)\",\"rgba(59,81,139,1)\",\"rgba(68,2,86,1)\",\"rgba(72,27,109,1)\",\"rgba(70,7,90,1)\",\"rgba(72,37,118,1)\",\"rgba(71,13,96,1)\",\"rgba(46,111,142,1)\",\"rgba(70,10,93,1)\",\"rgba(69,55,129,1)\",\"rgba(65,68,135,1)\",\"rgba(72,37,118,1)\",\"rgba(67,61,132,1)\",\"rgba(71,45,123,1)\",\"rgba(72,21,103,1)\",\"rgba(71,18,100,1)\",\"rgba(71,17,100,1)\",\"rgba(70,48,126,1)\",\"rgba(72,35,116,1)\",\"rgba(72,22,104,1)\",\"rgba(72,27,109,1)\",\"rgba(70,51,127,1)\",\"rgba(72,38,119,1)\",\"rgba(72,29,111,1)\",\"rgba(70,11,94,1)\",\"rgba(70,7,90,1)\",\"rgba(70,51,127,1)\",\"rgba(72,22,104,1)\",\"rgba(69,5,89,1)\",\"rgba(72,33,115,1)\",\"rgba(69,53,129,1)\",\"rgba(48,106,142,1)\",\"rgba(71,43,122,1)\",\"rgba(72,40,120,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,100,1)\",\"rgba(71,45,123,1)\",\"rgba(40,124,142,1)\",\"rgba(71,17,100,1)\",\"rgba(72,41,121,1)\",\"rgba(72,20,102,1)\",\"rgba(70,51,127,1)\",\"rgba(71,13,96,1)\",\"rgba(57,86,140,1)\",\"rgba(72,29,111,1)\",\"rgba(65,67,135,1)\",\"rgba(67,61,132,1)\",\"rgba(70,51,127,1)\",\"rgba(72,31,112,1)\",\"rgba(72,28,110,1)\",\"rgba(71,17,100,1)\",\"rgba(72,29,111,1)\",\"rgba(72,37,118,1)\",\"rgba(31,158,137,1)\",\"rgba(71,46,124,1)\",\"rgba(60,80,139,1)\",\"rgba(71,16,99,1)\",\"rgba(69,55,129,1)\",\"rgba(45,112,142,1)\",\"rgba(72,22,104,1)\",\"rgba(72,32,113,1)\",\"rgba(71,47,125,1)\",\"rgba(72,34,116,1)\",\"rgba(71,45,123,1)\",\"rgba(71,17,100,1)\",\"rgba(35,136,142,1)\",\"rgba(63,72,137,1)\",\"rgba(72,38,119,1)\",\"rgba(56,89,140,1)\",\"rgba(71,19,102,1)\",\"rgba(58,84,140,1)\",\"rgba(72,24,106,1)\",\"rgba(72,25,107,1)\",\"rgba(70,48,126,1)\",\"rgba(59,81,139,1)\",\"rgba(68,58,131,1)\",\"rgba(58,83,139,1)\",\"rgba(72,28,110,1)\",\"rgba(72,36,117,1)\",\"rgba(70,9,92,1)\",\"rgba(70,51,127,1)\",\"rgba(72,31,112,1)\",\"rgba(71,16,98,1)\",\"rgba(68,3,86,1)\",\"rgba(70,11,94,1)\",\"rgba(35,138,141,1)\",\"rgba(64,70,136,1)\",\"rgba(59,81,139,1)\",\"rgba(55,90,140,1)\",\"rgba(71,19,101,1)\",\"rgba(72,27,109,1)\",\"rgba(49,102,142,1)\",\"rgba(71,18,100,1)\",\"rgba(37,171,130,1)\",\"rgba(72,33,115,1)\",\"rgba(68,2,86,1)\",\"rgba(66,65,134,1)\",\"rgba(72,24,106,1)\",\"rgba(70,9,93,1)\",\"rgba(72,32,113,1)\",\"rgba(71,19,101,1)\",\"rgba(68,58,131,1)\",\"rgba(54,93,141,1)\",\"rgba(71,19,101,1)\",\"rgba(72,29,111,1)\",\"rgba(70,11,94,1)\",\"rgba(71,47,125,1)\",\"rgba(71,42,122,1)\",\"rgba(72,20,102,1)\",\"rgba(68,58,131,1)\",\"rgba(70,10,93,1)\",\"rgba(57,86,140,1)\",\"rgba(72,25,107,1)\",\"rgba(72,30,111,1)\",\"rgba(72,30,111,1)\",\"rgba(71,47,125,1)\",\"rgba(71,19,101,1)\",\"rgba(44,114,142,1)\",\"rgba(71,45,123,1)\",\"rgba(51,98,141,1)\",\"rgba(72,36,117,1)\",\"rgba(70,10,93,1)\",\"rgba(68,59,132,1)\",\"rgba(71,17,100,1)\",\"rgba(72,27,109,1)\",\"rgba(65,66,135,1)\",\"rgba(71,17,100,1)\",\"rgba(70,8,92,1)\",\"rgba(72,35,116,1)\",\"rgba(70,50,126,1)\",\"rgba(71,17,100,1)\",\"rgba(68,3,86,1)\",\"rgba(71,17,100,1)\",\"rgba(71,18,100,1)\",\"rgba(71,17,100,1)\",\"rgba(72,36,117,1)\",\"rgba(71,19,101,1)\",\"rgba(70,49,126,1)\",\"rgba(69,56,130,1)\",\"rgba(72,31,112,1)\",\"rgba(53,95,141,1)\",\"rgba(52,97,141,1)\",\"rgba(72,37,118,1)\",\"rgba(70,10,93,1)\",\"rgba(71,14,97,1)\",\"rgba(72,23,105,1)\",\"rgba(69,55,129,1)\",\"rgba(72,29,111,1)\",\"rgba(72,37,118,1)\",\"rgba(72,30,111,1)\",\"rgba(72,41,121,1)\",\"rgba(31,151,139,1)\",\"rgba(72,36,117,1)\",\"rgba(68,1,85,1)\",\"rgba(69,53,129,1)\",\"rgba(72,27,109,1)\",\"rgba(50,101,142,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(57,86,140,1)\",\"rgba(69,53,129,1)\",\"rgba(71,46,124,1)\",\"rgba(71,19,101,1)\",\"rgba(48,106,142,1)\",\"rgba(72,31,112,1)\",\"rgba(58,84,140,1)\",\"rgba(71,18,100,1)\",\"rgba(72,36,117,1)\",\"rgba(72,23,105,1)\",\"rgba(57,85,140,1)\",\"rgba(69,6,89,1)\",\"rgba(62,73,137,1)\",\"rgba(71,44,122,1)\",\"rgba(72,29,111,1)\",\"rgba(72,24,106,1)\",\"rgba(72,25,107,1)\",\"rgba(57,85,140,1)\",\"rgba(63,71,136,1)\",\"rgba(72,35,116,1)\",\"rgba(72,27,109,1)\",\"rgba(52,96,141,1)\",\"rgba(72,23,105,1)\",\"rgba(51,98,141,1)\",\"rgba(70,8,91,1)\",\"rgba(57,86,140,1)\",\"rgba(61,78,138,1)\",\"rgba(72,41,121,1)\",\"rgba(55,90,140,1)\",\"rgba(66,63,133,1)\",\"rgba(70,12,95,1)\",\"rgba(70,8,92,1)\",\"rgba(70,12,95,1)\",\"rgba(71,17,100,1)\",\"rgba(72,36,117,1)\",\"rgba(70,7,90,1)\",\"rgba(70,11,94,1)\",\"rgba(67,62,133,1)\",\"rgba(34,141,141,1)\",\"rgba(53,95,141,1)\",\"rgba(70,7,90,1)\",\"rgba(70,11,94,1)\",\"rgba(63,72,137,1)\",\"rgba(72,41,121,1)\",\"rgba(72,22,104,1)\",\"rgba(34,139,141,1)\",\"rgba(72,25,107,1)\",\"rgba(72,41,121,1)\",\"rgba(72,28,110,1)\",\"rgba(41,122,142,1)\",\"rgba(72,25,107,1)\",\"rgba(72,36,117,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(66,63,133,1)\",\"rgba(62,73,137,1)\",\"rgba(72,33,114,1)\",\"rgba(72,20,103,1)\",\"rgba(68,57,131,1)\",\"rgba(39,126,142,1)\",\"rgba(72,36,117,1)\",\"rgba(37,133,142,1)\",\"rgba(71,17,100,1)\",\"rgba(67,62,133,1)\",\"rgba(66,63,133,1)\",\"rgba(69,55,129,1)\",\"rgba(70,9,92,1)\",\"rgba(72,27,109,1)\",\"rgba(72,20,103,1)\",\"rgba(72,40,120,1)\",\"rgba(70,9,93,1)\",\"rgba(71,46,124,1)\",\"rgba(71,13,96,1)\",\"rgba(71,46,124,1)\",\"rgba(71,16,99,1)\",\"rgba(72,26,108,1)\",\"rgba(71,18,100,1)\",\"rgba(72,34,115,1)\",\"rgba(72,23,105,1)\",\"rgba(51,98,141,1)\",\"rgba(70,7,90,1)\",\"rgba(72,30,112,1)\",\"rgba(37,132,142,1)\",\"rgba(70,7,90,1)\",\"rgba(71,15,98,1)\",\"rgba(72,41,121,1)\",\"rgba(70,8,91,1)\",\"rgba(46,110,142,1)\",\"rgba(72,29,111,1)\",\"rgba(72,33,115,1)\",\"rgba(31,149,139,1)\",\"rgba(61,78,138,1)\",\"rgba(71,46,124,1)\",\"rgba(72,20,103,1)\",\"rgba(65,67,135,1)\",\"rgba(72,20,102,1)\",\"rgba(66,63,133,1)\",\"rgba(62,75,137,1)\",\"rgba(70,8,92,1)\",\"rgba(70,7,91,1)\",\"rgba(72,28,110,1)\",\"rgba(57,87,140,1)\",\"rgba(72,22,104,1)\",\"rgba(71,15,98,1)\",\"rgba(54,93,141,1)\",\"rgba(70,9,93,1)\",\"rgba(70,7,90,1)\",\"rgba(71,16,99,1)\",\"rgba(72,29,111,1)\",\"rgba(31,158,137,1)\",\"rgba(69,4,87,1)\",\"rgba(68,59,132,1)\",\"rgba(70,7,90,1)\",\"rgba(72,27,109,1)\",\"rgba(72,30,111,1)\",\"rgba(71,44,122,1)\",\"rgba(72,38,119,1)\",\"rgba(69,4,88,1)\",\"rgba(72,36,117,1)\",\"rgba(72,29,111,1)\",\"rgba(62,73,137,1)\",\"rgba(46,110,142,1)\",\"rgba(68,1,84,1)\",\"rgba(72,24,106,1)\",\"rgba(70,10,93,1)\",\"rgba(41,121,142,1)\",\"rgba(71,13,96,1)\",\"rgba(71,46,124,1)\",\"rgba(72,27,109,1)\",\"rgba(71,15,98,1)\",\"rgba(72,28,110,1)\",\"rgba(71,18,100,1)\",\"rgba(72,23,105,1)\",\"rgba(71,16,99,1)\",\"rgba(69,4,88,1)\",\"rgba(64,70,136,1)\",\"rgba(70,8,92,1)\",\"rgba(72,32,113,1)\",\"rgba(72,31,112,1)\",\"rgba(70,11,94,1)\",\"rgba(44,115,142,1)\",\"rgba(68,2,86,1)\",\"rgba(72,28,110,1)\",\"rgba(41,122,142,1)\",\"rgba(66,65,134,1)\",\"rgba(71,19,101,1)\",\"rgba(72,23,105,1)\",\"rgba(72,27,109,1)\",\"rgba(71,17,100,1)\",\"rgba(71,13,96,1)\",\"rgba(72,23,105,1)\",\"rgba(71,17,100,1)\",\"rgba(68,57,131,1)\",\"rgba(71,19,102,1)\",\"rgba(72,36,117,1)\",\"rgba(72,40,120,1)\",\"rgba(70,7,90,1)\",\"rgba(70,52,128,1)\",\"rgba(70,7,91,1)\",\"rgba(71,15,98,1)\",\"rgba(70,11,94,1)\",\"rgba(71,13,96,1)\",\"rgba(72,28,110,1)\",\"rgba(58,84,140,1)\",\"rgba(71,18,100,1)\",\"rgba(66,64,134,1)\",\"rgba(70,49,126,1)\",\"rgba(72,30,111,1)\",\"rgba(72,28,110,1)\",\"rgba(71,15,98,1)\",\"rgba(71,14,97,1)\",\"rgba(71,18,100,1)\",\"rgba(72,27,109,1)\",\"rgba(72,28,110,1)\",\"rgba(72,35,116,1)\",\"rgba(63,72,137,1)\",\"rgba(70,48,126,1)\",\"rgba(55,90,140,1)\",\"rgba(72,32,114,1)\",\"rgba(72,41,121,1)\",\"rgba(59,81,139,1)\",\"rgba(71,47,125,1)\",\"rgba(72,24,106,1)\",\"rgba(37,133,142,1)\",\"rgba(70,6,90,1)\",\"rgba(72,33,114,1)\",\"rgba(72,36,117,1)\",\"rgba(72,31,112,1)\",\"rgba(72,28,110,1)\",\"rgba(72,24,106,1)\",\"rgba(46,110,142,1)\",\"rgba(70,7,90,1)\",\"rgba(72,24,106,1)\",\"rgba(71,16,99,1)\",\"rgba(71,13,96,1)\",\"rgba(70,7,91,1)\",\"rgba(72,39,119,1)\",\"rgba(71,43,122,1)\",\"rgba(72,25,107,1)\",\"rgba(72,27,109,1)\",\"rgba(70,7,90,1)\",\"rgba(70,10,93,1)\",\"rgba(68,58,131,1)\",\"rgba(72,28,110,1)\",\"rgba(70,10,93,1)\",\"rgba(72,20,103,1)\",\"rgba(72,32,113,1)\",\"rgba(70,11,94,1)\",\"rgba(69,54,129,1)\",\"rgba(69,53,129,1)\",\"rgba(72,27,109,1)\",\"rgba(71,16,99,1)\",\"rgba(72,27,109,1)\",\"rgba(72,33,115,1)\",\"rgba(71,18,100,1)\",\"rgba(71,13,96,1)\",\"rgba(67,62,133,1)\",\"rgba(68,57,131,1)\",\"rgba(70,49,126,1)\"]}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":40.8401826484018,\"r\":7.30593607305936,\"b\":37.2602739726027,\"l\":48.9497716894977},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"What are people saying about the best and worst movies on IMDB?\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-5.06,5.06],\"tickmode\":\"array\",\"ticktext\":[\"-5.0\",\"-2.5\",\"0.0\",\"2.5\",\"5.0\"],\"tickvals\":[-5,-2.5,0,2.5,5],\"categoryorder\":\"array\",\"categoryarray\":[\"-5.0\",\"-2.5\",\"0.0\",\"2.5\",\"5.0\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"← Bad Movies - Good Movies →\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-5.06,5.06],\"tickmode\":\"array\",\"ticktext\":[\"-5.0\",\"-2.5\",\"0.0\",\"2.5\",\"5.0\"],\"tickvals\":[-5,-2.5,0,2.5,5],\"categoryorder\":\"array\",\"categoryarray\":[\"-5.0\",\"-2.5\",\"0.0\",\"2.5\",\"5.0\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"← Bad Reviews - Good Reviews →\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"width\":700,\"height\":700,\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false,\"displayModeBar\":false},\"source\":\"A\",\"attrs\":{\"103ac44aa7ca8\":{\"xintercept\":{},\"type\":\"scatter\"},\"103ac14de1cd0\":{\"yintercept\":{}},\"103ac235a664\":{\"x\":{},\"y\":{},\"size\":{},\"colour\":{},\"text\":{}}},\"cur_data\":\"103ac44aa7ca8\",\"visdat\":{\"103ac44aa7ca8\":[\"function (y) \",\"x\"],\"103ac14de1cd0\":[\"function (y) \",\"x\"],\"103ac235a664\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} And we are done and it looks amazing! With this dataviz we are able to see that the word overrated is mainly used in negative reviews about good movies. Likewise unfunny is used in bad reviews about bad movies. There is many more examples that I’ll let you explore by yourself.\nThanks for tagging along!\n References  The Words Men and Women Use When They Write About Love   ","date":1534464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534464000,"objectID":"ce5e7a2cb0bd35cb5f3a3c9d6aa0b662","permalink":"/2018/08/17/what-are-the-reviews-telling-us/","publishdate":"2018-08-17T00:00:00Z","relpermalink":"/2018/08/17/what-are-the-reviews-telling-us/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-20.\nIn this post we will look at a handful of movies reviews from imdb which I have scraped and placed in this repository movie reviews.","tags":null,"title":"What are the reviews telling us?","type":"post"},{"authors":null,"categories":["ggplot2","web scraping"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nThis blogpost will showcase an example of a workflow and its associated thought process when iterating though visualization styles working with ggplot2. For this reason will this post include a lot of sub par charts as you are seeing the steps not just the final product.\nWe will use census data concerning US trade with other nations which we scrape as well.\nSetting up We will using a standard set of packages, tidyverse for general data manipulation, rvest and httr for scraping and manipulation.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 library(httr)  Getting the data This project started when I found the following link https://www.census.gov/foreign-trade/balance/c4099.html. It include a month by month breakdown of U.S. trade in goods with Denmark from 1985 till the present. Unfortunately the data is given in yearly tables, so we have a little bit of munching to do. First we notice that the last part of the url include c4099, which after some googling reveals that 4099 is the country code for Denmark. The fill list of country trade codes are given on the following page https://www.census.gov/foreign-trade/schedules/c/countrycode.html which also include a .txt file so we don’t have to scrape. We will remove the first entry and last 6 since US doesn’t trade with itself.\ncontinent_df \u0026lt;- tibble(number = as.character(1:7), continent = c(\u0026quot;North America\u0026quot;, \u0026quot;Central America\u0026quot;, \u0026quot;South America\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Australia and Oceania\u0026quot;, \u0026quot;Africa\u0026quot;)) code_df \u0026lt;- read_lines(\u0026quot;https://www.census.gov/foreign-trade/schedules/c/country.txt\u0026quot;, skip = 5) %\u0026gt;% tibble(code = .) %\u0026gt;% separate(code, c(\u0026quot;code\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;iso\u0026quot;), sep = \u0026quot;\\\\|\u0026quot;) %\u0026gt;% mutate_all(trimws) %\u0026gt;% mutate(con_code = str_sub(code, 1, 1)) %\u0026gt;% filter(!is.na(iso), name != \u0026quot;United States of America\u0026quot;, con_code != 9) %\u0026gt;% left_join(continent_df, by = c(\u0026quot;con_code\u0026quot; = \u0026quot;number\u0026quot;)) %\u0026gt;% select(-con_code) With these code we create the targeted urls we will be scraping\ntarget_urls \u0026lt;- str_glue(\u0026quot;https://www.census.gov/foreign-trade/balance/c{code_df$code}.html\u0026quot;) We will be replication hrbrmstr’s scraping code found here since it works wonderfully.\ns_GET \u0026lt;- safely(GET) pb \u0026lt;- progress_estimated(length(target_urls)) map(target_urls, ~{ pb$tick()$print() Sys.sleep(5) s_GET(.x) }) -\u0026gt; httr_raw_responses write_rds(httr_raw_responses, \u0026quot;data/2018-us-trade-raw-httr-responses.rds\u0026quot;) good_responses \u0026lt;- keep(httr_raw_responses, ~!is.null(.x$result)) then we wrangle all the html files by extracting all the tables, parse the numeric variables and combining them to one table.\nwrangle \u0026lt;- function(x, name) { # Read html and extract tables read_html(x[[1]]) %\u0026gt;% html_nodes(\u0026quot;table\u0026quot;) %\u0026gt;% html_table() %\u0026gt;% # parse numeric columns map(~ mutate_at(.x, vars(Exports:Balance), funs(parse_number))) %\u0026gt;% bind_rows() %\u0026gt;% mutate(Country = name) } full_data \u0026lt;- map2_df(good_responses, code_df$code, wrangle) Lastly we do some polishing up with the date variables and join in the country information.\ntrade_data \u0026lt;- full_data %\u0026gt;% filter(!str_detect(Month, \u0026quot;TOTAL\u0026quot;)) %\u0026gt;% mutate(Date = parse_date(Month, format = \u0026quot;%B %Y\u0026quot;), Month = lubridate::month(Date), Year = lubridate::year(Date)) %\u0026gt;% left_join(code_df, by = c(\u0026quot;Country\u0026quot; = \u0026quot;code\u0026quot;)) Giving us this data to work with:\nglimpse(trade_data) ## Rows: 75,379 ## Columns: 10 ## $ Month \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,… ## $ Exports \u0026lt;dbl\u0026gt; 0.1, 0.4, 1.8, 0.2, 0.1, 1.3, 3.2, 0.6, 0.3, 0.6, 0.7, 1.4,… ## $ Imports \u0026lt;dbl\u0026gt; 1.0, 2.4, 2.2, 0.8, 0.2, 0.5, 0.8, 0.9, 0.4, 2.4, 0.5, 1.0,… ## $ Balance \u0026lt;dbl\u0026gt; -0.8, -2.0, -0.4, -0.6, -0.1, 0.7, 2.4, -0.3, -0.1, -1.8, 0… ## $ Country \u0026lt;dbl\u0026gt; 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010,… ## $ Date \u0026lt;date\u0026gt; 2018-01-01, 2018-02-01, 2018-03-01, 2018-04-01, 2017-01-01… ## $ Year \u0026lt;dbl\u0026gt; 2018, 2018, 2018, 2018, 2017, 2017, 2017, 2017, 2017, 2017,… ## $ name \u0026lt;chr\u0026gt; \u0026quot;Greenland\u0026quot;, \u0026quot;Greenland\u0026quot;, \u0026quot;Greenland\u0026quot;, \u0026quot;Greenland\u0026quot;, \u0026quot;Greenl… ## $ iso \u0026lt;chr\u0026gt; \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;, \u0026quot;GL\u0026quot;,… ## $ continent \u0026lt;chr\u0026gt; \u0026quot;North America\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;North A…  Lets get vizualising! Lets set a different theme for now.\ntheme_set(theme_minimal()) Lets start out nice and simple by plotting a simple scatter plot for just a single country to get a feel for the data.\ntrade_data %\u0026gt;% filter(name == \u0026quot;Greenland\u0026quot;) %\u0026gt;% ggplot(aes(Date, Balance)) + geom_point() + labs(title = \u0026quot;United States Trade Balance in Goods with Greenland\u0026quot;) Which looks good already! Lets see how it would look with as a line chart instead\ntrade_data %\u0026gt;% filter(name == \u0026quot;Greenland\u0026quot;) %\u0026gt;% ggplot(aes(Date, Balance)) + geom_line() + labs(title = \u0026quot;United States Trade Balance in Goods with Greenland\u0026quot;) it sure it quite jagged! Lets take a loot at the 4 biggest spiked to see if it is a indication of a trend\ntrade_data %\u0026gt;% filter(name == \u0026quot;Greenland\u0026quot;, Balance \u0026gt; 5) ## # A tibble: 4 x 10 ## Month Exports Imports Balance Country Date Year name iso continent ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 3 7.9 0.5 7.4 1010 2014-03-01 2014 Green… GL North Ame… ## 2 3 10.4 1 9.4 1010 2013-03-01 2013 Green… GL North Ame… ## 3 3 10.5 0.6 9.9 1010 2012-03-01 2012 Green… GL North Ame… ## 4 9 20 1.3 18.8 1010 2008-09-01 2008 Green… GL North Ame… Which didn’t give us much, 3 of the spikes happened in March and the last one a random September. It was worth a try, back to plotting! Lets see how a smooth curve would look overlaid the line chart\ntrade_data %\u0026gt;% filter(name == \u0026quot;Greenland\u0026quot;) %\u0026gt;% ggplot(aes(Date, Balance)) + geom_line() + geom_smooth(se = FALSE) + labs(title = \u0026quot;United States Trade Balance in Goods with Greenland\u0026quot;) Which looks nice in and off itself, however since this chart looks at the trade balance between two countries is the value 0 quite important and should be highlighted better. I will add a line behind the data points such that it highlights rather then hides\ntrade_data %\u0026gt;% filter(name == \u0026quot;Greenland\u0026quot;) %\u0026gt;% ggplot(aes(Date, Balance)) + geom_abline(slope = 0, intercept = 0, color = \u0026quot;orange\u0026quot;) + geom_line() + geom_smooth(se = FALSE) + labs(title = \u0026quot;United States Trade Balance in Goods with Greenland\u0026quot;) This gives us better indication for when the trade is positive or negative with respect to the United States. Lets take it up a notch and include a couple more countries. We remove the filter and add Country as the group aesthetic.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line() + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;) So we have 3 different problems I would like to fix right now. The scale between these different countries is massively different! The very negative balance of one country is making it hard to see what happens to the other countries. Secondly it is hard to distinguish the different countries since they are all the same color. And lastly there is some serious over plotting, this point is tired to the other problems so lets see if we can fix them one at a time.\nFirst lets transform the scales on the y axis such that we better can identify individual lines. We do this with the square root transformation which gives weights to values close to 0 and shrinks values far away form 0.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line() + scale_y_sqrt() + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With square root transformation\u0026quot;) ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 11918 row(s) containing missing values (geom_path). Oh no! We lost all the negative values. This happened because the normal square root operation only works with positive numbers. We fix this by using the signed square root which apply the square root to both the positive and negative as if they were positive and then signs them accordingly. for this we create a new transformation with the scales package.\nS_sqrt \u0026lt;- function(x) sign(x) * sqrt(abs(x)) IS_sqrt \u0026lt;- function(x) x ^ 2 * sign(x) S_sqrt_trans \u0026lt;- function() scales::trans_new(\u0026quot;S_sqrt\u0026quot;, S_sqrt, IS_sqrt) trade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line() + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) Much better! We will fix the the breaks a little bit too.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line() + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) Now lets solve the problem with over plotting, a standard trick is to introduce transparency, this is done using the alpha aesthetic. Lets start with 0.5 alpha.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line(alpha = 0.5) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) slightly better but not good enough, lets try 0.2\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) much better! Another thing we could do if coloring depending on the the continent.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country, color = continent)) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) This is quite messy, however we notice that the data for the African countries don’t cover the same range as the other countries. Lets see if there are some overall trends within each continent.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + geom_line(alpha = 0.2) + geom_smooth(aes(Date, Balance, group = continent, color = continent), se = FALSE) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation\u0026quot;) This gives some more tangible information. There is a upwards trend within North America for the last 10 years, where Asia have had a slow decline since the beginning of the data collection.\nNext lets see what happens when you facet depending on continent\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country)) + facet_wrap(~ continent) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation faceted depending on continent\u0026quot;) These look really nice, lets free up the scale on the y axis within each facet such that we can differentiate the lines better, on top of that lets re introduce the colors.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country, color = continent)) + facet_wrap(~ continent, scales = \u0026quot;free_y\u0026quot;) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation faceted depending on continent\u0026quot;) lets remove the color legend as the information is already present in the facet labels.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country, color = continent)) + facet_wrap(~ continent, scales = \u0026quot;free_y\u0026quot;) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation faceted depending on continent\u0026quot;) + guides(color = \u0026quot;none\u0026quot;) Lastly lets overlay the smooth continent average\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country, color = continent)) + facet_wrap(~ continent, scales = \u0026quot;free_y\u0026quot;) + geom_line(alpha = 0.2) + geom_smooth(aes(group = continent), color = \u0026quot;grey40\u0026quot;, se = FALSE) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation faceted depending on continent\u0026quot;) + guides(color = \u0026quot;none\u0026quot;) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; Unfortunately doesn’t add too much information so lets remove it again. Lastly lets update the the labels to reflect the the unit.\ntrade_data %\u0026gt;% ggplot(aes(Date, Balance, group = Country, color = continent)) + facet_wrap(~ continent, scales = \u0026quot;free_y\u0026quot;) + geom_line(alpha = 0.2) + coord_trans(y = \u0026quot;S_sqrt\u0026quot;) + scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500), minor_breaks = NULL) + labs(title = \u0026quot;United States Trade Balance in Goods with all countries\u0026quot;, subtitle = \u0026quot;With signed square root transformation faceted depending on continent\u0026quot;, y = \u0026quot;Balance (in millions of U.S. dollars on a nominal basis)\u0026quot;) + guides(color = \u0026quot;none\u0026quot;)  ","date":1528761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528761600,"objectID":"c398105f1cf63de47186ed0ac704e798","permalink":"/2018/06/12/ggplot2-trial-and-error-us-trade-data/","publishdate":"2018-06-12T00:00:00Z","relpermalink":"/2018/06/12/ggplot2-trial-and-error-us-trade-data/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nThis blogpost will showcase an example of a workflow and its associated thought process when iterating though visualization styles working with ggplot2.","tags":null,"title":"ggplot2 trial and error - US trade data","type":"post"},{"authors":null,"categories":["tidytext"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nThis post will be a short demonstration on how the the occurrence of emojis on twitter can be analysed using tidytools. We start of loading the necessary packages.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(rtweet) I have decided that for this example that I would focus on tweets that include the hash tags #happy and #sad in the hope that both would include a similar number of emojis but hopefully of different groups. We will use the rtweet package which already conforms to the tidy principles. Notice the retryonratelimit = TRUE argument as the combined number of tweets (10000 + 10000 = 20000) is larger the the 15 min limit of 18000.\ntweets_happy \u0026lt;- search_tweets(\u0026quot;#happy\u0026quot;, n = 10000, include_rts = FALSE) tweets_sad \u0026lt;- search_tweets(\u0026quot;#sad\u0026quot;, n = 10000, include_rts = FALSE, retryonratelimit = TRUE) As a safety will we save these tweets.\nwrite_as_csv(tweets_happy, \u0026quot;tweets_happy.csv\u0026quot;) write_as_csv(tweets_sad, \u0026quot;tweets_sad.csv\u0026quot;) Now we load this data.frame that contains information regarding the various emojis.\nemoji \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/EmilHvitfeldt/Emoji-table/master/emoji.csv\u0026quot;) Next we add the hash tag label as the emotion variable, next we tokenize all the tweets according to characters (this is done since a lot of the tweets didn’t use spaces emojis rendering them hard to detect.) and left join with the emoji data.frame such that we get the descriptions.\ntweets_all \u0026lt;- bind_rows( tweets_happy %\u0026gt;% mutate(emotion = \u0026quot;#happy\u0026quot;), tweets_sad %\u0026gt;% mutate(emotion = \u0026quot;#sad\u0026quot;) ) emoji_all \u0026lt;- unnest_tokens(tweets_all, word, text, token = \u0026quot;characters\u0026quot;) %\u0026gt;% select(word, emotion) %\u0026gt;% left_join(emoji, by = c(\u0026quot;word\u0026quot; = \u0026quot;utf\u0026quot;)) %\u0026gt;% filter(!is.na(shortname)) Lastly we create a simple faceted bar chart of the number of emojis used within each hash tag.\nemoji_all %\u0026gt;% count(word, emotion, shortname) %\u0026gt;% group_by(emotion) %\u0026gt;% arrange(desc(n)) %\u0026gt;% top_n(10, n) %\u0026gt;% ungroup() %\u0026gt;% mutate(emoji = reorder(shortname, n)) %\u0026gt;% ggplot(aes(emoji, n)) + geom_col() + facet_grid(emotion ~ ., scales = \u0026quot;free_y\u0026quot;) + coord_flip() + theme_minimal() + labs(title = \u0026quot;Emojis used in #happy and #sad tweets\u0026quot;, y = \u0026quot;Count\u0026quot;, x = \u0026quot;\u0026quot;) Using the emoji data.frame allows us to gain quick insight with the descriptive short names.\n","date":1528070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528070400,"objectID":"1dce3131753e02d14ef21cb1eae73853","permalink":"/2018/06/04/emoji-use-on-twitter/","publishdate":"2018-06-04T00:00:00Z","relpermalink":"/2018/06/04/emoji-use-on-twitter/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nThis post will be a short demonstration on how the the occurrence of emojis on twitter can be analysed using tidytools.","tags":null,"title":"Emoji use on Twitter","type":"post"},{"authors":null,"categories":["tidytext"],"content":" In my earlier post on binary text classification was one of the problems that occurred was the sheer size of the data when trying to fit a model. The bag of words method of having each column describe the occurrence of a specific word in each document (row) is appealing from a mathematical perspective, but gives rise for large sparse matrices which aren’t handled well by some models in R. This leads to slow running code at best and crashing at worst.\nWe will try to combat this problem by using something called word embedding which is a general term for the process of mapping textural information to a lower dimensional space. This is a special case of dimensionality reduction, and we will use the simple well known method Principal component analysis for our word embedding today. We are essentially trying to squeeze as much information into as little space as possible such that our models can run in a reasonable time.\nWe will use the same data as in the earlier post, and the PCA procedure is very inspired by Julia Silge recent post Understanding PCA using Stack Overflow data which you should read if you haven’t already!!\nData prepossessing We will use standard tidyverse tool set for this post. We will use randomForest model as this approach should be much faster.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(broom) library(randomForest) The data we will be using for this demonstration will be some social media disaster tweets discussed in this article. It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.\ndata \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv\u0026quot;) And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren’t classified, marked \"Can't Decide\" so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don’t actually have that much information in them as they are limited in characters and some only contain a couple of words.\nWe will at this stage remove what appears to be urls using some regex and str_replace_all, and we will select the columns id, disaster and text.\ndata_clean \u0026lt;- data %\u0026gt;% filter(choose_one != \u0026quot;Can\u0026#39;t Decide\u0026quot;) %\u0026gt;% mutate(id = `_unit_id`, disaster = choose_one == \u0026quot;Relevant\u0026quot;, text = str_replace_all(text, \u0026quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% select(id, disaster, text) We then extract all unigrams, bigram and remove stopwords.\ndata_counts \u0026lt;- map_df(1:2, ~ unnest_tokens(data_clean, word, text, token = \u0026quot;ngrams\u0026quot;, n = .x)) %\u0026gt;% anti_join(stop_words, by = \u0026quot;word\u0026quot;) We will only focus on the top 10000 most used words for the remainder of the analysis.\ntop10000 \u0026lt;- data_counts %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% slice(1:10000) %\u0026gt;% select(word) we will then count the words again, but this time we will count the word occurrence within each document and remove the underused words.\nunnested_words \u0026lt;- data_counts %\u0026gt;% count(id, word, sort = TRUE) %\u0026gt;% inner_join(top10000, by = \u0026quot;word\u0026quot;) We then cast the data.frame to a sparse matrix.\nsparse_word_matrix \u0026lt;- unnested_words %\u0026gt;% cast_sparse(id, word, n) In the last post we used this matrix for the modeling, but the size was quite obstacle.\ndim(sparse_word_matrix) ## [1] 10829 10000 We have a row for each document and a row for each of the top 10000 words, but most of the elements are empty so each of the variables don’t contain much information. We will do word embedding by applying PCA to the sparse word count matrix. Like Julia Silge we will use the wonderful irlba package that facilities PCA on sparse matrices. First we scale the matrix and then we apply PCA where we request 64 columns.\nThis stage will take some time, but that is the trade-off we will be making when using word embedding. We take some computation time up front in exchange for quick computation later down the line.\nword_scaled \u0026lt;- scale(sparse_word_matrix) word_pca \u0026lt;- irlba::prcomp_irlba(word_scaled, n = 64) Then we will create a meta data.frame to take care of tweets that disappeared when we cleaned them earlier.\nmeta \u0026lt;- tibble(id = as.numeric(dimnames(sparse_word_matrix)[[1]])) %\u0026gt;% left_join(data_clean[!duplicated(data_clean$id), ], by = \u0026quot;id\u0026quot;) Now we combine the PCA matrix with proper response variable (disaster/no-disaster) with the addition ot a training/testing split variable.\nclass_df \u0026lt;- data.frame(word_pca$x) %\u0026gt;% mutate(response = factor(meta$disaster), split = sample(0:1, NROW(meta), replace = TRUE, prob = c(0.2, 0.8))) We now have a data frame with 64 explanatory variables instead of the 10000 we started with. This a huge reduction which hopefully should pay off. For this demonstration will we try using two kinds of models. Standard logistic regression and a random forest model. Logistic regression is a good baseline which should be blazing fast now since the reduction have taken place and the random forest model which generally was quite slow should be more manageable this time around.\nmodel \u0026lt;- glm(response ~ ., data = filter(class_df, split == 1), family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred y_pred \u0026lt;- predict(model, type = \u0026quot;response\u0026quot;, newdata = filter(class_df, split == 0) %\u0026gt;% select(-response)) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading y_pred_logical \u0026lt;- if_else(y_pred \u0026gt; 0.5, 1, 0) (con \u0026lt;- table(y_pred_logical, filter(class_df, split == 0) %\u0026gt;% pull(response))) ## ## y_pred_logical FALSE TRUE ## 0 1180 631 ## 1 82 330 sum(diag(con)) / sum(con) ## [1] 0.6792623 it work fairly quickly and we get a decent accuracy of 68%. Remember this method isn’t meant to improve the accuracy but rather to improve the computational time.\nmodel \u0026lt;- randomForest(response ~ ., data = filter(class_df, split == 1)) y_pred \u0026lt;- predict(model, type = \u0026quot;class\u0026quot;, newdata = filter(class_df, split == 0) %\u0026gt;% select(-response)) (con \u0026lt;- table(y_pred, filter(class_df, split == 0) %\u0026gt;% pull(response))) ## ## y_pred FALSE TRUE ## FALSE 1106 379 ## TRUE 156 582 sum(diag(con)) / sum(con) ## [1] 0.7593342 This one takes slightly longer to run due to the number of trees, but it does give us the nifty 76% accuracy which is pretty good considering we only look at tweets.\nAnd this is all that there is to it! The dimensionality reduction method was able to reduce the number of variables while retaining most of the information within those variables such that we can run our procedures at a faster phase without much loss. There is still a lot of individual improvements to be done if this was to be used further, both in terms of hyper-parameter selection in the modeling choices but also the number of PCA variables that should be used in the final modelling. Remember that this is just one of the more simpler methods, with more advanced word representation methods being glove and word2vec.\n Data viz Since Julia did most of the legwork for the visualizations so we will take a look at how each of the words contribute to the first four components.\ntidied_pca \u0026lt;- bind_cols(Tag = colnames(word_scaled), tidy(word_pca$rotation)) %\u0026gt;% gather(PC, Contribution, PC1:PC4) ## Warning: \u0026#39;tidy.matrix\u0026#39; is deprecated. ## See help(\u0026quot;Deprecated\u0026quot;) tidied_pca %\u0026gt;% filter(PC %in% paste0(\u0026quot;PC\u0026quot;, 1:4)) %\u0026gt;% ggplot(aes(Tag, Contribution, fill = Tag)) + geom_col(show.legend = FALSE) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + labs(x = \u0026quot;Words\u0026quot;, y = \u0026quot;Relative importance in each principal component\u0026quot;) + facet_wrap(~ PC, ncol = 2) What we see is quite different then what Julia found in her study. We have just a few words doing most of the contributions in each of component. Lets zoom in to take a look at the words with the most influence on the different components:\nmap_df(c(-1, 1) * 20, ~ tidied_pca %\u0026gt;% filter(PC == \u0026quot;PC1\u0026quot;) %\u0026gt;% top_n(.x, Contribution)) %\u0026gt;% mutate(Tag = reorder(Tag, Contribution)) %\u0026gt;% ggplot(aes(Tag, Contribution, fill = Tag)) + geom_col(show.legend = FALSE, alpha = 0.8) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), axis.ticks.x = element_blank()) + labs(x = \u0026quot;Words\u0026quot;, y = \u0026quot;Relative importance in principle component\u0026quot;, title = \u0026quot;PC1\u0026quot;) We would like to see some sensible separation between the positive words and the negative words (with regard to contribution). However I haven’t been able to come up with a meaning full grouping for the first 3 components. The fourth on the other hand have all the positive influencing words containing numbers in one way or another.\nmap_df(c(-1, 1) * 20, ~ tidied_pca %\u0026gt;% filter(PC == \u0026quot;PC4\u0026quot;) %\u0026gt;% top_n(.x, Contribution)) %\u0026gt;% mutate(Tag = reorder(Tag, Contribution)) %\u0026gt;% ggplot(aes(Tag, Contribution, fill = Tag)) + geom_col(show.legend = FALSE, alpha = 0.8) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), axis.ticks.x = element_blank()) + labs(x = \u0026quot;Words\u0026quot;, y = \u0026quot;Relative importance in principle component\u0026quot;, title = \u0026quot;PC4\u0026quot;) This is all I have for this time. Hope you enjoyed it!\n ","date":1526947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526947200,"objectID":"ace51d0e26a04df6a6dc40b1ab267586","permalink":"/2018/05/22/using-pca-for-word-embedding-in-r/","publishdate":"2018-05-22T00:00:00Z","relpermalink":"/2018/05/22/using-pca-for-word-embedding-in-r/","section":"post","summary":"In my earlier post on binary text classification was one of the problems that occurred was the sheer size of the data when trying to fit a model. The bag of words method of having each column describe the occurrence of a specific word in each document (row) is appealing from a mathematical perspective, but gives rise for large sparse matrices which aren’t handled well by some models in R.","tags":null,"title":"Using PCA for word embedding in R","type":"post"},{"authors":null,"categories":["ggplot2"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nI will In this post explore the ethnic diversity of the student population in schools in California. We will utilize the data provided by California Department of Education that has Enrollment by School which includes “school-level enrollment by racial/ethnic designation, gender, and grade”.\nWe will combine this data with two other datasets that contain:\n Longitude and latitude of the schools Income information of the cities the schools are located in  Hopefully we will be able to draw some cool inference using these datasets while working though the complications you get when you combine datasets from the wild.\nLoading packages This will be a fairly standard data science exercise so we will stick to the tidyverse, rvest for scraping, patchwork for plot stitching and add a little fancyness with ggmap for local maps.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 library(patchwork) library(ggmap)  Enrollment data We start by downloading the dataset to our local disk and read it from there this is mainly to be nice, and for the fact the the download speed on the files we work with are a little slow. This can be done using readr’s read_tsv().\ndata \u0026lt;- readr::read_tsv(\u0026quot;filesenr.asp.txt\u0026quot;) To get an idea of the data lets have a quick glimpse()\nglimpse(data) ## Rows: 129,813 ## Columns: 23 ## $ CDS_CODE \u0026lt;chr\u0026gt; \u0026quot;33672490000001\u0026quot;, \u0026quot;33672490000001\u0026quot;, \u0026quot;33672490000001\u0026quot;, \u0026quot;3367… ## $ COUNTY \u0026lt;chr\u0026gt; \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Rivers… ## $ DISTRICT \u0026lt;chr\u0026gt; \u0026quot;San Jacinto Unified\u0026quot;, \u0026quot;San Jacinto Unified\u0026quot;, \u0026quot;San Jacinto … ## $ SCHOOL \u0026lt;chr\u0026gt; \u0026quot;Nonpublic, Nonsectarian Schools\u0026quot;, \u0026quot;Nonpublic, Nonsectarian… ## $ ETHNIC \u0026lt;dbl\u0026gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5,… ## $ GENDER \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;,… ## $ KDGN \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4… ## $ GR_1 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4… ## $ GR_2 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3… ## $ GR_3 \u0026lt;dbl\u0026gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6… ## $ GR_4 \u0026lt;dbl\u0026gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4… ## $ GR_5 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6… ## $ GR_6 \u0026lt;dbl\u0026gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_7 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_8 \u0026lt;dbl\u0026gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ UNGR_ELM \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_9 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_10 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_11 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_12 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ UNGR_SEC \u0026lt;dbl\u0026gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ ENR_TOTAL \u0026lt;dbl\u0026gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1… ## $ ADULT \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… and we notice that CDS_CODE is being read as a chr which is favorable in this case since we don’t actually want it as a integer but rather as an ID variable. If this column had been converted to integers we would have lost leading zeros which could lead to trouble.\n Longitude and latitude data The longitude and latitude of the schools are also available on California Department of Education’s website here. It includes quite a lot of information, a lot of it uninteresting for this project so will take a subset of it for further analysis. read_tsv complaints a little bit when reading it in, but after some crosschecking with the .xls file also provided, does it seem to be correct so we will ignore the error (don’t just ignore error all the time! most of the time they are telling you something important! in this case the problems stems from missing values and tab separated values don’t mix that nicely).\nlonglat_raw \u0026lt;- read_tsv(\u0026quot;pubschls.txt\u0026quot;) longlat \u0026lt;- longlat_raw %\u0026gt;% select(CDSCode, Latitude, Longitude, School, City) Lets have a peak at the data:\nglimpse(longlat) ## Rows: 10,014 ## Columns: 5 ## $ CDSCode \u0026lt;chr\u0026gt; \u0026quot;01100170000000\u0026quot;, \u0026quot;01100170109835\u0026quot;, \u0026quot;01100170112607\u0026quot;, \u0026quot;0110… ## $ Latitude \u0026lt;dbl\u0026gt; 37.65821, 37.52144, 37.80452, 37.86899, 37.78465, 37.84737,… ## $ Longitude \u0026lt;dbl\u0026gt; -122.0971, -121.9939, -122.2682, -122.2784, -122.2386, -122… ## $ School \u0026lt;chr\u0026gt; NA, \u0026quot;FAME Public Charter\u0026quot;, \u0026quot;Envision Academy for Arts \u0026amp; Tec… ## $ City \u0026lt;chr\u0026gt; \u0026quot;Hayward\u0026quot;, \u0026quot;Newark\u0026quot;, \u0026quot;Oakland\u0026quot;, \u0026quot;Berkeley\u0026quot;, \u0026quot;Oakland\u0026quot;, \u0026quot;Oak… we see that the the columns have been read in in appropriate ways. We recognize the CDS_CODE from before as CDSCode in this dataset which we will use to combine the datasets later.\n Income data Lastly we will get some income data, I found some fairly good data on Wikipedia. We use simple rvest tools to extract the table from the website and give it column names.\nWhile the income data is quite lovely, would it be outside the scope of this post to identify which census-designated place (CDP) each of the schools belong it. The second best option is to use the income data on a county by county basis. This will ofcause mean that we trade a bit of granularity for time. But hopefully it will still lead to some meaningful findings.\nurl \u0026lt;- \u0026quot;https://en.wikipedia.org/wiki/List_of_California_locations_by_income\u0026quot; income_data \u0026lt;- read_html(url) %\u0026gt;% html_nodes(\u0026quot;table\u0026quot;) %\u0026gt;% .[2] %\u0026gt;% html_table() %\u0026gt;% .[[1]] %\u0026gt;% set_names(c(\u0026quot;county\u0026quot;, \u0026quot;population\u0026quot;, \u0026quot;population_density\u0026quot;, \u0026quot;per_capita_income\u0026quot;, \u0026quot;median_household_income\u0026quot;, \u0026quot;median_family_income\u0026quot;)) lets take a look at the table:\nglimpse(income_data) ## Rows: 58 ## Columns: 6 ## $ county \u0026lt;chr\u0026gt; \u0026quot;Alameda\u0026quot;, \u0026quot;Alpine\u0026quot;, \u0026quot;Amador\u0026quot;, \u0026quot;Butte\u0026quot;, \u0026quot;Cala… ## $ population \u0026lt;chr\u0026gt; \u0026quot;1,559,308\u0026quot;, \u0026quot;1,202\u0026quot;, \u0026quot;37,159\u0026quot;, \u0026quot;221,578\u0026quot;, \u0026quot;4… ## $ population_density \u0026lt;chr\u0026gt; \u0026quot;2,109.8\u0026quot;, \u0026quot;1.6\u0026quot;, \u0026quot;62.5\u0026quot;, \u0026quot;135.4\u0026quot;, \u0026quot;44.0\u0026quot;, \u0026quot;1… ## $ per_capita_income \u0026lt;chr\u0026gt; \u0026quot;$36,439\u0026quot;, \u0026quot;$24,375\u0026quot;, \u0026quot;$27,373\u0026quot;, \u0026quot;$24,430\u0026quot;, \u0026quot;… ## $ median_household_income \u0026lt;chr\u0026gt; \u0026quot;$73,775\u0026quot;, \u0026quot;$61,343\u0026quot;, \u0026quot;$52,964\u0026quot;, \u0026quot;$43,165\u0026quot;, \u0026quot;… ## $ median_family_income \u0026lt;chr\u0026gt; \u0026quot;$90,822\u0026quot;, \u0026quot;$71,932\u0026quot;, \u0026quot;$68,765\u0026quot;, \u0026quot;$56,934\u0026quot;, \u0026quot;… here we see a couple of things that look weirds. Every column characters valued, which it shouldn’t be since 5 out of the 6 variables should be numerical.\nWe will use the wonderful parse_number function from the readr package to convert the character values to numeric.\nincome_data_clean \u0026lt;- income_data %\u0026gt;% mutate_at(vars(population:median_family_income), parse_number) a quick glimpse to see everything went well\nglimpse(income_data_clean) ## Rows: 58 ## Columns: 6 ## $ county \u0026lt;chr\u0026gt; \u0026quot;Alameda\u0026quot;, \u0026quot;Alpine\u0026quot;, \u0026quot;Amador\u0026quot;, \u0026quot;Butte\u0026quot;, \u0026quot;Cala… ## $ population \u0026lt;dbl\u0026gt; 1559308, 1202, 37159, 221578, 44921, 21424, 1… ## $ population_density \u0026lt;dbl\u0026gt; 2109.8, 1.6, 62.5, 135.4, 44.0, 18.6, 1496.0,… ## $ per_capita_income \u0026lt;dbl\u0026gt; 36439, 24375, 27373, 24430, 29296, 22211, 387… ## $ median_household_income \u0026lt;dbl\u0026gt; 73775, 61343, 52964, 43165, 54936, 50503, 797… ## $ median_family_income \u0026lt;dbl\u0026gt; 90822, 71932, 68765, 56934, 67100, 56472, 950… And we are good to go!\n Ethnic diversity To be able to compare the ethnic diversity between schools we need a measure that describes diversity to begin with. I’ll use the diversity index developed in 1991 by a researcher at the University of North Carolina at Chapel Hill. The index simply asks: “What is the probability that two people in a population picked at random will be from different ethnicity”. At first this can be seen as a daunting task, but if we look at it geometrically we notice that the calculations are quite straight forward.\nIf we imagine a population of 10 people split into two ethnicities, with 5 in each. Then we can draw the following outcome space:\ndiversity_plot(makeup = rep(5, 2)) (the code for diversity_plot is displayed in the end of the article for those interested.)\nWhere the colored squares represent random picks with the same ethnicity, light grey squares picks with different ethnicities. Dark grey indicate impossible picks (same person picked twice) and should be ignored. Now the diversity index can be calculated by dividing the number of light grey squares with the sum of light grey and colored squares.\nBefore we go on, lets look at how different populations have different diversity indexes. If we only have two groups, will the diversity score converge to 0.5 for large populations, however with small populations will the index be quite large since each person contribute such a big percentage of the group, making it harder to pick another one in the same group.\ndiversity_plot(rep(1, 2)) + diversity_plot(rep(2, 2)) + diversity_plot(rep(4, 2)) + diversity_plot(rep(8, 2)) Effectively giving that adding a single person from a new group will maximize the contribution to the index.\ndiversity_plot(c(rep(1, 2), 1, 1, 1)) + diversity_plot(c(rep(2, 2), 1, 1, 1)) + diversity_plot(c(rep(4, 2), 1, 1, 1)) + diversity_plot(c(rep(8, 2), 1, 1, 1)) diversity_plot(c(2, 3, 4, 5, 6) * 1) + diversity_plot(c(2, 3, 4, 5, 6) * 2) + diversity_plot(c(2, 3, 4, 5, 6) * 3) + diversity_plot(c(2, 3, 4, 5, 6) * 4) Now that we have seen the diversity index in use lets apply it to the data we have collected.\nWe would like to have the data in a different kind of tidy format, namely we want each row to represent each school. Taking another look at the data\nglimpse(data) ## Rows: 129,813 ## Columns: 23 ## $ CDS_CODE \u0026lt;chr\u0026gt; \u0026quot;33672490000001\u0026quot;, \u0026quot;33672490000001\u0026quot;, \u0026quot;33672490000001\u0026quot;, \u0026quot;3367… ## $ COUNTY \u0026lt;chr\u0026gt; \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Riverside\u0026quot;, \u0026quot;Rivers… ## $ DISTRICT \u0026lt;chr\u0026gt; \u0026quot;San Jacinto Unified\u0026quot;, \u0026quot;San Jacinto Unified\u0026quot;, \u0026quot;San Jacinto … ## $ SCHOOL \u0026lt;chr\u0026gt; \u0026quot;Nonpublic, Nonsectarian Schools\u0026quot;, \u0026quot;Nonpublic, Nonsectarian… ## $ ETHNIC \u0026lt;dbl\u0026gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5,… ## $ GENDER \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;F\u0026quot;,… ## $ KDGN \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4… ## $ GR_1 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4… ## $ GR_2 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3… ## $ GR_3 \u0026lt;dbl\u0026gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6… ## $ GR_4 \u0026lt;dbl\u0026gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4… ## $ GR_5 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6… ## $ GR_6 \u0026lt;dbl\u0026gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_7 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_8 \u0026lt;dbl\u0026gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ UNGR_ELM \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_9 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_10 \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_11 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ GR_12 \u0026lt;dbl\u0026gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ UNGR_SEC \u0026lt;dbl\u0026gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ ENR_TOTAL \u0026lt;dbl\u0026gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1… ## $ ADULT \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… and we would like to across each ETHNIC category within each school, ignoring GENDER (including gender could be a interesting question for another time). Luckily the total enrollment is already calculated for us ENR_TOTAL so we don’t have to do that manually.\nsmall_data \u0026lt;- data %\u0026gt;% select(CDS_CODE, ETHNIC, ENR_TOTAL) %\u0026gt;% group_by(CDS_CODE, ETHNIC) %\u0026gt;% summarize(ENR_TOTAL = sum(ENR_TOTAL)) %\u0026gt;% spread(ETHNIC, ENR_TOTAL, fill = 0) %\u0026gt;% ungroup() glimpse(small_data) ## Rows: 10,483 ## Columns: 10 ## $ CDS_CODE \u0026lt;chr\u0026gt; \u0026quot;01100170112607\u0026quot;, \u0026quot;01100170123968\u0026quot;, \u0026quot;01100170124172\u0026quot;, \u0026quot;01100… ## $ `0` \u0026lt;dbl\u0026gt; 0, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 23, 0, 4, 2, 7, 0,… ## $ `1` \u0026lt;dbl\u0026gt; 5, 6, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 3, 6, 0, 0, 4, 1, 1, … ## $ `2` \u0026lt;dbl\u0026gt; 8, 24, 161, 22, 0, 11, 0, 15, 0, 3, 30, 3, 98, 109, 49, 91, … ## $ `3` \u0026lt;dbl\u0026gt; 5, 2, 1, 0, 0, 2, 0, 23, 0, 2, 2, 0, 4, 16, 0, 1, 1, 9, 0, 0… ## $ `4` \u0026lt;dbl\u0026gt; 1, 2, 7, 5, 1, 1, 2, 2, 1, 0, 18, 0, 21, 35, 16, 34, 21, 104… ## $ `5` \u0026lt;dbl\u0026gt; 212, 125, 18, 96, 21, 109, 123, 452, 425, 21, 190, 8, 17, 97… ## $ `6` \u0026lt;dbl\u0026gt; 150, 23, 14, 75, 58, 95, 21, 84, 23, 1, 33, 6, 7, 81, 98, 63… ## $ `7` \u0026lt;dbl\u0026gt; 18, 16, 35, 115, 2, 13, 1, 1, 8, 10, 85, 12, 30, 98, 177, 16… ## $ `9` \u0026lt;dbl\u0026gt; 4, 7, 121, 57, 5, 11, 0, 10, 4, 2, 21, 2, 6, 41, 64, 62, 34,… Before we move on, lets reference the data documentation to see what each of the ETHNIC numbers mean. File Structure which states the following:\n Code 0 = Not reported Code 1 = American Indian or Alaska Native, Not Hispanic Code 2 = Asian, Not Hispanic Code 3 = Pacific Islander, Not Hispanic Code 4 = Filipino, Not Hispanic Code 5 = Hispanic or Latino Code 6 = African American, not Hispanic Code 7 = White, not Hispanic Code 9 = Two or More Races, Not Hispanic  So now we have two decisions we need to deal with before moving on. How to take care of the “Not reported” cases, and “Two or More Races, Not Hispanic”. The second point have been discussed before Updating the USA TODAY Diversity Index.\nLets start with the case of code 0. We notice that the category generally is quite small compared to the other groups, so we need to reason about what would happen if we drop them.\ndiversity_plot(c(10, 10, 2)) + diversity_plot(c(10, 10)) + diversity_plot(c(11, 11)) + diversity_plot(c(10, 10, 10, 3)) + diversity_plot(c(10, 10, 10)) + diversity_plot(c(11, 11, 11)) + plot_layout(nrow = 2) as we see that having them be a separate group (first column), gives a higher diversity score then ignoring them (second column) or adding them evenly to the remaining groups (third column). However ignoring them and spreading them gives mostly the same diversity index (when the group is small compared the the whole). Thus we will drop the “Not reported” column as it would otherwise inflate the diversity index.\nCoping with multi ethnicity is quite hard, and we will pick between four options.\n Assume everyone in “Two or More Races, Not Hispanic” is the same ethnicity Assume everyone in “Two or More Races, Not Hispanic” is all different ethnicities Ignore the group “Two or More Races, Not Hispanic” Distribute the group “Two or More Races, Not Hispanic” evenly into the other groups  Lets evaluate the different choices with some visualizations.\nAssume we have 3 main groups. with 5 in each, and an additional 3 people who have picked “Two or More Races, Not Hispanic”.\ndiversity_plot(c(5, 5, 5, 3)) + diversity_plot(c(5, 5, 5, rep(1, 3))) + diversity_plot(c(5, 5, 5)) + diversity_plot(c(6, 6, 6)) Option 3 and 4 both yields low diversity index considering that the choice of “Two or More Races, Not Hispanic” must indicate that they are different enough not to be put into one of the more precisely defined groups. The first option while appalling from a computational standpoint would treat a black-white mixture and an Asian-American Indian as members of the same group even though they had no racial identity in common. Thus We will work with the second option which in any case might overestimate the diversity of any given population as they oath to be some people “Two or More Races, Not Hispanic” with identical mixes (siblings would be a prime example).\nAfter deciding we can create a dplyr friendly function to calculate the diversity index based on a collection of columns. Here we denote y as the column denoting “Two or More Races, Not Hispanic”.\ndiversity \u0026lt;- function(..., y) { x \u0026lt;- sapply(list(...), cbind) total \u0026lt;- cbind(x, y) 1 - (rowSums(x ^ 2) - rowSums(x)) / (rowSums(total) ^ 2 - rowSums(total)) }  Bring back the data! We are finally able to calculate some diversity indexes!!! Using our recently made function within mutate gives us what we need.\ndata_diversity \u0026lt;- small_data %\u0026gt;% mutate(diversity = diversity(`1`, `2`, `3`, `4`, `5`, `6`, `7`, y = `9`)) Lets run a summary to see what ranges we get.\nsummary(data_diversity$diversity) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## 0.0000 0.3113 0.5216 0.4686 0.6351 1.0000 88 Interesting. All the number are between 0 and 1 which is good! that means that the diversity worked as intended. However there are 88 schools have NA valued diversity, lets look at those schools:\nfilter(data_diversity, is.na(diversity)) %\u0026gt;% head() ## # A tibble: 6 x 11 ## CDS_CODE `0` `1` `2` `3` `4` `5` `6` `7` `9` diversity ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 01612590121210 0 0 0 0 0 0 1 0 0 NaN ## 2 04755070000001 2 0 0 0 0 0 0 1 0 NaN ## 3 06615980000001 0 0 0 0 0 0 0 1 0 NaN ## 4 07618040000000 0 0 0 0 0 0 0 1 0 NaN ## 5 09619600000001 0 0 0 0 0 0 0 1 0 NaN ## 6 10622400114587 0 0 0 0 0 1 0 0 0 NaN So it seems like some of the schools have such a low number of kids that the calculations break down. We will exclude these schools for now.\ndata_diversity \u0026lt;- data_diversity %\u0026gt;% filter(!is.na(diversity)) Next lets look at the distribution of diversity indexes.\nggplot(data_diversity, aes(diversity)) + geom_histogram(binwidth = 0.01) We notice that most of the schools follow a nice shape, except a spike at 0 and 1. Lets look at the 0’s first:\ndata_diversity %\u0026gt;% filter(diversity == 0) %\u0026gt;% head(20) ## # A tibble: 20 x 11 ## CDS_CODE `0` `1` `2` `3` `4` `5` `6` `7` `9` diversity ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 016116800000… 0 0 0 0 0 0 2 0 0 0 ## 2 021002502300… 0 2 0 0 0 0 0 0 0 0 ## 3 076171300000… 0 0 0 0 0 0 0 6 0 0 ## 4 096195200000… 0 0 0 0 0 0 0 3 0 0 ## 5 096197800000… 0 0 0 0 0 0 0 4 0 0 ## 6 106212510304… 0 0 0 0 0 26 0 0 0 0 ## 7 106212510308… 0 0 0 0 0 2 0 0 0 0 ## 8 106215801089… 1 0 0 0 0 13 0 0 0 0 ## 9 106236401268… 0 0 0 0 0 47 0 0 0 0 ## 10 107380910301… 0 0 0 0 0 12 0 0 0 0 ## 11 107399900000… 0 0 0 0 0 3 0 0 0 0 ## 12 107512710302… 0 0 0 0 0 18 0 0 0 0 ## 13 107512710307… 0 0 0 0 0 5 0 0 0 0 ## 14 107523410303… 0 0 0 0 0 8 0 0 0 0 ## 15 107523460058… 0 0 0 0 0 174 0 0 0 0 ## 16 117656211300… 2 0 0 0 0 9 0 0 0 0 ## 17 126281000000… 0 0 0 0 0 0 0 2 0 0 ## 18 127538212300… 0 0 0 0 0 0 0 11 0 0 ## 19 127538261078… 0 0 0 0 0 0 0 7 0 0 ## 20 131013213301… 0 0 0 0 0 8 0 0 0 0 These all appear to be schools with just one race in them, most appear to be of category 5 which is “Hispanic or Latino”. Next lets take a look at the 1’s:\ndata_diversity %\u0026gt;% filter(diversity == 1) %\u0026gt;% head(20) ## # A tibble: 20 x 11 ## CDS_CODE `0` `1` `2` `3` `4` `5` `6` `7` `9` diversity ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 016112700000… 0 0 1 0 0 1 0 1 0 1 ## 2 046152300000… 0 0 0 0 0 0 0 1 1 1 ## 3 056155600000… 0 0 0 0 0 1 0 1 0 1 ## 4 076176200000… 0 0 0 0 0 0 1 1 0 1 ## 5 076177000000… 1 0 1 0 0 1 0 1 0 1 ## 6 097378300000… 1 1 0 0 0 1 0 1 0 1 ## 7 106215801202… 0 0 0 0 0 0 1 1 0 1 ## 8 117548111301… 0 0 0 0 0 1 0 1 0 1 ## 9 141014014300… 0 1 0 0 0 1 0 1 0 1 ## 10 156336200000… 0 0 0 0 0 1 1 0 0 1 ## 11 176402200000… 0 0 0 0 0 1 0 1 0 1 ## 12 196456819961… 0 0 0 0 0 1 0 1 0 1 ## 13 197343761144… 0 0 0 0 0 1 1 0 1 1 ## 14 207641420300… 0 0 0 0 0 1 0 1 0 1 ## 15 246578900000… 0 0 0 0 0 0 1 1 0 1 ## 16 257358525301… 0 0 0 0 0 0 0 1 1 1 ## 17 261026401286… 0 0 0 0 0 1 0 1 1 1 ## 18 286624100000… 0 0 0 0 0 1 0 1 0 1 ## 19 316684500000… 0 0 0 0 0 1 0 1 0 1 ## 20 336697701341… 0 0 0 0 0 1 1 1 0 1 As we have seen before, a diversity index of 1 can only happen if the maximal number of student in each category is 1. These schools seem to be rather small. Taking a look at the first school here\ndata %\u0026gt;% filter(CDS_CODE == \u0026quot;17640220000001\u0026quot;) %\u0026gt;% pull(SCHOOL) ## [1] \u0026quot;Nonpublic, Nonsectarian Schools\u0026quot; \u0026quot;Nonpublic, Nonsectarian Schools\u0026quot; and we get that it is a “Nonpublic, Nonsectarian Schools” which by further investigation shows that there are quite a few of those. Lets take a look at the total enrollment in each of the schools, we have seen a couple of instances with low enrollment and we wouldn’t want those to distort our view.\ntotal_students \u0026lt;- data %\u0026gt;% select(CDS_CODE:SCHOOL, ENR_TOTAL) %\u0026gt;% group_by(CDS_CODE) %\u0026gt;% summarise(ENR_TOTAL = sum(ENR_TOTAL)) %\u0026gt;% ungroup()  We also create a meta data.frame which stores the CDS_CODE along with county, district and school name.\nmeta \u0026lt;- data %\u0026gt;% select(CDS_CODE:SCHOOL) %\u0026gt;% distinct()  Looking at the distribution of total enrollment\ntotal_students %\u0026gt;% ggplot(aes(ENR_TOTAL)) + geom_histogram(bins = 1000) We see a big hump around the 600-700 mark, but also a big spike towards 0, lets take a closer look at the small values\ntotal_students %\u0026gt;% filter(ENR_TOTAL \u0026lt; 250) %\u0026gt;% ggplot(aes(ENR_TOTAL)) + geom_histogram(bins = 250) Here we make another choice that will ultimately change the outcome of the analysis. But I will restrict the investigation to school with a total enrollment of 50 or more.\ndata_big \u0026lt;- total_students %\u0026gt;% left_join(meta, by = \u0026quot;CDS_CODE\u0026quot;) %\u0026gt;% filter(ENR_TOTAL \u0026gt;= 50) Then we join that back to our enrollment data such that we have meta and diversity information in the same data.frame.\ndata_enrollment \u0026lt;- data_diversity %\u0026gt;% right_join(data_big, by = \u0026quot;CDS_CODE\u0026quot;) We start by looking to see if there is some correlation between total enrollment (ENR_TOTAL) and the diversity index (diversity). We fit with a simple linear model to begin with.\ndata_enrollment %\u0026gt;% ggplot(aes(ENR_TOTAL, diversity)) + geom_point(alpha = 0.1) + geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_minimal() + ylim(0, 1) + labs(x = \u0026quot;Enrollment\u0026quot;, y = \u0026quot;Diversity Index\u0026quot;, title = \u0026quot;Larger Californian schools tend to have a higher diversity index\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Its quite a small slope but still about 5% increase in diversity per 2000 students. Lets verify the correlation by checking the null hypothesis.\ndata_enrollment %\u0026gt;% mutate(ENR_TOTAL = ENR_TOTAL / 1000) %\u0026gt;% lm(diversity ~ ENR_TOTAL, data = .) %\u0026gt;% summary() ## ## Call: ## lm(formula = diversity ~ ENR_TOTAL, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5053 -0.1584 0.0533 0.1658 0.3926 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.447968 0.003489 128.399 \u0026lt; 2e-16 *** ## ENR_TOTAL 0.023239 0.004132 5.624 1.92e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.2111 on 9407 degrees of freedom ## Multiple R-squared: 0.003351, Adjusted R-squared: 0.003245 ## F-statistic: 31.63 on 1 and 9407 DF, p-value: 1.923e-08 We can’t reject the null hypothesis of no correlation and we get a better estimate of the slope to 2.3%. Of cause we can’t extrapolate the results outside the range of the existing data since the response variable is bounded.\n Where are we? Lets try to join the enrollment data to the geographical data so we can bring out some maps! The longlat dataset has quite a unfortunate number of missing values.\nlonglat$Latitude %\u0026gt;% is.na() %\u0026gt;% mean() ## [1] 0.2784102 But we will see if we can work with it anyways.\nmiss_lat_div \u0026lt;- data_enrollment %\u0026gt;% left_join(longlat, by = c(\u0026quot;CDS_CODE\u0026quot; = \u0026quot;CDSCode\u0026quot;)) %\u0026gt;% filter(is.na(Latitude)) %\u0026gt;% pull(diversity) have_lat_div \u0026lt;- data_enrollment %\u0026gt;% left_join(longlat, by = c(\u0026quot;CDS_CODE\u0026quot; = \u0026quot;CDSCode\u0026quot;)) %\u0026gt;% filter(!is.na(Latitude)) %\u0026gt;% pull(diversity) Sadly it turns out that the longitude and latitude is NOT missing at random as the distribution of the diversity index in not the same for missing and non-missing data.\ndensity(miss_lat_div) %\u0026gt;% plot() density(have_lat_div) %\u0026gt;% plot() We will still venture on with the remaining data, but with a higher caution then before. Hopefully we will still be able to see some clustering in the remaining data.\nmap \u0026lt;- map_data(\u0026quot;state\u0026quot;) %\u0026gt;% filter(region == \u0026quot;california\u0026quot;) map_point \u0026lt;- data_enrollment %\u0026gt;% left_join(longlat, by = c(\u0026quot;CDS_CODE\u0026quot; = \u0026quot;CDSCode\u0026quot;)) %\u0026gt;% filter(!is.na(Latitude)) map %\u0026gt;% ggplot(aes(long, lat, group = group)) + geom_polygon(fill = \u0026quot;grey80\u0026quot;) + coord_map() + theme_minimal() + geom_point(aes(Longitude, Latitude, color = diversity), data = map_point, inherit.aes = FALSE, alpha = 0.1) + scale_color_viridis_c(option = \u0026quot;A\u0026quot;) + labs(title = \u0026quot;Diversity of schools across California\u0026quot;) We don’t notice any mayor geographical trends. However there are still some gold nuggets here. Dark grey points are actually overlapping low-diversity index points since the alpha is set to 0.1 therefore we see a couple of instances in mid California where there are a couple of low-diversity index schools, with each point landing on a certain city.\nThe two mayor cities (Los Angeles and San Francisco) both have quite a few schools with high diversity, however Los Angeles have some quite dark spots. Lets zoom in to city level. Since we are getting more local, we will be using the ggmap package to quarry the Google Maps so we get a nice underlying map.\nLA_map \u0026lt;- get_map(location = \u0026#39;Los Angeles\u0026#39;, zoom = 9) ggmap(LA_map) + geom_point(data = map_point %\u0026gt;% filter(Longitude \u0026gt; -119, Longitude \u0026lt; -117, Latitude \u0026lt; 34.5, Latitude \u0026gt; 33.5), aes(Longitude, Latitude, color = diversity), alpha = 0.2) + scale_color_viridis_c(option = \u0026quot;A\u0026quot;) + labs(title = \u0026quot;Low diversity areas tends towards city centers \\nin Greater Los Angeles Area\u0026quot;) This maps shows much more interesting trends!! It appears that low diversity schools cluster together near city centers.\n Where is the money? We will end this post by taking a look at if the median household income in the county in which the school is located correlates with the diversity index we have calculated for each school.\nThis is simply done by joining the two data.frames together and piping them into ggplot.\ndata_enrollment %\u0026gt;% left_join(income_data_clean, by = c(\u0026quot;COUNTY\u0026quot; = \u0026quot;county\u0026quot;)) %\u0026gt;% ggplot(aes(median_household_income, diversity)) + geom_jitter(alpha = 0.1, width = 500) + geom_smooth(method = \u0026quot;lm\u0026quot;) + ylim(0, 1) + theme_minimal() + labs(x = \u0026quot;Median household income\u0026quot;, y = \u0026quot;Diversity Index\u0026quot;, title = \u0026quot;Higher diversity index in CA counties with high Median household income\u0026quot;) and we do indeed see a positive correlation between median household income and ethnic diversity in schools in California.\nThis is the end of this analysis, I hope you enjoyed it! See you again next time.\n Diversity plotting function diversity_plot \u0026lt;- function(makeup, fix = 0.45) { df \u0026lt;- tibble(id = seq_len(sum(makeup)), race = imap(makeup, ~ rep(.y, .x)) %\u0026gt;% unlist()) cols \u0026lt;- structure(rainbow(length(makeup)), names = as.character(seq_len(length(makeup)))) %\u0026gt;% c(\u0026quot;diag\u0026quot; = \u0026quot;grey30\u0026quot;, \u0026quot;other\u0026quot; = \u0026quot;grey70\u0026quot;) df1 \u0026lt;- crossing(df, df) %\u0026gt;% mutate(fill = case_when(id == id1 ~ \u0026quot;diag\u0026quot;, race == race1 ~ as.character(race), TRUE ~ \u0026quot;other\u0026quot;)) df1 %\u0026gt;% ggplot(aes(xmin = id - fix, xmax = id + fix, ymin = id1 - fix, ymax = id1 + fix)) + geom_rect(aes(fill = fill)) + theme(panel.background = element_blank(), panel.border = element_blank()) + scale_y_continuous(breaks = df[[\u0026quot;id\u0026quot;]], labels = df[[\u0026quot;race\u0026quot;]]) + scale_x_continuous(breaks = df[[\u0026quot;id\u0026quot;]], labels = df[[\u0026quot;race\u0026quot;]]) + coord_fixed() + scale_fill_manual(values = cols) + guides(fill = \u0026quot;none\u0026quot;) + labs(title = paste0(\u0026quot;The diversity score is \u0026quot;, signif(mean(df1$fill %in% c(\u0026quot;diag\u0026quot;, \u0026quot;other\u0026quot;)), digits = 3))) }  ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"80f9596a1b2aaf0ad3371c695a3ba1f7","permalink":"/2018/05/01/analysing-ethnic-diversity-in-californian-school/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/2018/05/01/analysing-ethnic-diversity-in-californian-school/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nI will In this post explore the ethnic diversity of the student population in schools in California.","tags":null,"title":"Analysing ethnic diversity in Californian school","type":"post"},{"authors":null,"categories":["ggpage","ggplot2"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nggpage version 0.2.0 In this post I will highlight a couple of the new features in the new update of my package ggpage.\nfirst we load the packages we need, which is tidyverse for general tidy tools, ggpage for visualization and finally rtweet and rvest for data collection.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(ggpage) library(rtweet) library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2  The basics The packages includes 2 main functions, ggpage_build and ggpage_plot that will transform the data in the right way and plot it respectively. The reason for the split of the functions is to allow additional transformations to be done on the tokenized data to be used in the plotting.\nThe package includes a example data set of the text Tinderbox by H.C. Andersen\ntinderbox %\u0026gt;% head() ## # A tibble: 6 x 2 ## text book ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026quot;A soldier came marching along the high road: \\\u0026quot;Left, right - le… The tinder-… ## 2 \u0026quot;had his knapsack on his back, and a sword at his side; he had b… The tinder-… ## 3 \u0026quot;and was now returning home. As he walked on, he met a very frig… The tinder-… ## 4 \u0026quot;witch in the road. Her under-lip hung quite down on her breast,… The tinder-… ## 5 \u0026quot;and said, \\\u0026quot;Good evening, soldier; you have a very fine sword, … The tinder-… ## 6 \u0026quot;knapsack, and you are a real soldier; so you shall have as much… The tinder-… This data set can be used directly with ggpage_build and ggpage_plot.\nggpage_build(tinderbox) %\u0026gt;% ggpage_plot() ## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead. ## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead. ## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead. ## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead. ggpage_build expects the column containing the text to be named “text” which it is in the tinderbox object. This visualization gets exiting when you start combining it with the other tools. We can show where the word “tinderbox” appears along with adding some page numbers.\nggpage_build(tinderbox) %\u0026gt;% mutate(tinderbox = word == \u0026quot;tinderbox\u0026quot;) %\u0026gt;% ggpage_plot(mapping = aes(fill = tinderbox), page.number = \u0026quot;top-left\u0026quot;) ## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead. ## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead. ## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead. ## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead. ## Warning: Use of `paper_number_data$page` is discouraged. Use `page` instead. And we see that the word tinderbox only appear 3 times in the middle of the story.\n Vizualizing tweets We can also use this to showcase a number of tweets. For this we will use the rtweet package. We will load in 100 tweets that contain the hash tag #rstats.\n## whatever name you assigned to your created app appname \u0026lt;- \u0026quot;********\u0026quot; ## api key (example below is not a real key) key \u0026lt;- \u0026quot;**********\u0026quot; ## api secret (example below is not a real key) secret \u0026lt;- \u0026quot;********\u0026quot; ## create token named \u0026quot;twitter_token\u0026quot; twitter_token \u0026lt;- create_token( app = appname, consumer_key = key, consumer_secret = secret) rstats_tweets \u0026lt;- rtweet::search_tweets(\u0026quot;#rstats\u0026quot;) %\u0026gt;% mutate(status_id = as.numeric(as.factor(status_id))) Since each tweet is too long by itself will we use the nest_paragraphs function to wrap the texts within each tweet. By passing the tweet id to page.col we will make it such that we have a tweet per page. Additionally we can indicate both whether the tweet is a retweet by coloring the paper blue if it is and green if it isn’t. Lastly we highlight where “rstats” is used.\nrstats_tweets %\u0026gt;% select(status_id, text) %\u0026gt;% nest_paragraphs(text) %\u0026gt;% ggpage_build(page.col = \u0026quot;status_id\u0026quot;, lpp = 4, ncol = 6) %\u0026gt;% mutate(rstats = word == \u0026quot;rstats\u0026quot;) %\u0026gt;% ggpage_plot(mapping = aes(fill = rstats), paper.show = TRUE, paper.color = ifelse(rstats_tweets$is_retweet, \u0026quot;lightblue\u0026quot;, \u0026quot;lightgreen\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;grey60\u0026quot;, \u0026quot;black\u0026quot;)) + labs(title = \u0026quot;100 #rstats tweets\u0026quot;, subtitle = \u0026quot;blue = retweet, green = original\u0026quot;) ## Warning: Use of `paper_data$xmin` is discouraged. Use `xmin` instead. ## Warning: Use of `paper_data$xmax` is discouraged. Use `xmax` instead. ## Warning: Use of `paper_data$ymin` is discouraged. Use `ymin` instead. ## Warning: Use of `paper_data$ymax` is discouraged. Use `ymax` instead. ## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead. ## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead. ## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead. ## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead.  Vizualizing documents Next we will look at the Convention on the Rights of the Child which we will scrape with rvest.\nurl \u0026lt;- \u0026quot;http://www.ohchr.org/EN/ProfessionalInterest/Pages/CRC.aspx\u0026quot; rights_text \u0026lt;- read_html(url) %\u0026gt;% html_nodes(\u0026#39;div[class=\u0026quot;boxtext\u0026quot;]\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% str_split(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() %\u0026gt;% str_wrap() %\u0026gt;% str_split(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() %\u0026gt;% data.frame(text = ., stringsAsFactors = FALSE) In this case will we remove the vertical space between the pages have it appear as a long paper like the website.\nThe wonderful case_when comes in vary handy here when we want to highlight multiple different words.\nfor the purpose of the “United Nations” was it necessary to check that the words “united” and “nations” only appeared in pairs.\nrights_text %\u0026gt;% ggpage_build(wtl = FALSE, y_space_pages = 0, ncol = 7) %\u0026gt;% mutate(highlight = case_when(word %in% c(\u0026quot;child\u0026quot;, \u0026quot;children\u0026quot;) ~ \u0026quot;child\u0026quot;, word %in% c(\u0026quot;right\u0026quot;, \u0026quot;rights\u0026quot;) ~ \u0026quot;rights\u0026quot;, word %in% c(\u0026quot;united\u0026quot;, \u0026quot;nations\u0026quot;) ~ \u0026quot;United Nations\u0026quot;, TRUE ~ \u0026quot;other\u0026quot;)) %\u0026gt;% ggpage_plot(mapping = aes(fill = highlight)) + scale_fill_manual(values = c(\u0026quot;darkgreen\u0026quot;, \u0026quot;grey\u0026quot;, \u0026quot;darkblue\u0026quot;, \u0026quot;darkred\u0026quot;)) + labs(title = \u0026quot;Word highlights in the \u0026#39;Convention on the Rights of the Child\u0026#39;\u0026quot;, fill = NULL) ## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead. ## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead. ## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead. ## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead. This is just a couple of different ways to use this package. I look forward to see what you guys can come up with.\n ","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523059200,"objectID":"1e0b8d6e8d2797f20cb726c9c46e1fbb","permalink":"/2018/04/07/ggpage-version-0.2.0-showcase/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/2018/04/07/ggpage-version-0.2.0-showcase/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nggpage version 0.2.0 In this post I will highlight a couple of the new features in the new update of my package ggpage.","tags":null,"title":"ggpage version 0.2.0 showcase","type":"post"},{"authors":null,"categories":["tidytext","caret"],"content":" the scope of this blog post is to show how to do binary text classification using standard tools such as tidytext and caret packages. One of if not the most common binary text classification task is the spam detection (spam vs non-spam) that happens in most email services but has many other application such as language identification (English vs non-English).\nIn this post I’ll showcase 5 different classification methods to see how they compare with this data. The methods all land on the less complex side of the spectrum and thus does not include creating complex deep neural networks.\nAn expansion of this subject is multiclass text classification which I might write about in the future.\nPackages We load the packages we need for this project. tidyverse for general data science work, tidytext for text manipulation and caret for modeling.\nlibrary(tidyverse) library(tidytext) library(caret)  Data The data we will be using for this demonstration will be some social media disaster tweets discussed in this article. It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.\ndata \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv\u0026quot;) And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren’t classified, marked \"Can't Decide\" so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don’t actually have that much information in them as they are limited in characters and some only contain a couple of words.\nWe will at this stage remove what appears to be urls using some regex and str_replace_all, and we will select the columns id, disaster and text.\ndata_clean \u0026lt;- data %\u0026gt;% filter(choose_one != \u0026quot;Can\u0026#39;t Decide\u0026quot;) %\u0026gt;% mutate(id = `_unit_id`, disaster = choose_one == \u0026quot;Relevant\u0026quot;, text = str_replace_all(text, \u0026quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% select(id, disaster, text) First we take a quick look at the distribution of classes and we see if the classes are balanced\ndata_clean %\u0026gt;% ggplot(aes(disaster)) + geom_bar() And we see that is fairly balanced so we don’t have to worry about sampling this time.\nThe representation we will be using in this post will be the bag-of-words representation in which we just count how many times each word appears in each tweet disregarding grammar and even word order (mostly).\nWe will construct a tf-idf vector model in which each unique word is represented as a column and each document (tweet in our case) is a row of the tf-idf values. This will create a very large matrix/data.frame (a column of each unique word in the total data set) which will overload a lot of the different models we can implement, furthermore will a lot of the words (or features in ML slang) not add considerably information. We have a trade off between information and computational speed.\nFirst we will remove all the stop words, this will insure that common words that usually don’t carry meaning doesn’t take up space (and time) in our model. Next will we only look at words that appear in 10 different tweets. Lastly we will be looking at both unigrams and bigrams to hopefully get a better information extraction.\ndata_counts \u0026lt;- map_df(1:2, ~ unnest_tokens(data_clean, word, text, token = \u0026quot;ngrams\u0026quot;, n = .x)) %\u0026gt;% anti_join(stop_words, by = \u0026quot;word\u0026quot;) %\u0026gt;% count(id, word, sort = TRUE) We will only look at words at appear in at least 10 different tweets.\nwords_10 \u0026lt;- data_counts %\u0026gt;% group_by(word) %\u0026gt;% summarise(n = n()) %\u0026gt;% filter(n \u0026gt;= 10) %\u0026gt;% select(word) we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix.\ndata_dtm \u0026lt;- data_counts %\u0026gt;% right_join(words_10, by = \u0026quot;word\u0026quot;) %\u0026gt;% bind_tf_idf(word, id, n) %\u0026gt;% cast_dtm(id, word, tf_idf) This leaves us with 2993 features. We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction.\nmeta \u0026lt;- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %\u0026gt;% left_join(data_clean[!duplicated(data_clean$id), ], by = \u0026quot;id\u0026quot;) We also create the index (based on the meta data.frame) to separate the data into a training and test set.\nset.seed(1234) trainIndex \u0026lt;- createDataPartition(meta$disaster, p = 0.8, list = FALSE, times = 1) since a lot of the methods take data.frames as inputs we will take the time and create these here:\ndata_df_train \u0026lt;- data_dtm[trainIndex, ] %\u0026gt;% as.matrix() %\u0026gt;% as.data.frame() data_df_test \u0026lt;- data_dtm[-trainIndex, ] %\u0026gt;% as.matrix() %\u0026gt;% as.data.frame() response_train \u0026lt;- meta$disaster[trainIndex] Now each row in the data.frame is a document/tweet (yay tidy principles!!).\n Missing tweets In the feature selection earlier we decided to turn our focus towards certain words and word-pairs, with that we also turned our focus AWAY from certain words. Since the tweets are fairly short in length it wouldn’t be surprising if a handful of the tweets completely skipped out focus as we noted earlier. Lets take a look at those tweets here.\ndata_clean %\u0026gt;% anti_join(meta, by = \u0026quot;id\u0026quot;) %\u0026gt;% head(25) %\u0026gt;% pull(text) We see that a lot of them appears to be part of urls that our regex didn’t detect, furthermore it appears that in those tweet the sole text was the url which wouldn’t have helped us in this case anyways.\n Modeling Now that we have the data all clean and tidy we will turn our heads towards modeling. We will be using the wonderful caret package which we will use to employ the following models\n Support vector machine Naive Bayes LogitBoost Random forest feed-forward neural networks  These where chosen because of their frequent use ( why SVM are good at text classification ) or because they are common in the classification field. They were also chosen because they where able to work with data with this number of variables in a reasonable time.\nFirst time around we will not use a resampling method.\ntrctrl \u0026lt;- trainControl(method = \u0026quot;none\u0026quot;)  SVM The first model will be the svmLinearWeights2 model from the LiblineaR package. Where we specify default parameters.\nsvm_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;svmLinearWeights2\u0026quot;, trControl = trctrl, tuneGrid = data.frame(cost = 1, Loss = 0, weight = 1)) We predict on the test data set based on the fitted model.\nsvm_pred \u0026lt;- predict(svm_mod, newdata = data_df_test) lastly we calculate the confusion matrix using the confusionMatrix function in the caret package.\nsvm_cm \u0026lt;- confusionMatrix(svm_pred, meta[-trainIndex, ]$disaster) svm_cm and we get an accuracy of 0.7461646.\n Naive-Bayes The second model will be the naive_bayes model from the naivebayes package. Where we specify default parameters.\nnb_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;naive_bayes\u0026quot;, trControl = trctrl, tuneGrid = data.frame(laplace = 0, usekernel = FALSE, adjust = FALSE)) We predict on the test data set based on the fitted model.\nnb_pred \u0026lt;- predict(nb_mod, newdata = data_df_test) calculate the confusion matrix\nnb_cm \u0026lt;- confusionMatrix(nb_pred, meta[-trainIndex, ]$disaster) nb_cm and we get an accuracy of 0.5564854.\n LogitBoost The third model will be the LogitBoost model from the caTools package. We don’t have to specify any parameters.\nlogitboost_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;LogitBoost\u0026quot;, trControl = trctrl) We predict on the test data set based on the fitted model.\nlogitboost_pred \u0026lt;- predict(logitboost_mod, newdata = data_df_test) calculate the confusion matrix\nlogitboost_cm \u0026lt;- confusionMatrix(logitboost_pred, meta[-trainIndex, ]$disaster) logitboost_cm and we get an accuracy of 0.632729.\n Random forest The fourth model will be the ranger model from the caTools package. Where we specify default parameters.\nrf_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;ranger\u0026quot;, trControl = trctrl, tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])), splitrule = \u0026quot;gini\u0026quot;, min.node.size = 1)) We predict on the test data set based on the fitted model.\nrf_pred \u0026lt;- predict(rf_mod, newdata = data_df_test) calculate the confusion matrix\nrf_cm \u0026lt;- confusionMatrix(rf_pred, meta[-trainIndex, ]$disaster) rf_cm and we get an accuracy of 0.7777778.\n nnet The fifth and final model will be the nnet model from the caTools package. Where we specify default parameters. We will also specify MaxNWts = 5000 such that it will work. It will need to be more then the number of columns multiplied the size.\nnnet_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;nnet\u0026quot;, trControl = trctrl, tuneGrid = data.frame(size = 1, decay = 5e-4), MaxNWts = 5000) We predict on the test data set based on the fitted model.\nnnet_pred \u0026lt;- predict(nnet_mod, newdata = data_df_test) calculate the confusion matrix\nnnet_cm \u0026lt;- confusionMatrix(nnet_pred, meta[-trainIndex, ]$disaster) nnet_cm and we get an accuracy of 0.7173408.\n Comparing models To see how the different models stack out we combine the metrics together in a data.frame.\nmod_results \u0026lt;- rbind( svm_cm$overall, nb_cm$overall, logitboost_cm$overall, rf_cm$overall, nnet_cm$overall ) %\u0026gt;% as.data.frame() %\u0026gt;% mutate(model = c(\u0026quot;SVM\u0026quot;, \u0026quot;Naive-Bayes\u0026quot;, \u0026quot;LogitBoost\u0026quot;, \u0026quot;Random forest\u0026quot;, \u0026quot;Neural network\u0026quot;)) visualizing the accuracy for the different models with the red line being the “No Information Rate” that is, having a model that just picks the model common class.\nmod_results %\u0026gt;% ggplot(aes(model, Accuracy)) + geom_point() + ylim(0, 1) + geom_hline(yintercept = mod_results$AccuracyNull[1], color = \u0026quot;red\u0026quot;) As you can see all but one approach does better then the “No Information Rate” on its first try before tuning the hyperparameters.\n Tuning hyperparameters After trying out the different models we saw quite a spread in performance. But it important to remember that the results might be because of good/bad default hyperparameters. There are a few different ways to handle this problem. I’ll show on of them here, grid search, on the SVM model so you get the idea.\nWe will be using 10-fold cross-validation and 3 repeats, which will slow down the procedure, but will try to limit and reduce overfitting. We will be using grid search approach to find optimal hyperparameters. For the sake of time have to fixed 2 of the hyperparameters and only let one vary. Remember that the time it takes to search though all combinations take a long time when then number of hyperparameters increase.\nfitControl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 3, repeats = 3, search = \u0026quot;grid\u0026quot;) We have decided to limit the search around the weight parameter’s default value 1.\nsvm_mod \u0026lt;- train(x = data_df_train, y = as.factor(response_train), method = \u0026quot;svmLinearWeights2\u0026quot;, trControl = fitControl, tuneGrid = data.frame(cost = 0.01, Loss = 0, weight = seq(0.5, 1.5, 0.1))) and once it have finished running we can plot the train object to see which value is highest.\nplot(svm_mod) And we see that it appear to be just around 1. It is important to search multiple parameters at the SAME TIME as it can not be assumed that the parameters are independent of each others. Only reason I didn’t do that here was to same the time.\nI will leave to you the reader to find out which of the models have the highest accuracy after doing parameter tuning.\nI hope you have enjoyed this overview of binary text classification.\n ","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522454400,"objectID":"0ae0edcb73b1b6a14fe38ddb157b3080","permalink":"/2018/03/31/binary-text-classification-with-tidytext-and-caret/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/2018/03/31/binary-text-classification-with-tidytext-and-caret/","section":"post","summary":"the scope of this blog post is to show how to do binary text classification using standard tools such as tidytext and caret packages. One of if not the most common binary text classification task is the spam detection (spam vs non-spam) that happens in most email services but has many other application such as language identification (English vs non-English).","tags":null,"title":"Binary text classification with Tidytext and caret","type":"post"},{"authors":null,"categories":["recreate","ggplot2"],"content":" This blogpost uses the old api for gganimate and will not work with current version. No update of this blogpost is planned for this moment.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data, or using data of a similar style.\nThe goal - A flowing sankey chart from nytimes In this excellent article Extensive Data Shows Punishing Reach of Racism for Black Boys by NYTimes includes a lot of very nice charts, both in motion and still. The chart that got biggest reception is the following:\n(see article for moving picture) We see a animated flow chart that follow the style of the classical Sankey chart. This chart will be the goal in this blog post, with 2 changes for brevity. firstly will I use randomly simulated data for my visualization and secondly will I not include the counters on the right-hand side of the chart and only show the creation of the counter on the left-hand as they are created in much the same fashion.\n R packages First we need some packages, but very few of those. Simply using tidyverse and gganimate for animation.\nlibrary(tidyverse) library(gganimate)  Single point We will start with animating a single point first. The path of each point closely resembles a sigmoid curve. I have used those in past visualizations, namely Visualizing trigrams with the Tidyverse.\nand we steal the function I created in that post\nsigmoid \u0026lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) { x \u0026lt;- seq(-scale, scale, length = n) y \u0026lt;- exp(x) / (exp(x) + 1) tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from, y = y * (y_to - y_from) + y_from) } And to get along with that we will have out data\nn_points \u0026lt;- 400 data \u0026lt;- tibble(from = rep(4, n_points), to = sample(1:4, n_points, TRUE), color = sample(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;), n_points, TRUE))  here the data is fairly clean and tidy, with numerical values for from and to but this endpoint should be able to be achieved in most any other appropriate type of data.\nTo simulate the path of a single data point we will use the custom sigmoid on the data for a single row. This gives us this smooth curve of points that resembles the path taken by the points in the original visualization.\nsigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]), n = 100, scale = 10) %\u0026gt;% ggplot(aes(x, y)) + geom_point() To set this in motion we will employ gganimate, for this we will add a time column to act as the frame.\np \u0026lt;- sigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]), n = 100, scale = 10) %\u0026gt;% mutate(time = row_number()) %\u0026gt;% ggplot(aes(x, y, frame = time)) + geom_point() gganimate(p) Which looks very nice so far. Next step is to have multiple points flowing towards different locations.\n multiple points To account for the multiple points we will wrap everything from last section inside a map_df to iterate over the rows. To avoid over plotting we introduce some uniform noise to each point.\np \u0026lt;- map_df(seq_len(nrow(data)), ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %\u0026gt;% mutate(time = row_number() + .x, y = y + runif(1, -0.25, 0.25))) %\u0026gt;% ggplot(aes(x, y, frame = time)) + geom_point() gganimate(p) Everything looks good so far, however the points all look the same, so we will do a little bit of beautification now rather then later. In addition to that will we save the data for the different components in different objects.\nthe following point_data have the modification with bind_cols that binds the information from the data data.frame to the final object. We include the color and removing all themes and guides.\npoint_data \u0026lt;- map_df(seq_len(nrow(data)), ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %\u0026gt;% mutate(time = row_number() + .x, y = y + runif(1, -0.25, 0.25), id = .x) %\u0026gt;% bind_cols(bind_rows(replicate(100, data[.x, -(1:2)], simplify = FALSE)))) p \u0026lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) + geom_point(shape = 15) + theme_void() + guides(color = \u0026quot;none\u0026quot;) gganimate(p, title_frame = FALSE) Which already looks way better. Next up to include animated counter on the left hand side that indicates how many points that have been introduced in the animation. This is simply done by counting how many have started their paths and afterwards padding to fill the length of the animation.\nstart_data_no_end \u0026lt;- point_data %\u0026gt;% group_by(id) %\u0026gt;% summarize(time = min(time)) %\u0026gt;% count(time) %\u0026gt;% arrange(time) %\u0026gt;% mutate(n = cumsum(n), x = 0.125, y = 2, n = str_c(\u0026quot;Follow the lives of \u0026quot;, n, \u0026quot; squares\u0026quot;)) # duplicating last number to fill gif start_data \u0026lt;- start_data_no_end %\u0026gt;% bind_rows( map_df(unique(point_data$time[point_data$time \u0026gt; max(start_data_no_end$time)]), ~ slice(start_data_no_end, nrow(start_data_no_end)) %\u0026gt;% mutate(time = .x)) ) This is added to our plot by the use of geom_text with a new data argument. We did some stringr magic to have a little annotation appear instead of the number itself. Important to have the hjust = 0 such that the annotation doesn’t move around too much.\np \u0026lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) + geom_point(shape = 15) + geom_text(data = start_data, hjust = 0, aes(label = n, frame = time, x = x, y = y), color = \u0026quot;black\u0026quot;) + theme_void() + guides(color = \u0026quot;none\u0026quot;) gganimate(p, title_frame = FALSE)  Ending boxes Like the original illustration there are some boxes where the points “land” in. these are very easily replicated. This will be done a little more programmatic such that it adapts to multiple outputs.\nending_box \u0026lt;- data %\u0026gt;% pull(to) %\u0026gt;% unique() %\u0026gt;% map_df(~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01), y = c(-0.3, 0.3, 0.3, -0.3, -0.3) + .x, id = .x)) We will add this in the same way as before, this time we will use geom_path to draw the box and frame = min(point_data$time) and cumulative = TRUE to have the boxes appear at the first frame and stay there forever.\np \u0026lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) + geom_point() + geom_text(data = start_data, aes(label = n, frame = time, x = x, y = y), color = \u0026quot;black\u0026quot;) + geom_path(data = ending_box, aes(x, y, group = id, frame = min(point_data$time), cumulative = TRUE), color = \u0026quot;grey70\u0026quot;) + theme_void() + coord_cartesian(xlim = c(-0.05, 1.15)) + guides(color = \u0026quot;none\u0026quot;) gganimate(p, title_frame = FALSE)  Filling the box Lastly do we want to fill the boxes as the points approach them. This is done by first figuring out when they appear at the end of their paths, and them drawing boxes at those points, this is done by the end_points and end_lines respectively.\nend_points \u0026lt;- point_data %\u0026gt;% group_by(id) %\u0026gt;% filter(time == max(time)) %\u0026gt;% ungroup() end_lines \u0026lt;- map_df(end_points$id, ~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01), y = c(-0.01, 0.01, 0.01, -0.01, -0.01) + as.numeric(end_points[.x, 2]), id = .x) %\u0026gt;% bind_cols(bind_rows(replicate(5, end_points[.x, -(1:2)], simplify = FALSE))) ) Like before we add the data in a new geom_, with cumulative = TRUE to let the “points” stay.\np \u0026lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) + geom_point() + geom_text(data = start_data, aes(label = n, frame = time, x = x, y = y), color = \u0026quot;black\u0026quot;) + geom_path(data = ending_box, aes(x, y, group = id, frame = min(point_data$time), cumulative = TRUE), color = \u0026quot;grey70\u0026quot;) + geom_polygon(data = end_lines, aes(x, y, fill = color, frame = time, group = id, cumulative = TRUE, color = color)) + theme_void() + coord_cartesian(xlim = c(-0.05, 1.15)) + guides(color = \u0026quot;none\u0026quot;, fill = \u0026quot;none\u0026quot;) gganimate(p, title_frame = FALSE) And this is what I have for you for now. Counters on the right hand side could be done in much the same way as we have seen here, but wouldn’t add much value to showcase that here.\n ","date":1521590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521590400,"objectID":"2371643ed96c46730d690c5f36fe500c","permalink":"/2018/03/21/recreate-sankey-flow-chart/","publishdate":"2018-03-21T00:00:00Z","relpermalink":"/2018/03/21/recreate-sankey-flow-chart/","section":"post","summary":"This blogpost uses the old api for gganimate and will not work with current version. No update of this blogpost is planned for this moment.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse.","tags":null,"title":"Recreate - Sankey flow chart","type":"post"},{"authors":null,"categories":["tidytext"],"content":" This code have been lightly revised to make sure it works as of 2018-12-19.\nText summarization In the realm of text summarization there two main paths:\n extractive summarization abstractive summarization  Where extractive scoring word and sentences according to some metric and then using that information to summarize the text. Usually done by copy/pasting (extracting) the most informative parts of the text.\nThe abstractive methods aims to build a semantic representation of the text and then use natural language generation techniques to generate text describing the informative parts.\nExtractive summarization is primarily the simpler task, with a handful of algorithms do will do the scoring. While with the advent of deep learning did NLP have a boost in abstractive summarization methods.\nIn this post will I focus on an example of a extractive summarization method called TextRank which is based on the PageRank algorithm that is used by Google to rank websites by their importance.\n TextRank Algorithm The TextRank algorithm is based on graph-based ranking algorithm. Generally used in web searches at Google, but have many other applications. Graph-based ranking algorithms try to decide the importance of a vertex by taking into account information about the entire graph rather then the vertex specific information. A typical piece of information would be information between relationships (edges) between the vertices.\nIn the NLP case we need to define the what we want to use as vertices and edges. In our case will we be using sentences as the vertices and words as the connection edges. So sentences with words that appear in many other sentences are seen as more important.\n Data preparation We start by loading the appropriate packages, which include tidyverse for general tasks, tidytext for text manipulations, textrank for the implementation of the TextRank algorithm and finally rvest to scrape an article to use as an example. The github for the textrank package can be found here.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(textrank) library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 To showcase this method I have randomly (EXTENSIVELY filtered political and controversial) selected an article as our guinea pig. The main body is selected using the html_nodes.\nurl \u0026lt;- \u0026quot;http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\u0026quot; article \u0026lt;- read_html(url) %\u0026gt;% html_nodes(\u0026#39;div[class=\u0026quot;padded\u0026quot;]\u0026#39;) %\u0026gt;% html_text() next we load the article into a tibble (since tidytext required the input as a data.frame). We start by tokenize according to sentences which is done by setting token = \"sentences\" in unnest_tokens. The tokenization is not always perfect using this tokenizer, but it have a low number of dependencies and is sufficient for this showcase. Lastly we add sentence number column and switch the order of the columns (textrank_sentences prefers the columns in a certain order).\narticle_sentences \u0026lt;- tibble(text = article) %\u0026gt;% unnest_tokens(sentence, text, token = \u0026quot;sentences\u0026quot;) %\u0026gt;% mutate(sentence_id = row_number()) %\u0026gt;% select(sentence_id, sentence) next we will tokenize again but this time to get words. In doing this we will retain the sentence_id column in our data.\narticle_words \u0026lt;- article_sentences %\u0026gt;% unnest_tokens(word, sentence) now we have all the sufficient input for the textrank_sentences function. However we will go one step further and remove the stop words in article_words since they would appear in most of the sentences and doesn’t really carry any information in them self.\narticle_words \u0026lt;- article_words %\u0026gt;% anti_join(stop_words, by = \u0026quot;word\u0026quot;)  Running TextRank Running the TextRank algorithm is easy, the textrank_sentences function only required 2 inputs.\n A data.frame with sentences A data.frame with tokens (in our case words) which are part of the each sentence  So we are ready to run\narticle_summary \u0026lt;- textrank_sentences(data = article_sentences, terminology = article_words) The output have its own printing method that displays the top 5 sentences:\narticle_summary ## Textrank on sentences, showing top 5 most important sentences found: ## 1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year. ## 2. fitbit says the tracker is designed for children eight years old and up. ## 3. sign up now check the box if you do not wish to receive promotional offers via email from time. ## 4. the fitbit ace looks a lot like the company’s alta tracker, but with a few child-friendly tweaks. ## 5. like many of fitbit’s other products, the fitbit ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long. Which in itself is pretty good.\n Digging deeper While the printing method is good, we can extract the information to good some further analysis. The information about the sentences is stored in sentences. It includes the information article_sentences plus the calculated textrank score.\narticle_summary[[\u0026quot;sentences\u0026quot;]] Lets begging by extracting the top 3 and bottom 3 sentences to see how they differ.\narticle_summary[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% arrange(desc(textrank)) %\u0026gt;% slice(1:3) %\u0026gt;% pull(sentence) ## [1] \u0026quot;fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.\u0026quot; ## [2] \u0026quot;fitbit says the tracker is designed for children eight years old and up.\u0026quot; ## [3] \u0026quot;sign up now check the box if you do not wish to receive promotional offers via email from time.\u0026quot; As expected these are the same sentences as we saw earlier. However the button sentences, doesn’t include the word fitbit (properly rather important word) and focuses more “other” things, like the reference to another product in the second sentence.\narticle_summary[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% arrange(textrank) %\u0026gt;% slice(1:3) %\u0026gt;% pull(sentence) ## [1] \u0026quot;contact us at editors@time.com.\u0026quot; ## [2] \u0026quot;by signing up you are agreeing to our terms of use and privacy policy thank you!\u0026quot; ## [3] \u0026quot;the $39.99 nabi compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\u0026quot; If we look at the article over time, it would be interesting to see where the important sentences appear.\narticle_summary[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% ggplot(aes(textrank_id, textrank, fill = textrank_id)) + geom_col() + theme_minimal() + scale_fill_viridis_c() + guides(fill = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Sentence\u0026quot;, y = \u0026quot;TextRank score\u0026quot;, title = \u0026quot;4 Most informative sentences appear within first half of sentences\u0026quot;, subtitle = \u0026#39;In article \u0026quot;Fitbits Newest Fitness Tracker Is Just for Kids\u0026quot;\u0026#39;, caption = \u0026quot;Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\u0026quot;)  Working with books??? Summaries help cut down the reading when used on articles. Would the same approach work on books? Lets see what happens when you exchange “sentence” in “article” with “chapter” in “book”. I’ll go to my old friend emma form the janeaustenr package. We will borrow some code from the Text Mining with R book to create the chapters. Remember that we want 1 chapter per row.\nemma_chapters \u0026lt;- janeaustenr::emma %\u0026gt;% tibble(text = .) %\u0026gt;% mutate(chapter_id = cumsum(str_detect(text, regex(\u0026quot;^chapter [\\\\divxlc]\u0026quot;, ignore_case = TRUE)))) %\u0026gt;% filter(chapter_id \u0026gt; 0) %\u0026gt;% group_by(chapter_id) %\u0026gt;% summarise(text = paste(text, collapse = \u0026#39; \u0026#39;)) and proceed as before to find the words and remove the stop words.\nemma_words \u0026lt;- emma_chapters %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% anti_join(stop_words, by = \u0026quot;word\u0026quot;) We run the textrank_sentences function again. It should still be very quick, as the bottleneck of the algorithm is more with the number of vertices rather then their individual size.\nemma_summary \u0026lt;- textrank_sentences(data = emma_chapters, terminology = emma_words) We will be careful not to use the standard printing method as it would print 5 whole chapter!!\nInstead we will look at the bar chart again to see if the important chapters appear in any particular order.\nemma_summary[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% ggplot(aes(textrank_id, textrank, fill = textrank_id)) + geom_col() + theme_minimal() + scale_fill_viridis_c(option = \u0026quot;inferno\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Chapter\u0026quot;, y = \u0026quot;TextRank score\u0026quot;, title = \u0026quot;Chapter importance in the novel Emma by Jane Austen\u0026quot;) + scale_x_continuous(breaks = seq(from = 0, to = 55, by = 5)) Which doesn’t appear to be the case in this particular text (which is properly good since skipping a chapter would be discouraged in a book like Emma). however it might prove helpful in non-chronological texts.\nThere is plenty more to look at but I’ll stop for now. If you have any feedback or suggestions please leave a comment, send a email emilhhvitfeldt@gmail.com or hit me up on twitter Emil_Hvitfeldt.\n ","date":1521072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521072000,"objectID":"cfd14f5d51263274db2e43125ea7b20b","permalink":"/2018/03/15/tidy-text-summarization-using-textrank/","publishdate":"2018-03-15T00:00:00Z","relpermalink":"/2018/03/15/tidy-text-summarization-using-textrank/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-19.\nText summarization In the realm of text summarization there two main paths:\n extractive summarization abstractive summarization  Where extractive scoring word and sentences according to some metric and then using that information to summarize the text.","tags":null,"title":"Tidy Text Summarization using TextRank","type":"post"},{"authors":null,"categories":["spacyr","ggplot2"],"content":" This code have been lightly revised to make sure it works as of 2020-04-21.\nWhat are we doing? The inspiration for this post is this beutiful vizualization from Mike Bostock. It nicely visualizes the co-occurrence of characters (when two characters appear in the same chapter) in the novel Les Misérables by Victor Hugo using the data collected by Jacques Bertin (and his assistants).\nThe way this post will differentiate itself from this is that we are going to collect the data ourselves using named entity recognition. Named entity recognition is the discipline of location and classifying named entities in text. Furthermore will we also try to cluster the characters according to their appearance in the novel.\ndisclaimer! I have of the time of writing this analysis not read of familiarized myself with Les Misérables in a attempt to show how a blind text analysis would run.\n Loading package and backend for this we will need tidyverse for general data science tasks, spacyr for the named entity recognition and igraph for some graph related transformation.\nlibrary(tidyverse) library(spacyr) library(igraph) We will be using the spacy NLP back-end as the parser for this analysis since it provides named entity recognition as one of its functionalities.\n Data Les Miserable is quite a long novel, in the terms of words and pages, however due to its age is it in the public domain and is easily available on Project Gutenberg.\nlesmis_raw \u0026lt;- gutenbergr::gutenberg_download(135) Looking thought the beginning of the text we notice how a large part of the beginning of the document is table of content and other information that isn’t of interest in this analysis. Manually checking leads to be discard the first 650 lines of the data. We will also add a chapter column using a regex.\nlesmis_line \u0026lt;- lesmis_raw %\u0026gt;% slice(-(1:650)) %\u0026gt;% mutate(chapter = cumsum(str_detect(text, \u0026quot;CHAPTER \u0026quot;))) For the use in cnlp_annotate() we need a data.frame where each row is a full chapter, with the 2 necessary columns id and text. This is accomplished using a simple map.\nlesmis \u0026lt;- map_df(seq_len(max(lesmis_line$chapter)), ~ tibble(id = .x, text = lesmis_line %\u0026gt;% filter(chapter == .x) %\u0026gt;% pull(text) %\u0026gt;% paste(collapse = \u0026quot; \u0026quot;))) Now we are all ready to run the spacy parser which will only take a couple of minutes.\nlesmis_obj \u0026lt;- spacy_parse(lesmis$text) ## Finding a python executable with spaCy installed... ## spaCy (language model: en_core_web_sm) is installed in /usr/bin/python ## successfully initialized (spaCy Version: 2.0.18, language model: en_core_web_sm) ## (python options: type = \u0026quot;python_executable\u0026quot;, value = \u0026quot;/usr/bin/python\u0026quot;) the output we are given nothing more then a simple tibble\nlesmis_obj ## doc_id sentence_id token_id token lemma pos entity ## 1 text1 1 1 SPACE ## 2 text1 1 2 CHAPTER chapter NOUN ## 3 text1 2 1 I -PRON- PRON ## 4 text1 3 1 — — PUNCT ## 5 text1 4 1 A a DET ## 6 text1 4 2 WOUND wound PROPN the entity information can be extracted using entity_extract()\nentity_extract(lesmis_obj) ## doc_id sentence_id entity entity_type ## 1 text2 1 EXPLAINING_A ORG ## 2 text2 1 NORP ## 3 text2 1 PHENOMENON ORG ## 4 text5 1 TOUSSAINT EVENT ## 5 text6 1 _CHAPTER_IV ORG ## 6 text7 1 LETTER ORG We see quite a few different entity_types, in fact lets take a quick look at the different types that are in this text\nentity_extract(lesmis_obj) %\u0026gt;% pull(entity_type) %\u0026gt;% unique() ## [1] \u0026quot;ORG\u0026quot; \u0026quot;NORP\u0026quot; \u0026quot;EVENT\u0026quot; \u0026quot;LANGUAGE\u0026quot; \u0026quot;GPE\u0026quot; \u0026quot;WORK\u0026quot; ## [7] \u0026quot;PERSON\u0026quot; \u0026quot;FAC\u0026quot; \u0026quot;PRODUCT\u0026quot; \u0026quot;LOC\u0026quot; \u0026quot;LAW\u0026quot; This labeling is explained here. After a bit of investigating I have decided that we only will look at “PERSON” and “ORG” (which is due in part to Napoleon being classified as a organisation.) Furthermore I will limit further analysis to about the 50 most mentioned characters. The rational behind this is that it hopefully would capture most of the important characters, with the weight that characters that are mentioned sparingly but consistently is more important then characters with high density in a few chapter. We will include a few more characters in case we have to exclude some of them after looking.\ntop_person_df \u0026lt;- entity_extract(lesmis_obj) %\u0026gt;% filter(entity_type %in% c(\u0026quot;ORG\u0026quot;, \u0026quot;PERSON\u0026quot;)) %\u0026gt;% count(entity, sort = TRUE) %\u0026gt;% slice(seq_len(60)) top_person_vec \u0026lt;- top_person_df %\u0026gt;% pull(entity) top_person_vec ## [1] \u0026quot;Marius\u0026quot; \u0026quot;Jean_Valjean\u0026quot; ## [3] \u0026quot;Cosette\u0026quot; \u0026quot;Javert\u0026quot; ## [5] \u0026quot;Thénardier\u0026quot; \u0026quot;Gavroche\u0026quot; ## [7] \u0026quot;Bishop\u0026quot; \u0026quot;Fantine\u0026quot; ## [9] \u0026quot;M._Madeleine\u0026quot; \u0026quot;Jondrette\u0026quot; ## [11] \u0026quot; \u0026quot; \u0026quot;M._Gillenormand\u0026quot; ## [13] \u0026quot;_\u0026quot; \u0026quot;Napoleon\u0026quot; ## [15] \u0026quot;Waterloo\u0026quot; \u0026quot;M._Leblanc\u0026quot; ## [17] \u0026quot; \u0026quot; \u0026quot;Madeleine\u0026quot; ## [19] \u0026quot;Grantaire\u0026quot; \u0026quot;Montparnasse\u0026quot; ## [21] \u0026quot;Fauchelevent\u0026quot; \u0026quot;Jean_Valjean_’s\u0026quot; ## [23] \u0026quot;Éponine\u0026quot; \u0026quot;Bossuet\u0026quot; ## [25] \u0026quot;Brujon\u0026quot; \u0026quot;Combeferre\u0026quot; ## [27] \u0026quot;Monseigneur\u0026quot; \u0026quot;M._Fauchelevent\u0026quot; ## [29] \u0026quot;Austerlitz\u0026quot; \u0026quot;Pontmercy\u0026quot; ## [31] \u0026quot;M._Mabeuf\u0026quot; \u0026quot;Champmathieu\u0026quot; ## [33] \u0026quot;Joly\u0026quot; \u0026quot;him:—\u0026quot; ## [35] \u0026quot;Louis_Philippe\u0026quot; \u0026quot;Nicolette\u0026quot; ## [37] \u0026quot;Voltaire\u0026quot; \u0026quot;Cæsar\u0026quot; ## [39] \u0026quot;Gillenormand\u0026quot; \u0026quot;Mayor\u0026quot; ## [41] \u0026quot;Monsieur\u0026quot; \u0026quot;Mademoiselle_Gillenormand\u0026quot; ## [43] \u0026quot;Lark\u0026quot; \u0026quot;Magnon\u0026quot; ## [45] \u0026quot;Théodule\u0026quot; \u0026quot;Bahorel\u0026quot; ## [47] \u0026quot;Louis_XVIII\u0026quot; \u0026quot;Mademoiselle_Baptistine\u0026quot; ## [49] \u0026quot;Blachevelle\u0026quot; \u0026quot;Blücher\u0026quot; ## [51] \u0026quot;Bonaparte\u0026quot; \u0026quot;Gorbeau\u0026quot; ## [53] \u0026quot;Jean_Prouvaire\u0026quot; \u0026quot;Laigle\u0026quot; ## [55] \u0026quot;Restoration\u0026quot; \u0026quot;Courfeyrac\u0026quot; ## [57] \u0026quot;Favourite\u0026quot; \u0026quot;Guelemer\u0026quot; ## [59] \u0026quot;Mabeuf\u0026quot; \u0026quot;Madame_Thénardier\u0026quot; After looking we see a few things we would like to fix before moving on. Firstly is “CHAPTER IV” and “CHAPTER VI” wrongly both classified as “ORG”s. \" “,”-\" and “exclaimed:–” and “Monsieur” have also been misclassified. “Jean Valjean’s” have been classified differently then “Jean Valjean” which is also the case with “Fauchelevent” and “M. Fauchelevent”, “M. Madeleine” and “Madeleine”, “M. Gillenormand”, “Gillenormand” and “Mademoiselle Gillenormand”. We will remove the miss-classifications here, and create a list of all the characters with all of their names. The list is named with the character’s main name for later subsetting.\ntop_person_vec_clean \u0026lt;- top_person_vec[-c(9, 13, 29, 34, 42, 56)] complications \u0026lt;- list(c(\u0026quot;Jean Valjean\u0026quot;, \u0026quot;Jean Valjean\u0026#39;s\u0026quot;), c(\u0026quot;Fauchelevent\u0026quot;, \u0026quot;M. Fauchelevent\u0026quot;), c(\u0026quot;Madeleine\u0026quot;, \u0026quot;M. Madeleine\u0026quot;), c(\u0026quot;Gillenormand\u0026quot;, \u0026quot;M. Gillenormand\u0026quot;, \u0026quot;Mademoiselle Gillenormand\u0026quot;)) characters \u0026lt;- setdiff(top_person_vec_clean, unlist(complications)) %\u0026gt;% as.list() %\u0026gt;% c(complications) names(characters) \u0026lt;- map_chr(characters, ~ .x[1]) We expand the grid of all possible co occurrences and count how many times they both occur within a chapter.\nco_occurrence \u0026lt;- expand.grid(map_chr(characters, ~ .x[1]), map_chr(characters, ~ .x[1])) %\u0026gt;% set_names(c(\u0026quot;person1\u0026quot;, \u0026quot;person2\u0026quot;)) %\u0026gt;% mutate(cooc = map2_dbl(person1, person2, ~ sum(str_detect(lesmis$text, str_c(.x, collapse = \u0026quot;|\u0026quot;)) \u0026amp; str_detect(lesmis$text, str_c(.y, collapse = \u0026quot;|\u0026quot;)))))  Visualize now that we have the co occurrence data we can make some visualizations!! (I will take care of labels etc in the end. Hang on!)\nco_occurrence %\u0026gt;% ggplot(aes(person1, person2, fill = cooc)) + geom_tile() So at a first glance is it hard to see anything due to the default color-scale and the fact that a couple of people, Jean Valjean and Marius, appear in a much higher number of chapters (perhaps they are main characters?). To get a more manageable scale we disregard co occurrence if they have been in less then 5 chapters together(remember that there are a total of 365 chapters in novel).\nco_occurrence_1 \u0026lt;- co_occurrence %\u0026gt;% mutate(cooc = ifelse(cooc \u0026gt; 5, log(cooc), NA)) co_occurrence_1 %\u0026gt;% ggplot(aes(person1, person2, fill = cooc)) + geom_tile() Now we finally see some of the fruit of our earlier work. It is definitely clear that there are groups of people that might form communities but it is unclear which and how many from this heat-map by itself. We would like to reorder the axis’s in the hope that it would create more clarity.\nThis data here can be seen as a Adjacency matrix here the row numbers are vertices and the tiles-values are the edges connecting them. So in a sense we would like to do some cluster analysis on this graph. This can be done by doing some Spectral Graph Partitioning in which we calculate the eigenvectors and sort the vertices by the second smallest eigenvector.\neigen \u0026lt;- co_occurrence_1 %\u0026gt;% # mutate(cooc = !is.na(cooc)) %\u0026gt;% igraph::graph_from_data_frame() %\u0026gt;% igraph::as_adj() %\u0026gt;% eigen() eigenvec2_sort \u0026lt;- data.frame(eigen = eigen$vectors[, length(eigen$values) - 1]) %\u0026gt;% mutate(row = row_number(), names = names(characters)) %\u0026gt;% arrange(eigen) eigen_names \u0026lt;- eigenvec2_sort %\u0026gt;% pull(names) We use sorted names to re-level the factors in the co occurrence data and see if it reveals more structure.\nco_occurrence_1 %\u0026gt;% mutate(person1 = factor(person1, levels = eigen_names), person2 = factor(person2, levels = eigen_names)) %\u0026gt;% ggplot(aes(person1, person2, fill = cooc)) + geom_tile() it isn’t much but it appears to have moved the data slight closer to the diagonal. We will still need to locate some communities in this data. this can be done using the plotted eigenvector.\neigenvec2_sort %\u0026gt;% pull(eigen) %\u0026gt;% plot(type = \u0026quot;o\u0026quot;) And what we are looking at is not their position but at the jumps. There can more easily be seen when we look at the diffs\neigenvec2_sort %\u0026gt;% pull(eigen) %\u0026gt;% diff() %\u0026gt;% plot() abline(h = 0.02) And after playing around a little it seems that 0.02 is a appropriate cutoff.\ncummunity_df \u0026lt;- eigenvec2_sort %\u0026gt;% mutate(community = c(0, diff(eigen) \u0026gt; 0.02) %\u0026gt;% cumsum()) %\u0026gt;% select(names, community) We will color-code the final visualization according to this clustering. So with a couple of joins\nco_occurrence_comm \u0026lt;- co_occurrence_1 %\u0026gt;% filter(!is.na(cooc)) %\u0026gt;% mutate(person1_chr = as.character(person1), person2_chr = as.character(person2), person1 = factor(person1, levels = eigen_names), person2 = factor(person2, levels = eigen_names)) %\u0026gt;% left_join(cummunity_df, by = c(\u0026quot;person1_chr\u0026quot; = \u0026quot;names\u0026quot;)) %\u0026gt;% left_join(cummunity_df, by = c(\u0026quot;person2_chr\u0026quot; = \u0026quot;names\u0026quot;)) %\u0026gt;% mutate(community = ifelse(community.x == community.y, community.x, NA), community = ifelse(!is.na(cooc), community, NA)) With a couple of final touch-ups and we arrive at the final result:\nco_occurrence_comm %\u0026gt;% ggplot(aes(person1, person2, alpha = cooc, fill = factor(community))) + geom_tile(color = \u0026quot;grey50\u0026quot;) + scale_alpha(range = c(0.5, 1)) + scale_fill_brewer(palette = \u0026quot;Set1\u0026quot;, na.value = \u0026quot;grey50\u0026quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1)) + guides(fill = \u0026quot;none\u0026quot;, alpha = \u0026quot;none\u0026quot;) + coord_fixed() + labs(x = NULL, y = NULL, title = \u0026quot;Les Misérables Co-occurrence\u0026quot;, subtitle = \u0026quot;with color-coded communities\u0026quot;)  Conclusion While I wasn’t able to find as full clusters as Jacques Bertin I still managed to get quite a lot of information out of the text regardless. I had fun in the progress and there are many more things I see myself doing with this new data set and spacyr.\nAnd while I couldn’t find a good way to include it in the main body of text. I almost finished the main analysis before realizing that Monsieur means. Mention your mistakes in your posts so others can learn from them!\n ","date":1519344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519344000,"objectID":"c2a4d1af8d0a879147b959044567c6d7","permalink":"/2018/02/23/co-occurrence-of-characters-in-les-miserable/","publishdate":"2018-02-23T00:00:00Z","relpermalink":"/2018/02/23/co-occurrence-of-characters-in-les-miserable/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2020-04-21.\nWhat are we doing? The inspiration for this post is this beutiful vizualization from Mike Bostock.","tags":null,"title":"Co Occurrence of Characters in Les Miserable","type":"post"},{"authors":null,"categories":[],"content":" This code have been lightly revised to make sure it works as of 2018-12-19.\nOverview Recently I stumbled across the Rvision package, which frankly looks amazing so far (still in development as this time of writing). So I decided to take it for a spin and show you girls/guys what I found.\n Setup So for this you will need a computer with a webcam and the Rvision package with its dependencies. It will use ROpenCVLite to access OpenCV’s functionalities. If not already installed, ROpenCVLite will be installed first by the command line below. Furthermore while not necessary for Rvision I have imported dplyr for general data manipulation.\n#devtools::install_github(\u0026quot;swarm-lab/Rvision\u0026quot;) library(Rvision) library(dplyr)  Minimal Setup - working with a photo We will start by simply loading a picture of a parrot. This is done using the function image, which creates an object of class Image. Image objects are pointers toward C++ objects stored in memory and will therefore not work with some functions in base R such sum, %%, etc.\nimg \u0026lt;- image(\u0026quot;parrot.jpg\u0026quot;) If we want to see the image we loaded we simply plot it:\nplot(img) For more information about the Image object we can turn to the property functions:\ndim(img) ## [1] 1595 1919 3 nrow(img) ## [1] 1595 ncol(img) ## [1] 1919 nchan(img) ## [1] 3 bitdepth(img) ## [1] \u0026quot;8U\u0026quot; colorspace(img) ## [1] \u0026quot;BGR\u0026quot;  Blurs Now that we have an Image object we can use some of tools at our disposal, which includes standard things like blurs:\nboxFilter(img, k_height = 25, k_width = 25) %\u0026gt;% plot() gaussianBlur(img, k_height = 25, k_width = 25, sigma_x = 5, sigma_y = 5) %\u0026gt;% plot() medianBlur(img, k_size = 25) %\u0026gt;% plot() sqrBoxFilter(img, k_height = 25, k_width = 25) %\u0026gt;% plot()  Operators Other kinds of operations can be done, such as changing the color space:\nchangeColorSpace(img, \u0026quot;GRAY\u0026quot;) %\u0026gt;% plot() And apply edge detection algorithms such as sobel and laplacian.\nsobel(img) %\u0026gt;% plot()  Draws The package also includes a number of drawing functions starting with the prefix draw, ending with Arrow, Circle, Ellipse, Line, Rectangle and text. These functions, unlike the others, modifies the Image object that is taken in, instead of returning another Image object.\nimg1 \u0026lt;- cloneImage(img) drawCircle(img1, x = 750, y = 750, radius = 200, color = \u0026quot;blue\u0026quot;, thickness = 10) plot(img1)  Blob detection By now we looked at a bunch of different functions but all of them have been used separately. Now lets combine them to detect something inside the picture.\nimg \u0026lt;- image(\u0026quot;balls.jpg\u0026quot;) plot(img) For our further calculations we need to know what color space this image is in\ncolorspace(img) ## [1] \u0026quot;BGR\u0026quot; Which is different then the correctly commonly used RGB. In the following code I tried to find all the blue balls. For that I used the split function to split the Image object into 3, one for each color channel. Then I used a do.call to return a object where the blue channel is more the 200, and the red and green are less then 200, in the hope that it would be enough to identify the blue color without also finding bright areas. This being a logical expression gives us a image file that is white when true and black when it isn’t. Lastly we used the medianBlur to remove any rough edges and flicker. (you can try comment out the medianBlur and see what changes)\nimg %\u0026gt;% split() %\u0026gt;% do.call(function(B, G, R) B \u0026gt; 200 \u0026amp; G \u0026lt; 200 \u0026amp; R \u0026lt; 200, .) %\u0026gt;% medianBlur() %\u0026gt;% plot() If we would like to highlight these balls on the original image we have to detect where these white blobs are and use the draw functions to draw on our original image. We use simpleBlobDetector and play around with the settings till be get something reasonable.\nblue_balls \u0026lt;- img %\u0026gt;% split() %\u0026gt;% do.call(function(B, G, R) B \u0026gt; 200 \u0026amp; G \u0026lt; 200 \u0026amp; R \u0026lt; 200, .) %\u0026gt;% medianBlur() %\u0026gt;% simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255, filter_by_convexity = FALSE, filter_by_inertia = FALSE, min_threshold = 0) blue_balls We use the cloneImage as it creates a new Image object such that the drawing doesn’t change the original Image object.\nimg1 \u0026lt;- cloneImage(img) for (i in seq_len(nrow(blue_balls))) { drawRectangle(image = img1, pt1_x = blue_balls$x[i] - 1 + blue_balls$size[i] / 2, pt1_y = blue_balls$y[i] - 1 + blue_balls$size[i] / 2, pt2_x = blue_balls$x[i] - 1 - blue_balls$size[i] / 2, pt2_y = blue_balls$y[i] - 1 - blue_balls$size[i] / 2, thickness = 3, color = \u0026quot;blue\u0026quot;) } plot(img) We see that it worked fairly well, it didn’t go all the way till the edges of the balls and it appeared to catch the blue artifact on the lower left side, but more careful ranges could take care of that problem.\n Streams Rvision also have a Stream object that we can utilize. the stream function creates a Stream object from the the camera connected to your computer. In my case number 0 is the webcam in my Macbook. Its corresponding function is release which closes the stream object. To capture something we use the handy readNext function that reads the next frame and returns a Image object.\nmy_stream \u0026lt;- stream(0) # 0 will start your default webcam in general. my_img \u0026lt;- readNext(my_stream) release(my_stream) Lets take a look at the image that was captured on my webcam.\nplot(my_img) and what a coincidence!! Its a handful of distinctly colored m\u0026amp;m’s against a dark background. Lets try against to locate the different colors, but before we do that let us reuse what we did earlier and make it into some custom functions:\nblob_fun \u0026lt;- function(img, fun, color = character()) { img %\u0026gt;% split() %\u0026gt;% do.call(fun, .) %\u0026gt;% medianBlur(15) %\u0026gt;% simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255, filter_by_convexity = FALSE, filter_by_inertia = FALSE, min_threshold = 0) %\u0026gt;% mutate(color = color) } multi_draw \u0026lt;- function(img, blobs) { if (nrow(blobs) \u0026gt; 0) { for (i in 1:nrow(blobs)) { drawRectangle(img, blobs$x[i] - 1 + blobs$size[i], blobs$y[i] - 1 + blobs$size[i], blobs$x[i] - 1 - blobs$size[i], blobs$y[i] - 1 - blobs$size[i], thickness = 5, color = blobs$color[1]) } } } Like before we found the blue balls by identifying the region in the BGR color space where its blue, we expand the same idea to the other colors. (I have not attempted brown as it is fairly similar in color to the table)\nblue \u0026lt;- function(B, G, R) B \u0026gt; 150 \u0026amp; R \u0026lt; 200 \u0026amp; G \u0026lt; 200 red \u0026lt;- function(B, G, R) R \u0026gt; 150 \u0026amp; B \u0026lt; 200 \u0026amp; G \u0026lt; 150 green \u0026lt;- function(B, G, R) G \u0026gt; 150 \u0026amp; B \u0026lt; 200 \u0026amp; R \u0026lt; 200 yellow \u0026lt;- function(B, G, R) G \u0026gt; 150 \u0026amp; B \u0026lt; 200 \u0026amp; B \u0026gt; 150 \u0026amp; R \u0026gt; 150 orange \u0026lt;- function(B, G, R) G \u0026gt; 150 \u0026amp; B \u0026lt; 150 \u0026amp; R \u0026gt; 150 Now we just have to run our custom blob detection function and custom drawing function for each color and see the final result\nblue_mms \u0026lt;- blob_fun(my_img, blue, \u0026quot;blue\u0026quot;) red_mms \u0026lt;- blob_fun(my_img, red, \u0026quot;red\u0026quot;) green_mms \u0026lt;- blob_fun(my_img, green, \u0026quot;green\u0026quot;) yellow_mms \u0026lt;- blob_fun(my_img, yellow, \u0026quot;yellow\u0026quot;) orange_mms \u0026lt;- blob_fun(my_img, orange, \u0026quot;orange\u0026quot;) multi_draw(my_img, blue_mms) multi_draw(my_img, red_mms) multi_draw(my_img, green_mms) multi_draw(my_img, yellow_mms) multi_draw(my_img, orange_mms) plot(my_img) And it is wonderful!\n Displays Last trip of the tour is a look at the Displays that Rvision facilitate. And in its simplest form, it creates a window where Image objects can be displayed. Which mean that we are able to do live m\u0026amp;m’s detection!!\nIn a minimal setup you would have this following chuck of code, which sets up a stream, a display and then populates that display with new images taken from the camera till you stop it. And then termination functions for the stream and display. However this is no different then a video feed.\nmy_stream \u0026lt;- stream(0) newDisplay(\u0026quot;Live test\u0026quot;, 360, 640) while(TRUE) { img \u0026lt;- readNext(my_stream) display(img, \u0026quot;Live test\u0026quot;, 25, 360, 640) } destroyDisplay(\u0026quot;Live test\u0026quot;) release(my_stream) So instead we will use the functions from earlier to detect and highlight the colored m\u0026amp;m’s!\nmy_stream \u0026lt;- stream(0) newDisplay(\u0026quot;Live test\u0026quot;, 360, 640) while(TRUE) { img \u0026lt;- readNext(my_stream) blue_mms \u0026lt;- blob_fun(img, blue, \u0026quot;blue\u0026quot;) red_mms \u0026lt;- blob_fun(img, red, \u0026quot;red\u0026quot;) green_mms \u0026lt;- blob_fun(img, green, \u0026quot;green\u0026quot;) yellow_mms \u0026lt;- blob_fun(img, yellow, \u0026quot;yellow\u0026quot;) orange_mms \u0026lt;- blob_fun(img, orange, \u0026quot;orange\u0026quot;) multi_draw(img, blue_mms) multi_draw(img, red_mms) multi_draw(img, green_mms) multi_draw(img, yellow_mms) multi_draw(img, orange_mms) display(img, \u0026quot;Live test\u0026quot;, 25, 360, 640) } destroyDisplay(\u0026quot;Live test\u0026quot;) release(my_stream) Its a little choppy but that might be because of my now quite old Macbook.\n Conclusion I had a blast working with Rvision and I look forward to use it is future projects! I would also recommend against using eatable data points as they tend to disappear over time.\n ","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518652800,"objectID":"1cef256c605ca9599ff98cb64d088f7f","permalink":"/2018/02/15/rvision-a-first-look/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/2018/02/15/rvision-a-first-look/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-19.\nOverview Recently I stumbled across the Rvision package, which frankly looks amazing so far (still in development as this time of writing).","tags":null,"title":"Rvision: A first look","type":"post"},{"authors":null,"categories":["tidytext"],"content":" Overview In this post we will\n talk about The Federalist Papers access and tidy the text using the tidytext package apply our model to the data to predict the author of the disputed papers   The Federalist Papers In the early days of The United States of America around the time when the Constitution was being signed did a series of articles published in various newspapers. These papers where writing under the false name Publius. It was later revealed to have been the collected works of Alexander Hamilton, James Madison and John Jay.\nThe Interesting thing in this was that the authorship of these papers were not consistent. In This is where we come in, in this blog post will we try to see if we are able to classify the troublesome papers.\nIf you would like to read more about this story including past attempts to solve this problem please read How Statistics Solved a 175-Year-Old Mystery About Alexander Hamilton by Ben Christopher.\n Libraries We will start by loading the libraries which includes glmnet that will be used to construct the predictive model later.\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(gutenbergr) library(glmnet)  Data We are lucky today because all of The Federalist Papers happens to be on gutenberg\npapers \u0026lt;- gutenberg_download(1404) head(papers, n = 10) ## # A tibble: 10 x 2 ## gutenberg_id text ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1404 \u0026quot;THE FEDERALIST PAPERS\u0026quot; ## 2 1404 \u0026quot;\u0026quot; ## 3 1404 \u0026quot;By Alexander Hamilton, John Jay, and James Madison\u0026quot; ## 4 1404 \u0026quot;\u0026quot; ## 5 1404 \u0026quot;\u0026quot; ## 6 1404 \u0026quot;\u0026quot; ## 7 1404 \u0026quot;\u0026quot; ## 8 1404 \u0026quot;FEDERALIST No. 1\u0026quot; ## 9 1404 \u0026quot;\u0026quot; ## 10 1404 \u0026quot;General Introduction\u0026quot; For the predictive modeling we are going to do later, I would like to divide each paper up into sentences. This is a rather complicated affair, but I will take a rather ad hoc approach that will be good enough for the purpose of this post. We will do this by collapsing all the lines together and splitting them by ., ! and ?’s.\npapers_sentences \u0026lt;- pull(papers, text) %\u0026gt;% str_c(collapse = \u0026quot; \u0026quot;) %\u0026gt;% str_split(pattern = \u0026quot;\\\\.|\\\\?|\\\\!\u0026quot;) %\u0026gt;% unlist() %\u0026gt;% tibble(text = .) %\u0026gt;% mutate(sentence = row_number()) We would like to assign each of these sentences to the corresponding article number and author. Thus we will first assign each of the 85 papers to the 3 authors and a group for the papers of interest.\nhamilton \u0026lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85) madison \u0026lt;- c(10, 14, 18:20, 37:48) jay \u0026lt;- c(2:5, 64) unknown \u0026lt;- c(49:58, 62:63) Next we will simple look for lines that include “FEDERALIST No” as they would indicate the start of a paper and then label them accordingly.\npapers_words \u0026lt;- papers_sentences %\u0026gt;% mutate(no = cumsum(str_detect(text, regex(\u0026quot;FEDERALIST No\u0026quot;, ignore_case = TRUE)))) %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% mutate(author = case_when(no %in% hamilton ~ \u0026quot;hamilton\u0026quot;, no %in% madison ~ \u0026quot;madison\u0026quot;, no %in% jay ~ \u0026quot;jay\u0026quot;, no %in% unknown ~ \u0026quot;unknown\u0026quot;), id = paste(no, sentence, sep = \u0026quot;-\u0026quot;)) lets take a quick count before we move on\npapers_words %\u0026gt;% count(author) ## # A tibble: 4 x 2 ## author n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 hamilton 114688 ## 2 jay 8539 ## 3 madison 45073 ## 4 unknown 24471 We see that Jay didn’t post as many articles as the other two gentlemen so we will exclude him from further analysis\npapers_words \u0026lt;- papers_words %\u0026gt;% filter(author != \u0026quot;jay\u0026quot;)  Predictive modeling To make this predictive model we will use the term-frequency matrix as our input and as the response will be an indicator that Hamilton wrote the paper. For this modeling we will use the glmnet package which fits a generalized linear model via penalized maximum likelihood. It is quite fast and works great with sparse matrix input, hence the term-frequency matrix.\nThe response is set to the binomial family because of the binary nature of the response (did Hamilton write the sentence).\nFirst we get the term-frequency matrix with the cast_ family in tidytext.\npapers_dtm \u0026lt;- papers_words %\u0026gt;% count(id, word, sort = TRUE) %\u0026gt;% cast_sparse(id, word, n) We will need to define a response variable, which we will do with a simple mutate, along with an indicator for our training set which will be the articles with known authors.\nmeta \u0026lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %\u0026gt;% left_join(papers_words[!duplicated(papers_words$id), ], by = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(y = as.numeric(author == \u0026quot;hamilton\u0026quot;), train = author != \u0026quot;unknown\u0026quot;) ## Warning: Column `id` joining factor and character vector, coercing into ## character vector We will use cross-validation to obtain the best value of the models tuning parameter. This part takes a couple of minutes.\npredictor \u0026lt;- papers_dtm[meta$train, ] response \u0026lt;- meta$y[meta$train] model \u0026lt;- cv.glmnet(predictor, response, family = \u0026quot;binomial\u0026quot;, alpha = 0.9) After running the model, we will add the predicted values to our meta data.frame.\nmeta \u0026lt;- meta %\u0026gt;% mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = \u0026quot;response\u0026quot;, s = model$lambda.1se) %\u0026gt;% as.numeric()) It is now time to visualize the results. First we will look at how the training set have been separated.\nmeta %\u0026gt;% filter(train) %\u0026gt;% ggplot(aes(factor(no), pred)) + geom_boxplot(aes(fill = author)) + theme_minimal() + labs(y = \u0026quot;predicted probability\u0026quot;, x = \u0026quot;Article number\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) + scale_fill_manual(values = c(\u0026quot;#304890\u0026quot;, \u0026quot;#6A7E50\u0026quot;)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) The box plot of predicted probabilities, one value for each sentence, for the 68 papers by Alexander Hamilton and James Madison. The probability represents the extent to which the model believe the sentence was written by Alexander Hamilton.\nLets see if this model can settle the dispute of the 12 papers. We will plot the predicted probabilities of the unknown papers alongside the training set.\nmeta %\u0026gt;% ggplot(aes(factor(no), pred)) + geom_boxplot(aes(fill = author)) + theme_minimal() + labs(y = \u0026quot;predicted probability\u0026quot;, x = \u0026quot;Article number\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) + scale_fill_manual(values = c(\u0026quot;#304890\u0026quot;, \u0026quot;#6A7E50\u0026quot;, \u0026quot;#D6BBD0\u0026quot;)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) we notice that the predicted probabilities don’t quite makes up able to determine who the original author is. This can be due to a variety of different reasons. One of them could be that Madison wrote them and Hamilton edited them.\nDespite the unsuccessful attempt to predict the secret author we still managed to showcase the method which while being unsuccessful in this case could provide useful in other cases.\n Working showcase Since the method proved unsuccessful in determining the secret author did I decide to add an example where the authorship is know. We will use the same data from earlier, only look at known Hamilton and Madison papers, train on some of them and show that the algorithm is able to detect the authorship of the other.\npapers_dtm \u0026lt;- papers_words %\u0026gt;% filter(author != \u0026quot;unknown\u0026quot;) %\u0026gt;% count(id, word, sort = TRUE) %\u0026gt;% cast_dtm(id, word, n) here we let the first 16 papers that they wrote be the test set and the rest be training set.\nmeta \u0026lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %\u0026gt;% left_join(papers_words[!duplicated(papers_words$id), ], by = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(y = as.numeric(author == \u0026quot;hamilton\u0026quot;), train = no \u0026gt; 20) ## Warning: Column `id` joining factor and character vector, coercing into ## character vector predictor \u0026lt;- papers_dtm[meta$train, ] %\u0026gt;% as.matrix() response \u0026lt;- meta$y[meta$train] model \u0026lt;- cv.glmnet(predictor, response, family = \u0026quot;binomial\u0026quot;, alpha = 0.9) meta \u0026lt;- meta %\u0026gt;% mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = \u0026quot;response\u0026quot;, s = model$lambda.1se) %\u0026gt;% as.numeric()) meta %\u0026gt;% ggplot(aes(factor(no), pred)) + geom_boxplot(aes(fill = author)) + theme_minimal() + labs(y = \u0026quot;predicted probability\u0026quot;, x = \u0026quot;Article number\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;) + scale_fill_manual(values = c(\u0026quot;#304890\u0026quot;, \u0026quot;#6A7E50\u0026quot;)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + geom_vline(aes(xintercept = 16.5), color = \u0026quot;red\u0026quot;) So we see that while it isn’t as crystal clear what what the test set predictions are giving us, they still give a pretty good indication.\n ","date":1517270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517270400,"objectID":"0ba56d1e20b1df17f91bad9c8160bb05","permalink":"/2018/01/30/predicting-authorship-in-the-federalist-papers-with-tidytext/","publishdate":"2018-01-30T00:00:00Z","relpermalink":"/2018/01/30/predicting-authorship-in-the-federalist-papers-with-tidytext/","section":"post","summary":"Overview In this post we will\n talk about The Federalist Papers access and tidy the text using the tidytext package apply our model to the data to predict the author of the disputed papers   The Federalist Papers In the early days of The United States of America around the time when the Constitution was being signed did a series of articles published in various newspapers.","tags":null,"title":"Predicting authorship in The Federalist Papers with tidytext","type":"post"},{"authors":null,"categories":["tidytext","ggplot2"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nIn this post I’ll go though how I created the data visualization I posted yesterday on twitter:\nTrying something new! Visualizing top trigrams in Jane Austen's Emma using #tidytext and #tidyverse! Blogpost coming soon! 🤗 #rstats #dataviz pic.twitter.com/Sy1fQJB5Ih — Emil Hvitfeldt (@Emil_Hvitfeldt) 23. januar 2018   What am I looking at? So for this particular data-viz I took novel Emma by Jane Austen, extracted all the trigrams (sentences of length 3), took the 150 most frequent ones and visualized those.\nThis visualization is layered horizontal tree graph where the 3 levels (vertical columns of words) correspond words that appear at the nth place in the trigrams, e.g. first column have the first words of the trigram, second column have middle words of trigrams etc. Up to 20 words in each column are kept and they are ordered and sized according to occurrence in the data.\nThe curves represent how often two words co-occur, with the color representing starting word and transparency related to frequency.\nAll code is presented in the following gist.\n Packages and parameters We will be using the following packages:\nlibrary(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(purrrlyr) And the overall parameters outlined in description are defined here:\nn_word \u0026lt;- 20 n_top \u0026lt;- 150 n_gramming \u0026lt;- 3  Trigrams If you have read Text Mining with R I’m sure you have encountered the janeaustenr package. We will use the Emma novel, and tidytext’s unnest_tokens to calculate the trigrams we need. We also specify the starting words.\ntrigrams \u0026lt;- tibble(text = janeaustenr::emma) %\u0026gt;% unnest_tokens(trigram, text, token = \u0026quot;ngrams\u0026quot;, n = n_gramming) start_words \u0026lt;- c(\u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;) next we find the top 150 trigrams using count and some regex magic. And we use those top words to filter such that we only will be looking at the top 150.\npattern \u0026lt;- str_c(\u0026quot;^\u0026quot;, start_words, \u0026quot; \u0026quot;, collapse = \u0026quot;|\u0026quot;) top_words \u0026lt;- trigrams %\u0026gt;% filter(str_detect(trigram, pattern)) %\u0026gt;% count(trigram, sort = TRUE) %\u0026gt;% slice(seq_len(n_top)) %\u0026gt;% pull(trigram) trigrams \u0026lt;- trigrams %\u0026gt;% filter(trigram %in% top_words)  Nodes Since we know that each trigram have a sample format, we can create a simple function to extract the nth word in a string.\nstr_nth_word \u0026lt;- function(x, n, sep = \u0026quot; \u0026quot;) { str_split(x, pattern = \u0026quot; \u0026quot;) %\u0026gt;% map_chr(~ .x[n]) } The following purrr::map_df\nExtracts the nth word in the trigram\n Counts and sorts the occurrences\n Grabs the top 20 words Equally space them along the y-axis  nodes \u0026lt;- map_df(seq_len(n_gramming), ~ trigrams %\u0026gt;% mutate(word = str_nth_word(trigram, .x)) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% slice(seq_len(n_word)) %\u0026gt;% mutate(y = seq(from = n_word + 1, to = 0, length.out = n() + 2)[seq_len(n()) + 1], x = .x)) plot of node positions Lets see the words so far:\nnodes %\u0026gt;% ggplot(aes(x, y, label = word)) + geom_text()   Edges When we look at the final visualization we see that the words are connected by curved lines. I achieved that by using a sigmoid curve and then transform it to match the starting and end points.\nsigmoid \u0026lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) { x \u0026lt;- seq(-scale, scale, length = n) y \u0026lt;- exp(x) / (exp(x) + 1) tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from, y = y * (y_to - y_from) + y_from) } The following function takes\n a list of trigrams a data.frame of “from” nodes a data.frame of “to” nodes  and returns a data.frame containing the data points for the curves wee need to draw with correct starting and ending points.\negde_lines \u0026lt;- function(trigram, from_word, to_word, scale = 5, n = 50, x_space = 0) { from_word \u0026lt;- from_word %\u0026gt;% select(-n) %\u0026gt;% set_names(c(\u0026quot;from\u0026quot;, \u0026quot;y_from\u0026quot;, \u0026quot;x_from\u0026quot;)) to_word \u0026lt;- to_word %\u0026gt;% select(-n) %\u0026gt;% set_names(c(\u0026quot;to\u0026quot;, \u0026quot;y_to\u0026quot;, \u0026quot;x_to\u0026quot;)) links \u0026lt;- crossing(from = from_word$from, to = to_word$to) %\u0026gt;% mutate(word_pair = paste(from, to), number = map_dbl(word_pair, ~ sum(str_detect(trigram$trigram, .x)))) %\u0026gt;% left_join(from_word, by = \u0026quot;from\u0026quot;) %\u0026gt;% left_join(to_word, by = \u0026quot;to\u0026quot;) links %\u0026gt;% by_row(~ sigmoid(x_from = .x$x_from + 0.2 + x_space, x_to = .x$x_to - 0.05, y_from = .x$y_from, y_to = .x$y_to, scale = scale, n = n) %\u0026gt;% mutate(word_pair = .x$word_pair, number = .x$number, from = .x$from)) %\u0026gt;% pull(.out) %\u0026gt;% bind_rows() } plot of first set of egdes Lets take a look at the first set of edges to see if it is working.\negde_lines(trigram = trigrams, from_word = filter(nodes, x == 1), to_word = filter(nodes, x == 2)) %\u0026gt;% filter(number \u0026gt; 0) %\u0026gt;% ggplot(aes(x, y, group = word_pair, alpha = number, color = from)) + geom_line()  Calculating all egdes For ease (and laziness) I have desired to calculate the edges in sections\n edges between first and second column edges between second and third column for words that start with “he” edges between second and third column for words that start with “she”  and combine by the end.\n# egdes between first and second column egde1 \u0026lt;- egde_lines(trigram = trigrams, from_word = filter(nodes, x == 1), to_word = filter(nodes, x == 2), n = 50) %\u0026gt;% filter(number \u0026gt; 0) %\u0026gt;% mutate(id = word_pair) # Words in second colunm ## That start with he second_word_he \u0026lt;- nodes %\u0026gt;% filter(x == 2) %\u0026gt;% select(-n) %\u0026gt;% left_join( trigrams %\u0026gt;% filter(str_nth_word(trigram, 1) == start_words[1]) %\u0026gt;% mutate(word = str_nth_word(trigram, 2)) %\u0026gt;% count(word), by = \u0026quot;word\u0026quot; ) %\u0026gt;% replace_na(list(n = 0)) ## That start with she second_word_she \u0026lt;- nodes %\u0026gt;% filter(x == 2) %\u0026gt;% select(-n) %\u0026gt;% left_join( trigrams %\u0026gt;% filter(str_nth_word(trigram, 1) == start_words[2]) %\u0026gt;% mutate(word = str_nth_word(trigram, 2)) %\u0026gt;% count(word), by = \u0026quot;word\u0026quot; ) %\u0026gt;% replace_na(list(n = 0)) # Words in third colunm ## That start with he third_word_he \u0026lt;- nodes %\u0026gt;% filter(x == 3) %\u0026gt;% select(-n) %\u0026gt;% left_join( trigrams %\u0026gt;% filter(str_nth_word(trigram, 1) == start_words[1]) %\u0026gt;% mutate(word = str_nth_word(trigram, 3)) %\u0026gt;% count(word), by = \u0026quot;word\u0026quot; ) %\u0026gt;% replace_na(list(n = 0)) ## That start with she third_word_she \u0026lt;- nodes %\u0026gt;% filter(x == 3) %\u0026gt;% select(-n) %\u0026gt;% left_join( trigrams %\u0026gt;% filter(str_nth_word(trigram, 1) == start_words[2]) %\u0026gt;% mutate(word = str_nth_word(trigram, 3)) %\u0026gt;% count(word), by = \u0026quot;word\u0026quot; ) %\u0026gt;% replace_na(list(n = 0)) # egdes between second and third column that starts with he egde2_he \u0026lt;- egde_lines(filter(trigrams, str_detect(trigram, paste0(\u0026quot;^\u0026quot;, start_words[1], \u0026quot; \u0026quot;))), second_word_he, third_word_he, n = 50) %\u0026gt;% mutate(y = y + 0.05, from = start_words[1], id = str_c(from, word_pair, sep = \u0026quot; \u0026quot;)) %\u0026gt;% filter(number \u0026gt; 0) # egdes between second and third column that starts with she egde2_she \u0026lt;- egde_lines(filter(trigrams, str_detect(trigram, paste0(\u0026quot;^\u0026quot;, start_words[2], \u0026quot; \u0026quot;))), second_word_she, third_word_she, n = 50) %\u0026gt;% mutate(y = y - 0.05, from = start_words[2], id = str_c(from, word_pair, sep = \u0026quot; \u0026quot;)) %\u0026gt;% filter(number \u0026gt; 0) # All edges edges \u0026lt;- bind_rows(egde1, egde2_he, egde2_she)   vizualisation Now we just add it all together. All labels, change colors, adjust xlim to fit words on the page.\np \u0026lt;- nodes %\u0026gt;% ggplot(aes(x, y, label = word, size = n)) + geom_text(hjust = 0, color = \u0026quot;#DDDDDD\u0026quot;) + theme_void() + geom_line(data = edges, aes(x, y, group = id, color = from, alpha = sqrt(number)), inherit.aes = FALSE) + theme(plot.background = element_rect(fill = \u0026quot;#666666\u0026quot;, colour = \u0026#39;black\u0026#39;), text = element_text(color = \u0026quot;#EEEEEE\u0026quot;, size = 15)) + guides(alpha = \u0026quot;none\u0026quot;, color = \u0026quot;none\u0026quot;, size = \u0026quot;none\u0026quot;) + xlim(c(0.9, 3.2)) + scale_color_manual(values = c(\u0026quot;#5EF1F1\u0026quot;, \u0026quot;#FA62D0\u0026quot;)) + labs(title = \u0026quot; Vizualizing trigrams in Jane Austen\u0026#39;s, Emma\u0026quot;) + scale_size(range = c(3, 8)) p  Notes There are a couple of differences between the Viz I posted online yesterday and the result here in this post due to a couple of mistakes found in the code during cleanup.\n Extra vizualisations n_word \u0026lt;- 20 n_top \u0026lt;- 150 n_gramming \u0026lt;- 3 trigrams \u0026lt;- tibble(text = janeaustenr::emma) %\u0026gt;% unnest_tokens(trigram, text, token = \u0026quot;ngrams\u0026quot;, n = n_gramming) start_words \u0026lt;- c(\u0026quot;i\u0026quot;, \u0026quot;you\u0026quot;) n_word \u0026lt;- 20 n_top \u0026lt;- 150 n_gramming \u0026lt;- 3 library(rvest) sherlock_holmes \u0026lt;- read_html(\u0026quot;https://sherlock-holm.es/stories/plain-text/cnus.txt\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_split(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() trigrams \u0026lt;- tibble(text = sherlock_holmes) %\u0026gt;% unnest_tokens(trigram, text, token = \u0026quot;ngrams\u0026quot;, n = n_gramming) start_words \u0026lt;- c(\u0026quot;holmes\u0026quot;, \u0026quot;watson\u0026quot;) ## Loading required package: xml2 ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 ## ## Attaching package: \u0026#39;rvest\u0026#39; ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## pluck ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## guess_encoding  ","date":1516665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516665600,"objectID":"9e526790244d1c82f535046dfe023326","permalink":"/2018/01/23/visualizing-trigrams-with-the-tidyverse/","publishdate":"2018-01-23T00:00:00Z","relpermalink":"/2018/01/23/visualizing-trigrams-with-the-tidyverse/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nIn this post I’ll go though how I created the data visualization I posted yesterday on twitter:","tags":null,"title":"Visualizing trigrams with the Tidyverse","type":"post"},{"authors":null,"categories":[],"content":" This code have been lightly revised to make sure it works as of 2018-12-18.\nPurrr tips and tricks If you like me started by only using map() and its cousins (map_df, map_dbl, etc) you are missing out a lot of what purrr have to offer! With the advent of #purrrresolution on twitter I’ll throw my 2 cents in in form of my bag of tips and tricks (which I’ll update in the future).\nFirst we load the packages:\nlibrary(tidyverse) library(repurrrsive) # datasets used in some of the examples. loading files Multiple files can be read and combined at once using map_df and read_cvs.\nfiles \u0026lt;- c(\u0026quot;2015.cvs\u0026quot;, \u0026quot;2016.cvs\u0026quot;, \u0026quot;2017.cvs\u0026quot;) map_df(files, read_csv) Combine with list.files to create magic1.\nfiles \u0026lt;- list.files(\u0026quot;../open-data/\u0026quot;, pattern = \u0026quot;^2017\u0026quot;, full.names = TRUE) full \u0026lt;- map_df(files, read_csv)  combine if you forget *_df the first time around. If you like me sometimes forget to end my map() with my desired out put. A last resort is to manually combine it in a second line if you don’t want to replace map() with map_df() (which is properly the better advice, but can be handy in a pinch).\nX \u0026lt;- map(1:10000, ~ data.frame(x = .x)) X \u0026lt;- bind_rows(X)  name shortcut in map provide “TEXT” to extract the element named “TEXT”. Follow 3 lines are equivalent.\nmap(got_chars, function(x) x[[\u0026quot;name\u0026quot;]]) map(got_chars, ~ .x[[\u0026quot;name\u0026quot;]]) map(got_chars, \u0026quot;name\u0026quot;) works the same with indexes.2\nmap(got_chars, function(x) x[[1]]) map(got_chars, ~ .x[[1]]) map(got_chars, 1)  use {} inside map If you don’t know how to write the proper anonymous function or you want some counter in your map(), you can use {} to construct your anonymous function.\nHere is a simple toy example that shows that you can write multiple lines inside map.\nmap(1:3, ~ { h \u0026lt;- .x + 2 g \u0026lt;- .x - 2 h + g }) map(1:3, ~ { Sys.sleep(10) cat(.x) .x }) This can be very handy if you want to be a responsible (websraping) pirate3.\nlibrary(httr) s_GET \u0026lt;- safely(GET) pb \u0026lt;- progress_estimated(length(target_urls)) map(target_urls, ~{ pb$tick()$print() Sys.sleep(5) s_GET(.x) }) -\u0026gt; httr_raw_responses  discard, keep and compact discard() and keep() will provide very valuable since they help you filter your list/vector based on certain predictors.\nThey can be useful in cases of webcraping where certain lines are to be ignored.\nlibrary(rvest) url \u0026lt;- \u0026quot;http://www.imdb.com/chart/boxoffice\u0026quot; read_html(url) %\u0026gt;% html_nodes(\u0026#39;tr\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% str_replace_all(\u0026quot;\\n +\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% trimws() %\u0026gt;% keep(~ str_extract(.x, \u0026quot;.$\u0026quot;) %in% 0:9) %\u0026gt;% discard(~ as.numeric(str_extract(.x, \u0026quot;.$\u0026quot;)) \u0026gt; 5) Where we here scrape Top Box Office (US) from IMDb.com and we use keep() to keeps all lines that end in a integer and discards() to discards all lines where the integer is more then 5.\ncompact() is a handy wrapper that removed all elements that are NULL.\n safely + compact If you have a function that sometimes throws an error, warning or for whatever reason isn’t entirely stable, you can use the wonder of safely() and compact(). safely() is a function that takes a function f() and returns a function safe_f() that returns a list with the elements result and error where result is the output of f() if it is able to run, and NULL otherwise. This means that we can create a function that will always work!\nunstable_function \u0026lt;- function() { ... } safe_function \u0026lt;- safely(unstable_function) map(data, ~ safe_function(.x)) %\u0026gt;% map(\u0026quot;result\u0026quot;) %\u0026gt;% compact() combining this with compact which removes all NULL values thus returning only the successful calls.\n Reduce purrr includes an little group of functions called reduce() (with its cousins reduce_right(), reduce2() and reduce2_right()) which iteratively combines from the left (right for reduce_right()) making\nreduce(list(x1, x2, x3), f) f(f(x1, x2), x3) equivalent.\nThis example4 comes from Colin Fay shows how to use reduce().\nregex_build \u0026lt;- function(list){ reduce(list, ~ paste(.x, .y, sep = \u0026quot;|\u0026quot;)) } regex_build(letters[1:5]) ## [1] \u0026quot;a|b|c|d|e\u0026quot; This example by Jason Becker5 shows how to easier label data using reduce_right.\n# Load a directory of .csv files that has each of the lookup tables lookups \u0026lt;- map(dir(\u0026#39;data/lookups\u0026#39;), read.csv, stringsAsFactors = FALSE) # Alternatively if you have a single lookup table with code_type as your # data attribute you\u0026#39;re looking up # lookups \u0026lt;- split(lookups, code_type) lookups$real_data \u0026lt;- read.csv(\u0026#39;data/real_data.csv\u0026#39;, stringsAsFactors = FALSE) real_data \u0026lt;- reduce_right(lookups, left_join)  pluck I find that subsetting list can be a hassle more often then not. But pluck() have really helped to alleviate those problems quite a bit.\nlist(A = list(\u0026quot;a1\u0026quot;,\u0026quot;a2\u0026quot;), B = list(\u0026quot;b1\u0026quot;, \u0026quot;b2\u0026quot;), C = list(\u0026quot;c1\u0026quot;, \u0026quot;c2\u0026quot;), D = list(\u0026quot;d1\u0026quot;, \u0026quot;d2\u0026quot;, \u0026quot;d3\u0026quot;)) %\u0026gt;% pluck(1)  head_while, tail_while purrr includes the twins head_while and tail_while which will gives you all the elements that satisfy the condition intill the first time it doesn’t.\nX \u0026lt;- sample(1:100) # This p \u0026lt;- function(X) !(X \u0026gt;= 10) X[seq(Position(p, X) - 1)] # is the same as this head_while(X, ~ .x \u0026gt;= 10)  rerun if you need to do some simulation studies rerun could prove very useful. It takes 2 arguments. .n is the number of times to run, and ... is the expression that have to be rerun.\nrerun(.n = 10, rnorm(10)) %\u0026gt;% map_df(~ tibble(mean = mean(.x), sd = sd(.x), median = median(.x)))  compose This little wonder of a function composes multiple functions to be applied in order from right to left.\nThis toy examples show how it works:\nsample(x = 1:6, size = 50, replace = TRUE) %\u0026gt;% table %\u0026gt;% sort %\u0026gt;% names dice1 \u0026lt;- function(n) sample(size = n, x = 1:6, replace = TRUE) dice_rank \u0026lt;- compose(names, sort, table, dice1) dice_rank(50) A more informative is found here6:\nlibrary(broom) tidy_lm \u0026lt;- compose(tidy, lm) tidy_lm(Sepal.Length ~ Species, data = iris) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 5.01 0.0728 68.8 1.13e-113 ## 2 Speciesversicolor 0.93 0.103 9.03 8.77e- 16 ## 3 Speciesvirginica 1.58 0.103 15.4 2.21e- 32  imap imap() is a handy little wrapper that acts as the indexed map(). Thus making it shorthand for map2(x, names(x), ...) when x have named and map2(x, seq_along(x), ...) when it doesn’t have names.\nimap_dbl(sample(10), ~ { cat(\u0026quot;draw nr\u0026quot;, .y, \u0026quot;is\u0026quot;, .x, \u0026quot;\\n\u0026quot;) .x }) or it could be used in conjunction with rerun() to easily add id to each sample.\nrerun(.n = 10, rnorm(10)) %\u0026gt;% imap_dfr(~ tibble(run = .y, mean = mean(.x), sd = sd(.x), median = median(.x)))   Sources http://ghement.ca/purrr.html\nhttp://statwonk.com/purrr.html\nhttps://maraaverick.rbind.io/2017/09/purrr-ty-posts/\nhttp://serialmentor.com/blog/2016/6/13/reading-and-combining-many-tidy-data-files-in-R\nhttp://colinfay.me/purrr-web-mining/\nhttp://colinfay.me/purrr-text-wrangling/\nhttp://colinfay.me/purrr-set-na/\nhttp://colinfay.me/purrr-mappers/\nhttp://colinfay.me/purrr-code-optim/\nhttp://colinfay.me/purrr-statistics/\n  ColinFay/df a list↩\n jennybc.github.io - Introduction to map(): extract elements↩\n Pirating Web Content Responsibly With R↩\n A Crazy Little Thing Called {purrr} - Part 2 : Text Wrangling↩\n Labeling Data with purrr↩\n A Crazy Little Thing Called {purrr} - Part 5: code optimization↩\n   ","date":1515369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515369600,"objectID":"fcfd6a51b14276335dc02cc42c6adcfb","permalink":"/2018/01/08/purrr-tips-and-tricks/","publishdate":"2018-01-08T00:00:00Z","relpermalink":"/2018/01/08/purrr-tips-and-tricks/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-18.\nPurrr tips and tricks If you like me started by only using map() and its cousins (map_df, map_dbl, etc) you are missing out a lot of what purrr have to offer!","tags":null,"title":"Purrr - tips and tricks","type":"post"},{"authors":null,"categories":["recreate","ggplot2","web scraping"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data, or using data of a similar style.\nThe goal - An annual sunshine record report I have recently read The Visual Display of Quantitative Information by Edward R Tufte, which I highly recommend. In the book the following chart was displayed which showed the sunshine record for each day day of the year.\nF.J. Monkhouse and H.R. Wilkinson, Maps and Diagrams (London, third edition 1971), 242-243.\nThe goal for the rest of this post is to create something similar. Since we don’t have direct access to the data, we will scrape some data for ourselves. All code will be shown together in the end of the post and in this gist\n R packages First we need some packages\nlibrary(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(lubridate) library(glue) library(ehlib) # devtools::install_github(\u0026quot;EmilHvitfeldt/ehlib\u0026quot;) The last package is my personal R package ehlib where I store some frequently used functions. If you do not wish to install/load this package just run the following code:\nstr_between \u0026lt;- function(string, start, end) { stringr::str_extract(string, stringr::str_c(start, \u0026#39;(.*?)\u0026#39;, end, collapse = \u0026#39;\u0026#39;)) %\u0026gt;% stringr::str_replace(start, \u0026quot;\u0026quot;) %\u0026gt;% stringr::str_replace(end, \u0026quot;\u0026quot;) } str_before \u0026lt;- function(string, pattern) { stringr::str_extract(string, stringr::str_c(\u0026quot;.+?(?=\u0026quot;, pattern, \u0026quot;)\u0026quot;)) }  Data collection So for this production we need, Weather information. But more specifically we need information about if the sun is shining for various times during the day, preferable for all days of the year. In addition sunrise and sunset times is also needed.\nWe will be scraping weather history from wunderground. On the button of the page https://www.wunderground.com/history/airport/KCQT/2018/1/1/DailyHistory.html we locate a table with “Time” and “Conditions”. Furthermore both sunrise and sunset times are present on the page.\nFor the website we need an airport code, year, month and day. Airport codes will have to be found manually by browsing the website. For a vector of all the days in a given year we use the following function that uses\nall_dates_in \u0026lt;- function(year) { if(ymd(glue::glue(\u0026quot;{year}0101\u0026quot;)) \u0026gt; as.Date(Sys.time())) { stop(\u0026quot;Please select a past or current year.\u0026quot;) } start \u0026lt;- ymd(glue::glue(\u0026quot;{year}0101\u0026quot;)) if(as.Date(Sys.time()) \u0026gt; ymd(glue::glue(\u0026quot;{year}1231\u0026quot;))) { end \u0026lt;- ymd(glue::glue(\u0026quot;{year}1231\u0026quot;)) } else { end \u0026lt;- as.Date(Sys.time()) } seq(start, end, by = \u0026quot;day\u0026quot;) } this function will work even if you pick a year that have not ended yet. As 2017 have just ended I though it would be appropriate to look back on that year.\nyear \u0026lt;- 2017 dates \u0026lt;- all_dates_in(year) head(dates) ## [1] \u0026quot;2017-01-01\u0026quot; \u0026quot;2017-01-02\u0026quot; \u0026quot;2017-01-03\u0026quot; \u0026quot;2017-01-04\u0026quot; \u0026quot;2017-01-05\u0026quot; ## [6] \u0026quot;2017-01-06\u0026quot; next we have a little function that creates a url from the airport code and the date. For safety we will wrap that function in purrr::safely.\nweather_data_html \u0026lt;- function(date, code) { url \u0026lt;- str_c(\u0026quot;https://www.wunderground.com/history/airport/\u0026quot;, code, \u0026quot;/\u0026quot;, year(date), \u0026quot;/\u0026quot;, month(date), \u0026quot;/\u0026quot;, mday(date), \u0026quot;/DailyHistory.html\u0026quot;) html_url \u0026lt;- read_html(url) } weather_data_html \u0026lt;- purrr::safely(weather_data_html) For this code-though will we be using airport code KCQT, which is placed in Los Angeles Downtown, CA.\nWe add some ‘crawl-delay’ of 5 seconds and let it run. Please remember that this will take over 30 minutes to run with a delay in place but we do it to be nice.\nairport_code \u0026lt;- \u0026quot;KCQT\u0026quot; full_data \u0026lt;- map(dates, ~{ weather_data_html(.x, airport_code) Sys.sleep(5) cat(month(.x), \u0026quot;/\u0026quot;, mday(.x), \u0026quot;\\n\u0026quot;, sep = \u0026quot;\u0026quot;) }) We can check whether all of the links went though.\nmap_lgl(full_data, ~ is.null(.x$error))  Data wrangling Since we will be working with times quite a lot in the section we will use the lubridate package quite some time. In addition to that package I have devised this following function to turn something of the form “2:51 PM” into the number of minutes after midnight.\nampm_minutes \u0026lt;- function(x) { as.numeric(str_between(x, \u0026quot;:\u0026quot;, \u0026quot; \u0026quot;)) + as.numeric(str_replace(str_before(x, \u0026quot;:\u0026quot;), \u0026quot;12\u0026quot;, \u0026quot;0\u0026quot;)) * 60 + 60 * 12 * str_detect(x, \u0026quot;PM\u0026quot;) } Next we have the main wrangling function that takes the input, extracts the sunrise, sunset times and add them to the table that is also extracted.\ndata_wrangling \u0026lt;- function(html_url, date) { # Sun rise time sun_rise \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;div[id=\u0026quot;astronomy-mod\u0026quot;] table\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% .[1] %\u0026gt;% str_between(\u0026quot;Time\\n\\t\\t\u0026quot;, \u0026quot;\\n\\t\\t\u0026quot;) # Sun set time sun_set \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;div[id=\u0026quot;astronomy-mod\u0026quot;] table\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% .[1] %\u0026gt;% str_between(\u0026quot;\\n\\t\\t\u0026quot;, \u0026quot;\\n\\t\\tCivil\u0026quot;) # Table table \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;table[id=\u0026quot;obsTable\u0026quot;]\u0026#39;) %\u0026gt;% html_table() %\u0026gt;% .[[1]] # Time column standardization is_daylight \u0026lt;- any(\u0026quot;Time (PDT)\u0026quot; == names(table), \u0026quot;Time (MDT)\u0026quot; == names(table), \u0026quot;Time (CDT)\u0026quot; == names(table), \u0026quot;Time (EDT)\u0026quot; == names(table)) time_names \u0026lt;- str_c(\u0026quot;Time\u0026quot;, c(\u0026quot; (PDT)\u0026quot;, \u0026quot; (MDT)\u0026quot;, \u0026quot; (CDT)\u0026quot;, \u0026quot; (EDT)\u0026quot;, \u0026quot; (PST)\u0026quot;, \u0026quot; (MST)\u0026quot;, \u0026quot; (CST)\u0026quot;, \u0026quot; (EST)\u0026quot;)) names(table) \u0026lt;- if_else(names(table) %in% time_names, \u0026quot;Time\u0026quot;, names(table)) table %\u0026gt;% mutate(sun_set = sun_set, sun_rise = sun_rise, date = date, yday = yday(date), day_minutes = ampm_minutes(Time) - is_daylight * 60, set_minutes = ampm_minutes(sun_set) - is_daylight * 60, rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60, sun_up = day_minutes \u0026gt; (rise_minutes + 90) \u0026amp; day_minutes \u0026lt; (set_minutes - 30)) } In this function we arbitrarily decide that the sun is up, if it is 90 minutes after sun rise and 30 minutes before sun set. This is done because out future visualization is being made with rectangles and the lag function, and to ensure that all the sunshine hours are within sun set and sun rise we have to put in some restrains.\nIt seems that the 30th of October doesn’t have hourly history data available so we will exclude it in the following:\nfull_data2 \u0026lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %\u0026gt;% data_wrangling(.y)) At this point it would be wise to save our data.\nsave(full_data2, file = glue(\u0026quot;{airport_code}-{year}.Rdata\u0026quot;))  Plotting data Now that we have all the data we need it is time to turn our heads to ggplot2. But before we do that lets create some axis breaks that we will need.\nx_axis \u0026lt;- dates %\u0026gt;% month() %\u0026gt;% table() %\u0026gt;% cumsum() names(x_axis) \u0026lt;- month.abb[1:12] y_axis \u0026lt;- 1:24 * 60 names(y_axis) \u0026lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), rep(c(\u0026quot;AM\u0026quot;, \u0026quot;PM\u0026quot;), each = 12)) So we start by creating a new condition for “Clear”, creating a new day_minutes variable to act as the other side for our sunshine rectangles and lastly remove all the observations where the sun isn’t up. Using geom_rect() to create all the little rectangles and geom_line()’s to show the sun set and sun rise, we lastly fiddle a little with the theme giving us the final result:\nfull_data2 %\u0026gt;% mutate(con = Conditions == \u0026quot;Clear\u0026quot;, day_minutes2 = lag(day_minutes)) %\u0026gt;% filter(sun_up) %\u0026gt;% ggplot(aes(fill = con)) + geom_rect(aes(xmin = yday, xmax = yday + 1, ymin = day_minutes, ymax = day_minutes2)) + geom_line(aes(yday, set_minutes)) + geom_line(aes(yday, rise_minutes)) + scale_fill_manual(values = c(\u0026quot;grey40\u0026quot;, NA)) + theme_minimal() + guides(fill = \u0026quot;none\u0026quot;) + theme( panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), axis.text.x.bottom = element_text(hjust = 1.7) ) + scale_x_continuous(breaks = x_axis, position = \u0026quot;right\u0026quot;) + scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) + labs(x = NULL, y = NULL, title = \u0026quot;Sunshine report of Los Angeles 2017\u0026quot;) ## Warning: Position guide is perpendicular to the intended axis. Did you mean to ## specify a different guide `position`? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position?  Extra ## Warning: Position guide is perpendicular to the intended axis. Did you mean to ## specify a different guide `position`? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position? ## Warning: guide_axis(): Discarding guide on merge. Do you have more than one ## guide with the same position?  Code library(rvest) library(tidyverse) library(lubridate) library(glue) #library(ehlib) # devtools::install_github(\u0026quot;EmilHvitfeldt/ehlib\u0026quot;) str_between \u0026lt;- function(string, start, end) { stringr::str_extract(string, stringr::str_c(start, \u0026#39;(.*?)\u0026#39;, end, collapse = \u0026#39;\u0026#39;)) %\u0026gt;% stringr::str_replace(start, \u0026quot;\u0026quot;) %\u0026gt;% stringr::str_replace(end, \u0026quot;\u0026quot;) } str_before \u0026lt;- function(string, pattern) { stringr::str_extract(string, stringr::str_c(\u0026quot;.+?(?=\u0026quot;, pattern, \u0026quot;)\u0026quot;)) } all_dates_in \u0026lt;- function(year) { if(ymd(glue::glue(\u0026quot;{year}0101\u0026quot;)) \u0026gt; as.Date(Sys.time())) { stop(\u0026quot;Please select a past or current year.\u0026quot;) } start \u0026lt;- ymd(glue::glue(\u0026quot;{year}0101\u0026quot;)) if(as.Date(Sys.time()) \u0026gt; ymd(glue::glue(\u0026quot;{year}1231\u0026quot;))) { end \u0026lt;- ymd(glue::glue(\u0026quot;{year}1231\u0026quot;)) } else { end \u0026lt;- as.Date(Sys.time()) } seq(start, end, by = \u0026quot;day\u0026quot;) } airport_code \u0026lt;- \u0026quot;KCQT\u0026quot; full_data \u0026lt;- map(dates, ~{ weather_data_html(.x, airport_code) Sys.sleep(5) cat(month(dates), \u0026quot;/\u0026quot;, mday(dates), \u0026quot;\\n\u0026quot;, sep = \u0026quot;\u0026quot;) }) map_lgl(full_data, ~ is.null(.x$error)) ampm_minutes \u0026lt;- function(x) { as.numeric(str_between(x, \u0026quot;:\u0026quot;, \u0026quot; \u0026quot;)) + as.numeric(str_replace(str_before(x, \u0026quot;:\u0026quot;), \u0026quot;12\u0026quot;, \u0026quot;0\u0026quot;)) * 60 + 60 * 12 * str_detect(x, \u0026quot;PM\u0026quot;) } data_wrangling \u0026lt;- function(html_url, date) { # Sun rise time sun_rise \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;div[id=\u0026quot;astronomy-mod\u0026quot;] table\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% .[1] %\u0026gt;% str_between(\u0026quot;Time\\n\\t\\t\u0026quot;, \u0026quot;\\n\\t\\t\u0026quot;) # Sun set time sun_set \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;div[id=\u0026quot;astronomy-mod\u0026quot;] table\u0026#39;) %\u0026gt;% html_text() %\u0026gt;% .[1] %\u0026gt;% str_between(\u0026quot;\\n\\t\\t\u0026quot;, \u0026quot;\\n\\t\\tCivil\u0026quot;) # Table table \u0026lt;- html_url %\u0026gt;% html_nodes(\u0026#39;table[id=\u0026quot;obsTable\u0026quot;]\u0026#39;) %\u0026gt;% html_table() %\u0026gt;% .[[1]] # Time column standardization is_daylight \u0026lt;- any(\u0026quot;Time (PDT)\u0026quot; == names(table), \u0026quot;Time (MDT)\u0026quot; == names(table), \u0026quot;Time (CDT)\u0026quot; == names(table), \u0026quot;Time (EDT)\u0026quot; == names(table)) time_names \u0026lt;- str_c(\u0026quot;Time\u0026quot;, c(\u0026quot; (PDT)\u0026quot;, \u0026quot; (MDT)\u0026quot;, \u0026quot; (CDT)\u0026quot;, \u0026quot; (EDT)\u0026quot;, \u0026quot; (PST)\u0026quot;, \u0026quot; (MST)\u0026quot;, \u0026quot; (CST)\u0026quot;, \u0026quot; (EST)\u0026quot;)) names(table) \u0026lt;- if_else(names(table) %in% time_names, \u0026quot;Time\u0026quot;, names(table)) table %\u0026gt;% mutate(sun_set = sun_set, sun_rise = sun_rise, date = date, yday = yday(date), day_minutes = ampm_minutes(Time) - is_daylight * 60, set_minutes = ampm_minutes(sun_set) - is_daylight * 60, rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60, sun_up = day_minutes \u0026gt; (rise_minutes + 90) \u0026amp; day_minutes \u0026lt; (set_minutes - 30)) } full_data2 \u0026lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %\u0026gt;% data_wrangling(.y)) x_axis \u0026lt;- dates %\u0026gt;% month() %\u0026gt;% table() %\u0026gt;% cumsum() names(x_axis) \u0026lt;- month.abb[1:12] y_axis \u0026lt;- 1:24 * 60 names(y_axis) \u0026lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), rep(c(\u0026quot;AM\u0026quot;, \u0026quot;PM\u0026quot;), each = 12)) full_data2 %\u0026gt;% mutate(con = Conditions == \u0026quot;Clear\u0026quot;, day_minutes2 = lag(day_minutes)) %\u0026gt;% filter(sun_up) %\u0026gt;% ggplot(aes(fill = con)) + geom_rect(aes(xmin = yday, xmax = yday + 1, ymin = day_minutes, ymax = day_minutes2)) + geom_line(aes(yday, set_minutes)) + geom_line(aes(yday, rise_minutes)) + scale_fill_manual(values = c(\u0026quot;grey40\u0026quot;, NA)) + theme_minimal() + guides(fill = \u0026quot;none\u0026quot;) + theme( panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank(), axis.text.x.bottom = element_text(hjust = 1.7) ) + scale_x_continuous(breaks = x_axis, position = \u0026quot;right\u0026quot;) + scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) + labs(x = NULL, y = NULL, title = \u0026quot;Sunshine report of Los Angeles 2017\u0026quot;)  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"553a6c868922ffdf01e8e2e091c464a0","permalink":"/2018/01/01/recreate-sunshine-report/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/2018/01/01/recreate-sunshine-report/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nHello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse.","tags":null,"title":"Recreate - Sunshine Report","type":"post"},{"authors":null,"categories":["tidytext","web scraping"],"content":"         This code have been lightly revised to make sure it works as of 2018-12-16.\nAfter attending useR!2017 for the first time, which great pleasure and new connections made. I decided to see if I could extract some of the information available in the public schedule. So as with my last post I’ll do a bit of scraping followed by a few visualizations.\nPackages library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(utils) library(plotly) library(ltm) require(visNetwork)  Web scraping I found this task easiest with the help of purrr:map(). First we find the full schedules at the following links\nhttps://user2017.sched.com/2017-07-04/overview (Tuesday)\nhttps://user2017.sched.com/2017-07-05/overview (Wednesday)\nhttps://user2017.sched.com/2017-07-06/overview (Thursday)\nhttps://user2017.sched.com/2017-07-07/overview (Friday)\nthen we read the entire page into a tibble along with a day variable.\nday \u0026lt;- c(\u0026quot;Tuesday\u0026quot;, \u0026quot;Wednesday\u0026quot;, \u0026quot;Thursday\u0026quot;, \u0026quot;Friday\u0026quot;) link \u0026lt;- paste0(\u0026quot;https://user2017.sched.com/2017-07-0\u0026quot;, 4:7, \u0026quot;/overview\u0026quot;, sep = \u0026quot;\u0026quot;) event0 \u0026lt;- map2_df(link, day, ~ tibble(text = readLines(.x), day = .y)) then with the help of stringr we extract the desired information from the document, following the idiom that “multiple simple regex are better then one complicated one”. I also filtered out most non-talk events.\nevents \u0026lt;- event0 %\u0026gt;% filter(str_detect(text, \u0026quot;\u0026lt;span class=\u0026#39;\u0026quot;) | str_detect(text, \u0026quot;\u0026lt;/h3\u0026gt;\u0026quot;), !str_detect(text, \u0026quot;REGISTRATION\u0026quot;), !str_detect(text, \u0026quot;COFFEE BREAK\u0026quot;), !str_detect(text, \u0026quot;LUNCH\u0026quot;), !str_detect(text, \u0026quot;WELCOME\u0026quot;), !str_detect(text, \u0026quot;Poster\u0026quot;), !str_detect(text, \u0026quot;RIOT SESSION\u0026quot;), !str_detect(text, \u0026quot;Buses\u0026quot;), !str_detect(text, \u0026quot;Dinner\u0026quot;), !str_detect(text, \u0026quot;CLOSING\u0026quot;)) %\u0026gt;% mutate(time = str_extract(text, \u0026quot;\u0026lt;h3\u0026gt;.{1,7}\u0026quot;), # time time = str_replace(time, \u0026quot;\u0026lt;h3\u0026gt; *\u0026quot;, \u0026quot;\u0026quot;), id = str_extract(text, \u0026quot;id=\u0026#39;\\\\S{32}\u0026quot;), # id id = str_replace(id, \u0026quot;id=\u0026#39;\u0026quot;, \u0026quot;\u0026quot;), name = str_extract(text, str_c(id, \u0026quot;.*\u0026quot;)), # name name = str_replace(name, str_c(id, \u0026quot;\u0026#39;\u0026gt;\u0026quot;), \u0026quot;\u0026quot;), name = str_extract(name, \u0026quot;^.*(?=( \u0026lt;span))\u0026quot;), room = str_extract(text, \u0026#39;vs\u0026quot;\u0026gt;(.*?)\u0026lt;\u0026#39;), room = str_replace(room, \u0026#39;vs\u0026quot;\u0026gt;\u0026#39;, \u0026quot;\u0026quot;), room = str_replace(room, \u0026#39;\u0026lt;\u0026#39;,\u0026quot;\u0026quot;)) %\u0026gt;% # room fill(time) %\u0026gt;% filter(!str_detect(text, \u0026quot;\u0026lt;h3\u0026gt;\u0026quot;)) %\u0026gt;% dplyr::select(-text) lets take a look at what we have by now just to see that we have what we want.\nhead(events) ## # A tibble: 6 x 5 ## day time id name room ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Tuesday 9:30am 893eab219225a0990770… Data Carpentry: Open and Reproduc… 2.02 ## 2 Tuesday 9:30am 30c0eebdc887f3ad3aef… Dose-response analysis using R 4.02 ## 3 Tuesday 9:30am 57ce234e5ce9082da3cc… Geospatial visualization using R 4.03 ## 4 Tuesday 9:30am 95b110146486b0a5f802… Introduction to Bayesian inferenc… 2.01 ## 5 Tuesday 9:30am 7294f7df20ab1a7c37df… Introduction to parallel computin… 3.01 ## 6 Tuesday 9:30am f15703fe51e89294f2b5… Rcpp: From Simple Examples to Mac… PLENA… Now that we have all the information about the different events we can scrape every event page to find its attendees. This following chuck of code might seem a little hard at first, it helps to notice that there is a second tibble inside the big tibble.\npeople \u0026lt;- map_df(events$id, ~ tibble(attendee = tibble(text = readLines( str_c(\u0026quot;https://user2017.sched.com/event-goers/\u0026quot;, .x))) %\u0026gt;% filter(str_detect(text, \u0026quot; +\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;)) %\u0026gt;% .$text %\u0026gt;% str_split(., \u0026quot;li\u0026gt;\u0026lt;li\u0026quot;) %\u0026gt;% unlist(), id = .x) %\u0026gt;% mutate(attendee = str_replace(attendee, \u0026quot;(.*?)title=\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;), attendee = str_replace(attendee, \u0026quot;\\\u0026quot;\u0026gt;\u0026lt;(.*)\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% filter(!str_detect(attendee, \u0026quot;venue\u0026quot;), !str_detect(attendee, \u0026quot;Private\u0026quot;))) lets again take a look at what we have by now just to see that we have what we want.\nhead(people) ## # A tibble: 6 x 2 ## attendee id ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026quot; \u0026lt;li\u0026gt;\u0026lt;a href=\\\u0026quot;/\\\u0026quot;\u0026gt;Schedule\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026quot; 893eab219225a09907704… ## 2 \u0026quot; … 893eab219225a09907704… ## 3 \u0026quot;Marc Trunjer Kusk Nielsen\u0026quot; 893eab219225a09907704… ## 4 \u0026quot;lvaudor\u0026quot; 893eab219225a09907704… ## 5 \u0026quot;Alan Ponce\u0026quot; 893eab219225a09907704… ## 6 \u0026quot;bpiccolo\u0026quot; 893eab219225a09907704…  visualizations With a data set with this many possibilities the options are quite few, so here I’ll just list a few of the ones I found handy. So first we just do a simple bubble plot, this will be done with left_join’s and count and piped straight into ggplot.\nleft_join(events, people, by = \u0026quot;id\u0026quot;) %\u0026gt;% count(id) %\u0026gt;% left_join(events, by = \u0026quot;id\u0026quot;) %\u0026gt;% filter(day == \u0026quot;Friday\u0026quot;) %\u0026gt;% ggplot(aes(time, room, size = n)) + geom_point() + theme_bw() + scale_size(range = c(5, 20)) + labs(title = \u0026quot;useR!2017 Friday schedule\u0026quot;, x = \u0026quot;\u0026quot;) Since both our room and time were simply character vectors, the ordering is not right. This can be fixed by setting the levels correctly. Here I have the ordered vectored for both room and time.\ntime_levels \u0026lt;- c(\u0026quot;9:15am\u0026quot;, \u0026quot;9:30am\u0026quot;, \u0026quot;11:00am\u0026quot;, \u0026quot;11:18am\u0026quot;, \u0026quot;11:30am\u0026quot;, \u0026quot;11:36am\u0026quot;, \u0026quot;11:54am\u0026quot;, \u0026quot;12:12pm\u0026quot;, \u0026quot;1:15pm\u0026quot;, \u0026quot;1:30pm\u0026quot;, \u0026quot;1:48pm\u0026quot;, \u0026quot;2:00pm\u0026quot;, \u0026quot;2:06pm\u0026quot;, \u0026quot;2:24pm\u0026quot;, \u0026quot;2:42pm\u0026quot;, \u0026quot;3:30pm\u0026quot;, \u0026quot;3:45pm\u0026quot;, \u0026quot;4:00pm\u0026quot;, \u0026quot;4:45pm\u0026quot;, \u0026quot;4:55pm\u0026quot;, \u0026quot;5:00pm\u0026quot;, \u0026quot;5:05pm\u0026quot;, \u0026quot;5:30pm\u0026quot;, \u0026quot;5:35pm\u0026quot;, \u0026quot;5:40pm\u0026quot;, \u0026quot;5:45pm\u0026quot;, \u0026quot;5:50pm\u0026quot;, \u0026quot;5:55pm\u0026quot;, \u0026quot;6:00pm\u0026quot;, \u0026quot;6:05pm\u0026quot;, \u0026quot;6:10pm\u0026quot;, \u0026quot;6:15pm\u0026quot;, \u0026quot;6:20pm\u0026quot;, \u0026quot;7:00pm\u0026quot;) room_levels \u0026lt;- c(\u0026quot;PLENARY\u0026quot;, \u0026quot;2.01\u0026quot;, \u0026quot;2.02\u0026quot;, \u0026quot;3.01\u0026quot;, \u0026quot;3.02\u0026quot;, \u0026quot;4.01\u0026quot;, \u0026quot;4.02\u0026quot;) and we deal with it with a single mutate like so\nleft_join(events, people, by = \u0026quot;id\u0026quot;) %\u0026gt;% count(id) %\u0026gt;% left_join(events, by = \u0026quot;id\u0026quot;) %\u0026gt;% mutate(time = factor(time, time_levels), room = factor(room, room_levels)) %\u0026gt;% filter(day == \u0026quot;Friday\u0026quot;) %\u0026gt;% ggplot(aes(time, room, size = n)) + geom_point() + theme_bw() + scale_size(range = c(5, 20)) + labs(title = \u0026quot;useR!2017 Friday schedule\u0026quot;, x = \u0026quot;\u0026quot;) another way to visualize it would be to use a stacked bar chart like so\np \u0026lt;- left_join(events, people, by = \u0026quot;id\u0026quot;) %\u0026gt;% count(id) %\u0026gt;% left_join(events, by = \u0026quot;id\u0026quot;) %\u0026gt;% filter(day == \u0026quot;Thursday\u0026quot;) %\u0026gt;% mutate(time = factor(time, time_levels), room = factor(room, rev(room_levels))) %\u0026gt;% ggplot(aes(time, fill = room, text = name)) + geom_bar(aes(weight = n)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = \u0026quot;useR!2017 Thursday schedule\u0026quot;, x = \u0026quot;\u0026quot;) p or with a bit of interactivity plotly::ggplotly can be used so that is possible to hover over each event to see name and size.\nggplotly(p, tooltip = c(\u0026quot;n\u0026quot;, \u0026quot;name\u0026quot;), width = 900, height = 555)  {\"x\":{\"data\":[{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[741,472,513,611,695,662,675,570,488,449,341,453,300,480,391,335,267,345,341,224],\"x\":[3,4,5,6,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],\"y\":[62,259,343,200,91,296,76,120,153,90,47,222,100,58,95,133,169,119,129,80],\"text\":[\"n: 62\nmoodler: A new R package to easily fetch data from Moodle\",\"n: 259\nCan you keep a secret?\",\"n: 343\nScraping data with rvest and purrr\",\"n: 200\njug: Building Web APIs for R\",\"n: 91\nInteractive graphs for blind and print disabled people\",\"n: 296\nPackage ggiraph: a ggplot2 Extension for Interactive Graphics\",\"n: 76\nVisual funnel plot inference for meta-analysis\",\"n: 120\nmapedit - interactive manipulation of spatial objects\",\"n: 153\nExploring and presenting maps with **tmap**\",\"n: 90\nR in a small-sized bank's risk management\",\"n: 47\n**eventstudies**: An *R* package for conducting event studies and a platform for methodological research on event studies\",\"n: 222\nAutomatic Machine Learning in R\",\"n: 100\nR in a Pharmaceutical Company\",\"n: 58\nUsing R to Analyze Healthcare Cost of Older Patients Using Personal Emergency Response Service\",\"n: 95\nStatistics hitting the business front line\",\"n: 133\nAn example of Shiny tool at Nestlé R\u0026D, an enabler to guide product developers in designing gluten free biscuits\",\"n: 169\nUsing R for optimal beer recipe selection\",\"n: 119\nCandy Crush R Saga\",\"n: 129\nGamifyr: Transforming Machine Learning Tasks into Games with Shiny\",\"n: 80\nUltra-Fast Data Mining With The R-KDB+ Interface\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"4.02\",\"legendgroup\":\"4.02\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[629,417,460,568,400,293,418,272,413,355,308,190,320,300],\"x\":[3,4,5,6,19,20,21,22,23,24,25,26,27,28],\"y\":[112,55,53,43,49,48,35,28,67,36,27,77,25,41],\"text\":[\"n: 112\nBayesian social network analysis with Bergm\",\"n: 55\ndifNLR: Detection of potentional gender/minority bias with extensions of logistic regression\",\"n: 53\n**BradleyTerryScalable**: Ranking items scalably with the Bradley-Terry model\",\"n: 43\nIRT test equating with the R package equateIRT\",\"n: 49\nDNA methylation-based classification of human central nervous system tumors\",\"n: 48\nMultivariate statistics for PAT data analysis: short overview of existing R packages and methods\",\"n: 35\nR in research on microbial mutation rates\",\"n: 28\nPlasmid Profiler: Comparative Analysis of Plasmid Content in WGS Data\",\"n: 67\nApplication of R and Shiny in multiomics understanding of blood cancer biology and drug response\",\"n: 36\nSimulate phenotype(s) with epistatic interactions\",\"n: 27\nIntroducing the DynNom package for the generation of dynamic nomograms\",\"n: 77\nGraduate from plot to ggplot2: Using R to visualize the story of Ebola survivors in the PREVAIL III Ebola Natural History Study\",\"n: 25\n**BivRegBLS**, a new *R* package: Tolerance Intervals and Errors-in-Variables Regressions in Method Comparison Studies\",\"n: 41\nWhat is missing from the meta-analysis landscape?\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(196,154,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"4.01\",\"legendgroup\":\"4.01\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[511,412,510,342,402,250,177,270,219,322,271,260,146,228,219],\"x\":[8,9,10,11,12,19,20,21,22,23,24,25,26,27,28],\"y\":[184,250,165,228,86,150,116,148,53,91,84,48,44,92,81],\"text\":[\"n: 184\nReinforcementLearning: A package for replicating human behavior in R\",\"n: 250\nDeep Learning for Natural Language Processing in R\",\"n: 165\nR4ML: A Scalable R for Machine Learning\",\"n: 228\nComputer Vision and Image Recognition algorithms for R users\",\"n: 86\nDepth and depth-based classification with R package **ddalpha**\",\"n: 150\nR Blogging with blogdown and GitHub\",\"n: 116\n**redmineR** and the story of automating *useR!2017* abstract review process\",\"n: 148\nThe current state of naming conventions in R\",\"n: 53\nAn Introduction to the r2anki-package\",\"n: 91\nrOpenGov: community project for open government data\",\"n: 84\nR.gov: making R work for government\",\"n: 48\nnsoAPI - retrieving data from National Statistical Offices with R\",\"n: 44\nJurimetrics: quantitative analysis of judicial decisions using R\",\"n: 92\nShiny Apps for Maths and Stats Exercises\",\"n: 81\nWorking with R when internet is not reliable\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(83,180,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"3.02\",\"legendgroup\":\"3.02\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[592,366,407,479,542,439,280,362,222,241,185,107,157,143,196,195,134,97,151,154,88],\"x\":[3,4,5,6,7,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],\"y\":[37,51,53,89,107,72,132,148,120,161,65,70,113,76,126,76,126,49,77,65,136],\"text\":[\"n: 37\n**rags2ridges**: A One-Stop-Go for Network Modeling of Precision Matrices\",\"n: 51\nVarious Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in *R*\",\"n: 53\nfactorMerger: a set of tools to support results from post hoc testing\",\"n: 89\nEstimating the Parameters of a Continuous-Time Markov Chain from Discrete-Time Data with ctmcd\",\"n: 107\nMCMC Output Analysis Using R package mcmcse\",\"n: 72\nAn Efficient Algorithm for Solving Large Fixed Effects OLS Problems with Clustered Standard Error Estimation\",\"n: 132\nR Package glmm: Likelihood-Based Inference for Generalized Linear Mixed Models\",\"n: 148\n**countreg**: Tools for count data regression\",\"n: 120\nHow to Use (R)Stan to Estimate Models in External R Packages\",\"n: 161\nbrms: Bayesian Multilevel Models using Stan\",\"n: 65\nThe cutpointr package: Improved and tidy estimation of optimal cutpoints\",\"n: 70\nPreparing Datetime Data with Padr\",\"n: 113\nR in Minecraft\",\"n: 76\nDigital Signal Processing with R\",\"n: 126\nData Analysis Using Hierarchical Generalized Linear Models with R\",\"n: 76\nThe R package bigstatsr: Memory- and Computation-Efficient Statistical Tools for Big Matrices\",\"n: 126\nAdvanced R Solutions -- A Bookdown Project\",\"n: 49\nFunctional Input Validation with valaddin\",\"n: 77\nROI - R Optimization Infrastructure\",\"n: 65\nsimmer: Discrete-Event Simulation for R\",\"n: 136\nData Error! But where?\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,192,148,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"3.01\",\"legendgroup\":\"3.01\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[521,157,233,257,272,354,229,336,193,57,28,22,24,39,46,50,30,24,35],\"x\":[3,4,5,6,7,8,9,10,11,19,20,21,22,23,24,25,26,27,28],\"y\":[71,209,174,222,270,85,51,26,29,128,79,135,119,157,149,84,67,127,119],\"text\":[\"n: 71\nHosting Data Packages via `drat`: A Case Study with Hurricane Exposure Data\",\"n: 209\nClouds, Containers and R, towards a global hub for reproducible and collaborative data science\",\"n: 174\ncodebookr: Codebooks in *R*\",\"n: 222\nShow me the errors you didn't look for\",\"n: 270\nAutomatically archiving reproducible studies with Docker\",\"n: 85\nThe **renjin** package: Painless Just-in-time Compilation for High Performance R\",\"n: 51\nAn LLVM-based Compiler Toolkit for R\",\"n: 26\nPerformance Benchmarking of the R Programming Environment on Knight's Landing\",\"n: 29\n*GNU R* on a Programmable Logic Controller (PLC) in an Embedded-Linux Environment\",\"n: 128\nR and Tableau Integration: A Case approach\",\"n: 79\nThe dataCompareR package\",\"n: 135\nUse of templates within an R package to create a (semi-)automated analysis workflow and/or report\",\"n: 119\ngraphiT: an interactive, user-friendly tool to produce graphics based on the grammar of graphics' principles\",\"n: 157\n**heatmaply**: an *R* package for creating interactive cluster heatmaps\",\"n: 149\nPlot Colour Helper – Finally an easy way to pick colours for your R plots!\",\"n: 84\nbsplus: Using Twitter Bootstrap to extend your Shiny app\",\"n: 67\nTAGS - Table Assorting Guided System: an HTML widget to create multiple tables from Excel spreadsheets\",\"n: 127\nObject-oriented markdown in R to facilitate collaboration\",\"n: 119\nStrategies for Reproducible Research with Packrat\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,182,235,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"2.02\",\"legendgroup\":\"2.02\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[485,74,176,167,196,114,117,133,66,149,0,0,0,0,0,0,0,0,0,0,0],\"x\":[3,4,5,6,7,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],\"y\":[36,83,57,90,76,240,112,203,127,92,57,28,22,24,39,46,50,30,24,35,88],\"text\":[\"n: 36\nUsing the alphabetr package to determine paired T cell receptor sequences\",\"n: 83\nDifferentiation of brain tumor tissue using hierarchical non-negative matrix factorization\",\"n: 57\nBiosignature-Based Drug Design: from high dimensional data to business impact\",\"n: 90\nInteractive and Reproducible Research for RNA Sequencing Analysis\",\"n: 76\nStochastic Gradient Descent Log-Likelihood Estimation in the Cox Proportional Hazards Model with Applications to The Cancer Genome Atlas Data\",\"n: 240\nR-based computing with big data on disk\",\"n: 112\nDaff: diff, patch and merge for data.frames\",\"n: 203\nodbc - A modern database interface\",\"n: 127\nImproving DBI\",\"n: 92\n*implyr**: A **dplyr** Backend for a Apache Impala\",\"n: 57\nrdwd – manage German weather observations\",\"n: 28\neseis – A toolbox to weld seismologic and geomorphic data analysis\",\"n: 22\nAn R Decision Support Framework for the Identification of BMP in Catchments\",\"n: 24\nReproducible research in computational subsurface hydrology - First steps in R with RMODFLOW and RMT3DMS\",\"n: 39\nUsing an R package as platform for harmonized cleaning of data from RTI MicroPEM air quality sensors\",\"n: 46\nmap data from **naturalearth** : aiming for sustainability through specialisation and **rOpenSci**\",\"n: 50\nOpenSpat, spread the spatial world\",\"n: 30\nsmires -- Calculating Hydrological Metrics for Univariate Time Series\",\"n: 24\nminimalRSD and FMC: R packages to construct cost efficient experimental designs\",\"n: 35\nICtest: Estimating the Number of Interesting Components\",\"n: 88\nBetter Confidence Intervals for Quantiles\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(165,138,255,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"2.01\",\"legendgroup\":\"2.01\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],\"y\":[274,448,485,74,176,167,196,114,117,133,66,149,328,525,37,36,31,33],\"text\":[\"n: 274\nSponsor Talk Open Analytics\",\"n: 448\nKEYNOTE: Dose-response analysis: considering dose both as qualitative factor and quantitative covariate- using R*\",\"n: 485\nShow Me Your Model: tools for visualisation of statistical models\",\"n: 74\nQuantitative fisheries advice using R and FLR\",\"n: 176\n*jamovi*: a spreadsheet for R\",\"n: 167\nThe growing popularity of R in data journalism\",\"n: 196\nFFTrees: An R package to create, visualise and use fast and frugal decision trees\",\"n: 114\nCommunity-based learning and knowledge sharing\",\"n: 117\nData Carpentry: Teaching Reproducible Data Driven Discovery\",\"n: 133\nStatistics in Action with R: an educative platform\",\"n: 66\nA quasi-experiment for the influence of the user interface on the acceptance of R\",\"n: 149\nThe analysis of R learning styles with R\",\"n: 328\nSponsor Talk Rstudio\",\"n: 525\nKEYNOTE: Parallel Computation in R: What We Want, and How We (Might) Get It\",\"n: 37\nSPONSOR TALK EODA\",\"n: 36\nSPONSOR TALK ORACLE\",\"n: 31\nSPONSOR TALK MANGO SOLUTIONS\",\"n: 33\nSPONSOR TALK ALTERYX\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(251,97,215,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"PLENARY\",\"legendgroup\":\"PLENARY\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.3817104776009,\"r\":7.30593607305936,\"b\":37.8531000277654,\"l\":48.9497716894977},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"useR!2017 Thursday schedule\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,29.6],\"tickmode\":\"array\",\"ticktext\":[\"9:15am\",\"9:30am\",\"11:00am\",\"11:18am\",\"11:36am\",\"11:54am\",\"12:12pm\",\"1:30pm\",\"1:48pm\",\"2:06pm\",\"2:24pm\",\"2:42pm\",\"3:30pm\",\"3:45pm\",\"4:45pm\",\"4:55pm\",\"5:00pm\",\"5:05pm\",\"5:30pm\",\"5:35pm\",\"5:40pm\",\"5:45pm\",\"5:50pm\",\"5:55pm\",\"6:00pm\",\"6:05pm\",\"6:10pm\",\"6:15pm\",\"6:20pm\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"categoryorder\":\"array\",\"categoryarray\":[\"9:15am\",\"9:30am\",\"11:00am\",\"11:18am\",\"11:36am\",\"11:54am\",\"12:12pm\",\"1:30pm\",\"1:48pm\",\"2:06pm\",\"2:24pm\",\"2:42pm\",\"3:30pm\",\"3:45pm\",\"4:45pm\",\"4:55pm\",\"5:00pm\",\"5:05pm\",\"5:30pm\",\"5:35pm\",\"5:40pm\",\"5:45pm\",\"5:50pm\",\"5:55pm\",\"6:00pm\",\"6:05pm\",\"6:10pm\",\"6:15pm\",\"6:20pm\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-45,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-47.9,1005.9],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"250\",\"500\",\"750\",\"1000\"],\"tickvals\":[0,250,500,750,1000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"250\",\"500\",\"750\",\"1000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"count\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.943817833581613},\"annotations\":[{\"text\":\"room\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"width\":900,\"height\":555,\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"9efd52dcd1ab\":{\"weight\":{},\"x\":{},\"fill\":{},\"text\":{},\"type\":\"bar\"}},\"cur_data\":\"9efd52dcd1ab\",\"visdat\":{\"9efd52dcd1ab\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Network graph To make our-self a simple network graph will I be using the visNetwork package, which have a lovely vignette. So here first of all to create a manageable graph I’ll subset all the Wednesday talks in room 4.02, which was the “Shiny I” and “Text Mining” block.\nsub_data \u0026lt;- left_join(events, people, by = \u0026quot;id\u0026quot;) %\u0026gt;% filter(day == \u0026quot;Wednesday\u0026quot;, room == \u0026quot;4.02\u0026quot;) %\u0026gt;% dplyr::select(name, attendee, time) I this graph I will let each node be a event and let the edges be to which degree they share attendees. So we start\nnode_size \u0026lt;- sub_data %\u0026gt;% group_by(name, time) %\u0026gt;% summarize(n = n()) to find how many attendees the events share with each other we first find all the different pairs of events with utils::combn function and with purrr and inner_join finds how many they have in common. Since utils::combn gives its result in a matrix we have to fiddle just a bit to our way back to a tibble.\nconn \u0026lt;- combn(node_size$name, 2) %\u0026gt;% as.tibble() %\u0026gt;% map_int(~ inner_join(sub_data %\u0026gt;% filter(name == .x[1]), sub_data %\u0026gt;% filter(name == .x[2]), by = \u0026quot;attendee\u0026quot;) %\u0026gt;% nrow()) %\u0026gt;% rbind(combn(node_size$name, 2)) %\u0026gt;% t() %\u0026gt;% as.tibble() ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: The `x` argument of `as_tibble.matrix()` must have column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. names(conn) \u0026lt;- c(\u0026quot;n\u0026quot;, \u0026quot;from\u0026quot;, \u0026quot;to\u0026quot;) conn ## # A tibble: 45 x 3 ## n from to ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 21 A Tidy Data Model for Natural Lan… bradio: Add data music widgets to y… ## 2 57 A Tidy Data Model for Natural Lan… Developing and deploying large scal… ## 3 82 A Tidy Data Model for Natural Lan… How we built a Shiny App for 700 us… ## 4 84 A Tidy Data Model for Natural Lan… Interacting with databases from Shi… ## 5 84 A Tidy Data Model for Natural Lan… manifestoR - a tool for data journa… ## 6 99 A Tidy Data Model for Natural Lan… Neural Embeddings and NLP with R an… ## 7 83 A Tidy Data Model for Natural Lan… ShinyProxy ## 8 155 A Tidy Data Model for Natural Lan… Text Analysis and Text Mining Using… ## 9 168 A Tidy Data Model for Natural Lan… Text mining, the tidy way ## 10 46 bradio: Add data music widgets to… Developing and deploying large scal… ## # … with 35 more rows for the node tibble we need to supply it with a id column, but I will also supply it with a label (name of the event), size (number of people in the event) and color (what book is this event in. green = Shiny I, blue = Text Mining).\nShiny_I \u0026lt;- c(\u0026quot;11:00am\u0026quot;, \u0026quot;11:18am\u0026quot;, \u0026quot;11:36am\u0026quot;, \u0026quot;11:54am\u0026quot;, \u0026quot;12:12pm\u0026quot;) Text_Mining \u0026lt;- c(\u0026quot;1:30pm\u0026quot;, \u0026quot;1:48pm\u0026quot;, \u0026quot;2:06pm\u0026quot;, \u0026quot;2:24pm\u0026quot;, \u0026quot;2:42pm\u0026quot;) nodes \u0026lt;- node_size %\u0026gt;% mutate(id = name, label = str_wrap(name, width = 20), size = n / 20, color = case_when( time %in% Shiny_I ~ \u0026quot;lightgreen\u0026quot;, time %in% Text_Mining ~ \u0026quot;lightblue\u0026quot; )) for the edge tibble it needs from and to columns that matches with the id in the node tibble. I will also supply with a constant color column (because if omitted it would borrow from the node coloring) and a width column to indicate how many attendees they share. This is again done with a couple of left_joins and the connectivity is the average percentage of attendees they share. Lastly we remove any edge with less then 0.5 connectivity to clear out the graph.\nedges \u0026lt;- conn %\u0026gt;% left_join(node_size %\u0026gt;% dplyr::select(-time) %\u0026gt;% rename(n_from = n), by = c(\u0026quot;from\u0026quot; = \u0026quot;name\u0026quot;)) %\u0026gt;% left_join(node_size %\u0026gt;% dplyr::select(-time) %\u0026gt;% rename(n_to = n), by = c(\u0026quot;to\u0026quot; = \u0026quot;name\u0026quot;)) %\u0026gt;% mutate(n = as.numeric(n), n_to = as.numeric(n_to), n_from = as.numeric(n_from), connectivity = (n / n_from + n / n_to) / 2, width = connectivity * 10, color = \u0026quot;grey\u0026quot;) %\u0026gt;% filter(connectivity \u0026gt; 0.5) Which yields us with the wonderful graph which show a somehow clear divide between the two blocks.\nvisNetwork(nodes, edges, width = \u0026quot;100%\u0026quot;)  {\"x\":{\"nodes\":{\"name\":[\"A Tidy Data Model for Natural Language Processing\",\"bradio: Add data music widgets to your business intelligence dashboards\",\"Developing and deploying large scale Shiny applications for non-life insurance\",\"How we built a Shiny App for 700 users?\",\"Interacting with databases from Shiny\",\"manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software\",\"Neural Embeddings and NLP with R and Spark\",\"ShinyProxy\",\"Text Analysis and Text Mining Using R\",\"Text mining, the tidy way\"],\"time\":[\"2:06pm\",\"11:18am\",\"12:12pm\",\"11:00am\",\"11:36am\",\"1:48pm\",\"2:42pm\",\"11:54am\",\"2:24pm\",\"1:30pm\"],\"n\":[209,73,191,307,294,138,199,289,251,329],\"id\":[\"A Tidy Data Model for Natural Language Processing\",\"bradio: Add data music widgets to your business intelligence dashboards\",\"Developing and deploying large scale Shiny applications for non-life insurance\",\"How we built a Shiny App for 700 users?\",\"Interacting with databases from Shiny\",\"manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software\",\"Neural Embeddings and NLP with R and Spark\",\"ShinyProxy\",\"Text Analysis and Text Mining Using R\",\"Text mining, the tidy way\"],\"label\":[\"A Tidy Data Model\\nfor Natural Language\\nProcessing\",\"bradio: Add data\\nmusic widgets\\nto your business\\nintelligence\\ndashboards\",\"Developing and\\ndeploying large\\nscale Shiny\\napplications for\\nnon-life insurance\",\"How we built a Shiny\\nApp for 700 users?\",\"Interacting with\\ndatabases from Shiny\",\"manifestoR - a\\ntool for data\\njournalists, a\\nsource for text\\nminers and a\\nprototype for\\nreproducibility\\nsoftware\",\"Neural Embeddings\\nand NLP with R and\\nSpark\",\"ShinyProxy\",\"Text Analysis and\\nText Mining Using R\",\"Text mining, the\\ntidy way\"],\"size\":[10.45,3.65,9.55,15.35,14.7,6.9,9.95,14.45,12.55,16.45],\"color\":[\"lightblue\",\"lightgreen\",\"lightgreen\",\"lightgreen\",\"lightgreen\",\"lightblue\",\"lightblue\",\"lightgreen\",\"lightblue\",\"lightblue\"]},\"edges\":{\"n\":[84,155,168,61,144,137,146,212,201,203,110,188],\"from\":[\"A Tidy Data Model for Natural Language Processing\",\"A Tidy Data Model for Natural Language Processing\",\"A Tidy Data Model for Natural Language Processing\",\"bradio: Add data music widgets to your business intelligence dashboards\",\"Developing and deploying large scale Shiny applications for non-life insurance\",\"Developing and deploying large scale Shiny applications for non-life insurance\",\"Developing and deploying large scale Shiny applications for non-life insurance\",\"How we built a Shiny App for 700 users?\",\"How we built a Shiny App for 700 users?\",\"Interacting with databases from Shiny\",\"manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software\",\"Text Analysis and Text Mining Using R\"],\"to\":[\"manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software\",\"Text Analysis and Text Mining Using R\",\"Text mining, the tidy way\",\"Interacting with databases from Shiny\",\"How we built a Shiny App for 700 users?\",\"Interacting with databases from Shiny\",\"ShinyProxy\",\"Interacting with databases from Shiny\",\"ShinyProxy\",\"ShinyProxy\",\"Text mining, the tidy way\",\"Text mining, the tidy way\"],\"n_from\":[209,209,209,73,191,191,191,307,307,294,138,251],\"n_to\":[138,251,329,294,307,294,289,294,289,289,329,329],\"connectivity\":[0.505304763886,0.67957833736823,0.657233024534256,0.521549715776722,0.611491038081757,0.591631940734409,0.634794108588924,0.705821090651244,0.675112428569818,0.696449167902455,0.565723976917316,0.660216277746158],\"width\":[5.05304763886,6.7957833736823,6.57233024534256,5.21549715776722,6.11491038081757,5.91631940734409,6.34794108588924,7.05821090651244,6.75112428569818,6.96449167902455,5.65723976917316,6.60216277746158],\"color\":[\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\"]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\"},\"manipulation\":{\"enabled\":false}},\"groups\":null,\"width\":\"100%\",\"height\":null,\"idselection\":{\"enabled\":false},\"byselection\":{\"enabled\":false},\"main\":null,\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\"},\"evals\":[],\"jsHooks\":[]} I hope you enjoyed this post and I would love you see any and all visualization or analysis you might have regarding this data.\n  ","date":1500508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500508800,"objectID":"7f03ce193d95e7d1d884fe64f4571448","permalink":"/2017/07/20/analysing-user2017-schedule-data/","publishdate":"2017-07-20T00:00:00Z","relpermalink":"/2017/07/20/analysing-user2017-schedule-data/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nAfter attending useR!2017 for the first time, which great pleasure and new connections made.","tags":null,"title":"Analysing useR!2017 schedule data","type":"post"},{"authors":null,"categories":["tidytext","web scraping"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nLately I have been wondering how to quantify how repetitive a text is, specifically how repetitive are the lyrics to songs. I’m by no means the first one, Colin Morris did a great piece on language compression with his Are Pop Lyrics Getting More Repetitive? which i highly recommend you go read. Instead of looking at pop lyrics will we instead be focusing some popular musicals to see if general patterns emerge within each show.\nMy plan is to use the magic of the tidyverse with the inclusion of tidytext to find the percentage of repeated consecutive sequences of words() also called n-grams) of varying length and then compare the results. We will use rvest to extract the lyrics from the web. However for larger data needs official APIs are recommended.\nExtracting song lyrics library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(tidytext) library(rvest) ## Warning: package \u0026#39;xml2\u0026#39; was built under R version 3.6.2 We will first take a peek at a specific song,\nthe_stars_look_down \u0026lt;- \u0026quot;https://genius.com/Elton-john-the-stars-look-down-lyrics\u0026quot; the_stars_look_down %\u0026gt;% read_lines() %\u0026gt;% head(20) ## [1] \u0026quot;\u0026quot; ## [2] \u0026quot;\u0026quot; ## [3] \u0026quot;\u0026lt;!DOCTYPE html\u0026gt;\u0026quot; ## [4] \u0026quot;\u0026lt;html class=\\\u0026quot;snarly apple_music_player--enabled bagon_song_page--enabled song_stories_public_launch--enabled react_forums--disabled\\\u0026quot; xmlns=\\\u0026quot;http://www.w3.org/1999/xhtml\\\u0026quot; xmlns:fb=\\\u0026quot;http://www.facebook.com/2008/fbml\\\u0026quot; lang=\\\u0026quot;en\\\u0026quot; xml:lang=\\\u0026quot;en\\\u0026quot;\u0026gt;\u0026quot; ## [5] \u0026quot; \u0026lt;head\u0026gt;\u0026quot; ## [6] \u0026quot; \u0026lt;base target=\u0026#39;_top\u0026#39; href=\\\u0026quot;//genius.com/\\\u0026quot;\u0026gt;\u0026quot; ## [7] \u0026quot;\u0026quot; ## [8] \u0026quot; \u0026lt;script type=\\\u0026quot;text/javascript\\\u0026quot;\u0026gt;\u0026quot; ## [9] \u0026quot;//\u0026lt;![CDATA[\u0026quot; ## [10] \u0026quot;\u0026quot; ## [11] \u0026quot; var _sf_startpt=(new Date()).getTime();\u0026quot; ## [12] \u0026quot; if (window.performance \u0026amp;\u0026amp; performance.mark) {\u0026quot; ## [13] \u0026quot; window.performance.mark(\u0026#39;parse_start\u0026#39;);\u0026quot; ## [14] \u0026quot; }\u0026quot; ## [15] \u0026quot;\u0026quot; ## [16] \u0026quot;//]]\u0026gt;\u0026quot; ## [17] \u0026quot;\u0026lt;/script\u0026gt;\u0026quot; ## [18] \u0026quot;\u0026quot; ## [19] \u0026quot;\u0026lt;title\u0026gt;Elton John – The Stars Look Down Lyrics | Genius Lyrics\u0026lt;/title\u0026gt;\u0026quot; ## [20] \u0026quot;\u0026quot; And we find a whole of lines of no interest of us, which is to be expected. After some digging I manage to find that the lyrics are packed between p tags we can do this with rvest’s html_nodes().\nthe_stars_look_down_lyrics \u0026lt;- the_stars_look_down %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;p\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_split(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() %\u0026gt;% tibble(text = .) the_stars_look_down_lyrics ## # A tibble: 142 x 1 ## text ## \u0026lt;chr\u0026gt; ## 1 [Spoken OVERHEAD VOICE] ## 2 For over seventy years miners of Durham County have come together once a yea… ## 3 [Spoken HERBERT MORRISON] ## 4 I want you men of the pits to come through. I want these streets clean of na… ## 5 [ENSEMBLE, MINERS] ## 6 Through the dark, and ## 7 Through the hunger ## 8 Through the night and ## 9 Through the fear ## 10 Through the fight and ## # … with 132 more rows We notice some of the rows are indications of who is talking, these are quickly dealt with by a filter. Now we employ our tidytext arsenal with unnest_tokens and we find all the bi-grams (pairs of consecutive words). (notice how they overlap)\nthe_stars_look_down_lyrics %\u0026gt;% filter(!str_detect(text, \u0026quot;\\\\[\u0026quot;)) %\u0026gt;% unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) ## # A tibble: 719 x 1 ## bigram ## \u0026lt;chr\u0026gt; ## 1 for over ## 2 over seventy ## 3 seventy years ## 4 years miners ## 5 miners of ## 6 of durham ## 7 durham county ## 8 county have ## 9 have come ## 10 come together ## # … with 709 more rows now from this we can summarize to find the number of unique bigrams and by extension the percentage of repeated bigrams.\nthe_stars_look_down_lyrics %\u0026gt;% filter(!str_detect(text, \u0026quot;\\\\[\u0026quot;)) %\u0026gt;% unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% summarise(length = length(bigram), unique = length(unique(bigram))) %\u0026gt;% mutate(repetition = 1 - unique / length) ## # A tibble: 1 x 3 ## length unique repetition ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 719 447 0.378 So we see that in this particular song around 38% bigrams are present at least twice. We will expect this percentage to be strictly decreasing for \\(n\\) increasing, but what we are interested in the both the rate it is decreasing but also the general level.\nNow we generalizing this procedure using some purrr to give us a nice data.frame out in the end. The range 1:5 was picked after some trial and error, and it seemed to me that most trends died out around the 5-6 mark rendering data points over rather uninteresting.\nsongfun \u0026lt;- function(hyperlink, repnum = 1:5) { lyrics \u0026lt;- hyperlink %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;p\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_split(\u0026quot;\\n\u0026quot;) %\u0026gt;% unlist() %\u0026gt;% tibble(text = .) map_dfr(repnum, ~ lyrics %\u0026gt;% unnest_tokens(ngram, text, token = \u0026quot;ngrams\u0026quot;, n = .x) %\u0026gt;% summarise(n = .x, length = length(ngram), unique = length(unique(ngram))) %\u0026gt;% mutate(repetition = 1 - unique / length, name = hyperlink)) } Now to try this out, we plug in the link again, and pipe the result into ggplot to give us a nice visualization\nsongfun(\u0026quot;https://genius.com/Elton-john-the-stars-look-down-lyrics\u0026quot;) %\u0026gt;% ggplot(aes(n, repetition)) + geom_line() + coord_cartesian(ylim = 0:1) from this plot alone we can see that roughly 3/4 of the words used in the song are used more then twice, while on the other end of the scale just shy of 25% of the 5-grams are used more then once. This plot by itself doesn’t provide too much meaningful information by itself. So next step is to gather information for more songs to compare.\nThis function takes a link to an album page, and uses similar techniques used earlier to detect the song in the album, find the lyrics with songfun, process it and spits out a data.frame.\nalbumfun \u0026lt;- function(hlink, ...) { song_links \u0026lt;- tibble(text = readLines(hlink)) %\u0026gt;% filter(str_detect(text, \u0026quot; \u0026lt;a href=\\\u0026quot;https://genius.com/\u0026quot;)) %\u0026gt;% mutate(text = str_replace(text, \u0026quot;\u0026lt;a href=\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% mutate(text = str_replace(text, \u0026quot;\\\u0026quot; class=\\\u0026quot;u-display_block\\\u0026quot;\u0026gt;\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% mutate(text = str_replace(text, \u0026quot; *\u0026quot;, \u0026quot;\u0026quot;)) %\u0026gt;% mutate(song = str_replace(text, \u0026quot;https://genius.com/\u0026quot;, \u0026quot;\u0026quot;)) nsongs \u0026lt;- nrow(song_links) out \u0026lt;- tibble() for (i in 1:nsongs) { ting \u0026lt;- songfun(song_links$text[i], ...) out \u0026lt;- rbind(out, ting) } out %\u0026gt;% mutate(album = hlink) }  Analysis We use our function to get the data for a number of different musicals.\nbillyelliot \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Elton-john/Billy-elliot-the-musical-original-london-cast-recording\u0026quot;) thebookofmormon \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Book-of-mormon/The-book-of-mormon-original-broadway-recording\u0026quot;) lionking \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/The-lion-king/The-lion-king-original-broadway-cast-recording\u0026quot;) avenueq \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Robert-lopez-and-jeff-marx/Avenue-q-original-broadway-cast-recording\u0026quot;) oklahoma \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Richard-rodgers/Oklahoma-original-motion-picture-soundtrack\u0026quot;) soundofmusic \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Richard-rodgers/The-sound-of-music-original-soundtrack-recording\u0026quot;) intheheights \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Lin-manuel-miranda/In-the-heights-original-broadway-cast-recording\u0026quot;) lemiserables \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Les-miserables-original-broadway-cast/Les-miserables-1987-original-broadway-cast\u0026quot;) phantomoftheopera \u0026lt;- albumfun(hlink = \u0026quot;https://genius.com/albums/Andrew-lloyd-webber/The-phantom-of-the-opera-original-london-cast-recording\u0026quot;) and a quick explorative plot tells us that it is working as intended, we see some variation both slopes and offset, telling us that Billy Elliot have some range in its songs.\nbillyelliot %\u0026gt;% ggplot(aes(n, repetition)) + geom_line(aes(group = name)) + labs(title = \u0026quot;Billy Elliot\u0026quot;) + coord_cartesian(ylim = 0:1) to further compare we bind all the data.frames together for ease of handling\nmusical_names \u0026lt;- c(\u0026quot;The Phantom of the Opera\u0026quot;, \u0026quot;The Book of Mormon\u0026quot;, \u0026quot;Billy Elliot\u0026quot;, \u0026quot;Les Miserables\u0026quot;, \u0026quot;In the Heights\u0026quot;, \u0026quot;Oklahoma\u0026quot;, \u0026quot;The Sound of music\u0026quot;, \u0026quot;Avenue Q\u0026quot;, \u0026quot;Lion King\u0026quot;) rbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma, soundofmusic, intheheights, lemiserables, phantomoftheopera) %\u0026gt;% mutate(album = factor(album, label = musical_names)) %\u0026gt;% ggplot(aes(n, repetition)) + geom_line(aes(group = name), alpha = 0.5) + facet_wrap(~ album) Wow, here we clearly see some differences in lyrical styles for the different musical, from the evenness of the soundtrack to “In the Heights” to the range of “Lion King”. To try having them all in the same graph would be overwhelming. However we could still plot the trend of each album in the same plot, fading out individual songs.\nrbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma, soundofmusic, intheheights, lemiserables, phantomoftheopera) %\u0026gt;% ggplot(aes(n, repetition)) + coord_cartesian(ylim = 0:1) + geom_line(aes(group = name), alpha = 0.05) + geom_smooth(aes(group = album, color = album), se = FALSE) + theme_bw() + labs(title = \u0026quot;Repetition in musicals\u0026quot;) + scale_colour_brewer(palette = \u0026quot;Set1\u0026quot;, name = \u0026quot;Musical\u0026quot;, labels = c(\u0026quot;The Phantom of the Opera\u0026quot;, \u0026quot;The Book of Mormon\u0026quot;, \u0026quot;Billy Elliot\u0026quot;, \u0026quot;Les Miserables\u0026quot;, \u0026quot;In the Heights\u0026quot;, \u0026quot;Oklahoma\u0026quot;, \u0026quot;The Sound of music\u0026quot;, \u0026quot;Avenue Q\u0026quot;, \u0026quot;Lion King\u0026quot;)) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;  ","date":1496620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496620800,"objectID":"f5d884f8ad75aafa42246a34df37bbf7","permalink":"/2017/06/05/repetition-in-musicals-with-tidytext/","publishdate":"2017-06-05T00:00:00Z","relpermalink":"/2017/06/05/repetition-in-musicals-with-tidytext/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nLately I have been wondering how to quantify how repetitive a text is, specifically how repetitive are the lyrics to songs.","tags":null,"title":"Repetition in musicals with tidytext","type":"post"},{"authors":null,"categories":["ggplot2"],"content":" This code have been lightly revised to make sure it works as of 2018-12-16.\nWith Reporters Without Borders coming out with its 2017 World Press Freedom Index in the same week as Hadley Wickham coming out with the emo(ji) package, I had no choice but to explore both of them at the same time.\nDisclaimer! This post is not an exercise in statistical inference but rather a proof of concept of how to use the emo(ji) package with ggplot2.\nLoading packages library(hrbrthemes) library(tidyverse) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.6.2 library(stringr) library(tibble) # remotes::install_github(\u0026#39;hadley/emo\u0026#39;) library(emo) The hrbrthemes is not necessary for this project but it is one of my personal favorite ggplot2 themes.\n Gathering data Here we collect the data from Reporters Without Borders, emoji flags and The World Bank (so we have something to plot against).\n2017 World Press Freedom Index We have the 2017 World Press Freedom Index (direct download link) data which we load in as normal.\n(freedom_index \u0026lt;- read_csv(\u0026quot;https://rsf.org/sites/default/files/index_format_upload_2017-v2_1_0.csv\u0026quot;)) ## # A tibble: 180 x 12 ## ISO Rank FR_Country EN_country ES_country `Underlying sit… ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 NOR 1 Norvège Norway Noruega 760 ## 2 SWE 2 Suede Sweden Suecia 759 ## 3 FIN 3 Finlande Finland Finlandia 892 ## 4 DNK 4 Danemark Denmark Dinamarca 1036 ## 5 NLD 5 Pays-Bas Netherlan… Países Ba… 963 ## 6 CRI 6 Costa Rica Costa Rica Costa Rica 1193 ## 7 CHE 7 Suisse Switzerla… Suiza 1213 ## 8 JAM 8 Jamaïque Jamaica Jamaica 1273 ## 9 BEL 9 Belgique Belgium Bélgica 1247 ## 10 ISL 10 Islande Iceland Islandia 1303 ## # … with 170 more rows, and 6 more variables: `Abuse score 2016` \u0026lt;chr\u0026gt;, ## # `Overall Score 2016` \u0026lt;dbl\u0026gt;, `Progression RANK` \u0026lt;dbl\u0026gt;, `Rank 2015` \u0026lt;dbl\u0026gt;, ## # `Score 2015` \u0026lt;dbl\u0026gt;, Zone \u0026lt;chr\u0026gt; and we see that a total of 180 countries have a score (Overall Score 2016).\n GDP per capita To have something somehow meaningful to compare the freedom index to. I’ve found some data about GDP per capita, mostly because I figured it would have data for quite a lot of the countries covered by the freedom index. So from The World Bank (direct download link) which we load in as normal.\n(gdp_pcap \u0026lt;- read_csv(\u0026quot;API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv\u0026quot;, skip = 2)) which have quite a few variables but for now we will just focus on the 2015 variable as the 2016 appears empty. Now that we have two data sets which we would like to combine, a general question would be if the gdp_pcap data have information matching our 180 countries. So with the following bit of code we join the two datasets together by the ICO ALPHA-3 Code available in both datasets and select the countries who don’t have a value for the year 2015.\nleft_join(freedom_index, gdp_pcap, by = c(\u0026quot;ISO\u0026quot; = \u0026quot;Country Code\u0026quot;)) %\u0026gt;% filter(is.na(`2015`)) %\u0026gt;% select(EN_country) ## # A tibble: 12 x 1 ## EN_country ## \u0026lt;chr\u0026gt; ## 1 Liechtenstein ## 2 Andorra ## 3 OECS ## 4 Taiwan ## 5 Papua New Guinea ## 6 Cyprus North ## 7 Kosovo ## 8 Venezuela ## 9 Libya ## 10 Syrian Arab Republic ## 11 Eritrea ## 12 Democratic People\u0026#39;s Republic of Korea which leaves us with 166 countries. I could have looked for the data for these countries, but that is outside the reach for this post.\n Flag emoji I would like to use the different flag emojis\n## 🇦🇨🇦🇩🇦🇪🇦🇫🇦🇬🇦🇮🇦🇱🇦🇲🇦🇴🇦🇶🇦🇷🇦🇸🇦🇹🇦🇺🇦🇼🇦🇽🇦🇿🇧🇦🇧🇧🇧🇩🇧🇪🇧🇫🇧🇬🇧🇭🇧🇮🇧🇯🇧🇱🇧🇲🇧🇳🇧🇴🇧🇶🇧🇷🇧🇸🇧🇹🇧🇻🇧🇼🇧🇾🇧🇿🇨🇦🇨🇨🇨🇩🇨🇫🇨🇬🇨🇭🇨🇮🇨🇰🇨🇱🇨🇲🇨🇳🇨🇴🇨🇵🇨🇷🇨🇺🇨🇻🇨🇼🇨🇽🇨🇾🇨🇿🇩🇪🇩🇪🇩🇬🇩🇯🇩🇰🇩🇲🇩🇴🇩🇿🇪🇦🇪🇨🇪🇪🇪🇬🇪🇭🇪🇷🇪🇸🇪🇹🇪🇺🇫🇮🇫🇯🇫🇰🇫🇲🇫🇴🇫🇷🇬🇦🇬🇧🇬🇧🇬🇩🇬🇪🇬🇫🇬🇬🇬🇭🇬🇮🇬🇱🇬🇲🇬🇳🇬🇵🇬🇶🇬🇷🇬🇸🇬🇹🇬🇺🇬🇼🇬🇾🇭🇰🇭🇲🇭🇳🇭🇷🇭🇹🇭🇺🇮🇨🇮🇩🇮🇪🇮🇱🇮🇲🇮🇳🇮🇴🇮🇶🇮🇷🇮🇸🇮🇹🇯🇪🇯🇲🇯🇴🇯🇵🇰🇪🇰🇬🇰🇭🇰🇮🇰🇲🇰🇳🇰🇵🇰🇷🇰🇼🇰🇾🇰🇿🇱🇦🇱🇧🇱🇨🇱🇮🇱🇰🇱🇷🇱🇸🇱🇹🇱🇺🇱🇻🇱🇾🇲🇦🇲🇨🇲🇩🇲🇪🇲🇫🇲🇬🇲🇭🇲🇰🇲🇱🇲🇲🇲🇳🇲🇴🇲🇵🇲🇶🇲🇷🇲🇸🇲🇹🇲🇺🇲🇻🇲🇼🇲🇽🇲🇾🇲🇿🇳🇦🇳🇨🇳🇪🇳🇫🇳🇬🇳🇮🇳🇱🇳🇴🇳🇵🇳🇷🇳🇺🇳🇿🇴🇲🇵🇦🇵🇪🇵🇫🇵🇬🇵🇭🇵🇰🇵🇱🇵🇲🇵🇳🇵🇷🇵🇸🇵🇹🇵🇼🇵🇾🇶🇦🇷🇪🇷🇴🇷🇸🇷🇺🇷🇼🇸🇦🇸🇧🇸🇨🇸🇩🇸🇪🇸🇬🇸🇭🇸🇮🇸🇯🇸🇰🇸🇱🇸🇲🇸🇳🇸🇴🇸🇷🇸🇸🇸🇹🇸🇻🇸🇽🇸🇾🇸🇿🇹🇦🇹🇨🇹🇩🇹🇫🇹🇬🇹🇭🇹🇯🇹🇰🇹🇱🇹🇲🇹🇳🇹🇴🇹🇷🇹🇹🇹🇻🇹🇼🇹🇿🇺🇦🇺🇬🇺🇲🇺🇳🇺🇸🇺🇸🇺🇾🇺🇿🇻🇦🇻🇨🇻🇪🇻🇬🇻🇮🇻🇳🇻🇺🇼🇫🇼🇸🇽🇰🇾🇪🇾🇹🇿🇦🇿🇲🇿🇼🏴󠁧󠁢󠁥󠁮󠁧󠁿🏴󠁧󠁢󠁳󠁣󠁴󠁿🏴󠁧󠁢󠁷󠁬󠁳󠁿 which all can be found with the new emo(ji) package\nemo::ji_find(\u0026quot;flag\u0026quot;) ## # A tibble: 264 x 2 ## name emoji ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Ascension_Island 🇦🇨 ## 2 andorra 🇦🇩 ## 3 united_arab_emirates 🇦🇪 ## 4 afghanistan 🇦🇫 ## 5 antigua_barbuda 🇦🇬 ## 6 anguilla 🇦🇮 ## 7 albania 🇦🇱 ## 8 armenia 🇦🇲 ## 9 angola 🇦🇴 ## 10 antarctica 🇦🇶 ## # … with 254 more rows we first notice that the first two emojis are not country flags, and that the name of the countries are not on same format as what we have from earlier, so we replace the underscores with spaces and translate everything to lowercase before joining. This time by country name. Again we check for missed joints.\nleft_join(freedom_index, gdp_pcap, by = c(\u0026quot;ISO\u0026quot; = \u0026quot;Country Code\u0026quot;)) %\u0026gt;% mutate(EN_country = tolower(EN_country)) %\u0026gt;% left_join(emo::ji_find(\u0026quot;flag\u0026quot;) %\u0026gt;% mutate(name = str_replace_all(name, \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;)) %\u0026gt;% filter(name != \u0026quot;japan\u0026quot;, name != \u0026quot;crossed flags\u0026quot;), by = c(\u0026quot;EN_country\u0026quot; = \u0026quot;name\u0026quot;)) %\u0026gt;% filter(!is.na(`2015`)) %\u0026gt;% filter(is.na(emoji)) %\u0026gt;% select(EN_country) ## # A tibble: 22 x 1 ## EN_country ## \u0026lt;chr\u0026gt; ## 1 germany ## 2 spain ## 3 trinidad and tobago ## 4 france ## 5 united kingdom ## 6 united states ## 7 italy ## 8 south korea ## 9 bosnia and herzegovina ## 10 japan ## # … with 12 more rows Which is quite a few. It turns out that the naming convention for the emoji names have not been that consistent, “de” used instead of “germany” etc. To clear up code later on we make a new emoji tibble with all the changes.\nnewemoji \u0026lt;- emo::ji_find(\u0026quot;flag\u0026quot;) %\u0026gt;% mutate(name = str_replace_all(string = name, pattern = \u0026quot;_\u0026quot;, replacement = \u0026quot; \u0026quot;)) %\u0026gt;% filter(name != \u0026quot;japan\u0026quot;, name != \u0026quot;crossed flags\u0026quot;) %\u0026gt;% mutate(name = str_replace(name, \u0026quot;^de$\u0026quot;, \u0026quot;germany\u0026quot;), name = str_replace(name, \u0026quot;^es$\u0026quot;, \u0026quot;spain\u0026quot;), name = str_replace(name, \u0026quot;^trinidad tobago$\u0026quot;, \u0026quot;trinidad and tobago\u0026quot;), name = str_replace(name, \u0026quot;^fr$\u0026quot;, \u0026quot;france\u0026quot;), name = str_replace(name, \u0026quot;^uk$\u0026quot;, \u0026quot;united kingdom\u0026quot;), name = str_replace(name, \u0026quot;^us$\u0026quot;, \u0026quot;united states\u0026quot;), name = str_replace(name, \u0026quot;^it$\u0026quot;, \u0026quot;italy\u0026quot;), name = str_replace(name, \u0026quot;^kr$\u0026quot;, \u0026quot;south korea\u0026quot;), name = str_replace(name, \u0026quot;^bosnia herzegovina$\u0026quot;, \u0026quot;bosnia and herzegovina\u0026quot;), name = str_replace(name, \u0026quot;^guinea bissau$\u0026quot;, \u0026quot;guinea-bissau\u0026quot;), name = str_replace(name, \u0026quot;^cote divoire$\u0026quot;, \u0026quot;ivory coast\u0026quot;), name = str_replace(name, \u0026quot;^timor leste$\u0026quot;, \u0026quot;east timor\u0026quot;), name = str_replace(name, \u0026quot;^congo brazzaville$\u0026quot;, \u0026quot;congo\u0026quot;), name = str_replace(name, \u0026quot;^palestinian territories$\u0026quot;, \u0026quot;palestine\u0026quot;), name = str_replace(name, \u0026quot;^ru$\u0026quot;, \u0026quot;russian federation\u0026quot;), name = str_replace(name, \u0026quot;^congo kinshasa$\u0026quot;, \u0026quot;the democratic republic of the congo\u0026quot;), name = str_replace(name, \u0026quot;^tr$\u0026quot;, \u0026quot;turkey\u0026quot;), name = str_replace(name, \u0026quot;^brunei$\u0026quot;, \u0026quot;brunei darussalam\u0026quot;), name = str_replace(name, \u0026quot;^laos$\u0026quot;, \u0026quot;lao people\u0026#39;s democratic republic\u0026quot;), name = str_replace(name, \u0026quot;^cn$\u0026quot;, \u0026quot;china\u0026quot;), name = str_replace(name, \u0026quot;^jp$\u0026quot;, \u0026quot;japan\u0026quot;)) newemoji ## # A tibble: 264 x 2 ## name emoji ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Ascension Island 🇦🇨 ## 2 andorra 🇦🇩 ## 3 united arab emirates 🇦🇪 ## 4 afghanistan 🇦🇫 ## 5 antigua barbuda 🇦🇬 ## 6 anguilla 🇦🇮 ## 7 albania 🇦🇱 ## 8 armenia 🇦🇲 ## 9 angola 🇦🇴 ## 10 antarctica 🇦🇶 ## # … with 254 more rows   Plotting it all with ggplot2 Now with all the preparation done we do a naive first plot.\nleft_join(freedom_index, gdp_pcap, by = c(\u0026quot;ISO\u0026quot; = \u0026quot;Country Code\u0026quot;)) %\u0026gt;% mutate(EN_country = tolower(EN_country)) %\u0026gt;% left_join(newemoji, by = c(\u0026quot;EN_country\u0026quot; = \u0026quot;name\u0026quot;)) %\u0026gt;% ggplot(aes(x = `2015`, y = `Overall Score 2016`)) + geom_text(aes(label = emoji)) ## Warning: Removed 14 rows containing missing values (geom_text). But wait, we have a couple of problems:\n The emojis don’t show up. The freedom score is 100 times to much as the actual. The gdp_pcap is quite skewed.  But these are not problems too great for us. It turns out that R’s graphical devices don’t support AppleColorEmoji font. We can alleviate the that problem by saving the plot as a svg file. And we will do a simple log transformation of the gdp_pcap.\nOur final plot is thus the following:\nleft_join(freedom_index, gdp_pcap, by = c(\u0026quot;ISO\u0026quot; = \u0026quot;Country Code\u0026quot;)) %\u0026gt;% mutate(EN_country = tolower(EN_country), `Overall Score 2016` = `Overall Score 2016` / 100) %\u0026gt;% left_join(newemoji, by = c(\u0026quot;EN_country\u0026quot; = \u0026quot;name\u0026quot;)) %\u0026gt;% ggplot(aes(x = `2015`, y = `Overall Score 2016`)) + stat_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;grey\u0026quot;, se = FALSE) + geom_text(aes(label = emoji)) + scale_x_log10() + annotation_logticks(sides = \u0026quot;b\u0026quot;) + theme_ipsum() + labs(x = \u0026quot;GDP per capita (current US$)\u0026quot;, y = \u0026quot;2017 World Press Freedom Index\u0026quot;, title = \u0026quot;Countries with high GDP per capita\\ntend to have low Freedom Index\u0026quot;, subtitle = \u0026quot;Visualized with emojis\u0026quot;)  ","date":1493164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493164800,"objectID":"557f54ee6254fe1c0edf2db93f39a148","permalink":"/2017/04/26/2017-world-press-freedom-index-with-emojis/","publishdate":"2017-04-26T00:00:00Z","relpermalink":"/2017/04/26/2017-world-press-freedom-index-with-emojis/","section":"post","summary":"This code have been lightly revised to make sure it works as of 2018-12-16.\nWith Reporters Without Borders coming out with its 2017 World Press Freedom Index in the same week as Hadley Wickham coming out with the emo(ji) package, I had no choice but to explore both of them at the same time.","tags":null,"title":"2017 World Press Freedom Index with emojis","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]