<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Academic</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 19 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>tidytuesday: Part-of-Speech and textrecipes with The Office</title>
      <link>/2020/03/19/tidytuesday-part-of-speech-and-textrecipes-with-the-office/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/19/tidytuesday-part-of-speech-and-textrecipes-with-the-office/</guid>
      <description>


&lt;p&gt;I‚Äôm ready for my second &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#tidytuesday&lt;/a&gt; and as a massive &lt;a href=&#34;https://www.imdb.com/title/tt0386676/&#34;&gt;The Office&lt;/a&gt; fan this dataset is right up my alley. In this post, you will read how to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the R wrapper &lt;a href=&#34;http://spacyr.quanteda.io/&#34;&gt;spacyr&lt;/a&gt; of &lt;a href=&#34;https://spacy.io/&#34;&gt;spacy&lt;/a&gt; to extract part of speech tags&lt;/li&gt;
&lt;li&gt;Use a custom tokenizer in conjunction with &lt;a href=&#34;https://tidymodels.github.io/textrecipes/dev/&#34;&gt;textrecipes&lt;/a&gt; package&lt;/li&gt;
&lt;li&gt;Do hyperparameter tuning with the &lt;a href=&#34;https://github.com/tidymodels/tune&#34;&gt;tune&lt;/a&gt; package&lt;/li&gt;
&lt;li&gt;Try to predict the author of each line in the show&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I‚Äôll put a little more effort into the explorative charts then I usually do.
I‚Äôll not be explaining each line of code for those, but you are encouraged to play around with them yourself.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages üì¶&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(schrute)
library(tidytext)
library(tidymodels)
library(tokenizers)
library(textrecipes)
library(spacyr)
library(paletteer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be using the &lt;a href=&#34;https://bradlindblad.github.io/schrute/index.html&#34;&gt;schrute&lt;/a&gt; package which includes the dataset for the week.
&lt;a href=&#34;https://github.com/juliasilge/tidytext&#34;&gt;tidytext&lt;/a&gt; and &lt;a href=&#34;https://github.com/ropensci/tokenizers&#34;&gt;tokenizers&lt;/a&gt; to do data exploration for the text.
&lt;a href=&#34;http://spacyr.quanteda.io/&#34;&gt;spacyr&lt;/a&gt; to access the spacy to perform part of speech tagging.
&lt;a href=&#34;https://github.com/tidymodels&#34;&gt;tidymodels&lt;/a&gt; and
&lt;a href=&#34;https://tidymodels.github.io/textrecipes/dev/&#34;&gt;textrecipes&lt;/a&gt; to do to the preprocessing and modeling.
And lastly, we use &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer&#34;&gt;paletteer&lt;/a&gt; to get pretty color palettes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring the data ‚õè&lt;/h2&gt;
&lt;p&gt;The data comes with a lot of different variables. We will be focusing on &lt;code&gt;character&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;.
First, let us take a look at how many lines each character has&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theoffice %&amp;gt;%
  count(character, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Micheal, Dwight, Jim, and Pam are dominating the charts.
This is unsurprising since they are some of the main characters having a central role in the episodes they appear in.
This will be too many classes for the scope of this post so I‚Äôll limit it to the top 5 characters with the most lines since the number drops off more after the first 5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_office &amp;lt;- theoffice %&amp;gt;%
  select(character, text) %&amp;gt;%
  filter(character %in% c(&amp;quot;Michael&amp;quot;, &amp;quot;Dwight&amp;quot;, &amp;quot;Jim&amp;quot;, &amp;quot;Pam&amp;quot;, &amp;quot;Andy&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us take a lot at how many words each line in the script is.
This is going to be a problem for us later on as predicting with shorter text is harder than longer text as there is less information in it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_office %&amp;gt;%
  mutate(n_words = count_words(text)) %&amp;gt;%
  ggplot(aes(n_words, color = character)) +
  geom_density(binwidth = 1, key_glyph = draw_key_timeseries) +
  xlim(c(0, 50)) +
  scale_color_paletteer_d(&amp;quot;nord::aurora&amp;quot;) +
  labs(x = &amp;quot;Number of words&amp;quot;, y = &amp;quot;Density&amp;quot;, color = NULL,
       title = &amp;quot;Distribution of line length in The Office&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, 
        plot.title.position = &amp;quot;plot&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These lines are thankfully pretty similar, which will make it easier for us to make a good predictive model.
However, we can still see some differences.
Pam and Jim both have shorter lines than the rest,
and Michael and Andy both have fewer shorter lines in exchange for more long lines.&lt;/p&gt;
&lt;p&gt;We will be also be exploring &lt;a href=&#34;https://en.wikipedia.org/wiki/Part-of-speech_tagging&#34;&gt;part of speech tagging&lt;/a&gt; and for that, we will be using the spacyr package.
It isn‚Äôt always needed but I‚Äôm going to explicitly initialize the spacy model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spacy_initialize(model = &amp;quot;en_core_web_sm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the spacyr package outputs in this nice format with &lt;code&gt;doc_id&lt;/code&gt;, &lt;code&gt;sentence_id&lt;/code&gt;, &lt;code&gt;token_id&lt;/code&gt;, &lt;code&gt;token&lt;/code&gt; and &lt;code&gt;pos&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spacy_parse(small_office$text[1], entity = FALSE, lemma = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally I would just analyze the data in this format.
But since I have to create a custom wrapper for textrecipes anyway I‚Äôll do the remaining of the text mining in tidytext.
textrecipes requires that the tokenizer returns the tokens in a list format similar to the tokenizers in &lt;strong&gt;tokenizers&lt;/strong&gt;.
The following function takes a character vector and returns the part of speech tags in a list format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spacy_pos &amp;lt;- function(x) {
  tokens &amp;lt;- spacy_parse(x, entity = FALSE, lemma = FALSE)
  token_list &amp;lt;- split(tokens$pos, tokens$doc_id)
  names(token_list) &amp;lt;- gsub(&amp;quot;text&amp;quot;, &amp;quot;&amp;quot;, names(token_list))
  res &amp;lt;- unname(token_list[as.character(seq_along(x))])
  empty &amp;lt;- lengths(res) == 0
  res[empty] &amp;lt;- lapply(seq_len(sum(empty)), function(x) character(0))
  res
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Little example to showcase the function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example_string &amp;lt;- c(&amp;quot;Hello there pig&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;One more pig here&amp;quot;)

spacy_pos(x = example_string)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use a custom tokenizer by simply passing it to the &lt;code&gt;token&lt;/code&gt; argument.
This is going to take a little longer than normal since POS tagging takes longer than simply tokenizing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_office_tokens &amp;lt;- small_office %&amp;gt;%
  unnest_tokens(text, text, token = spacy_pos, to_lower = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is a chart of the number of each part of speech tags.
The meaning of the acronyms can be found &lt;a href=&#34;https://spacy.io/api/annotation&#34;&gt;here&lt;/a&gt; if you click on the Universal Part-of-speech Tags button.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colors &amp;lt;- rep(paletteer_d(&amp;quot;rcartocolor::Pastel&amp;quot;), length.out = 16)

small_office_tokens %&amp;gt;%
  count(text) %&amp;gt;%
  ggplot(aes(n, reorder(text, n), fill = reorder(text, n))) +
  geom_col() +
  labs(x = NULL, y = NULL, title = &amp;quot;Part of Speech tags in The Office&amp;quot;) +
  scale_fill_manual(values = colors) +
  guides(fill = &amp;quot;none&amp;quot;) +
  theme_minimal() +
  theme(plot.title.position = &amp;quot;plot&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I found it initially surprising that punctuation (&lt;code&gt;PUNCT&lt;/code&gt;) was leading the chart.
But after thinking about it a little bit it, I can imagine it has something to do with all the lines being very short and having to end in some kind of punctuation.&lt;/p&gt;
&lt;p&gt;We can facet this by the character to see who uses what part of speech.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_office_tokens %&amp;gt;%
  count(character, text) %&amp;gt;%
  group_by(character) %&amp;gt;%
  mutate(prop = n / sum(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(forcats::fct_rev(reorder(text, n)), prop, fill = character)) +
  geom_col(position = &amp;quot;dodge&amp;quot;) +
  scale_fill_paletteer_d(&amp;quot;nord::aurora&amp;quot;) +
  labs(x = NULL, y = NULL, fill = NULL,
       title = &amp;quot;Part of speech tags by main character in The Office&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;top&amp;quot;, 
        plot.title.position = &amp;quot;plot&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I don‚Äôt immediately see anything popping out at me, but it is a very pretty chart otherwise.
I feel like I have seen enough, lets get to modeling!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-Ô∏è&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling ‚öôÔ∏è&lt;/h2&gt;
&lt;p&gt;Not that we have gotten a look at the data lets get to modeling.
First we need to do a test/train split which we can do with &lt;a href=&#34;https://github.com/tidymodels/yardstick&#34;&gt;yardstick&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
office_split &amp;lt;- initial_split(small_office, strata = character)
office_test &amp;lt;- testing(office_split)
office_train &amp;lt;- training(office_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we are going to prepare the preprocessing steps.
We will be using the custom part of speech tokenizer we defined earlier to include part of speech tag counts as features in our model.
Since this data is going to a little sparse will we also include &lt;a href=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;bi-grams&lt;/a&gt; of the data.
To this, we first create a copy of the text variable and apply the tokenizers to each copy.
Lastly will be also be doing some downsampling of the data to handle the imbalance in the data.
This calculation will once again take a little while since the part of speech calculations takes a minute or two.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- recipe(character ~ text, data = small_office) %&amp;gt;%
  # Deal with imbalance
  step_downsample(character) %&amp;gt;%
  # Create copy of text variable
  step_mutate(text_copy = text) %&amp;gt;%
  # Tokenize the two text columns
  step_tokenize(text, token = &amp;quot;ngrams&amp;quot;, options = list(n = 2)) %&amp;gt;%
  step_tokenize(text_copy, custom_token = spacy_pos) %&amp;gt;%
  # Filter to only keep the most 100 frequent n-grams
  step_tokenfilter(text, max_tokens = 100) %&amp;gt;%
  # Calculate tf-idf for both sets of tokens
  step_tfidf(text, text_copy) %&amp;gt;%
  prep()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now extract the processed data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;office_test_prepped &amp;lt;- bake(rec, office_test)
office_train_prepped &amp;lt;- juice(rec)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To do the actual modeling we will be using &lt;code&gt;multinom_reg()&lt;/code&gt; with &lt;code&gt;&#34;glmnet&#34;&lt;/code&gt; as the engine.
This model has two hyperparameters, which we will be doing a grid search over to find optimal values.
We specify that we want to tune these parameters by passing &lt;code&gt;tune()&lt;/code&gt; to them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tune_spec &amp;lt;- multinom_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)
tune_spec&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we set up a bootstrap sampler and grid to optimize over.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(12345)
office_boot &amp;lt;- bootstraps(office_train_prepped, strata = character, times = 10)

hyper_grid &amp;lt;- grid_regular(penalty(), mixture(), levels = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we pass all the objects to &lt;code&gt;tune_grid()&lt;/code&gt;.
It is also possible to combine our recipe and model object into a workflow object to pass to tune_grid instead.
However, since the preprocessing step took so long and we didn‚Äôt vary anything it makes more sense time-wise to use &lt;code&gt;tune_grid()&lt;/code&gt; with a formula instead.
I also set &lt;code&gt;control = control_grid(verbose = TRUE)&lt;/code&gt; so I get a live update of how far the calculations are going.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123456)
fitted_grid &amp;lt;- tune_grid(
  formula = character ~ .,
  model = tune_spec,
  resamples = office_boot,
  grid = hyper_grid,
  control = control_grid(verbose = TRUE)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now look at the best performing models with &lt;code&gt;show_best()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_grid %&amp;gt;%
  show_best(&amp;quot;roc_auc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can use the values from the best performing model to fit our final model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_model &amp;lt;- tune_spec %&amp;gt;%
  update(penalty = 0.005994843, mixture = 1 / 3) %&amp;gt;%
  fit(character ~ ., data = office_train_prepped)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation üìê&lt;/h2&gt;
&lt;p&gt;Now that we have our final model we can predict on our test set and look at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;confusion matrix&lt;/a&gt; to see how well we did.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_cols(
  predict(final_model, office_test_prepped),
  office_test_prepped
) %&amp;gt;%
  conf_mat(truth = character, estimate = .pred_class) %&amp;gt;%
  autoplot(type = &amp;quot;heatmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are not going too well.
It is doing best at predicting Michael correctly, and it seems to confuse Dwight and Michael a little bit.&lt;/p&gt;
&lt;p&gt;Let us investigate the cases that didn‚Äôt go too well.
We can get the individual class probabilities by setting &lt;code&gt;type = &#34;prob&#34;&lt;/code&gt; in &lt;code&gt;predict()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class_predictions &amp;lt;- predict(final_model, office_test_prepped, type = &amp;quot;prob&amp;quot;)
class_predictions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do some wrangling to get the 5 worst predicted texts for each character:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_cols(
  class_predictions,
  office_test
) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;.pred_&amp;quot;)) %&amp;gt;%
  filter(gsub(&amp;quot;.pred_&amp;quot;, &amp;quot;&amp;quot;, name) == character) %&amp;gt;%
  group_by(character) %&amp;gt;%
  arrange(value) %&amp;gt;%
  slice(1:5) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(-name, -value) %&amp;gt;%
  reactable::reactable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the first striking thing here is that many of the lines are quite short, with most of Pam‚Äôs being 5 words or less. On the other hand, all the wrongly predicted lines for Michael are quite a bit longer than the rest.&lt;/p&gt;
&lt;p&gt;We can also get the best predicted lines for each character by flipping the sign with &lt;code&gt;desc()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_cols(
  class_predictions,
  office_test
) %&amp;gt;%
  pivot_longer(starts_with(&amp;quot;.pred_&amp;quot;)) %&amp;gt;%
  filter(gsub(&amp;quot;.pred_&amp;quot;, &amp;quot;&amp;quot;, name) == character) %&amp;gt;%
  group_by(character) %&amp;gt;%
  arrange(desc(value)) %&amp;gt;%
  slice(1:5) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(-name, -value) %&amp;gt;%
  reactable::reactable()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing I noticed is that many of Pam‚Äôs lines start with ‚ÄúOh my‚Äù and that might have been a unique character trait that got picked up in the bi-grams.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Word Rank Slope Charts</title>
      <link>/2020/03/17/word-rank-slope-charts/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/17/word-rank-slope-charts/</guid>
      <description>


&lt;p&gt;I have been working on visualizing how different kinds of words are used in texts and I finally found a good visualization style with the &lt;a href=&#34;https://datavizproject.com/data-type/slope-chart/&#34;&gt;slope chart&lt;/a&gt;.
More specifically I‚Äôm thinking of two groups of paired words.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages üì¶&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(hcandersenr)
library(tidytext)
library(paletteer)
library(ggrepel)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;minimal-example-1Ô∏è‚É£&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimal Example 1Ô∏è‚É£&lt;/h2&gt;
&lt;p&gt;First I‚Äôll walk you through a minimal example of how the chart is created.
Afterward, I have created a function to automate the whole procedure so we can quickly iterate.
We start by an example of gendered words in fairly tales by H.C. Andersen using the &lt;a href=&#34;https://github.com/EmilHvitfeldt/hcandersenr&#34;&gt;hcandersenr&lt;/a&gt; package.
We start by generating a data.frame of paired words.
This is easily done using the &lt;code&gt;tribble()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gender_words &amp;lt;- tribble(
  ~men, ~women,
  &amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;,
  &amp;quot;his&amp;quot;, &amp;quot;her&amp;quot;,
  &amp;quot;man&amp;quot;, &amp;quot;woman&amp;quot;,
  &amp;quot;men&amp;quot;, &amp;quot;women&amp;quot;,
  &amp;quot;boy&amp;quot;, &amp;quot;girl&amp;quot;,
  &amp;quot;he&amp;#39;s&amp;quot;, &amp;quot;she&amp;#39;s&amp;quot;,
  &amp;quot;he&amp;#39;d&amp;quot;, &amp;quot;she&amp;#39;d&amp;quot;,
  &amp;quot;he&amp;#39;ll&amp;quot;, &amp;quot;she&amp;#39;ll&amp;quot;,
  &amp;quot;himself&amp;quot;, &amp;quot;herself&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we are going to tokenize and count the tokens in the corpus,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ordered_words &amp;lt;- hcandersen_en %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  count(word, sort = TRUE) %&amp;gt;% 
  pull(word)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we are going to get the index for each word, which we will put on a log scale since it will be &lt;a href=&#34;https://en.wikipedia.org/wiki/Zipf%27s_law&#34;&gt;easier to visualize&lt;/a&gt;.
Next, we will calculate a slope between the points and add the correct labels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gender_words_plot &amp;lt;- gender_words %&amp;gt;%
  mutate(male_index = match(men, ordered_words),
         female_index = match(women, ordered_words)) %&amp;gt;%
  mutate(slope = log10(male_index) - log10(female_index)) %&amp;gt;%
  pivot_longer(male_index:female_index) %&amp;gt;%
  mutate(value = log10(value),
         label = ifelse(name == &amp;quot;male_index&amp;quot;, men, women)) %&amp;gt;%
  mutate(name = factor(name, c(&amp;quot;male_index&amp;quot;, &amp;quot;female_index&amp;quot;), c(&amp;quot;men&amp;quot;, &amp;quot;women&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we are going to manually calculate the limits to make sure a diverging color scale will have the colors &lt;a href=&#34;https://www.hvitfeldt.me/blog/center-continuous-palettes-in-ggplot2/&#34;&gt;done directly&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limit &amp;lt;- max(abs(gender_words_plot$slope)) * c(-1, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we just put everything into ggplot2 and voila!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gender_words_plot %&amp;gt;%
  ggplot(aes(name, value, group = women, label = label)) +
  geom_line(aes(color = slope)) +
  scale_y_reverse(labels = function(x) 10 ^ x) +
  geom_text() +
  guides(color = &amp;quot;none&amp;quot;) +
  scale_color_distiller(type = &amp;quot;div&amp;quot;, limit = limit) +
  theme_minimal() +
  theme(panel.border = element_blank(), panel.grid.major.x = element_blank()) +
  labs(x = NULL, y = &amp;quot;Word Rank&amp;quot;) +
  labs(title = &amp;quot;Masculine gendered words appeared more often in H.C. Andersen&amp;#39;s fairy tales&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-17-word-rank-slope-charts/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;make-it-into-a-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Make it into a function ‚ú®&lt;/h2&gt;
&lt;p&gt;This function is mostly the same as the code you saw earlier.
Main difference is using &lt;code&gt;.data&lt;/code&gt; from &lt;a href=&#34;https://rlang.r-lib.org/reference/tidyeval-data.html&#34;&gt;rlang&lt;/a&gt; to generalize.
The function also includes other beautifications such as improved themes and theme support with &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer&#34;&gt;paletteer&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_fun &amp;lt;- function(words, ref, palette = &amp;quot;scico::roma&amp;quot;, ...) {
  
  names &amp;lt;- colnames(ref)
  
  ordered_words &amp;lt;- names(sort(table(words), decreasing = TRUE))

  plot_data &amp;lt;- ref %&amp;gt;%
    mutate(index1 = match(.data[[names[1]]], ordered_words),
           index2 = match(.data[[names[2]]], ordered_words)) %&amp;gt;%
    mutate(slope = log10(index1) - log10(index2)) %&amp;gt;%
    pivot_longer(index1:index2) %&amp;gt;%
    mutate(value = log10(value),
           label = ifelse(name == &amp;quot;index1&amp;quot;, 
                          .data[[names[1]]], 
                          .data[[names[2]]]),
           name = factor(name, c(&amp;quot;index1&amp;quot;, &amp;quot;index2&amp;quot;), names))
  
  limit &amp;lt;- max(abs(plot_data$slope)) * c(-1, 1)

  plot_data %&amp;gt;%
    ggplot(aes(name, value, group = .data[[names[2]]], label = label)) +
    geom_line(aes(color = slope), size = 1) +
    scale_y_reverse(labels = function(x) round(10 ^ x)) +
    geom_text_repel(data = subset(plot_data, name == names[1]),
                    aes(segment.color = slope),
                    nudge_x       = -0.1,
                    segment.size  = 1,
                    direction     = &amp;quot;y&amp;quot;,
                    hjust         = 1) + 
    geom_text_repel(data = subset(plot_data, name == names[2]),
                    aes(segment.color = slope),
                    nudge_x       = 0.1,
                    segment.size  = 1,
                    direction     = &amp;quot;y&amp;quot;,
                    hjust         = 0) + 
    scale_color_paletteer_c(palette, 
                            limit = limit,
                            aesthetics = c(&amp;quot;color&amp;quot;, &amp;quot;segment.color&amp;quot;), 
                            ...) +
    guides(color = &amp;quot;none&amp;quot;, segment.color = &amp;quot;none&amp;quot;) +
    theme_minimal() +
    theme(panel.border = element_blank(), 
          panel.grid.major.x = element_blank(), axis.text.x = element_text(size = 15)) +
    labs(x = NULL, y = &amp;quot;Word Rank&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can recreate the previous chart with ease&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ref &amp;lt;- tribble(
  ~Men, ~Women,
  &amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;,
  &amp;quot;his&amp;quot;, &amp;quot;her&amp;quot;,
  &amp;quot;man&amp;quot;, &amp;quot;woman&amp;quot;,
  &amp;quot;men&amp;quot;, &amp;quot;women&amp;quot;,
  &amp;quot;boy&amp;quot;, &amp;quot;girl&amp;quot;,
  &amp;quot;he&amp;#39;s&amp;quot;, &amp;quot;she&amp;#39;s&amp;quot;,
  &amp;quot;he&amp;#39;d&amp;quot;, &amp;quot;she&amp;#39;d&amp;quot;,
  &amp;quot;he&amp;#39;ll&amp;quot;, &amp;quot;she&amp;#39;ll&amp;quot;,
  &amp;quot;himself&amp;quot;, &amp;quot;herself&amp;quot;
)

words &amp;lt;- hcandersen_en %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  pull(word)

plot_fun(words, ref, direction = -1) +
  labs(title = &amp;quot;Masculine gendered words appeared more often in H.C. Andersen&amp;#39;s fairy tales&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown aesthetics: segment.colour

## Warning: Ignoring unknown aesthetics: segment.colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-17-word-rank-slope-charts/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gallery&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gallery üñº&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ref &amp;lt;- tribble(
  ~Men, ~Women,
  &amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;,
  &amp;quot;his&amp;quot;, &amp;quot;her&amp;quot;,
  &amp;quot;man&amp;quot;, &amp;quot;woman&amp;quot;,
  &amp;quot;men&amp;quot;, &amp;quot;women&amp;quot;,
  &amp;quot;boy&amp;quot;, &amp;quot;girl&amp;quot;,
  &amp;quot;himself&amp;quot;, &amp;quot;herself&amp;quot;
)

words &amp;lt;- janeaustenr::austen_books() %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  pull(word)

plot_fun(words, ref, direction = -1) +
  labs(title = &amp;quot;Masculine gendered words appeared less often in Jane Austen Novels&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown aesthetics: segment.colour

## Warning: Ignoring unknown aesthetics: segment.colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-17-word-rank-slope-charts/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More examples using the &lt;a href=&#34;https://github.com/EmilHvitfeldt/tidygutenbergr&#34;&gt;tidygutenbergr&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ref &amp;lt;- tribble(
  ~Men, ~Women,
  &amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;,
  &amp;quot;his&amp;quot;, &amp;quot;her&amp;quot;,
  &amp;quot;man&amp;quot;, &amp;quot;woman&amp;quot;,
  &amp;quot;men&amp;quot;, &amp;quot;women&amp;quot;,
  &amp;quot;boy&amp;quot;, &amp;quot;girl&amp;quot;,
  &amp;quot;he&amp;#39;s&amp;quot;, &amp;quot;she&amp;#39;s&amp;quot;,
  &amp;quot;himself&amp;quot;, &amp;quot;herself&amp;quot;
)

words &amp;lt;- tidygutenbergr::phantom_of_the_opera() %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  pull(word)

plot_fun(words, ref, &amp;quot;scico::berlin&amp;quot;) +
  labs(title = &amp;quot;Masculine gendered words appeared more often in Phantom of the Opera&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown aesthetics: segment.colour

## Warning: Ignoring unknown aesthetics: segment.colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-17-word-rank-slope-charts/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ref &amp;lt;- tribble(
  ~Positive, ~Negative,
  &amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;,
  &amp;quot;pretty&amp;quot;, &amp;quot;ugly&amp;quot;,
  &amp;quot;friendly&amp;quot;, &amp;quot;hostile&amp;quot;
)

words &amp;lt;- tidygutenbergr::dracula() %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;%
  pull(word)

plot_fun(words, ref, palette = &amp;quot;scico::tokyo&amp;quot;) +
  labs(title = &amp;quot;Positive adjectives appeared more often in Dracula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown aesthetics: segment.colour

## Warning: Ignoring unknown aesthetics: segment.colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-17-word-rank-slope-charts/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using stm to Investigate if Stemming is Appropriate</title>
      <link>/2020/03/16/using-stm-to-investigate-if-stemming-is-appropriate/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/2020/03/16/using-stm-to-investigate-if-stemming-is-appropriate/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Photo by Plush Design Studio on Unsplash&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is known that topic modeling does not benefit from stemming &lt;a href=&#34;https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf&#34;&gt;ref&lt;/a&gt;.
I propose a workflow to investigate if stemming is appropriate as a method for data reduction.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take all the tokens and apply the stemming algorithm you would like to test&lt;/li&gt;
&lt;li&gt;Construct a list of words that should be equal under stemming&lt;/li&gt;
&lt;li&gt;Apply a topic model to your original data&lt;/li&gt;
&lt;li&gt;Predict the topic for each word created in 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If grouped words are predicted to the same topic then we assume that stemming would not make much of a difference.
If the words are predicted to be indifferent topics then we have a suspicion that the stemmed and unstemmed words have different uses and stemming would be ill-advised.&lt;/p&gt;
&lt;p&gt;First, we load the packages we will be using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stm)
library(hcandersenr)
library(SnowballC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;SnowballC&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a first test, we pick 3 fairy tales by H.C. Andersens using the &lt;a href=&#34;https://github.com/EmilHvitfeldt/hcandersenr&#34;&gt;hcandersenr&lt;/a&gt; package.
To create multiple ‚Äúdocuments‚Äù for each fairy tale we start by tokenizing to sentences.
Then we give each sentence a unique identifier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fairy_tales &amp;lt;- hcandersen_en %&amp;gt;%
  filter(book %in% c(&amp;quot;The fir tree&amp;quot;, &amp;quot;The tinder-box&amp;quot;, &amp;quot;Thumbelina&amp;quot;)) %&amp;gt;%
  unnest_tokens(token, text, token = &amp;quot;sentences&amp;quot;) %&amp;gt;%
  group_by(book) %&amp;gt;%
  mutate(sentence = row_number()) %&amp;gt;%
  ungroup() %&amp;gt;%
  unite(document, book, sentence)

fairy_tales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 501 x 2
##    document       token                                                         
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                                                         
##  1 The fir tree_1 &amp;quot;far down in the forest, where the warm sun and the fresh air‚Ä¶
##  2 The fir tree_2 &amp;quot;the sun shone, and the soft air fluttered its leaves, and th‚Ä¶
##  3 The fir tree_3 &amp;quot;sometimes the children would bring a large basket of raspber‚Ä¶
##  4 The fir tree_4 &amp;quot;which made it feel more unhappy than before.&amp;quot;                
##  5 The fir tree_5 &amp;quot;and yet all this while the tree grew a notch or joint taller‚Ä¶
##  6 The fir tree_6 &amp;quot;still, as it grew, it complained.&amp;quot;                           
##  7 The fir tree_7 &amp;quot;\&amp;quot;oh!&amp;quot;                                                       
##  8 The fir tree_8 &amp;quot;how i wish i were as tall as the other trees, then i would s‚Ä¶
##  9 The fir tree_9 &amp;quot;i should have the birds building their nests on my boughs, a‚Ä¶
## 10 The fir tree_‚Ä¶ &amp;quot;the tree was so discontented, that it took no pleasure in th‚Ä¶
## # ‚Ä¶ with 491 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we unnest the tokens to words and create a new variable of the stemmed words&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fairy_tales_tokens &amp;lt;- fairy_tales %&amp;gt;%
  unnest_tokens(token, token) %&amp;gt;%
  mutate(token_stem = wordStem(token))

fairy_tales_tokens&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,577 x 3
##    document       token  token_stem
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     
##  1 The fir tree_1 far    far       
##  2 The fir tree_1 down   down      
##  3 The fir tree_1 in     in        
##  4 The fir tree_1 the    the       
##  5 The fir tree_1 forest forest    
##  6 The fir tree_1 where  where     
##  7 The fir tree_1 the    the       
##  8 The fir tree_1 warm   warm      
##  9 The fir tree_1 sun    sun       
## 10 The fir tree_1 and    and       
## # ‚Ä¶ with 10,567 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a look at all the times where stemming we can look at all the times stemming yields a different token.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;different &amp;lt;- fairy_tales_tokens %&amp;gt;%
  select(token, token_stem) %&amp;gt;%
  filter(token != token_stem) %&amp;gt;%
  unique()

different&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 759 x 2
##    token      token_stem
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;     
##  1 resting    rest      
##  2 pretty     pretti    
##  3 little     littl     
##  4 was        wa        
##  5 happy      happi     
##  6 wished     wish      
##  7 its        it        
##  8 companions companion 
##  9 pines      pine      
## 10 firs       fir       
## # ‚Ä¶ with 749 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we have 759 different tokens.
But since stemming can collapse multiple different tokens into one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;different %&amp;gt;%
  count(token_stem, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 672 x 2
##    token_stem     n
##    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
##  1 seiz           4
##  2 leav           3
##  3 live           3
##  4 look           3
##  5 place          3
##  6 plai           3
##  7 pleas          3
##  8 sai            3
##  9 trembl         3
## 10 appear         2
## # ‚Ä¶ with 662 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;different&lt;/code&gt; data.frame and construct a list of words that would land in the same bucket after stemming.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stem_buckets &amp;lt;- split(different$token, different$token_stem) %&amp;gt;%
  imap(~ c(.x, .y))

stem_buckets[21:25]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $anxiou
## [1] &amp;quot;anxious&amp;quot; &amp;quot;anxiou&amp;quot; 
## 
## $anyth
## [1] &amp;quot;anything&amp;quot; &amp;quot;anyth&amp;quot;   
## 
## $apart
## [1] &amp;quot;apartment&amp;quot; &amp;quot;apart&amp;quot;    
## 
## $appear
## [1] &amp;quot;appearance&amp;quot; &amp;quot;appeared&amp;quot;   &amp;quot;appear&amp;quot;    
## 
## $appl
## [1] &amp;quot;apples&amp;quot; &amp;quot;apple&amp;quot;  &amp;quot;appl&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that ‚Äúanxiou‚Äù and ‚Äúanxious‚Äù would look the same after stemming, likewise will ‚Äúapples‚Äù, ‚Äúapple‚Äù and ‚Äúappl‚Äù.
The main point of this exercise is to see if the words in these groups of words end up in the topic when during topic modeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stm_model &amp;lt;- fairy_tales_tokens %&amp;gt;%
  count(document, token) %&amp;gt;%
  cast_sparse(document, token, n) %&amp;gt;%
  stm(K = 3, verbose = FALSE)

stm_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A topic model with 3 topics, 501 documents and a 1518 word dictionary.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, I fit the model to 3 topics because I knew that would be the right number since I picked the data.
When doing this on your data you should run multiple models with a varying number of topics to find the best one.
For more information please read &lt;a href=&#34;https://juliasilge.com/blog/evaluating-stm/&#34;&gt;Training, Evaluating, and Interpreting Topic Models&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/juliasilge&#34;&gt;Julia Silge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have a &lt;code&gt;stm&lt;/code&gt; model and a list of words,
We can inspect the model object to check if multiple words are put in the same topic.
Below is a function that will take a vector of characters and a &lt;code&gt;stm&lt;/code&gt; model and return &lt;code&gt;TRUE&lt;/code&gt; if all the words appear in the same topic and &lt;code&gt;FALSE&lt;/code&gt; if they don‚Äôt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stm_match &amp;lt;- function(x, model) {
  topics &amp;lt;- tidy(model) %&amp;gt;%
  filter(term %in% x) %&amp;gt;%
  group_by(term) %&amp;gt;%
  top_n(1, beta) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(topic) %&amp;gt;%
  n_distinct()
  
  topics == 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, if we pass the words ‚Äúbelieved‚Äù and ‚Äúbeliev‚Äù&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stm_match(c(&amp;quot;believed&amp;quot;, &amp;quot;believ&amp;quot;), stm_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that they did end up in the same bucket.
If we instead pass in ‚Äúdog‚Äù and ‚Äúhappy‚Äù they land in different topics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stm_match(c(&amp;quot;dog&amp;quot;, &amp;quot;happy&amp;quot;), stm_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of this is not perfect, there is still some uncertainty but it is a good first step to evaluate if stemming is appropriate for your application.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tested &amp;lt;- tibble(terms = stem_buckets,
                 stem = names(stem_buckets)) %&amp;gt;%
  mutate(match = map_lgl(terms, stm_match, stm_model))

tested&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 672 x 3
##    terms        stem      match
##    &amp;lt;named list&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;lgl&amp;gt;
##  1 &amp;lt;chr [2]&amp;gt;    a         FALSE
##  2 &amp;lt;chr [2]&amp;gt;    abl       TRUE 
##  3 &amp;lt;chr [2]&amp;gt;    abov      TRUE 
##  4 &amp;lt;chr [2]&amp;gt;    accompani TRUE 
##  5 &amp;lt;chr [2]&amp;gt;    ach       TRUE 
##  6 &amp;lt;chr [2]&amp;gt;    admir     TRUE 
##  7 &amp;lt;chr [2]&amp;gt;    adorn     TRUE 
##  8 &amp;lt;chr [2]&amp;gt;    ag        TRUE 
##  9 &amp;lt;chr [2]&amp;gt;    ala       TRUE 
## 10 &amp;lt;chr [2]&amp;gt;    alarm     TRUE 
## # ‚Ä¶ with 662 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we‚Äôll look at the distribution of &lt;code&gt;TRUE&lt;/code&gt;s and &lt;code&gt;FALSE&lt;/code&gt;s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tested %&amp;gt;%  
  ggplot(aes(match)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-16-stm-stemming/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So it looks like most of the word groups were put into the same topic during modeling.
This is a good sign.
Please note that this category includes a lot of false positives.
This is happening because &lt;code&gt;stm_match()&lt;/code&gt; also returns true for a case where one of the words appears in the model and all other words don‚Äôt.
So for the case of ‚Äúaccompanied‚Äù and ‚Äúaccompani‚Äù, the word ‚Äúaccompanied‚Äù was present in one of the topics, but the word ‚Äúaccompani‚Äù was not present in the original data and hence did not appear in any of the topics.
In this case, the &lt;code&gt;TRUE&lt;/code&gt; value we are getting is saying that the data doesn‚Äôt provide enough evidence to indicate that stemming would be bad.
By looking at a sample of &lt;code&gt;TRUE&lt;/code&gt; cases we see that a lot of them are happening because the stemmed word isn‚Äôt being used, like the words ‚Äúaliv‚Äù, ‚Äúalon‚Äù and ‚Äúalwai‚Äù.
On the other side, we have that the words ‚Äúallowed‚Äù and ‚Äúallow‚Äù are both real words AND they appeared in the same topic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tested %&amp;gt;%
  filter(match) %&amp;gt;%
  slice(10:15) %&amp;gt;%
  pull(terms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;alighted&amp;quot; &amp;quot;alight&amp;quot;  
## 
## [[2]]
## [1] &amp;quot;alive&amp;quot; &amp;quot;aliv&amp;quot; 
## 
## [[3]]
## [1] &amp;quot;allowed&amp;quot; &amp;quot;allow&amp;quot;  
## 
## [[4]]
## [1] &amp;quot;alone&amp;quot; &amp;quot;alon&amp;quot; 
## 
## [[5]]
## [1] &amp;quot;already&amp;quot; &amp;quot;alreadi&amp;quot;
## 
## [[6]]
## [1] &amp;quot;always&amp;quot; &amp;quot;alwai&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turning our head to the &lt;code&gt;FALSE&lt;/code&gt; cases.
These cases will not have any false positives as both words would have to appear in the original corpus for them to put into different topics.
These cases are still not going to be perfect, but will again be an indication.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tested %&amp;gt;%
  filter(!match) %&amp;gt;%
  pull(terms) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;as&amp;quot; &amp;quot;a&amp;quot; 
## 
## [[2]]
## [1] &amp;quot;appearance&amp;quot; &amp;quot;appeared&amp;quot;   &amp;quot;appear&amp;quot;    
## 
## [[3]]
## [1] &amp;quot;backs&amp;quot; &amp;quot;back&amp;quot; 
## 
## [[4]]
## [1] &amp;quot;beginning&amp;quot; &amp;quot;begin&amp;quot;    
## 
## [[5]]
## [1] &amp;quot;beside&amp;quot;  &amp;quot;besides&amp;quot; &amp;quot;besid&amp;quot;  
## 
## [[6]]
## [1] &amp;quot;birds&amp;quot; &amp;quot;bird&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the list I would advise you to look over carefully.
Check to make sure that you are okay with the number and count of misgroupings you would get by applying stemming.&lt;/p&gt;
&lt;details closed&gt;
&lt;p&gt;&lt;summary&gt; &lt;span title=&#34;Click to Expand&#34;&gt; current session info &lt;/span&gt; &lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 setting  value                       
 version  R version 3.6.0 (2019-04-26)
 os       macOS Mojave 10.14.6        
 system   x86_64, darwin15.6.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 ctype    en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2020-04-21                  

‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 package     * version date       lib source        
 assertthat    0.2.1   2019-03-21 [1] CRAN (R 3.6.0)
 backports     1.1.6   2020-04-05 [1] CRAN (R 3.6.0)
 blogdown      0.18    2020-03-04 [1] CRAN (R 3.6.0)
 bookdown      0.18    2020-03-05 [1] CRAN (R 3.6.0)
 broom         0.5.5   2020-02-29 [1] CRAN (R 3.6.0)
 cellranger    1.1.0   2016-07-27 [1] CRAN (R 3.6.0)
 cli           2.0.2   2020-02-28 [1] CRAN (R 3.6.0)
 clipr         0.7.0   2019-07-23 [1] CRAN (R 3.6.0)
 codetools     0.2-16  2018-12-24 [1] CRAN (R 3.6.0)
 colorspace    1.4-1   2019-03-18 [1] CRAN (R 3.6.0)
 crayon        1.3.4   2017-09-16 [1] CRAN (R 3.6.0)
 data.table    1.12.8  2019-12-09 [1] CRAN (R 3.6.0)
 DBI           1.1.0   2019-12-15 [1] CRAN (R 3.6.0)
 dbplyr        1.4.2   2019-06-17 [1] CRAN (R 3.6.0)
 desc          1.2.0   2018-05-01 [1] CRAN (R 3.6.0)
 details     * 0.2.1   2020-01-12 [1] CRAN (R 3.6.0)
 digest        0.6.25  2020-02-23 [1] CRAN (R 3.6.0)
 dplyr       * 0.8.5   2020-03-07 [1] CRAN (R 3.6.0)
 ellipsis      0.3.0   2019-09-20 [1] CRAN (R 3.6.0)
 evaluate      0.14    2019-05-28 [1] CRAN (R 3.6.0)
 fansi         0.4.1   2020-01-08 [1] CRAN (R 3.6.0)
 farver        2.0.3   2020-01-16 [1] CRAN (R 3.6.0)
 forcats     * 0.5.0   2020-03-01 [1] CRAN (R 3.6.0)
 fs            1.4.1   2020-04-04 [1] CRAN (R 3.6.0)
 generics      0.0.2   2018-11-29 [1] CRAN (R 3.6.0)
 ggplot2     * 3.3.0   2020-03-05 [1] CRAN (R 3.6.0)
 glue          1.4.0   2020-04-03 [1] CRAN (R 3.6.0)
 gtable        0.3.0   2019-03-25 [1] CRAN (R 3.6.0)
 haven         2.2.0   2019-11-08 [1] CRAN (R 3.6.0)
 hcandersenr * 0.2.0   2019-01-19 [1] CRAN (R 3.6.0)
 hms           0.5.3   2020-01-08 [1] CRAN (R 3.6.0)
 htmltools     0.4.0   2019-10-04 [1] CRAN (R 3.6.0)
 httr          1.4.1   2019-08-05 [1] CRAN (R 3.6.0)
 janeaustenr   0.1.5   2017-06-10 [1] CRAN (R 3.6.0)
 jsonlite      1.6.1   2020-02-02 [1] CRAN (R 3.6.0)
 knitr       * 1.28    2020-02-06 [1] CRAN (R 3.6.0)
 labeling      0.3     2014-08-23 [1] CRAN (R 3.6.0)
 lattice       0.20-41 2020-04-02 [1] CRAN (R 3.6.0)
 lifecycle     0.2.0   2020-03-06 [1] CRAN (R 3.6.0)
 lubridate     1.7.8   2020-04-06 [1] CRAN (R 3.6.0)
 magrittr      1.5     2014-11-22 [1] CRAN (R 3.6.0)
 Matrix        1.2-18  2019-11-27 [1] CRAN (R 3.6.0)
 modelr        0.1.6   2020-02-22 [1] CRAN (R 3.6.0)
 munsell       0.5.0   2018-06-12 [1] CRAN (R 3.6.0)
 nlme          3.1-145 2020-03-04 [1] CRAN (R 3.6.0)
 pillar        1.4.3   2019-12-20 [1] CRAN (R 3.6.0)
 pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 3.6.0)
 plyr          1.8.6   2020-03-03 [1] CRAN (R 3.6.0)
 png           0.1-7   2013-12-03 [1] CRAN (R 3.6.0)
 purrr       * 0.3.3   2019-10-18 [1] CRAN (R 3.6.0)
 R6            2.4.1   2019-11-12 [1] CRAN (R 3.6.0)
 Rcpp          1.0.4.6 2020-04-09 [1] CRAN (R 3.6.0)
 readr       * 1.3.1   2018-12-21 [1] CRAN (R 3.6.0)
 readxl        1.3.1   2019-03-13 [1] CRAN (R 3.6.0)
 reprex        0.3.0   2019-05-16 [1] CRAN (R 3.6.0)
 reshape2      1.4.4   2020-04-09 [1] CRAN (R 3.6.2)
 rlang         0.4.5   2020-03-01 [1] CRAN (R 3.6.0)
 rmarkdown     2.1     2020-01-20 [1] CRAN (R 3.6.0)
 rprojroot     1.3-2   2018-01-03 [1] CRAN (R 3.6.0)
 rstudioapi    0.11    2020-02-07 [1] CRAN (R 3.6.0)
 rvest         0.3.5   2019-11-08 [1] CRAN (R 3.6.0)
 scales        1.1.0   2019-11-18 [1] CRAN (R 3.6.0)
 sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 3.6.0)
 SnowballC   * 0.7.0   2020-04-01 [1] CRAN (R 3.6.2)
 stm         * 1.3.5   2019-12-17 [1] CRAN (R 3.6.0)
 stringi       1.4.6   2020-02-17 [1] CRAN (R 3.6.0)
 stringr     * 1.4.0   2019-02-10 [1] CRAN (R 3.6.0)
 tibble      * 3.0.1   2020-04-20 [1] CRAN (R 3.6.2)
 tidyr       * 1.0.2   2020-01-24 [1] CRAN (R 3.6.0)
 tidyselect    1.0.0   2020-01-27 [1] CRAN (R 3.6.0)
 tidytext    * 0.2.3   2020-03-04 [1] CRAN (R 3.6.0)
 tidyverse   * 1.3.0   2019-11-21 [1] CRAN (R 3.6.0)
 tokenizers    0.2.1   2018-03-29 [1] CRAN (R 3.6.0)
 utf8          1.1.4   2018-05-24 [1] CRAN (R 3.6.0)
 vctrs         0.2.4   2020-03-10 [1] CRAN (R 3.6.0)
 withr         2.1.2   2018-03-15 [1] CRAN (R 3.6.0)
 xfun          0.13    2020-04-13 [1] CRAN (R 3.6.2)
 xml2          1.3.0   2020-04-01 [1] CRAN (R 3.6.2)
 yaml          2.2.1   2020-02-01 [1] CRAN (R 3.6.0)

[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use prismatic with after_scale() for finer control of colors in ggplot2</title>
      <link>/2020/02/25/use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/2020/02/25/use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/</guid>
      <description>


&lt;p&gt;With the release of version 3.3.0 of ggplot2 came the ability to have more control over the aesthetic evaluation.
This allows us to modify the colors of the mapped palettes with &lt;a href=&#34;https://github.com/EmilHvitfeldt/prismatic&#34;&gt;prismatic&lt;/a&gt; now easier than ever.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages üì¶&lt;/h2&gt;
&lt;p&gt;We load the essential packages to wrangle, collect data (we will use tweets), scrape websites and handle emojis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(prismatic)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Suppose you have a simple bar chart and you have added colors to each bar.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds, aes(cut)) +
  geom_bar(aes(fill = cut))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, suppose you would like to add a border around each bar.
Traditionally you could add a single color like black but it isn‚Äôt that satisfying as it doesn‚Äôt have any relation to the mapped colors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds, aes(cut)) +
  geom_bar(aes(fill = cut), color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;now that &lt;code&gt;after_scale()&lt;/code&gt; is available for us we can base the color based on the mapped fill colors.
Below I have used &lt;code&gt;clr_darken()&lt;/code&gt; to create a border that is just slightly darker than the fill color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds, aes(cut)) +
  geom_bar(aes(fill = cut, color = after_scale(clr_darken(fill, 0.3))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;this could also have been done in reverse by supplying the color and modifying the fill after.
Notice how we are able to chain multiple color modifications together.
Here we hare taking the color, then desaturating it followed by some lighting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds, aes(cut)) +
  geom_bar(aes(color = cut, 
               fill = after_scale(clr_lighten(clr_desaturate(color), 
                                              space = &amp;quot;combined&amp;quot;))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you only need to specify one color directly you can use the &lt;code&gt;stage()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds, aes(cut)) +
  geom_bar(aes(fill = stage(start = cut, 
                            after_scale = clr_lighten(fill, space = &amp;quot;combined&amp;quot;))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-06-use-prismatic-with-after_scale-for-finer-control-of-colors-in-ggplot2/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;details closed&gt;
&lt;p&gt;&lt;summary&gt; &lt;span title=&#34;Click to Expand&#34;&gt; current session info &lt;/span&gt; &lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 setting  value                       
 version  R version 3.6.0 (2019-04-26)
 os       macOS Mojave 10.14.6        
 system   x86_64, darwin15.6.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 ctype    en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2020-04-20                  

‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 package     * version    date       lib source                     
 assertthat    0.2.1      2019-03-21 [1] CRAN (R 3.6.0)             
 backports     1.1.6      2020-04-05 [1] CRAN (R 3.6.0)             
 blogdown      0.18       2020-03-04 [1] CRAN (R 3.6.0)             
 bookdown      0.18       2020-03-05 [1] CRAN (R 3.6.0)             
 cli           2.0.2      2020-02-28 [1] CRAN (R 3.6.0)             
 clipr         0.7.0      2019-07-23 [1] CRAN (R 3.6.0)             
 codetools     0.2-16     2018-12-24 [1] CRAN (R 3.6.0)             
 colorspace    1.4-1      2019-03-18 [1] CRAN (R 3.6.0)             
 crayon        1.3.4      2017-09-16 [1] CRAN (R 3.6.0)             
 desc          1.2.0      2018-05-01 [1] CRAN (R 3.6.0)             
 details     * 0.2.1      2020-01-12 [1] CRAN (R 3.6.0)             
 digest        0.6.25     2020-02-23 [1] CRAN (R 3.6.0)             
 dplyr         0.8.5      2020-03-07 [1] CRAN (R 3.6.0)             
 ellipsis      0.3.0      2019-09-20 [1] CRAN (R 3.6.0)             
 emo           0.0.0.9000 2019-12-18 [1] Github (hadley/emo@3f03b11)
 evaluate      0.14       2019-05-28 [1] CRAN (R 3.6.0)             
 fansi         0.4.1      2020-01-08 [1] CRAN (R 3.6.0)             
 farver        2.0.3      2020-01-16 [1] CRAN (R 3.6.0)             
 generics      0.0.2      2018-11-29 [1] CRAN (R 3.6.0)             
 ggplot2     * 3.3.0      2020-03-05 [1] CRAN (R 3.6.0)             
 glue          1.4.0      2020-04-03 [1] CRAN (R 3.6.0)             
 gtable        0.3.0      2019-03-25 [1] CRAN (R 3.6.0)             
 htmltools     0.4.0      2019-10-04 [1] CRAN (R 3.6.0)             
 httr          1.4.1      2019-08-05 [1] CRAN (R 3.6.0)             
 knitr       * 1.28       2020-02-06 [1] CRAN (R 3.6.0)             
 labeling      0.3        2014-08-23 [1] CRAN (R 3.6.0)             
 lifecycle     0.2.0      2020-03-06 [1] CRAN (R 3.6.0)             
 lubridate     1.7.8      2020-04-06 [1] CRAN (R 3.6.0)             
 magrittr      1.5        2014-11-22 [1] CRAN (R 3.6.0)             
 munsell       0.5.0      2018-06-12 [1] CRAN (R 3.6.0)             
 pillar        1.4.3      2019-12-20 [1] CRAN (R 3.6.0)             
 pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 3.6.0)             
 png           0.1-7      2013-12-03 [1] CRAN (R 3.6.0)             
 prismatic   * 0.2.0.9000 2020-03-15 [1] local                      
 purrr         0.3.3      2019-10-18 [1] CRAN (R 3.6.0)             
 R6            2.4.1      2019-11-12 [1] CRAN (R 3.6.0)             
 Rcpp          1.0.4.6    2020-04-09 [1] CRAN (R 3.6.0)             
 rlang         0.4.5      2020-03-01 [1] CRAN (R 3.6.0)             
 rmarkdown     2.1        2020-01-20 [1] CRAN (R 3.6.0)             
 rprojroot     1.3-2      2018-01-03 [1] CRAN (R 3.6.0)             
 rstudioapi    0.11       2020-02-07 [1] CRAN (R 3.6.0)             
 scales        1.1.0      2019-11-18 [1] CRAN (R 3.6.0)             
 sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 3.6.0)             
 stringi       1.4.6      2020-02-17 [1] CRAN (R 3.6.0)             
 stringr       1.4.0      2019-02-10 [1] CRAN (R 3.6.0)             
 tibble        3.0.0      2020-03-30 [1] CRAN (R 3.6.2)             
 tidyselect    1.0.0      2020-01-27 [1] CRAN (R 3.6.0)             
 vctrs         0.2.4      2020-03-10 [1] CRAN (R 3.6.0)             
 viridisLite   0.3.0      2018-02-01 [1] CRAN (R 3.6.0)             
 withr         2.1.2      2018-03-15 [1] CRAN (R 3.6.0)             
 xfun          0.13       2020-04-13 [1] CRAN (R 3.6.2)             
 xml2          1.3.0      2020-04-01 [1] CRAN (R 3.6.2)             
 yaml          2.2.1      2020-02-01 [1] CRAN (R 3.6.0)             

[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deploy your bookdown project to Netlify with Github Actions</title>
      <link>/2020/01/20/deploy-your-bookdown-project-to-netlify-with-github-actions/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/01/20/deploy-your-bookdown-project-to-netlify-with-github-actions/</guid>
      <description>


&lt;p&gt;With the new &lt;a href=&#34;https://github.com/features/actions&#34;&gt;Github Actions&lt;/a&gt; comes many possibilities.
Some new and some old.
One of the benefits is that you don‚Äôt have to use third-party applications to do continuous integration.&lt;/p&gt;
&lt;p&gt;This post will show you how you can set up a &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;bookdown&lt;/a&gt; site with &lt;a href=&#34;https://netlify.com/&#34;&gt;Netlify&lt;/a&gt; using Github Actions.
This was previously and still is possible to do with Travis-CI.&lt;/p&gt;
&lt;p&gt;This post wouldn‚Äôt have been possible without &lt;a href=&#34;https://twitter.com/jimhester_&#34;&gt;Jim Hester‚Äôs&lt;/a&gt; work on &lt;a href=&#34;https://github.com/r-lib/actions&#34;&gt;Github Actions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are transferring a book from Travis-CI build look at the notes at the end of this post.&lt;/p&gt;
&lt;div id=&#34;create-repository&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Repository&lt;/h2&gt;
&lt;p&gt;First, you need to create a bookdown repository.
For this, I suggest you follow the &lt;a href=&#34;https://bookdown.org/yihui/bookdown/get-started.html&#34;&gt;Getting Started&lt;/a&gt; chapter from the &lt;a href=&#34;https://bookdown.org/yihui/bookdown/&#34;&gt;Bookdown Book&lt;/a&gt; and download the GitHub repository &lt;a href=&#34;https://github.com/rstudio/bookdown-demo&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/bookdown-demo&lt;/a&gt; as a &lt;a href=&#34;https://github.com/rstudio/bookdown-demo/archive/master.zip&#34;&gt;Zip file&lt;/a&gt;, then unzip it locally.
I recommend that you change the name of the &lt;code&gt;.Rproj&lt;/code&gt; file so isn‚Äôt the default value.&lt;/p&gt;
&lt;p&gt;The next step isn‚Äôt necessary but is still highly recommended.
Go fill in the information in the &lt;code&gt;DESCRIPTION&lt;/code&gt; file.
Most importantly the &lt;code&gt;Package&lt;/code&gt; and &lt;code&gt;Title&lt;/code&gt; fields.
The &lt;code&gt;Package&lt;/code&gt; field will be used as the name of the repository and the &lt;code&gt;Title&lt;/code&gt; field will be the description of the repository once it hits Github.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connect-to-github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connect to Github&lt;/h2&gt;
&lt;p&gt;Now we want to connect our repository to Github.
For this, I will use the &lt;a href=&#34;https://usethis.r-lib.org/&#34;&gt;usethis&lt;/a&gt; package which is wonderful for things like this.
If you haven‚Äôt used &lt;strong&gt;usethis&lt;/strong&gt; before please go do the &lt;a href=&#34;https://usethis.r-lib.org/articles/articles/usethis-setup.html&#34;&gt;usethis setup&lt;/a&gt; before moving forward.&lt;/p&gt;
&lt;p&gt;Simply add Git to the repository by running &lt;code&gt;usethis::use_git()&lt;/code&gt; and connect it to Github with &lt;code&gt;usethis::use_github()&lt;/code&gt;.
This should open up a webpage with the newly linked repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-netlify-account&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Netlify account&lt;/h2&gt;
&lt;p&gt;If you haven‚Äôt already got a Netlify account, go to &lt;a href=&#34;https://www.netlify.com/&#34;&gt;netlify.com/&lt;/a&gt; to create one for free.
I have it set up with Github for easier interaction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-netlify-site&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Netlify site&lt;/h2&gt;
&lt;p&gt;Once you have logged into Netlify go to your team page and to create a ‚ÄúNew site from Git‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;netlify-teams.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Select Github for Continuous Deployment&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-new-site.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we need to select the GitHub repository.
Depending on how many repositories you have you can find it in the list or search for it with the search bar.
Once you have found it click the little arrow to the right of it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pick-a-repository.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Don‚Äôt touch any of the settings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;deploy-settings.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila!
Here is your new site, it is currently empty.
Now click on the ‚ÄúSite settings‚Äù button&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;new-site.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;copy the API ID and save it, you will need it in a little bit.
If you lose it you can always come back here and copy it again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;site-id.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You might have noticed that the website is completely random.
If you click on the ‚ÄúChange site name‚Äù button you can set a new prefix name.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;set-name.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Scroll down to get the Status Badge, you can copy this too and put it in the top of your README.md file if you want.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;status-badge.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-a-netlify-personal-access-token&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get a Netlify personal access token&lt;/h2&gt;
&lt;p&gt;Scroll all the way up and click on your icon in the top right corner.
Then go to ‚ÄúUser settings‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;icon.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;go to ‚ÄúApplications‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;user-settings.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And click on ‚ÄúNew access token‚Äù to create a personal access token&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;personal-access-token.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The description of your token isn‚Äôt important but try to make it related to your book so you remember.
Click ‚ÄúGenerate token‚Äù when you are done.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;generate-token.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is your authentication token.
Copy it and don‚Äôt lose it!
Once you leave this site you can get it back.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;created-token.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;store-your-secrets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Store Your Secrets&lt;/h2&gt;
&lt;p&gt;Now that you have the API ID and personal access token go back to your Github repository and go to ‚ÄúSettings‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;github-settings.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;go to ‚ÄúSecrets‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;github-secrets.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Click on ‚ÄúAdd a new secret‚Äù&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;add-new-secret.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You need to do this twice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one named ‚ÄúNETLIFY_AUTH_TOKEN‚Äù where you put the personal access token as the value and,&lt;/li&gt;
&lt;li&gt;one named ‚ÄúNETLIFY_SITE_ID‚Äù where you put the API ID as the value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;fill-in-secret.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-github-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Github workflow&lt;/h2&gt;
&lt;p&gt;Now add the GitHub workflow file.&lt;/p&gt;
&lt;p&gt;For this you will need version &lt;code&gt;1.5.1.9000&lt;/code&gt; or higher of &lt;strong&gt;usethis&lt;/strong&gt; for it to work.
You can get the newest version of usethis from github with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;r-lib/usethis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then you run &lt;code&gt;use_github_action(&#34;bookdown.yaml&#34;)&lt;/code&gt; which will create the .yaml file in the right directory for you.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-renvsnapshot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Run renv::snapshot&lt;/h2&gt;
&lt;p&gt;Install the &lt;a href=&#34;https://github.com/rstudio/renv&#34;&gt;renv&lt;/a&gt; package and run &lt;code&gt;renv::snapshot()&lt;/code&gt;.
This will ensure the package versions remain consistent across builds.&lt;/p&gt;
&lt;p&gt;Once you need more packages, add them to the description like you normally would with an R package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;push-changes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Push changes&lt;/h2&gt;
&lt;p&gt;And that is everything you need to do, just commit the workflow file and the &lt;strong&gt;renv&lt;/strong&gt; files you created and the website should build for you.&lt;/p&gt;
&lt;p&gt;My example can be found &lt;a href=&#34;https://bookdown-github-actions-netlify.netlify.com/&#34;&gt;here&lt;/a&gt; with the &lt;a href=&#34;https://github.com/EmilHvitfeldt/bookdown-github-actions-netlify&#34;&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;the line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;netlify deploy --prod --dir _book&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in the workflow file it the one that deploys the built book to Netlify.
It defaults to the &lt;code&gt;_book&lt;/code&gt; folder.
In the &lt;code&gt;_bookdown.yml&lt;/code&gt; file you can change the output folder.
So if you have set it to &lt;code&gt;output_dir: &#34;docs&#34;&lt;/code&gt; then you need to change the deploy option to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;netlify deploy --prod --dir docs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real Emojis in ggplot2</title>
      <link>/2020/01/02/real-emojis-in-ggplot2/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/01/02/real-emojis-in-ggplot2/</guid>
      <description>


&lt;p&gt;I have been trying to use &lt;a href=&#34;https://en.wikipedia.org/wiki/Emoji&#34;&gt;Emojis&lt;/a&gt; for a long time.
It was actually part of my very first &lt;a href=&#34;https://www.hvitfeldt.me/blog/2017-world-press-freedom-index-with-emojis/&#34;&gt;post&lt;/a&gt; on this blog.
Others have made progress such as with &lt;a href=&#34;https://cran.r-project.org/web/packages/emojifont/vignettes/emojifont.html&#34;&gt;emojifont&lt;/a&gt;, but it is not using the classical &lt;a href=&#34;https://en.wikipedia.org/wiki/Apple_Color_Emoji&#34;&gt;Apple Color Emoji&lt;/a&gt; font which is the most commonly recognized.
I made a breakthrough when I was writing the &lt;a href=&#34;https://www.hvitfeldt.me/packagecalendar/2019/&#34;&gt;packagecalander&lt;/a&gt; entry on &lt;a href=&#34;https://github.com/clauswilke/ggtext&#34;&gt;ggtext&lt;/a&gt;.
While the method is the best I have found it does have some cons.&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actually works&lt;/li&gt;
&lt;li&gt;Doesn‚Äôt require the use of SVG&lt;/li&gt;
&lt;li&gt;Previews nicely&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relies on experimental package &lt;strong&gt;ggtext&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Needs web scraping&lt;/li&gt;
&lt;li&gt;Required access to the internet to render&lt;/li&gt;
&lt;li&gt;Size can‚Äôt be adjusted using the size aesthetic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in all, it is a fair trade for my needs.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages üì¶&lt;/h2&gt;
&lt;p&gt;We load the essential packages to wrangle, collect data (we will use tweets), scrape websites and handle emojis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtweet)
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(&amp;quot;clauswilke/ggtext&amp;quot;)
library(ggtext)
library(emo)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the tweets üê¶&lt;/h2&gt;
&lt;p&gt;For a simple dataset where we find emojis I‚Äôm going to get some tweets with the word ‚Äúhappy‚Äù.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happy &amp;lt;- search_tweets(&amp;quot;happy&amp;quot;, include_rts = FALSE, n = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can use the &lt;code&gt;ji_extract_all()&lt;/code&gt; function from the &lt;a href=&#34;https://github.com/hadley/emo&#34;&gt;emo&lt;/a&gt; package.
This will give us a list of emojis so we can use the &lt;code&gt;unnest()&lt;/code&gt; function to get back to a tidy format.
I‚Äôm going to do a simple &lt;code&gt;count()&lt;/code&gt; of the emojis for the following visualizations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happy_emojis &amp;lt;- happy %&amp;gt;%
  mutate(emoji = emo::ji_extract_all(text)) %&amp;gt;%
  unnest(cols = c(emoji)) %&amp;gt;%
  count(emoji, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is where the magic happens.
We don‚Äôt have a way to displays emojis in &lt;strong&gt;ggplot2&lt;/strong&gt;, but we can use &lt;strong&gt;ggtext&lt;/strong&gt; to embed images into the text using HTML.
Now we just need to get an image of each emoji.
The following function will accept an emoji as a string and return the URL to a .png of that emoji.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emoji_to_link &amp;lt;- function(x) {
  paste0(&amp;quot;https://emojipedia.org/emoji/&amp;quot;,x) %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;tr td a&amp;quot;) %&amp;gt;%
    .[1] %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    paste0(&amp;quot;https://emojipedia.org/&amp;quot;, .) %&amp;gt;%
    read_html() %&amp;gt;%
    html_node(&amp;#39;div[class=&amp;quot;vendor-image&amp;quot;] img&amp;#39;) %&amp;gt;%
    html_attr(&amp;quot;src&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then this function will take that URL and construct the necessary HTML code to show the emoji PNGs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;link_to_img &amp;lt;- function(x, size = 25) {
  paste0(&amp;quot;&amp;lt;img src=&amp;#39;&amp;quot;, x, &amp;quot;&amp;#39; width=&amp;#39;&amp;quot;, size, &amp;quot;&amp;#39;/&amp;gt;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To be courteous we are only going to scrape the emojis we are actually going to use.
So we will &lt;code&gt;slice()&lt;/code&gt; the 10 most frequent emojis.
We will also be adding a 5 second delay using &lt;code&gt;slowly()&lt;/code&gt; and &lt;code&gt;rate_delay()&lt;/code&gt; from &lt;strong&gt;purrr&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_happy &amp;lt;- happy_emojis %&amp;gt;%
  slice(1:10) %&amp;gt;%
  mutate(url = map_chr(emoji, slowly(~emoji_to_link(.x), rate_delay(1))),
         label = link_to_img(url))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;emoji-scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;emoji-scatter plot üìà&lt;/h2&gt;
&lt;p&gt;Now we can use the &lt;code&gt;geom_richtext()&lt;/code&gt; function from &lt;strong&gt;ggtext&lt;/strong&gt; to create a emoji scatter chart.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_happy %&amp;gt;%
  ggplot(aes(emoji, n, label = label)) +
  geom_richtext(aes(y = n), fill = NA, label.color = NA, # remove background and outline
                label.padding = grid::unit(rep(0, 4), &amp;quot;pt&amp;quot;) # remove padding
  ) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-02-real-emojis-in-ggplot2/index_files/figure-html/plot1-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a little off, so lets other these by counts and put them over a bar chart.
I‚Äôm also going to the x-axis ticks and text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;offset &amp;lt;- max(top_happy$n) / 20

top_happy %&amp;gt;%
  ggplot(aes(fct_reorder(emoji, n, .desc = TRUE), n, label = label)) +
  geom_col() +
  geom_richtext(aes(y = n + offset), fill = NA, label.color = NA,
                label.padding = grid::unit(rep(0, 4), &amp;quot;pt&amp;quot;)
  ) +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  labs(x = NULL) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-02-real-emojis-in-ggplot2/index_files/figure-html/plot2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;emojis-in-labels-and-text&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Emojis in labels and text üìä&lt;/h2&gt;
&lt;p&gt;We are not only limited to using emojis in the geoms.
We can set the text element using emojis to &lt;code&gt;element_markdown()&lt;/code&gt;.
Below we have the same bar chart as above but with the emoji as labels below instead of on top.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_happy %&amp;gt;%
  ggplot(aes(fct_reorder(label, n, .desc = TRUE), n)) +
  geom_col() +
  theme_minimal() +
  theme(axis.text.x = element_markdown()) +
  labs(x = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-02-real-emojis-in-ggplot2/index_files/figure-html/plot3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-splash-of-color&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding a splash of color üåà&lt;/h2&gt;
&lt;p&gt;We can employ a little more scraping and color calculations to had colors to the bars according to the colors of the emoji.
The following function takes a URL to a .png file and returns the most common color that isn‚Äôt pure black or pure white.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_emoji_color &amp;lt;- function(x) {
  data &amp;lt;- png::readPNG(RCurl::getURLContent(x))
  color_freq &amp;lt;- names(sort(table(rgb(data[,,1], data[,,2], data[,,3])), 
                           decreasing = TRUE))
  setdiff(color_freq, c(&amp;quot;#FFFFFF&amp;quot;, &amp;quot;#000000&amp;quot;))[1]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We apply this to all the emoji URLs and color the bars accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data &amp;lt;- top_happy %&amp;gt;%
  mutate(color = map_chr(url, slowly(~mean_emoji_color(.x), rate_delay(1))))

plot_data %&amp;gt;%
  ggplot(aes(fct_reorder(label, n, .desc = TRUE), 
             color = color, 
             fill = unclass(prismatic::clr_lighten(color, 0.4)), n)) +
  geom_col() +
  scale_fill_identity() +
  scale_color_identity() +
  theme_minimal() +
  theme(axis.text.x = element_markdown()) +
  labs(x = NULL, y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Emojis used in (small sample) of &amp;#39;happy&amp;#39; tweets&amp;quot;,
       subtitle = &amp;quot;Displayed in ggplot2!!!&amp;quot;,
       caption = &amp;quot;@Emil_Hvitfeldt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-02-real-emojis-in-ggplot2/index_files/figure-html/plotdata-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-note&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final note üóí&lt;/h2&gt;
&lt;p&gt;If you want to use emojis in the text you need to call &lt;code&gt;theme_*()&lt;/code&gt; before &lt;code&gt;theme()&lt;/code&gt; such that &lt;code&gt;element_markdown()&lt;/code&gt; isn‚Äôt being overwritten.&lt;/p&gt;
&lt;details closed&gt;
&lt;p&gt;&lt;summary&gt; &lt;span title=&#34;Click to Expand&#34;&gt; current session info &lt;/span&gt; &lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 setting  value                       
 version  R version 3.6.0 (2019-04-26)
 os       macOS Mojave 10.14.6        
 system   x86_64, darwin15.6.0        
 ui       X11                         
 language (EN)                        
 collate  en_US.UTF-8                 
 ctype    en_US.UTF-8                 
 tz       America/Los_Angeles         
 date     2020-04-21                  

‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 package     * version    date       lib source                            
 askpass       1.1        2019-01-13 [1] CRAN (R 3.6.0)                    
 assertthat    0.2.1      2019-03-21 [1] CRAN (R 3.6.0)                    
 backports     1.1.6      2020-04-05 [1] CRAN (R 3.6.0)                    
 bitops        1.0-6      2013-08-17 [1] CRAN (R 3.6.0)                    
 blogdown      0.18       2020-03-04 [1] CRAN (R 3.6.0)                    
 bookdown      0.18       2020-03-05 [1] CRAN (R 3.6.0)                    
 broom         0.5.5      2020-02-29 [1] CRAN (R 3.6.0)                    
 cellranger    1.1.0      2016-07-27 [1] CRAN (R 3.6.0)                    
 cli           2.0.2      2020-02-28 [1] CRAN (R 3.6.0)                    
 clipr         0.7.0      2019-07-23 [1] CRAN (R 3.6.0)                    
 codetools     0.2-16     2018-12-24 [1] CRAN (R 3.6.0)                    
 colorspace    1.4-1      2019-03-18 [1] CRAN (R 3.6.0)                    
 crayon        1.3.4      2017-09-16 [1] CRAN (R 3.6.0)                    
 curl          4.3        2019-12-02 [1] CRAN (R 3.6.0)                    
 DBI           1.1.0      2019-12-15 [1] CRAN (R 3.6.0)                    
 dbplyr        1.4.2      2019-06-17 [1] CRAN (R 3.6.0)                    
 desc          1.2.0      2018-05-01 [1] CRAN (R 3.6.0)                    
 details     * 0.2.1      2020-01-12 [1] CRAN (R 3.6.0)                    
 digest        0.6.25     2020-02-23 [1] CRAN (R 3.6.0)                    
 dplyr       * 0.8.5      2020-03-07 [1] CRAN (R 3.6.0)                    
 ellipsis      0.3.0      2019-09-20 [1] CRAN (R 3.6.0)                    
 emo         * 0.0.0.9000 2019-12-18 [1] Github (hadley/emo@3f03b11)       
 evaluate      0.14       2019-05-28 [1] CRAN (R 3.6.0)                    
 fansi         0.4.1      2020-01-08 [1] CRAN (R 3.6.0)                    
 farver        2.0.3      2020-01-16 [1] CRAN (R 3.6.0)                    
 forcats     * 0.5.0      2020-03-01 [1] CRAN (R 3.6.0)                    
 fs            1.4.1      2020-04-04 [1] CRAN (R 3.6.0)                    
 generics      0.0.2      2018-11-29 [1] CRAN (R 3.6.0)                    
 ggplot2     * 3.3.0      2020-03-05 [1] CRAN (R 3.6.0)                    
 ggtext      * 0.1.0      2019-12-13 [1] Github (clauswilke/ggtext@cc8ea0c)
 glue          1.4.0      2020-04-03 [1] CRAN (R 3.6.0)                    
 gridtext      0.1.1      2020-02-24 [1] CRAN (R 3.6.0)                    
 gtable        0.3.0      2019-03-25 [1] CRAN (R 3.6.0)                    
 haven         2.2.0      2019-11-08 [1] CRAN (R 3.6.0)                    
 hms           0.5.3      2020-01-08 [1] CRAN (R 3.6.0)                    
 htmltools     0.4.0      2019-10-04 [1] CRAN (R 3.6.0)                    
 httr          1.4.1      2019-08-05 [1] CRAN (R 3.6.0)                    
 jsonlite      1.6.1      2020-02-02 [1] CRAN (R 3.6.0)                    
 knitr       * 1.28       2020-02-06 [1] CRAN (R 3.6.0)                    
 labeling      0.3        2014-08-23 [1] CRAN (R 3.6.0)                    
 lattice       0.20-41    2020-04-02 [1] CRAN (R 3.6.0)                    
 lifecycle     0.2.0      2020-03-06 [1] CRAN (R 3.6.0)                    
 lubridate     1.7.8      2020-04-06 [1] CRAN (R 3.6.0)                    
 magrittr      1.5        2014-11-22 [1] CRAN (R 3.6.0)                    
 markdown      1.1        2019-08-07 [1] CRAN (R 3.6.0)                    
 modelr        0.1.6      2020-02-22 [1] CRAN (R 3.6.0)                    
 munsell       0.5.0      2018-06-12 [1] CRAN (R 3.6.0)                    
 nlme          3.1-145    2020-03-04 [1] CRAN (R 3.6.0)                    
 openssl       1.4.1      2019-07-18 [1] CRAN (R 3.6.0)                    
 pillar        1.4.3      2019-12-20 [1] CRAN (R 3.6.0)                    
 pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 3.6.0)                    
 png           0.1-7      2013-12-03 [1] CRAN (R 3.6.0)                    
 prettyunits   1.1.1      2020-01-24 [1] CRAN (R 3.6.0)                    
 prismatic     0.2.0.9000 2020-03-15 [1] local                             
 progress      1.2.2      2019-05-16 [1] CRAN (R 3.6.0)                    
 purrr       * 0.3.3      2019-10-18 [1] CRAN (R 3.6.0)                    
 R6            2.4.1      2019-11-12 [1] CRAN (R 3.6.0)                    
 Rcpp          1.0.4.6    2020-04-09 [1] CRAN (R 3.6.0)                    
 RCurl         1.98-1.1   2020-01-19 [1] CRAN (R 3.6.0)                    
 readr       * 1.3.1      2018-12-21 [1] CRAN (R 3.6.0)                    
 readxl        1.3.1      2019-03-13 [1] CRAN (R 3.6.0)                    
 reprex        0.3.0      2019-05-16 [1] CRAN (R 3.6.0)                    
 rlang         0.4.5      2020-03-01 [1] CRAN (R 3.6.0)                    
 rmarkdown     2.1        2020-01-20 [1] CRAN (R 3.6.0)                    
 rprojroot     1.3-2      2018-01-03 [1] CRAN (R 3.6.0)                    
 rstudioapi    0.11       2020-02-07 [1] CRAN (R 3.6.0)                    
 rtweet      * 0.7.0      2020-01-08 [1] CRAN (R 3.6.0)                    
 rvest       * 0.3.5      2019-11-08 [1] CRAN (R 3.6.0)                    
 scales        1.1.0      2019-11-18 [1] CRAN (R 3.6.0)                    
 selectr       0.4-2      2019-11-20 [1] CRAN (R 3.6.0)                    
 sessioninfo   1.1.1      2018-11-05 [1] CRAN (R 3.6.0)                    
 stringi       1.4.6      2020-02-17 [1] CRAN (R 3.6.0)                    
 stringr     * 1.4.0      2019-02-10 [1] CRAN (R 3.6.0)                    
 tibble      * 3.0.1      2020-04-20 [1] CRAN (R 3.6.2)                    
 tidyr       * 1.0.2      2020-01-24 [1] CRAN (R 3.6.0)                    
 tidyselect    1.0.0      2020-01-27 [1] CRAN (R 3.6.0)                    
 tidyverse   * 1.3.0      2019-11-21 [1] CRAN (R 3.6.0)                    
 vctrs         0.2.4      2020-03-10 [1] CRAN (R 3.6.0)                    
 withr         2.1.2      2018-03-15 [1] CRAN (R 3.6.0)                    
 xfun          0.13       2020-04-13 [1] CRAN (R 3.6.2)                    
 xml2        * 1.3.0      2020-04-01 [1] CRAN (R 3.6.2)                    
 yaml          2.2.1      2020-02-01 [1] CRAN (R 3.6.0)                    

[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Paletteer version 1.0.0</title>
      <link>/2019/12/18/paletteer-version-1.0.0/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/2019/12/18/paletteer-version-1.0.0/</guid>
      <description>


&lt;p&gt;I‚Äôm over-the-moon excited to announce the release of version 1.0.0 of &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer&#34;&gt;paletteer&lt;/a&gt;.
This version comes with breaking changes and major quality of life improvements.
I will unironically name this the ‚Äúfirst useable version‚Äù for reasons that will be obvious later in this post.&lt;/p&gt;
&lt;div id=&#34;breaking-changes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breaking Changes üí•&lt;/h2&gt;
&lt;p&gt;There has been a significant change in syntax for this version.
For versions &amp;lt;= 0.2.1 the way to specify a palette was done using the arguments &lt;code&gt;package&lt;/code&gt; and &lt;code&gt;palette&lt;/code&gt;.
Both could be taken as both string or unquoted strings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# versions &amp;lt;= 0.2.1
paletteer_c(&amp;quot;gameofthrones&amp;quot;, &amp;quot;baratheon&amp;quot;, 10)

paletteer_d(nord, halifax_harbor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While convinient and cool to use &lt;a href=&#34;https://edwinth.github.io/blog/nse/&#34;&gt;NSE&lt;/a&gt;,
tt was not very useful and I had &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer/issues/17&#34;&gt;several&lt;/a&gt; &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer/issues/13&#34;&gt;people&lt;/a&gt; &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer/issues/53&#34;&gt;complaining&lt;/a&gt;.
I realized that using NSE wasn‚Äôt a good fit at all for this package.
This means that from version 1.0.0 and moving forward only strings will be used to specify palettes.&lt;/p&gt;
&lt;p&gt;Secondly, I have eliminated the &lt;code&gt;package&lt;/code&gt; argument and from now on all specification is done on the form &lt;code&gt;package::palette&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# versions &amp;gt;= 1.0.0
paletteer_c(&amp;quot;gameofthrones::baratheon&amp;quot;, 10)

paletteer_d(&amp;quot;nord::halifax_harbor&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above change is the most likely to break your earlier code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;autocomplete&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Autocomplete üéâ&lt;/h2&gt;
&lt;p&gt;The biggest downside to the original version of &lt;strong&gt;paletteer&lt;/strong&gt; and later version was the lack of discoverability.
Unless you knew the palette you wanted and the EXACT spelling you couldn‚Äôt really use &lt;strong&gt;paletteer&lt;/strong&gt;.
Sure you could browse &lt;code&gt;palettes_c_names&lt;/code&gt; and &lt;code&gt;palettes_d_names&lt;/code&gt; like some caveman,
but to be honest the package felt more like a novelty project than a useful tool.&lt;/p&gt;
&lt;p&gt;All of this changes with version 1.0.0 üéâ!
Simply starting by typing &lt;code&gt;paletteer_d()&lt;/code&gt; or any of the other related functions and simply hit tab.
This will prompt all the names of available palettes which you then can search through using fuzzy search.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;complete1.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This change is the single biggest improvement to this package.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discoverability ‚úÖ&lt;/li&gt;
&lt;li&gt;No more missspellings ‚úÖ&lt;/li&gt;
&lt;li&gt;Total awesomeness ‚úÖ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And yes, it also work with the &lt;code&gt;scale_*_paletteer()&lt;/code&gt; functions üôå&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;complete2.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prismatic-integration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prismatic integration üíé&lt;/h2&gt;
&lt;p&gt;You can see from the first gif that the output is a little more colorful then what you are used to.
This all comes from the &lt;a href=&#34;https://github.com/EmilHvitfeldt/prismatic&#34;&gt;prismatic&lt;/a&gt; package I released earlier this year.
The &lt;strong&gt;prismatic&lt;/strong&gt; colors objects that are returned from all &lt;strong&gt;paletteer&lt;/strong&gt; functions will be printed with colorful backgrounds provided that the &lt;a href=&#34;https://github.com/r-lib/crayon&#34;&gt;crayon&lt;/a&gt; package is available, otherwise, it will just print normally.
This is great for when you want to take a quick look at the colors you are about to use.
Please note that the background can only take &lt;a href=&#34;https://github.com/r-lib/crayon#256-colors&#34;&gt;256&lt;/a&gt; different colors.
Some palettes will fit nicely inside these 256 values and will display nicely (viridis::magma) below, while other palettes with a lot of value will show weird jumps in colors (gameofthrones::greyjoy)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;prismatic1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want more accurate color depictions you can simply &lt;code&gt;plot()&lt;/code&gt; the output to see the real colors&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(paletteer_c(&amp;quot;viridis::magma&amp;quot;, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-19-paletteer-version-1-0-0/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(paletteer_c(&amp;quot;gameofthrones::greyjoy&amp;quot;, 100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-12-19-paletteer-version-1-0-0/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-color-palettes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More color palettes üåà&lt;/h2&gt;
&lt;p&gt;It wouldn‚Äôt be a &lt;strong&gt;paletteer&lt;/strong&gt; release without more palettes.
And this release is no different!
This update brings us 654 new palettes!!! from 19 different packages bringing out total up to 1759.
I did a little live-tweeting while implementing these packages so you can take a look at the newly included palettes here:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
I&#39;ll be adding a whole bunch of new palettes to {paletteer} tonight! üåà&lt;br&gt;&lt;br&gt;Read this thread if you want to see the new colorful goodies coming your way!&lt;br&gt;&lt;br&gt;‚ù§Ô∏èüíôüíöüß°üíõüíú&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/c0qK27nc4N&#34;&gt;pic.twitter.com/c0qK27nc4N&lt;/a&gt;
&lt;/p&gt;
‚Äî Emil Hvitfeldt (&lt;span class=&#34;citation&#34;&gt;@Emil_Hvitfeldt&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Emil_Hvitfeldt/status/1203508809269800962?ref_src=twsrc%5Etfw&#34;&gt;December 8, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;That is all I have for you this time around if you create or find more palette packages please go over and file &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer/issues&#34;&gt;an issue&lt;/a&gt; so they can be included as well.
Thank you!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Refactoring Tests</title>
      <link>/2019/11/25/refactoring-tests/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/2019/11/25/refactoring-tests/</guid>
      <description>


&lt;p&gt;We all know the saying&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you‚Äôve written the same code 3 times, write a function&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, I would like to expend that to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you‚Äôre written the same test 3 times, write a test function&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;During my lasting packages such as &lt;a href=&#34;https://github.com/EmilHvitfeldt/prismatic&#34;&gt;prismatic&lt;/a&gt;,
I found myself copy-pasting tests around whenever I needed to test a new function.
I realized that the refactoring practices I try to apply in my general code writing,
wasn‚Äôt carried over to the way I was writing my tests.
I would frequency copy-paste hundreds of lines of tests only to replace the function name.
In this post will I go over a refactoring scenario I am working on at the moment.&lt;/p&gt;
&lt;div id=&#34;the-copy-pasted-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The copy-pasted test&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;prismatic&lt;/strong&gt; package includes almost a dozen different functions that work mostly the same way.
They all take the same type of arguments, return the returns in the same fashion and so on.
This leads me to have a great overlap between what tests I‚Äôm performing for each function.&lt;/p&gt;
&lt;p&gt;Taking a look at the following code chuck we see a test that makes sure that the function &lt;code&gt;clr_alpha()&lt;/code&gt; complain when given the wrong type of the first argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;quot;complain when `col` type is wrong.&amp;quot;, {
  expect_error(clr_alpha(&amp;quot;not a color&amp;quot;))

  expect_error(clr_alpha(list(pal = &amp;quot;#000000&amp;quot;)))

  expect_error(clr_alpha(character()))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When looking at the same test for &lt;code&gt;clr_mix()&lt;/code&gt; we see that it is a carbon copy except for the function name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;quot;it complains when col type is wrong.&amp;quot;, {
  expect_error(clr_mix(&amp;quot;not a color&amp;quot;))

  expect_error(clr_mix(list(pal = &amp;quot;#000000&amp;quot;)))

  expect_error(clr_mix(character()))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I‚Äôm going to propose 2 different styles of refactoring,
with the main difference being how RStudio returns the error when tests are not met.&lt;/p&gt;
&lt;div id=&#34;fix-1---plays-well-with-error-messages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fix #1 - Plays well with error messages&lt;/h2&gt;
&lt;p&gt;The first solution is to wrap the inside of your test into a function.
The above test would create the refactored testing-function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_wrong_input &amp;lt;- function(clr_) {
  expect_error(clr_(&amp;quot;not a color&amp;quot;))

  expect_error(clr_(list(pal = &amp;quot;#000000&amp;quot;)))

  expect_error(clr_(character()))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the test would be changed to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;quot;it complains when col type is wrong.&amp;quot;, {
  test_wrong_input(clr_alpha)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this change will perform the tests,
and adding tests for the new function would only need 1 change in the test instead of 3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;quot;it complains when col type is wrong.&amp;quot;, {
  test_wrong_input(clr_mix)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More importantly, let‚Äôs imagine that we want to extend the types of wrong inputs we what to screen.
Now we simply just need to add it once and it propagates to all the functions.&lt;/p&gt;
&lt;p&gt;The main benefit of this refactoring style is that when an error appears,
It will denote the line where the test broke.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;style1-error.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fix-2---less-typing-worse-error-message&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fix #2 - Less typing, worse error message&lt;/h2&gt;
&lt;p&gt;The second solution is to wrap the entire testing statement inside a function.
For this example, the function would look like this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_wrong_input &amp;lt;- function(clr_) {
  test_that(&amp;quot;it complains when col type is wrong.&amp;quot;, {
  expect_error(clr_(&amp;quot;not a color&amp;quot;))

  expect_error(clr_(list(pal = &amp;quot;#000000&amp;quot;)))

  expect_error(clr_(character()))
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the testing would look like&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_wrong_input(clr_mix)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reduces the number of lines needed for each test from 3 down to 1.
However, it comes with a downside.
When an error appears &lt;strong&gt;testthat&lt;/strong&gt; will give the location of the definition of the test-function,
not the location from where it was called.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;style2-error.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
We can still see that the error happens inside the ‚Äúalpha‚Äù Context,
but it is slightly harder to track down.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fix-2.1---ugly-hack-to-give-me-the-location&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fix #2.1 - ugly hack to give me the location&lt;/h2&gt;
&lt;p&gt;The second solution can be made slightly better by making the description of the test more informative.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_wrong_input &amp;lt;- function(clr_) {
  test_that(paste0(&amp;quot;test_wrong_input: &amp;quot;,
                   deparse(substitute(clr_)),
                   &amp;quot;() complains when col type is wrong.&amp;quot;), {
  expect_error(clr_(&amp;quot;not a color&amp;quot;))

  expect_error(clr_(list(pal = &amp;quot;#000000&amp;quot;)))

  expect_error(clr_(&amp;quot;pink&amp;quot;))
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;style2.5-error.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It takes more work upfront when writing the test functions.
But it gives a compromise between the brevity of test files and the clarity of the debugging page.&lt;/p&gt;
&lt;p&gt;Thanks for reading along! I hope you found it useful!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Manipulating colors with {prismatic}</title>
      <link>/2019/10/01/manipulating-colors-with-prismatic/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/2019/10/01/manipulating-colors-with-prismatic/</guid>
      <description>


&lt;p&gt;I‚Äôm happy to announce my newest package &lt;a href=&#34;https://github.com/EmilHvitfeldt/prismatic&#34;&gt;prismatic&lt;/a&gt; which facilitates simple manipulations of colors. I had been working on this package online and offline for some time, but the &lt;a href=&#34;https://github.com/tidyverse/ggplot2/pull/3534&#34;&gt;promise of easy manipulation of mapped data in ggplot2&lt;/a&gt; forced me to get some work done to get this package out before ggplot2 version 3.3.0. (as of time of writing.)&lt;/p&gt;
&lt;p&gt;This post will go over some of the finer details with lots of pretty pictures!&lt;/p&gt;
&lt;div id=&#34;loading-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading Packages&lt;/h2&gt;
&lt;p&gt;The prismatic package is fairly low dependency with only 1 import being &lt;a href=&#34;https://github.com/thomasp85/farver&#34;&gt;farver&lt;/a&gt; for lightning fast conversion between color spaces. I have also loaded the &lt;a href=&#34;http://colorspace.r-forge.r-project.org/&#34;&gt;colorspace&lt;/a&gt; package, from which some of the following functions have been inspired. I will use colorspace to enable plotting of multiple color palettes side by side, but I will not showcase the code each time. Go to the end of the post for example code for comparison plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(prismatic)
library(colorspace) # for plotting functions
library(magrittr) # for the glorious pipe&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;let-me-see-the-colors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let me see the colors!!&lt;/h2&gt;
&lt;p&gt;If you have seen my work, you will properly know that I &lt;a href=&#34;https://github.com/EmilHvitfeldt/r-color-palettes&#34;&gt;like&lt;/a&gt; &lt;a href=&#34;https://github.com/EmilHvitfeldt/paletteer&#34;&gt;colors&lt;/a&gt; &lt;a href=&#34;https://github.com/EmilHvitfeldt/quickpalette&#34;&gt;alot&lt;/a&gt;! But being also to quickly inspect some colors have always been a little too much work. Now all you have to do it pass your colors to &lt;code&gt;color()&lt;/code&gt; (or &lt;code&gt;colour()&lt;/code&gt; for our friends across the pond) to get a &lt;color&gt; object which has a nice &lt;code&gt;plot()&lt;/code&gt; method&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rainbow(10) %&amp;gt;% color() %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hcl.colors(25) %&amp;gt;% color() %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scico::scico(256, palette = &amp;quot;buda&amp;quot;) %&amp;gt;% color() %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which I would like to think is one of the main features of the package. If you happens to have &lt;a href=&#34;https://github.com/r-lib/crayon&#34;&gt;crayon&lt;/a&gt; available you will see a approximation of the colors with a filled in background (this limited to 256 colors so you milage might very, when in doubt use &lt;code&gt;plot()&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;print.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is the extent of what the color object can do.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;manipulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manipulations&lt;/h2&gt;
&lt;p&gt;The second star of the package is the &lt;a href=&#34;https://emilhvitfeldt.github.io/prismatic/reference/index.html&#34;&gt;collection of functions&lt;/a&gt; to manipulate the colors. All these functions have a couple of things in common.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They all start with &lt;code&gt;clr_&lt;/code&gt; for easy auto completion in your favorite IDE.&lt;/li&gt;
&lt;li&gt;They all take a vector of colors as the first argument and results a colors object of the same length.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;these two facts make the function super pipe friendly.&lt;/p&gt;
&lt;div id=&#34;saturation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saturation&lt;/h3&gt;
&lt;p&gt;The two functions &lt;code&gt;clr_saturate()&lt;/code&gt; and &lt;code&gt;clr_desaturate()&lt;/code&gt; both modifies the &lt;a href=&#34;https://en.wikipedia.org/wiki/HSL_and_HSV#Saturation&#34;&gt;saturation&lt;/a&gt; of a color. It takes a single additional argument to specifying the degree of which the (de)saturation should occur. These values should be between 0(nothing happens) and 1(full on power!).&lt;/p&gt;
&lt;p&gt;notice how you don‚Äôt have to call &lt;code&gt;color()&lt;/code&gt; on the output of &lt;code&gt;clr_desaturate()&lt;/code&gt; as it already returns a colors object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hcl.colors(10, &amp;quot;plasma&amp;quot;) %&amp;gt;%
  clr_desaturate(0.8) %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Example done with Mango palette from &lt;a href=&#34;https://github.com/johannesbjork/LaCroixColoR&#34;&gt;LaCroixColoR&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seeing-life-in-black-and-white&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seeing life in black and white&lt;/h3&gt;
&lt;p&gt;Turns out there is a &lt;a href=&#34;https://www.tannerhelland.com/3643/grayscale-image-algorithm-vb6/&#34;&gt;lot of different ways&lt;/a&gt; to turn colors into grayscale. Prismatic has implemented a handful of these. Notice how the viridis palette is still working once you have it transformed to black and white.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hcl.colors(10) %&amp;gt;%
  clr_greyscale() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Be advised that not all of these methods are meant to be perceptually uniform.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Negate&lt;/h3&gt;
&lt;p&gt;Negation of a color is pretty simple. it will just pick the opposite color in &lt;a href=&#34;https://en.wikipedia.org/wiki/RGB_color_space&#34;&gt;RGB space&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;terrain.colors(10) %&amp;gt;%
  clr_negate() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mixing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mixing&lt;/h3&gt;
&lt;p&gt;Mixing is just adding colors together. Thus my mixing a color with red would make a color more red.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rainbow(10) %&amp;gt;%
  clr_mix(&amp;quot;red&amp;quot;) %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rotation&lt;/h3&gt;
&lt;p&gt;the &lt;code&gt;clr_rotate()&lt;/code&gt; function will take a color and rotate its hue, which is a way walk around the rainbow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;terrain.colors(10) %&amp;gt;%
  clr_rotate(90) %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;color-blindness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Color blindness&lt;/h3&gt;
&lt;p&gt;also includes 3 functions (&lt;code&gt;clr_protan()&lt;/code&gt;, &lt;code&gt;clr_deutan()&lt;/code&gt; and &lt;code&gt;clr_tritan()&lt;/code&gt;) to simulate colorblindness. These functions has a &lt;code&gt;severity&lt;/code&gt; argument to control the strength of the deficiency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hcl.colors(10) %&amp;gt;%
  clr_deutan() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;light-and-darkness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Light and darkness&lt;/h3&gt;
&lt;p&gt;Lastly we have functions to simulate lightness and darkness. This is surprisingly hard to do and no one way works great all the time. Please refer to the excellent &lt;a href=&#34;https://arxiv.org/abs/1903.06490&#34;&gt;colorspace paper&lt;/a&gt; for more information. These functions (&lt;code&gt;clr_lighten()&lt;/code&gt; and &lt;code&gt;clr_darken()&lt;/code&gt;) also include a &lt;code&gt;space&lt;/code&gt; argument to determine the space in which to perform the transformation. Please try each of these to find the optimal method for your use case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rainbow(10) %&amp;gt;%
  clr_darken() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-01-manipulating-colors-with-prismatic/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison Code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;swatchplot(
  list(
    saturate = rbind(&amp;quot;0&amp;quot; = clr_rotate(terrain.colors(10),  0),
                     &amp;quot;60&amp;quot; = clr_rotate(terrain.colors(10),  60),
                     &amp;quot;120&amp;quot; = clr_rotate(terrain.colors(10),  120),
                     &amp;quot;180&amp;quot; = clr_rotate(terrain.colors(10),  180),
                     &amp;quot;240&amp;quot; = clr_rotate(terrain.colors(10),  240),
                     &amp;quot;300&amp;quot; = clr_rotate(terrain.colors(10),  300)),
    desaturate = rbind(&amp;quot;0&amp;quot; = clr_rotate(hcl.colors(10),  0),
                       &amp;quot;60&amp;quot; = clr_rotate(hcl.colors(10),  60),
                       &amp;quot;120&amp;quot; = clr_rotate(hcl.colors(10),  120),
                       &amp;quot;180&amp;quot; = clr_rotate(hcl.colors(10),  180),
                       &amp;quot;240&amp;quot; = clr_rotate(hcl.colors(10),  240),
                       &amp;quot;300&amp;quot; = clr_rotate(hcl.colors(10),  300))
  ),
  nrow = 7, line = 2.5
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Authorship classification with tidymodels and textrecipes</title>
      <link>/2019/08/09/authorship-classification-with-tidymodels-and-textrecipes/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/2019/08/09/authorship-classification-with-tidymodels-and-textrecipes/</guid>
      <description>


&lt;p&gt;In this post we will revisit on of my earlier &lt;a href=&#34;https://www.hvitfeldt.me/blog/predicting-authorship-in-the-federalist-papers-with-tidytext/&#34;&gt;blogposts&lt;/a&gt; where I tried to use tidytext and glmnet to predict the authorship of the anonymous Federalist Papers. If you want more information regarding the data, please read the old post. In the post we will try to achieve the same goal, but use the &lt;a href=&#34;https://www.tidyverse.org/articles/2018/08/tidymodels-0-0-1/&#34;&gt;tidymodels&lt;/a&gt; framework.&lt;/p&gt;
&lt;div id=&#34;loading-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels) # Modeling framework
library(textrecipes) # extension to preprocessing engine to handle text
library(stringr) # String modification
library(gutenbergr) # Portal to download the Federalist Papers
library(tokenizers) # Tokenization engine
library(furrr) # to be able to fit the models in parallel&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fetching-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fetching the Data&lt;/h2&gt;
&lt;p&gt;The text is provided from the &lt;a href=&#34;https://www.gutenberg.org/wiki/Main_Page&#34;&gt;Gutenberg Project&lt;/a&gt;. A simple search reveals that the Federalist Papers have the id of 1404.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers &amp;lt;- gutenberg_download(1404)
papers&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;shaping-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;shaping data&lt;/h2&gt;
&lt;p&gt;This is the first we will deviate from the original post in that we will divide the text into paragraphs instead of sentences as we did in the last post. Hopefully this will strike a good balance between size of each observation and the number of observations.&lt;/p&gt;
&lt;p&gt;In the following pipe we:
- &lt;code&gt;pull()&lt;/code&gt; out the text vector
- paste together the strings with &lt;code&gt;\n&lt;/code&gt; to denote line-breaks
- tokenize into paragraphs
- put it in a tibble
- create a variable &lt;code&gt;no&lt;/code&gt; to denote which paper the paragraph is in
- add &lt;code&gt;author&lt;/code&gt; variable to denote author
- remove preamble text&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# attribution numbers
hamilton &amp;lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
madison &amp;lt;- c(10, 14, 18:20, 37:48)
jay &amp;lt;- c(2:5, 64)
unknown &amp;lt;- c(49:58, 62:63)

papers_paragraphs &amp;lt;- papers %&amp;gt;%
  pull(text) %&amp;gt;%
  str_c(collapse = &amp;quot;\n&amp;quot;) %&amp;gt;%
  tokenize_paragraphs() %&amp;gt;%
  unlist() %&amp;gt;%
  tibble(text = .) %&amp;gt;%
  mutate(no = cumsum(str_detect(text, regex(&amp;quot;FEDERALIST No&amp;quot;,
                                            ignore_case = TRUE)))) %&amp;gt;%
  mutate(author = case_when(no %in% hamilton ~ &amp;quot;hamilton&amp;quot;,
                            no %in% madison ~ &amp;quot;madison&amp;quot;,
                            no %in% jay ~ &amp;quot;jay&amp;quot;,
                            no %in% unknown ~ &amp;quot;unknown&amp;quot;)) %&amp;gt;%
  filter(no &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;class-balance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Class Balance&lt;/h2&gt;
&lt;p&gt;There is quite a bit inbalence between the classes. For the remaining of the analysis will we exclude all the papers written by &lt;code&gt;Jay&lt;/code&gt;, partly because it is a small class, but more importantly because he isn‚Äôt suspected to be the mystery author.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_paragraphs %&amp;gt;%
  count(author) %&amp;gt;%
  ggplot(aes(author, n)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is wroth remembering that we don‚Äôt have the true answer, much more like in real world problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting the Data&lt;/h2&gt;
&lt;p&gt;Here we will use the &lt;code&gt;rsample&lt;/code&gt; package to split the data into a testing, validation and training dataset. We will let the testing dataset be all the paragraphs where &lt;code&gt;author == &#34;unknown&#34;&lt;/code&gt; and the training and validation datasets being the paragraphs written by Hamilton and Madison. &lt;code&gt;intial_split()&lt;/code&gt; will insure that each dataset with have the same proportions with respect to the &lt;code&gt;author&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_split &amp;lt;- papers_paragraphs %&amp;gt;%
  filter(author %in% c(&amp;quot;hamilton&amp;quot;, &amp;quot;madison&amp;quot;)) %&amp;gt;%
  initial_split(strata = author)

training_data &amp;lt;- training(data_split)

validation_data &amp;lt;- testing(data_split)

testing_data &amp;lt;- papers_paragraphs %&amp;gt;%
  filter(author == &amp;quot;unknown&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-data-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;specifying data preprocessing&lt;/h2&gt;
&lt;p&gt;We will go with a rather simple preprocessing. start by specifying a recipe where &lt;code&gt;author&lt;/code&gt; is to be predicted, and we only want to use the &lt;code&gt;text&lt;/code&gt; data. Here we make sure to use the training dataset. We then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tokenize according to (n-grams)[&lt;a href=&#34;https://www.tidytextmining.com/ngrams.html&#34; class=&#34;uri&#34;&gt;https://www.tidytextmining.com/ngrams.html&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;only keep the 250 most frequent tokens&lt;/li&gt;
&lt;li&gt;calculate the (term frequency‚Äìinverse document frequency)[&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;up-sample the observation to achieve class balance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and finally prep the recipe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- recipe(author ~ text, data = training_data) %&amp;gt;%
  step_tokenize(text, token = &amp;quot;ngrams&amp;quot;, options = list(n = 3)) %&amp;gt;%
  step_tokenfilter(text, max_tokens = 250) %&amp;gt;%
  step_tfidf(text) %&amp;gt;%
  step_upsample(author) %&amp;gt;%
  prep()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply Preprocessing&lt;/h2&gt;
&lt;p&gt;Now we apply the prepped recipe to get back the processed datasets. Note that I have used shorter names for processed datasets (&lt;code&gt;train_data&lt;/code&gt; vs &lt;code&gt;training_data&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data &amp;lt;- juice(rec)
val_data &amp;lt;- bake(rec, new_data = validation_data)
test_data &amp;lt;- bake(rec, new_data = testing_data)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the Models&lt;/h2&gt;
&lt;p&gt;This time I‚Äôm going to try to run some (random forests)[&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Random_forest&lt;/a&gt;]. And that would be fairly easy to use. First we specify the the model type (&lt;code&gt;rand_forest&lt;/code&gt;) the type (&lt;code&gt;classification&lt;/code&gt;) and the engine (&lt;code&gt;randomForest&lt;/code&gt;). Next we fit the model to the training dataset, predict it on the validation datasets, add the true value and calculate the accuracy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rand_forest(&amp;quot;classification&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;randomForest&amp;quot;) %&amp;gt;%
  fit(author ~ ., data = train_data) %&amp;gt;%
  predict(new_data = val_data) %&amp;gt;%
  mutate(truth = val_data$author) %&amp;gt;%
  accuracy(truth, .pred_class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However we want to try some different hyper-parameter values to make sure we are using the best we can. The &lt;code&gt;dials&lt;/code&gt; allows us to do hyper-parameter searching in a fairly easy way. First we will create a parameter_grid, where we will vary the number of trees in our forest (&lt;code&gt;trees()&lt;/code&gt;) and the number of predictors to be randomly sampled. We give it some reasonable ranges, and say that we want 5 levels for each parameter, resulting in &lt;code&gt;5 * 5 = 25&lt;/code&gt; parameter pairs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;param_grid &amp;lt;- grid_regular(range_set(trees(), c(50, 250)), 
                           range_set(mtry(), c(1, 15)), levels = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we create a model specification where we use &lt;code&gt;varying()&lt;/code&gt; to denote that these parameters are to be varying.
Then we &lt;code&gt;merge()&lt;/code&gt; the model specification into the parameter grid such that we have a tibble of model specifications&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_spec &amp;lt;- rand_forest(&amp;quot;classification&amp;quot;, mtry = varying(), trees = varying()) %&amp;gt;%
  set_engine(&amp;quot;randomForest&amp;quot;)

param_grid &amp;lt;- param_grid %&amp;gt;%
  mutate(specs = merge(., rf_spec))

param_grid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want to iterate through the model specification. We will here create a function that will take a model specification, fit it to the training data, predict according to the validation data, calculate the accuracy and return it as a single number. Create this function makes it so we can use &lt;code&gt;map()&lt;/code&gt; over all the model specifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_one_spec &amp;lt;- function(model) {
  model %&amp;gt;%
    fit(author ~ ., data = train_data) %&amp;gt;%
    predict(new_data = val_data) %&amp;gt;%
    mutate(truth = val_data$author) %&amp;gt;%
    accuracy(truth, .pred_class) %&amp;gt;%
    pull(.estimate)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this is a fairly small dataset, I‚Äôll showcase how we can parallize the calculations. Since we have a framework where are we &lt;code&gt;map()&lt;/code&gt;‚Äôing over the specification it is a obvious case for the &lt;code&gt;furrr&lt;/code&gt; package. (if you don‚Äôt want or isn‚Äôt able to to run your models on multiple cores, simply delete &lt;code&gt;plan(multicore)&lt;/code&gt; and turn &lt;code&gt;future_map_dbl()&lt;/code&gt; to &lt;code&gt;map_dbl()&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan(multicore)
final &amp;lt;- param_grid %&amp;gt;%
  mutate(accuracy = future_map_dbl(specs, fit_one_spec))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can try to visualize the optimal hyper-parameters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final %&amp;gt;%
  mutate_at(vars(trees:mtry), factor) %&amp;gt;%
  ggplot(aes(mtry, trees, fill = accuracy)) +
  geom_tile() +
  scale_fill_viridis_c()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we clearly see that only having 1 predictor to split with it sub-optimal, but otherwise having a low number of predictors are to be preferred. We can use &lt;code&gt;arrange()&lt;/code&gt; to look at the top parameter pairs&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arrange(final, desc(accuracy)) %&amp;gt;%
  slice(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we pick &lt;code&gt;trees == 100&lt;/code&gt; and &lt;code&gt;mtry == 4&lt;/code&gt; as our hyper-parameters. And we use these to fit our final model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_model &amp;lt;- rf_spec %&amp;gt;%
  update(trees = 100, mtry = 4) %&amp;gt;%
  fit(author ~ ., data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-the-unknown-papers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predicting the unknown papers&lt;/h2&gt;
&lt;p&gt;Lastly we predict on the unknown papers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_predict &amp;lt;- testing_data %&amp;gt;% 
  bind_cols(predict(final_model, new_data = test_data)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can‚Äôt calculate an accuracy or any other metric, as we don‚Äôt know the true value. However we can see how the the different paragraphs have been classified within each paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_predict %&amp;gt;%
  count(no, .pred_class) %&amp;gt;%
  mutate(no = factor(no)) %&amp;gt;%
  group_by(no) %&amp;gt;%
  mutate(highest = n == max(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can visualize the results, and it looks like from this limited analysis that Hamilton is the author of mysterious papers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_predict %&amp;gt;%
  count(no, .pred_class) %&amp;gt;%
  mutate(no = factor(no),
         .pred_class = factor(.pred_class, 
                              levels = c(&amp;quot;hamilton&amp;quot;, &amp;quot;madison&amp;quot;), 
                              labels = c(&amp;quot;Hamilton&amp;quot;, &amp;quot;Madison&amp;quot;))) %&amp;gt;%
  group_by(no) %&amp;gt;%
  mutate(highest = n == max(n)) %&amp;gt;%
  ggplot(aes(no, n, fill = .pred_class, alpha = highest)) +
  scale_alpha_ordinal(range = c(0.5, 1)) +
  geom_col(position = &amp;quot;dodge&amp;quot;, color = &amp;quot;black&amp;quot;) +
  theme_minimal() +
  scale_fill_manual(values = c(&amp;quot;#304890&amp;quot;, &amp;quot;#6A7E50&amp;quot;)) +
  guides(alpha = &amp;quot;none&amp;quot;) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  labs(x = &amp;quot;Federalist Paper Number&amp;quot;,
       y = &amp;quot;Number of paragraphs&amp;quot;,
       fill = &amp;quot;Author&amp;quot;,
       title = &amp;quot;Hamilton were predicted more often to be the author then\nMadison in all but 1 Paper&amp;quot;)
    &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post we have touched a lot of different topics; tidymodels, text preprocessing, parallel computing etc. Since we have covered so many topics have left each section not covered it a lot of detail. In a more proper analysis you would want to try some different models and different ways to do the preprocessing.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Creating RStudio addin to modify selection</title>
      <link>/2019/07/30/creating-rstudio-addin-to-modify-selection/</link>
      <pubDate>Tue, 30 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/2019/07/30/creating-rstudio-addin-to-modify-selection/</guid>
      <description>


&lt;div id=&#34;the-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The problem&lt;/h1&gt;
&lt;p&gt;Lately there have been some well deservered buzz around addins in RStudio, &lt;a href=&#34;https://github.com/milesmcbain/datapasta&#34;&gt;datapasta&lt;/a&gt; being one and &lt;a href=&#34;https://gitlab.com/hrbrmstr/hrbraddins&#34;&gt;hrbraddins&lt;/a&gt; being another highly liked ones.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
I find datapasta helpful for creating little tibbles for teaching. I&#39;ll find some interesting data online and just copy and paste the table directly into the correct format. You can also set up keyboard shortcuts, because who doesn&#39;t love a keyboard shortcut. Thanks &lt;a href=&#34;https://twitter.com/MilesMcBain?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@MilesMcBain&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://t.co/deaZVVYYDu&#34;&gt;pic.twitter.com/deaZVVYYDu&lt;/a&gt;
&lt;/p&gt;
‚Äî We are R-Ladies (&lt;span class=&#34;citation&#34;&gt;@WeAreRLadies&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/WeAreRLadies/status/1153284810191847425?ref_src=twsrc%5Etfw&#34;&gt;July 22, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
My keyboard shortcut for this lil&#39; function gets quite the workout‚Ä¶&lt;br&gt;üì∫ ‚Äúhrbraddins::bare_combine()‚Äù by &lt;a href=&#34;https://twitter.com/hrbrmstr?ref_src=twsrc%5Etfw&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@hrbrmstr&lt;/span&gt;&lt;/a&gt; &lt;a href=&#34;https://t.co/8dwqNEso0B&#34;&gt;https://t.co/8dwqNEso0B&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/gyqz2mUE0Y&#34;&gt;pic.twitter.com/gyqz2mUE0Y&lt;/a&gt;
&lt;/p&gt;
‚Äî Mara Averick (&lt;span class=&#34;citation&#34;&gt;@dataandme&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/dataandme/status/1155842512743030785?ref_src=twsrc%5Etfw&#34;&gt;July 29, 2019&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;All of this is done with &lt;a href=&#34;https://www.rstudio.com/resources/webinars/understanding-add-ins/&#34;&gt;RStudio Addins&lt;/a&gt; using the &lt;a href=&#34;https://github.com/rstudio/rstudioapi&#34;&gt;rstudioapi&lt;/a&gt; r package.&lt;/p&gt;
&lt;p&gt;A lot of the popular addins follows the same simple formula&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;extract highlighted text&lt;/li&gt;
&lt;li&gt;modify extracted text&lt;/li&gt;
&lt;li&gt;replace highlighted text with modified text.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;if your problem can be solved with the above steps, then this post is for you.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The solution&lt;/h1&gt;
&lt;p&gt;Once you have found the name of your addin, go to your package directory, or &lt;a href=&#34;https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/&#34;&gt;create a new package&lt;/a&gt;. Then we use &lt;a href=&#34;https://usethis.r-lib.org/&#34;&gt;usethis&lt;/a&gt; to create a .R file for the function and to create the necessary infrastructure to let RStudio know it is a Addin.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_r(&amp;quot;name_of_your_addin&amp;quot;)
use_addin(&amp;quot;name_of_your_addin&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;inst/rstudio/addins.dcf&lt;/code&gt; file will be populated to make a binding between your function to the addins menu. From here you will in &lt;code&gt;Name&lt;/code&gt; to change the text of the button in the drop-down menu and change the &lt;code&gt;description&lt;/code&gt; to change the hover text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Name: New Addin Name
Description: New Addin Description
Binding: name_of_your_addin
Interactive: false&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;now you can go back to the .R to write your function. Below is the minimal code needed. Just replace &lt;code&gt;any_function&lt;/code&gt; with a function that takes a string and returns a modified string. build the package and you are done!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example &amp;lt;- function() {
  
  # Gets The active Documeent
  ctx &amp;lt;- rstudioapi::getActiveDocumentContext()

  # Checks that a document is active
  if (!is.null(ctx)) {
    
    # Extracts selection as a string
    selected_text &amp;lt;- ctx$selection[[1]]$text

    # modify string
    selected_text &amp;lt;- any_function(selected_text)
    
    # replaces selection with string
    rstudioapi::modifyRange(ctx$selection[[1]]$range, selected_text)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examples---slugify&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples - slugify&lt;/h1&gt;
&lt;p&gt;While I was writing this post I created an addin to turn the title of the blog post into a slug i could use. I replaced&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_text &amp;lt;- any_function(selected_text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selected_text &amp;lt;- stringr::str_to_lower(selected_text)
selected_text &amp;lt;- stringr::str_replace_all(selected_text, &amp;quot; &amp;quot;, &amp;quot;-&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gave me this little gem of a addin!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/hbrPc6d.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Changing Glyph in legend in ggplot2</title>
      <link>/2019/06/17/changing-glyph-in-legend-in-ggplot2/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/2019/06/17/changing-glyph-in-legend-in-ggplot2/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The newest version of &lt;a href=&#34;https://www.tidyverse.org/articles/2019/06/ggplot2-3-2-0/&#34;&gt;ggplot2 3.2.0&lt;/a&gt; gave us the ability to change the glyph in the legend like so&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(economics_long, aes(date, value01, colour = variable)) +
  geom_line(key_glyph = &amp;quot;timeseries&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And they can likewise be specified with the &lt;code&gt;draw_key_*&lt;/code&gt; functions as well&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(economics_long, aes(date, value01, colour = variable)) +
  geom_line(key_glyph = draw_key_timeseries)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;showcase&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Showcase&lt;/h1&gt;
&lt;p&gt;The following is all the available &lt;code&gt;draw_key_*&lt;/code&gt; functions in ggplot2. Notice that the dark gray color in dotplot and polygon is a result of a unspecified &lt;code&gt;fill&lt;/code&gt; aesthetic. Code to generate these figures can be found at the end of this post.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-5.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-6.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-7.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-8.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-9.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-10.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-11.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-12.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-13.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-14.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-15.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-3-16.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;custom-glyph-key&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Custom glyph key&lt;/h1&gt;
&lt;p&gt;Since the &lt;code&gt;draw_key_*&lt;/code&gt; function just return a grob, you can use spend some time and create your own custom glyphs! Taking inspiration from &lt;code&gt;draw_key_boxplot&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;draw_key_boxplot
## function (data, params, size) 
## {
##     grobTree(linesGrob(0.5, c(0.1, 0.25)), linesGrob(0.5, c(0.75, 
##         0.9)), rectGrob(height = 0.5, width = 0.75), linesGrob(c(0.125, 
##         0.875), 0.5), gp = gpar(col = data$colour %||% &amp;quot;grey20&amp;quot;, 
##         fill = alpha(data$fill %||% &amp;quot;white&amp;quot;, data$alpha), lwd = (data$size %||% 
##             0.5) * .pt, lty = data$linetype %||% 1))
## }
## &amp;lt;bytecode: 0x7ff0874a8c08&amp;gt;
## &amp;lt;environment: namespace:ggplot2&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will I try to make a glyph by myself using both points and lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(grid)
library(rlang)
draw_key_smile &amp;lt;- function(data, params, size) {
  grobTree(
    pointsGrob(0.25, 0.75, size = unit(.25, &amp;quot;npc&amp;quot;), pch = 16),
    pointsGrob(0.75, 0.75, size = unit(.25, &amp;quot;npc&amp;quot;), pch = 16),
    linesGrob(c(0.9, 0.87, 0.78, 0.65, 0.5, 0.35, 0.22, 0.13, 0.1), 
              c(0.5, 0.35, 0.22, 0.13, 0.1, 0.13, 0.22, 0.35, 0.5)),
    gp = gpar(
      col = data$colour %||% &amp;quot;grey20&amp;quot;,
      fill = alpha(data$fill %||% &amp;quot;white&amp;quot;, data$alpha),
      lwd = (data$size %||% 0.5) * .pt,
      lty = data$linetype %||% 1
    )
  )
}

ggplot(economics_long, aes(date, value01, colour = variable)) +
  geom_line(key_glyph = draw_key_smile)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-06-17-changing-glyph-in-ggplot2/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And it looks so happy!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(magrittr)
library(ggplot2)
library(grid)

draws &amp;lt;- ls(getNamespace(&amp;quot;ggplot2&amp;quot;), pattern = &amp;quot;^draw_key_&amp;quot;)

legend_fun &amp;lt;- function(x) {
  ggg &amp;lt;- economics_long %&amp;gt;%
    mutate(variable = factor(variable, labels = paste(&amp;quot;Option&amp;quot;, LETTERS[1:5]))) %&amp;gt;%
    ggplot(aes(date, value01, colour = variable)) +
  geom_line(key_glyph = get(x)) +
    labs(color = x) 
  
  legend &amp;lt;- cowplot::get_legend(ggg)
  
  grid.newpage()
  grid.draw(legend)
}

purrr::walk(draws[1:12], legend_fun)
p &amp;lt;- ggplot(mtcars, aes(wt, mpg, label = rownames(mtcars))) + 
  geom_text(aes(colour = factor(ceiling(seq_len(nrow(mtcars)) %% 5), labels = paste(&amp;quot;Option&amp;quot;, LETTERS[1:5])))) +
  labs(color = &amp;quot;draw_key_text&amp;quot;)
legend &amp;lt;- cowplot::get_legend(p)

grid.newpage()
grid.draw(legend)
purrr::walk(draws[14:16], legend_fun)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Custom Profiler in R</title>
      <link>/2019/05/25/custom-profiler-in-r/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      <guid>/2019/05/25/custom-profiler-in-r/</guid>
      <description>


&lt;p&gt;This blogpost is going to describe how to write a customizable profiling function. If you are not familiar with profiling read the &lt;a href=&#34;https://adv-r.hadley.nz/perf-measure.html#profiling&#34;&gt;Profiling&lt;/a&gt; section of &lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advanced R&lt;/a&gt; to familiarize yourself, I‚Äôll wait.&lt;/p&gt;
&lt;p&gt;‚Ä¶&lt;/p&gt;
&lt;p&gt;Welcome back!&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;While these packages aren‚Äôt strictly needed since most of what we are doing is happening in base R, am I still loading in &lt;code&gt;tidyverse&lt;/code&gt; to do some easier string manipulations and plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;profiling-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Profiling basics&lt;/h2&gt;
&lt;p&gt;You have properly used the &lt;a href=&#34;https://rstudio.github.io/profvis/index.html&#34;&gt;profvis&lt;/a&gt; package. It is an amazing package and I use it on a daily basis. However, the amount of information you get can be overwhelming at times depending on your profiling goals.&lt;/p&gt;
&lt;p&gt;Lets propose in this scenario that we take in some data, scale and center it, apply &lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;PCA&lt;/a&gt; while only keeping the components that explain 90% of the variance and lastly apply &lt;a href=&#34;https://www.datanovia.com/en/lessons/clara-in-r-clustering-large-applications/&#34;&gt;CLARA&lt;/a&gt; clustering and return the classification.&lt;/p&gt;
&lt;p&gt;The code to do that is contained in the following chunk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_threshold &amp;lt;- function(x, threshold) {
  data_pca &amp;lt;- prcomp(x, scale. = TRUE)
  total_var &amp;lt;- sum(data_pca$sdev ^ 2)
  num_comp &amp;lt;- which.max(cumsum(data_pca$sdev ^ 2 / total_var) &amp;gt;= threshold)
  data_pca$x[, seq_len(num_comp)]
}

pca_kmeans &amp;lt;- function(x, threshold = 0.9, centers = 2) {
  data_matrix &amp;lt;- as.matrix(x)
  data_pca &amp;lt;- pca_threshold(data_matrix, threshold = threshold)
  data_kmeans &amp;lt;- cluster::clara(data_pca, k = centers)
  data_kmeans$cluster
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create some data, and run profvis on it&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;large_data &amp;lt;- diamonds %&amp;gt;%
  select_if(is.numeric) %&amp;gt;%
  sample_n(100000, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;profvis::profvis({
  pca_kmeans(large_data)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we get the following information back.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;profvis-flame.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;profvis-data.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;It is very infomrative, but it is also giving a LOT of information. Lets propose we want to know the percentage of the computation time is used to do the PCA calculations. In the &lt;code&gt;profvis&lt;/code&gt; framework you would need to do the calculation manually. All while waiting for the html widget to load.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-idea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Idea&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;profvis&lt;/code&gt; uses the &lt;code&gt;Rprof&lt;/code&gt; function internally to inspect what is happening. By using &lt;code&gt;Rprof&lt;/code&gt; directly we can extract the profile and calculate out out output/metrix.&lt;/p&gt;
&lt;p&gt;The base profiling steps are&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- tempfile()
Rprof(tmp)
##################
# Code goes here #
##################
Rprof(NULL)
profile &amp;lt;- readLines(tmp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This chunk will set up a temporary file, start the profiler and set it to write to the temporary file, stop the profiler and read the result from the profiler.&lt;/p&gt;
&lt;p&gt;Trying it with our code we get&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- tempfile()
Rprof(tmp)
x &amp;lt;- pca_kmeans(large_data)
Rprof(NULL)
profile &amp;lt;- readLines(tmp)

head(profile)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample.interval=20000&amp;quot;                                                                                                            
## [2] &amp;quot;\&amp;quot;aperm.default\&amp;quot; \&amp;quot;aperm\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;
## [3] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [4] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [5] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [6] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets see what these lines mean. first we notice that the first line is just denoting the sample interval, so we can ignore that for now. Lets look at the next line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;\&amp;quot;aperm.default\&amp;quot; \&amp;quot;aperm\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a snapshot of the ‚Äúcall-stack‚Äù, and it reads inside-out. So we have that &lt;code&gt;aperm.default&lt;/code&gt; is called inside &lt;code&gt;aperm&lt;/code&gt; which is called inside &lt;code&gt;apply&lt;/code&gt; which is called inside &lt;code&gt;scale.default&lt;/code&gt; and so on and so forth all the way up to &lt;code&gt;pca_kmeans&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now that we know how &lt;code&gt;Rprof&lt;/code&gt; works, we can write some code that checks whether ‚Äúpca_threshold‚Äù appear in the call-stack and then find the percentage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Solution&lt;/h2&gt;
&lt;p&gt;We can now create a function that will calculate the percentage of the time is being spend in a certain function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prof_procentage &amp;lt;- function(expr, pattern) {
  tmp &amp;lt;- tempfile()
  Rprof(tmp)
  expr
  Rprof(NULL)
  profile &amp;lt;- readLines(tmp)[-1]
  
  mean(grepl(pattern, profile))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can now easily be used on our calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prof_procentage(
  x &amp;lt;- pca_kmeans(large_data),
  pattern = &amp;quot;pca_threshold&amp;quot;
)
## [1] 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this is how to create a custom profiler. Simple modify the last line in the skeleton function &lt;code&gt;prof_procentage&lt;/code&gt; to change its behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-extensions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;the Extensions&lt;/h2&gt;
&lt;p&gt;The sky‚Äôs the limit! you are only limited by your regex abilities. You can also change the output. In the last example I returned a numeric of the percentage, But we can also have the output be a plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prof_procentage_plot &amp;lt;- function(expr, pattern) {
  tmp &amp;lt;- tempfile()
  Rprof(tmp)
  expr
  Rprof(NULL)
  profile &amp;lt;- readLines(tmp)[-1]
  
  data.frame(x = grepl(pattern, profile)) %&amp;gt;%
    ggplot(aes(x)) +
    geom_bar()
}

prof_procentage_plot(
  x &amp;lt;- pca_kmeans(large_data),
  pattern = &amp;quot;pca_threshold&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-25-custom-profiler-in-r/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-follow-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The follow-up&lt;/h2&gt;
&lt;p&gt;After my initial announcement of this post I got a helpful tweet from &lt;a href=&#34;https://twitter.com/hadleywickham/status/1132737794760237059&#34;&gt;Hadley Wickham&lt;/a&gt; about the &lt;code&gt;profvis::parse_rprof()&lt;/code&gt;. In essence it will help you parse the file you write with &lt;code&gt;Rprof&lt;/code&gt; to help you get to your answer faster and safer.&lt;/p&gt;
&lt;p&gt;So some output like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample.interval=20000&amp;quot;                                                                                                            
## [2] &amp;quot;\&amp;quot;aperm.default\&amp;quot; \&amp;quot;aperm\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;
## [3] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [4] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [5] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;          
## [6] &amp;quot;\&amp;quot;is.na\&amp;quot; \&amp;quot;FUN\&amp;quot; \&amp;quot;apply\&amp;quot; \&amp;quot;scale.default\&amp;quot; \&amp;quot;scale\&amp;quot; \&amp;quot;prcomp.default\&amp;quot; \&amp;quot;prcomp\&amp;quot; \&amp;quot;pca_threshold\&amp;quot; \&amp;quot;pca_kmeans\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Will be transformed to a nice data.frame with &lt;code&gt;profvis::parse_rprof()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    time depth          label filenum linenum memalloc meminc filename
## 1     1     9  aperm.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 2     1     8          aperm      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 3     1     7          apply      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 4     1     6  scale.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 5     1     5          scale      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 6     1     4 prcomp.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 7     1     3         prcomp      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 8     1     2  pca_threshold      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 9     1     1     pca_kmeans      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 10    2     9          is.na      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 11    2     8            FUN      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 12    2     7          apply      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 13    2     6  scale.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 14    2     5          scale      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 15    2     4 prcomp.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 16    2     3         prcomp      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 17    2     2  pca_threshold      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 18    2     1     pca_kmeans      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 19    3     9          is.na      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 20    3     8            FUN      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 21    3     7          apply      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 22    3     6  scale.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 23    3     5          scale      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 24    3     4 prcomp.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 25    3     3         prcomp      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 26    3     2  pca_threshold      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 27    3     1     pca_kmeans      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 28    4     9          is.na      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 29    4     8            FUN      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 30    4     7          apply      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 31    4     6  scale.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 32    4     5          scale      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 33    4     4 prcomp.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 34    4     3         prcomp      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 35    4     2  pca_threshold      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 36    4     1     pca_kmeans      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 37    5     9          is.na      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 38    5     8            FUN      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 39    5     7          apply      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 40    5     6  scale.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 41    5     5          scale      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 42    5     4 prcomp.default      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 43    5     3         prcomp      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 44    5     2  pca_threshold      NA      NA        0      0     &amp;lt;NA&amp;gt;
## 45    5     1     pca_kmeans      NA      NA        0      0     &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Center continuous palettes in ggplot2</title>
      <link>/2019/05/21/center-continuous-palettes-in-ggplot2/</link>
      <pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate>
      <guid>/2019/05/21/center-continuous-palettes-in-ggplot2/</guid>
      <description>


&lt;p&gt;Using a divergent color palette can be beneficial when you want to draw attention to some values compared to a fixed point. Like temperature around freezing, monetary values around zero and so on. However it can be hard to align 0 to the middle of a continuous color scale. This post will explain how to do this correctly for &lt;code&gt;scale_colour_distiller&lt;/code&gt; and &lt;code&gt;scale_fill_distiller&lt;/code&gt;, and this will also work for extension packages such as &lt;a href=&#34;https://github.com/thomasp85/scico&#34;&gt;scico&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;packages-and-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Packages and data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(scico)

theme_set(theme_minimal())

example_data &amp;lt;- data.frame(name = letters[1:10],
                           value = -2:7 + 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The problem&lt;/h1&gt;
&lt;p&gt;First lets construct a simple chart, we have a bar chart where some of the bars go up, and some of the bars go down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(example_data, aes(name, value)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next lets add some color by assigning the value to the fill aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(example_data, aes(name, value, fill = value)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using a sequential palette for a chart like this doesn‚Äôt give us much insight. Lets add a divergent scale with &lt;code&gt;scale_fill_gradient2()&lt;/code&gt;. While it is doing its job, you still have to define the colors yourself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(example_data, aes(name, value, fill = value)) +
  geom_col() +
  scale_fill_gradient2()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lets instead use the &lt;code&gt;scale_fill_distiller()&lt;/code&gt; function to access the continuous versions of the &lt;a href=&#34;http://colorbrewer2.org&#34;&gt;brewer scales&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(example_data, aes(name, value, fill = value)) +
  geom_col() +
  scale_fill_distiller(type = &amp;quot;div&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But look! some of the upwards facing bars are colored green instead of orange.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The solution&lt;/h1&gt;
&lt;p&gt;The solution is to manually specify the the limits of the color palette such that the center of the palette appears in the middle of the range. This is simply done by finding the absolute maximum of the range of the variable to are mapping to the color. We then set the limits to go from negative max to positive max, thus making zero appear in the middle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limit &amp;lt;- max(abs(example_data$value)) * c(-1, 1)

ggplot(example_data, aes(name, value, fill = value)) +
  geom_col() +
  scale_fill_distiller(type = &amp;quot;div&amp;quot;, limit = limit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This approach also works with the &lt;code&gt;scico&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limit &amp;lt;- max(abs(example_data$value)) * c(-1, 1)

ggplot(example_data, aes(name, value, fill = value)) +
  geom_col() +
  scale_fill_scico(palette = &amp;quot;cork&amp;quot;, limit = limit) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-21-center-continuous-palettes-in-ggplot2/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Circle Love - making hearts with circles</title>
      <link>/2019/05/08/circle-love-making-hearts-with-circles/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>/2019/05/08/circle-love-making-hearts-with-circles/</guid>
      <description>


&lt;div id=&#34;why-are-we-here&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why are we here?&lt;/h1&gt;
&lt;p&gt;Some days ago I say this little cute pen and it sparked something inside me.&lt;/p&gt;
&lt;p class=&#34;codepen&#34; data-height=&#34;265&#34; data-theme-id=&#34;0&#34; data-default-tab=&#34;html,result&#34; data-user=&#34;chrisgannon&#34; data-slug-hash=&#34;EJBxRx&#34; style=&#34;height: 265px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;&#34; data-pen-title=&#34;Heart is Home&#34;&gt;
&lt;span&gt;See the Pen &lt;a href=&#34;https://codepen.io/chrisgannon/pen/EJBxRx/&#34;&gt;
Heart is Home&lt;/a&gt; by Chris Gannon (&lt;a href=&#34;https://codepen.io/chrisgannon&#34;&gt;&lt;span class=&#34;citation&#34;&gt;@chrisgannon&lt;/span&gt;&lt;/a&gt;)
on &lt;a href=&#34;https://codepen.io&#34;&gt;CodePen&lt;/a&gt;.&lt;/span&gt;
&lt;/p&gt;
&lt;script async src=&#34;https://static.codepen.io/assets/embed/ei.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;I throw together some lines of code and took my first splash into using &lt;a href=&#34;https://github.com/r-spatial/sf&#34;&gt;simple Features&lt;/a&gt;. This post is not meant as an introduction to sf, a great introduction to the sf objects is made by &lt;a href=&#34;https://www.jessesadler.com/post/simple-feature-objects/&#34;&gt;Jesse Sadler&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Loading packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(sf)
library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;first-run&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First run&lt;/h1&gt;
&lt;p&gt;First we create the center shape. I have gone for heart shape, for which I found a parametric expression, I have wrapped all of this in a little function such that I can specify the number of points the polygon has.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heart_fun &amp;lt;- function(n) {
  t &amp;lt;- c(seq(0, 2 * pi, length.out = n), 0)
  
  out &amp;lt;- data.frame(
    x = c(16 * sin(t) ^ 3),
    y = 13 * cos(t) - 5 * cos(2 * t) - 2 * cos(3 * t) - cos(4 * t)
  )
  out &amp;lt;- as.matrix(out)
  out &amp;lt;- list(out)
  st_polygon(out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets check that the function works&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heart_fun(100)
## POLYGON ((0 5, 0.004082058 5.082247, 0.03245962 5.325084, 0.1084517 5.716992, 0.2534598 6.239393, 0.4860975 6.867539, 0.8214215 7.571701, 1.270293 8.31857, 1.838891 9.072817, 2.528404 9.798711, 3.334892 10.46172, 4.24935 11.03003, 5.25795 11.47583, 6.342465 11.77642, 7.480851 11.915, 8.647981 11.88112, 9.816481 11.67082, 10.95766 11.28641, 12.04251 10.736, 13.04268 10.03268, 13.93146 9.193568, 14.68474 8.238708, 15.28179 7.189845, 15.70606 6.069255, 15.94569 4.898625, 15.99396 3.698075, 15.8495 2.485356, 15.51639 1.275288, 15.00393 0.07943237, 14.32642 -1.093982, 13.50257 -2.239884, 12.5549 -3.355982, 11.50893 -4.442201, 10.3923 -5.5, 9.233833 -6.531618, 8.062492 -7.539309, 6.906432 -8.524629, 5.792014 -9.487815, 4.742924 -10.42731, 3.77938 -11.33948, 2.917472 -12.21848, 2.168659 -13.05638, 1.539432 -13.84345, 1.031163 -14.56857, 0.6401401 -15.21987, 0.3577924 -15.78537, 0.1710904 -16.25367, 0.06311066 -16.61466, 0.01374229 -16.86016, 0.0005110288 -16.9844, -0.0005110288 -16.9844, -0.01374229 -16.86016, -0.06311066 -16.61466, -0.1710904 -16.25367, -0.3577924 -15.78537, -0.6401401 -15.21987, -1.031163 -14.56857, -1.539432 -13.84345, -2.168659 -13.05638, -2.917472 -12.21848, -3.77938 -11.33948, -4.742924 -10.42731, -5.792014 -9.487815, -6.906432 -8.524629, -8.062492 -7.539309, -9.233833 -6.531618, -10.3923 -5.5, -11.50893 -4.442201, -12.5549 -3.355982, -13.50257 -2.239884, -14.32642 -1.093982, -15.00393 0.07943237, -15.51639 1.275288, -15.8495 2.485356, -15.99396 3.698075, -15.94569 4.898625, -15.70606 6.069255, -15.28179 7.189845, -14.68474 8.238708, -13.93146 9.193568, -13.04268 10.03268, -12.04251 10.736, -10.95766 11.28641, -9.816481 11.67082, -8.647981 11.88112, -7.480851 11.915, -6.342465 11.77642, -5.25795 11.47583, -4.24935 11.03003, -3.334892 10.46172, -2.528404 9.798711, -1.838891 9.072817, -1.270293 8.31857, -0.8214215 7.571701, -0.4860975 6.867539, -0.2534598 6.239393, -0.1084517 5.716992, -0.03245962 5.325084, -0.004082058 5.082247, -2.350945e-46 5, 0 5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and that it plots correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(heart_fun(100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We also create a helper function to create a unit circle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circle_fun &amp;lt;- function(n) {
  t &amp;lt;- c(seq(0, 2 * pi, length.out = n), 0)
  
  out &amp;lt;- data.frame(
    x = sin(t),
    y = cos(t)
  )
  out &amp;lt;- as.matrix(out)
  out &amp;lt;- list(out)
  st_polygon(out)
}

plot(circle_fun(100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we have a heart shape, lets check the boundaries of that shape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;st_bbox(heart_fun(100))
##      xmin      ymin      xmax      ymax 
## -15.99396 -16.98440  15.99396  11.91500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets generate a sf polygon of both the heart and circle polygon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circle &amp;lt;- circle_fun(100)
heart &amp;lt;- heart_fun(100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we want to generate a list of candidate points where we try to place circles. for now we will just randomly sample between -25 and 25 on the x axis and -20 and 20 on the y axis. then we will save them as a sf object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points &amp;lt;- data.frame(x = runif(250, -25, 25),
                     y = runif(250, -20, 20)) %&amp;gt;% 
  sf::st_as_sf(coords = c(1, 2))

plot(points)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we will filter the points such that we only consider points that are outside the heart shape.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points &amp;lt;- points[!lengths(st_intersects(points, heart)), ]
plot(points)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next we will loop through every single point and calculate the distance (using &lt;code&gt;st_distance&lt;/code&gt;) from the point to the heart. then we will place a circle on that point and scale it such that is has a radius equal to the distance we calculated. That way the heart shape should show given enough points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_polygons &amp;lt;- map(points[[1]],
    ~ (circle * st_distance(heart, .x, by_element = TRUE)) + .x) %&amp;gt;%
  st_sfc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(all_polygons)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we get something nice! however some of the circle become quite big. Lets bound the radius and give it some variation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bound &amp;lt;- function(x, limit) {
  ifelse(x &amp;gt; limit, runif(1, limit / 4, limit), x)
}

all_polygons &amp;lt;- map(points[[1]],
    ~ (circle * bound(st_distance(heart, .x, by_element = TRUE), 4)) + .x) %&amp;gt;%
  st_sfc()

plot(all_polygons)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now lets turn this into a data.frame and extract the x and y coordinate so we can use them for coloring.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotting_data &amp;lt;- data.frame(all_polygons) %&amp;gt;%
  mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]),
         y = map_dbl(geometry, ~st_centroid(.x)[[2]])) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have everything we need we will turn to &lt;code&gt;ggplot2&lt;/code&gt; to pretty everything up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotting_data %&amp;gt;%
  ggplot() +
  geom_sf(aes(color = y, geometry = geometry), alpha = 0.2, fill = NA) +
  coord_sf(datum = NA) +
  theme_void() + 
  guides(color = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we are done! It looks nice and pretty, now there is a bunch of things we can change.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;color scales&lt;/li&gt;
&lt;li&gt;coloring patterns&lt;/li&gt;
&lt;li&gt;circle arrangement (rectangle, circle, buffer)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;one-function-plotting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One function plotting&lt;/h1&gt;
&lt;p&gt;Everything from before is not wrapper up nice and tight in this function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circle_heart &amp;lt;- function(n, center_sf, outside_sf, outside_filter = &amp;quot;None&amp;quot;, plotting_margin = 5, ...) {
  
  bound &amp;lt;- function(x, limit) {
    ifelse(x &amp;gt; limit, runif(1, limit / 4, limit), x)
  }
  
  range &amp;lt;- st_bbox(center_sf)
  points &amp;lt;- data.frame(x = runif(n, range[[&amp;quot;xmin&amp;quot;]] - plotting_margin, 
                                    range[[&amp;quot;xmax&amp;quot;]] + plotting_margin),
                       y = runif(n, range[[&amp;quot;ymin&amp;quot;]] - plotting_margin, 
                                    range[[&amp;quot;ymax&amp;quot;]] + plotting_margin)) %&amp;gt;% 
    sf::st_as_sf(coords = c(1, 2))
  
  if (outside_filter == &amp;quot;buffer&amp;quot;) {
    points &amp;lt;- st_intersection(points, st_buffer(center_sf, plotting_margin))
  } 
  
  points &amp;lt;- points[!lengths(st_intersects(points, center_sf)), ]
  
  all_polygons &amp;lt;- map(points[[1]],
    ~ (outside_sf * bound(st_distance(center_sf, .x, by_element = TRUE), 4)) + .x) %&amp;gt;%
  st_sfc()
  
  plotting_data &amp;lt;- data.frame(all_polygons) %&amp;gt;%
  mutate(x = map_dbl(geometry, ~st_centroid(.x)[[1]]),
         y = map_dbl(geometry, ~st_centroid(.x)[[2]])) 
  
  plotting_data %&amp;gt;%
    ggplot() +
    geom_sf(..., mapping = aes(geometry = geometry)) +
    coord_sf(datum = NA) +
    theme_void()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It returns a simple ggplot2 object that we then can further modify to our visual liking.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circle_heart(300, heart_fun(100), circle_fun(100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A handful of examples&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- circle_heart(300, heart_fun(100), circle_fun(100), 
                   plotting_margin = 10, fill = NA) +
  aes(color = sin(x / y)) +
  scale_color_viridis_c() +
  guides(color = &amp;quot;none&amp;quot;)

p2 &amp;lt;- circle_heart(300, heart_fun(100), circle_fun(100), 
                   outside_filter = &amp;quot;buffer&amp;quot;, plotting_margin = 10, color = NA, alpha = 0.4) +
  aes(fill = cos(x / y)) +
  scale_fill_viridis_c(option = &amp;quot;A&amp;quot;) +
  guides(fill = &amp;quot;none&amp;quot;)

p3 &amp;lt;- circle_heart(300, heart_fun(100), circle_fun(5), 
                   outside_filter = &amp;quot;buffer&amp;quot;, plotting_margin = 10, color = NA, alpha = 0.4) +
  aes(fill = x + y) +
  scale_fill_gradient(low = &amp;quot;pink&amp;quot;, high = &amp;quot;black&amp;quot;) +
  guides(fill = &amp;quot;none&amp;quot;)

p4 &amp;lt;- circle_heart(500, heart_fun(100), circle_fun(4), 
                   outside_filter = &amp;quot;buffer&amp;quot;, plotting_margin = 10, color = NA, alpha = 0.4) +
  aes(fill = atan2(y, x)) +
  scale_fill_gradientn(colours = rainbow(256)) +
  guides(fill = &amp;quot;none&amp;quot;)

p5 &amp;lt;- circle_heart(300, heart_fun(100), circle_fun(10), 
                   outside_filter = &amp;quot;buffer&amp;quot;, plotting_margin = 10, color = NA, alpha = 0.4) +
  aes(fill = factor(floor(x * y) %% 8)) +
  scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;) +
  guides(fill = &amp;quot;none&amp;quot;)

p6 &amp;lt;- circle_heart(500, heart_fun(100), heart_fun(100) / 20, 
                   outside_filter = &amp;quot;buffer&amp;quot;, plotting_margin = 10, color = &amp;quot;grey70&amp;quot;, alpha = 0.4) +
  aes(fill = (y %% 4) * (x %% 1)) +
  scale_fill_gradientn(colours = cm.colors(256)) +
  guides(fill = &amp;quot;none&amp;quot;)

p1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-08-circle-love-making-hearts-with-circles/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text Classification with Tidymodels</title>
      <link>/2018/12/29/text-classification-with-tidymodels/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/2018/12/29/text-classification-with-tidymodels/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/font-awesome/css/all.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/font-awesome/css/v4-shims.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have previously used this blog to talk about text classification a couple of times. &lt;a href=&#34;https://github.com/tidymodels/tidymodels&#34;&gt;tidymodels&lt;/a&gt; have since then seen quite a bit of progress. I did in addition get the &lt;a href=&#34;https://github.com/tidymodels/textrecipes&#34;&gt;textrecipes&lt;/a&gt; package on CRAN, which provides extra steps to &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; package from tidymodels.&lt;/p&gt;
&lt;p&gt;Seeing the always wonderful post by Julia Silge on &lt;a href=&#34;https://juliasilge.com/blog/tidy-text-classification/&#34;&gt;text classification with tidy data principles&lt;/a&gt; encouraged me to show how the same workflow also can be accomplished in tidymodels.&lt;/p&gt;
&lt;p&gt;To give this post a little spice will we only be using stop words. Yes, you read that right, we will only keep stop words. Words you are often encouraged to exclude as they don‚Äôt provide much information. We will challenge that assumption in this post! To have a baseline for our stop word model will I be using the same data as Julia used in her post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data we will be using is the text from &lt;em&gt;Pride and Prejudice&lt;/em&gt; and text from &lt;em&gt;The War of the Worlds&lt;/em&gt;. These texts can we get from &lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt; using the &lt;a href=&#34;https://github.com/ropensci/gutenbergr&#34;&gt;gutenbergr&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(gutenbergr)

titles &amp;lt;- c(
  &amp;quot;The War of the Worlds&amp;quot;,
  &amp;quot;Pride and Prejudice&amp;quot;
)
books &amp;lt;- gutenberg_works(title %in% titles) %&amp;gt;%
  gutenberg_download(meta_fields = &amp;quot;title&amp;quot;) %&amp;gt;%
  mutate(title = as.factor(title)) %&amp;gt;%
  select(-gutenberg_id)

books
## # A tibble: 19,504 x 2
##    text                                                      title              
##    &amp;lt;chr&amp;gt;                                                     &amp;lt;fct&amp;gt;              
##  1 &amp;quot;The War of the Worlds&amp;quot;                                   The War of the Wor‚Ä¶
##  2 &amp;quot;&amp;quot;                                                        The War of the Wor‚Ä¶
##  3 &amp;quot;by H. G. Wells [1898]&amp;quot;                                   The War of the Wor‚Ä¶
##  4 &amp;quot;&amp;quot;                                                        The War of the Wor‚Ä¶
##  5 &amp;quot;&amp;quot;                                                        The War of the Wor‚Ä¶
##  6 &amp;quot;     But who shall dwell in these worlds if they be&amp;quot;     The War of the Wor‚Ä¶
##  7 &amp;quot;     inhabited? .  .  .  Are we or they Lords of the&amp;quot;    The War of the Wor‚Ä¶
##  8 &amp;quot;     World? .  .  .  And how are all things made for ma‚Ä¶ The War of the Wor‚Ä¶
##  9 &amp;quot;          KEPLER (quoted in The Anatomy of Melancholy)&amp;quot;  The War of the Wor‚Ä¶
## 10 &amp;quot;&amp;quot;                                                        The War of the Wor‚Ä¶
## # ‚Ä¶ with 19,494 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(deviating from Julia, will we drop the &lt;code&gt;gutenberg_id&lt;/code&gt; variable as it is redundant, remove the &lt;code&gt;document&lt;/code&gt; variable as it isn‚Äôt needed in the tidymodels framework and set the &lt;code&gt;title&lt;/code&gt; variable as a factor as it works better with tidymodels used later on.)&lt;/p&gt;
&lt;p&gt;I‚Äôm going to quote Julia to explain the modeling problem we are facing;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have the text data now, and let‚Äôs frame the kind of prediction problem we are going to work on. Imagine that we take each book and cut it up into lines, like strips of paper (‚ú® confetti ‚ú®) with an individual line on each paper. Let‚Äôs train a model that can take an individual line and give us a probability that this book comes from Pride and Prejudice vs.¬†from The War of the Worlds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So that is fairly straight-forward task, we already have the data as we want in &lt;code&gt;books&lt;/code&gt;. Before we go on lets investigate the class imbalance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;books %&amp;gt;%
  ggplot(aes(title)) +
  geom_bar() +
  theme_minimal() +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Number of Strips in &amp;#39;Pride and Prejudice&amp;#39; and &amp;#39;The War of the Worlds&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-29-text-classification-with-tidymodels/index_files/figure-html/proportional-plot-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is a little uneven, but we will carry on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stop-words&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stop words&lt;/h2&gt;
&lt;p&gt;Lets first have a talk about stop words. These are the words that are needed for the sentences to be structurally sound, but doesn‚Äôt add any information. however such a concept as ‚Äúnon-informational‚Äù is quite abstract and is bound to be highly domain specific. We will be using the English snowball stop word lists provided by the &lt;a href=&#34;https://github.com/quanteda/stopwords&#34;&gt;stopwords&lt;/a&gt; package (because that is what textrecipes naively uses).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stopwords)
stopwords(language = &amp;quot;en&amp;quot;, source = &amp;quot;snowball&amp;quot;) %&amp;gt;% sort()
##   [1] &amp;quot;a&amp;quot;          &amp;quot;about&amp;quot;      &amp;quot;above&amp;quot;      &amp;quot;after&amp;quot;      &amp;quot;again&amp;quot;     
##   [6] &amp;quot;against&amp;quot;    &amp;quot;all&amp;quot;        &amp;quot;am&amp;quot;         &amp;quot;an&amp;quot;         &amp;quot;and&amp;quot;       
##  [11] &amp;quot;any&amp;quot;        &amp;quot;are&amp;quot;        &amp;quot;aren&amp;#39;t&amp;quot;     &amp;quot;as&amp;quot;         &amp;quot;at&amp;quot;        
##  [16] &amp;quot;be&amp;quot;         &amp;quot;because&amp;quot;    &amp;quot;been&amp;quot;       &amp;quot;before&amp;quot;     &amp;quot;being&amp;quot;     
##  [21] &amp;quot;below&amp;quot;      &amp;quot;between&amp;quot;    &amp;quot;both&amp;quot;       &amp;quot;but&amp;quot;        &amp;quot;by&amp;quot;        
##  [26] &amp;quot;can&amp;#39;t&amp;quot;      &amp;quot;cannot&amp;quot;     &amp;quot;could&amp;quot;      &amp;quot;couldn&amp;#39;t&amp;quot;   &amp;quot;did&amp;quot;       
##  [31] &amp;quot;didn&amp;#39;t&amp;quot;     &amp;quot;do&amp;quot;         &amp;quot;does&amp;quot;       &amp;quot;doesn&amp;#39;t&amp;quot;    &amp;quot;doing&amp;quot;     
##  [36] &amp;quot;don&amp;#39;t&amp;quot;      &amp;quot;down&amp;quot;       &amp;quot;during&amp;quot;     &amp;quot;each&amp;quot;       &amp;quot;few&amp;quot;       
##  [41] &amp;quot;for&amp;quot;        &amp;quot;from&amp;quot;       &amp;quot;further&amp;quot;    &amp;quot;had&amp;quot;        &amp;quot;hadn&amp;#39;t&amp;quot;    
##  [46] &amp;quot;has&amp;quot;        &amp;quot;hasn&amp;#39;t&amp;quot;     &amp;quot;have&amp;quot;       &amp;quot;haven&amp;#39;t&amp;quot;    &amp;quot;having&amp;quot;    
##  [51] &amp;quot;he&amp;quot;         &amp;quot;he&amp;#39;d&amp;quot;       &amp;quot;he&amp;#39;ll&amp;quot;      &amp;quot;he&amp;#39;s&amp;quot;       &amp;quot;her&amp;quot;       
##  [56] &amp;quot;here&amp;quot;       &amp;quot;here&amp;#39;s&amp;quot;     &amp;quot;hers&amp;quot;       &amp;quot;herself&amp;quot;    &amp;quot;him&amp;quot;       
##  [61] &amp;quot;himself&amp;quot;    &amp;quot;his&amp;quot;        &amp;quot;how&amp;quot;        &amp;quot;how&amp;#39;s&amp;quot;      &amp;quot;i&amp;quot;         
##  [66] &amp;quot;i&amp;#39;d&amp;quot;        &amp;quot;i&amp;#39;ll&amp;quot;       &amp;quot;i&amp;#39;m&amp;quot;        &amp;quot;i&amp;#39;ve&amp;quot;       &amp;quot;if&amp;quot;        
##  [71] &amp;quot;in&amp;quot;         &amp;quot;into&amp;quot;       &amp;quot;is&amp;quot;         &amp;quot;isn&amp;#39;t&amp;quot;      &amp;quot;it&amp;quot;        
##  [76] &amp;quot;it&amp;#39;s&amp;quot;       &amp;quot;its&amp;quot;        &amp;quot;itself&amp;quot;     &amp;quot;let&amp;#39;s&amp;quot;      &amp;quot;me&amp;quot;        
##  [81] &amp;quot;more&amp;quot;       &amp;quot;most&amp;quot;       &amp;quot;mustn&amp;#39;t&amp;quot;    &amp;quot;my&amp;quot;         &amp;quot;myself&amp;quot;    
##  [86] &amp;quot;no&amp;quot;         &amp;quot;nor&amp;quot;        &amp;quot;not&amp;quot;        &amp;quot;of&amp;quot;         &amp;quot;off&amp;quot;       
##  [91] &amp;quot;on&amp;quot;         &amp;quot;once&amp;quot;       &amp;quot;only&amp;quot;       &amp;quot;or&amp;quot;         &amp;quot;other&amp;quot;     
##  [96] &amp;quot;ought&amp;quot;      &amp;quot;our&amp;quot;        &amp;quot;ours&amp;quot;       &amp;quot;ourselves&amp;quot;  &amp;quot;out&amp;quot;       
## [101] &amp;quot;over&amp;quot;       &amp;quot;own&amp;quot;        &amp;quot;same&amp;quot;       &amp;quot;shan&amp;#39;t&amp;quot;     &amp;quot;she&amp;quot;       
## [106] &amp;quot;she&amp;#39;d&amp;quot;      &amp;quot;she&amp;#39;ll&amp;quot;     &amp;quot;she&amp;#39;s&amp;quot;      &amp;quot;should&amp;quot;     &amp;quot;shouldn&amp;#39;t&amp;quot; 
## [111] &amp;quot;so&amp;quot;         &amp;quot;some&amp;quot;       &amp;quot;such&amp;quot;       &amp;quot;than&amp;quot;       &amp;quot;that&amp;quot;      
## [116] &amp;quot;that&amp;#39;s&amp;quot;     &amp;quot;the&amp;quot;        &amp;quot;their&amp;quot;      &amp;quot;theirs&amp;quot;     &amp;quot;them&amp;quot;      
## [121] &amp;quot;themselves&amp;quot; &amp;quot;then&amp;quot;       &amp;quot;there&amp;quot;      &amp;quot;there&amp;#39;s&amp;quot;    &amp;quot;these&amp;quot;     
## [126] &amp;quot;they&amp;quot;       &amp;quot;they&amp;#39;d&amp;quot;     &amp;quot;they&amp;#39;ll&amp;quot;    &amp;quot;they&amp;#39;re&amp;quot;    &amp;quot;they&amp;#39;ve&amp;quot;   
## [131] &amp;quot;this&amp;quot;       &amp;quot;those&amp;quot;      &amp;quot;through&amp;quot;    &amp;quot;to&amp;quot;         &amp;quot;too&amp;quot;       
## [136] &amp;quot;under&amp;quot;      &amp;quot;until&amp;quot;      &amp;quot;up&amp;quot;         &amp;quot;very&amp;quot;       &amp;quot;was&amp;quot;       
## [141] &amp;quot;wasn&amp;#39;t&amp;quot;     &amp;quot;we&amp;quot;         &amp;quot;we&amp;#39;d&amp;quot;       &amp;quot;we&amp;#39;ll&amp;quot;      &amp;quot;we&amp;#39;re&amp;quot;     
## [146] &amp;quot;we&amp;#39;ve&amp;quot;      &amp;quot;were&amp;quot;       &amp;quot;weren&amp;#39;t&amp;quot;    &amp;quot;what&amp;quot;       &amp;quot;what&amp;#39;s&amp;quot;    
## [151] &amp;quot;when&amp;quot;       &amp;quot;when&amp;#39;s&amp;quot;     &amp;quot;where&amp;quot;      &amp;quot;where&amp;#39;s&amp;quot;    &amp;quot;which&amp;quot;     
## [156] &amp;quot;while&amp;quot;      &amp;quot;who&amp;quot;        &amp;quot;who&amp;#39;s&amp;quot;      &amp;quot;whom&amp;quot;       &amp;quot;why&amp;quot;       
## [161] &amp;quot;why&amp;#39;s&amp;quot;      &amp;quot;will&amp;quot;       &amp;quot;with&amp;quot;       &amp;quot;won&amp;#39;t&amp;quot;      &amp;quot;would&amp;quot;     
## [166] &amp;quot;wouldn&amp;#39;t&amp;quot;   &amp;quot;you&amp;quot;        &amp;quot;you&amp;#39;d&amp;quot;      &amp;quot;you&amp;#39;ll&amp;quot;     &amp;quot;you&amp;#39;re&amp;quot;    
## [171] &amp;quot;you&amp;#39;ve&amp;quot;     &amp;quot;your&amp;quot;       &amp;quot;yours&amp;quot;      &amp;quot;yourself&amp;quot;   &amp;quot;yourselves&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this list contains 175 words. Many of these words will at first glance pass the ‚Äúnon-informational‚Äù test. However if you look at it more you will realize that many of these can have meaning in certain contexts. The word ‚Äúi‚Äù for example will be used more in blog posts then legal documents. Secondly there appear to be quite a lot of negation words, ‚Äúwouldn‚Äôt‚Äù, ‚Äúdon‚Äôt‚Äù, ‚Äúdoesn‚Äôt‚Äù and ‚Äúmustn‚Äôt‚Äù just to list a few. This is another reminder that constructing your own stop word list can be highly beneficial for your project as the default list might not work in your field.&lt;/p&gt;
&lt;p&gt;While these words are assumed to have little information, the distribution of them and the relational information contained with how the stop word are used compared to each other might give us some information anyways. One author might use negations more often then another, maybe someon really like to use the word ‚Äúnor‚Äù. These kind of features can be extracted as the distributional information, or in other words ‚Äúcounts‚Äù. We will count how often each stop word appear and hope that some of the words can divide the authors. Next we have the order of which words appear in. This is related to writing style, some authors might write ‚Äú‚Ä¶ will you please‚Ä¶‚Äù while others might use ‚Äú‚Ä¶ you will handle‚Ä¶‚Äù. The way each word combination is used might be worth a little bit of information. We will capture the relational information with ngrams.&lt;/p&gt;
&lt;p&gt;We will briefly showcase how this works with an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentence &amp;lt;- &amp;quot;This an example sentence that is used to explain the concept of ngrams.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to extract the ngrams we will use the &lt;a href=&#34;https://github.com/ropensci/tokenizers&#34;&gt;tokenizers&lt;/a&gt; package (also default in textrecipes). Here we can get all the trigrams (ngrams of length 3).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tokenizers)
tokenize_ngrams(sentence, n = 3)
## [[1]]
##  [1] &amp;quot;this an example&amp;quot;       &amp;quot;an example sentence&amp;quot;   &amp;quot;example sentence that&amp;quot;
##  [4] &amp;quot;sentence that is&amp;quot;      &amp;quot;that is used&amp;quot;          &amp;quot;is used to&amp;quot;           
##  [7] &amp;quot;used to explain&amp;quot;       &amp;quot;to explain the&amp;quot;        &amp;quot;explain the concept&amp;quot;  
## [10] &amp;quot;the concept of&amp;quot;        &amp;quot;concept of ngrams&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;however we would also like to the singular word counts (unigrams) and bigrams (ngrams of length 2). This can easily be done by setting the &lt;code&gt;n_min&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tokenize_ngrams(sentence, n = 3, n_min = 1)
## [[1]]
##  [1] &amp;quot;this&amp;quot;                  &amp;quot;this an&amp;quot;               &amp;quot;this an example&amp;quot;      
##  [4] &amp;quot;an&amp;quot;                    &amp;quot;an example&amp;quot;            &amp;quot;an example sentence&amp;quot;  
##  [7] &amp;quot;example&amp;quot;               &amp;quot;example sentence&amp;quot;      &amp;quot;example sentence that&amp;quot;
## [10] &amp;quot;sentence&amp;quot;              &amp;quot;sentence that&amp;quot;         &amp;quot;sentence that is&amp;quot;     
## [13] &amp;quot;that&amp;quot;                  &amp;quot;that is&amp;quot;               &amp;quot;that is used&amp;quot;         
## [16] &amp;quot;is&amp;quot;                    &amp;quot;is used&amp;quot;               &amp;quot;is used to&amp;quot;           
## [19] &amp;quot;used&amp;quot;                  &amp;quot;used to&amp;quot;               &amp;quot;used to explain&amp;quot;      
## [22] &amp;quot;to&amp;quot;                    &amp;quot;to explain&amp;quot;            &amp;quot;to explain the&amp;quot;       
## [25] &amp;quot;explain&amp;quot;               &amp;quot;explain the&amp;quot;           &amp;quot;explain the concept&amp;quot;  
## [28] &amp;quot;the&amp;quot;                   &amp;quot;the concept&amp;quot;           &amp;quot;the concept of&amp;quot;       
## [31] &amp;quot;concept&amp;quot;               &amp;quot;concept of&amp;quot;            &amp;quot;concept of ngrams&amp;quot;    
## [34] &amp;quot;of&amp;quot;                    &amp;quot;of ngrams&amp;quot;             &amp;quot;ngrams&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we get unigrams, bigrams and trigrams in one. But wait, we wanted to limit our focus to stop words. Here is how the end result will look once we exclude all non-stop words and perform the ngram operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tokenize_words(sentence) %&amp;gt;%
  unlist() %&amp;gt;%
  intersect(stopwords(language = &amp;quot;en&amp;quot;, source = &amp;quot;snowball&amp;quot;)) %&amp;gt;%
  paste(collapse = &amp;quot; &amp;quot;) %&amp;gt;%
  print() %&amp;gt;%
  tokenize_ngrams(n = 3, n_min = 1)
## [1] &amp;quot;this an that is to the of&amp;quot;
## [[1]]
##  [1] &amp;quot;this&amp;quot;         &amp;quot;this an&amp;quot;      &amp;quot;this an that&amp;quot; &amp;quot;an&amp;quot;           &amp;quot;an that&amp;quot;     
##  [6] &amp;quot;an that is&amp;quot;   &amp;quot;that&amp;quot;         &amp;quot;that is&amp;quot;      &amp;quot;that is to&amp;quot;   &amp;quot;is&amp;quot;          
## [11] &amp;quot;is to&amp;quot;        &amp;quot;is to the&amp;quot;    &amp;quot;to&amp;quot;           &amp;quot;to the&amp;quot;       &amp;quot;to the of&amp;quot;   
## [16] &amp;quot;the&amp;quot;          &amp;quot;the of&amp;quot;       &amp;quot;of&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have quite a reduction in ngrams then the full sentence, but hopefully there is some information within.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-testing-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training &amp;amp; testing split&lt;/h2&gt;
&lt;p&gt;Before we start modeling we need to split our data into a testing and training set. This is easily done using the &lt;a href=&#34;https://github.com/tidymodels/rsample&#34;&gt;rsample&lt;/a&gt; package from tidymodels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
## Warning: package &amp;#39;rsample&amp;#39; was built under R version 3.6.2
set.seed(1234) 

books_split &amp;lt;- initial_split(books, strata = &amp;quot;title&amp;quot;, p = 0.75)
train_data &amp;lt;- training(books_split)
test_data &amp;lt;- testing(books_split)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;Next step is to do the preprocessing. For this will we use the &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;recipes&lt;/a&gt; from tidymodels. This allows us to specify a preprocessing design that can be train on the training data and applied to the training and testing data alike. I created textrecipes as recipes doesn‚Äôt naively support text preprocessing.&lt;/p&gt;
&lt;p&gt;I‚Äôm are going to replicate Julia‚Äôs preprocessing here to make comparisons easier for myself. Notice the &lt;code&gt;step_filter()&lt;/code&gt; call, the original text have quite a lot of empty lines and these don‚Äôt contain any textual information at all so we will filter away these observations. Note also that we could have used &lt;code&gt;all_predictors()&lt;/code&gt; instead of &lt;code&gt;text&lt;/code&gt; at it is the only predictor we have.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(textrecipes)
julia_rec &amp;lt;- recipe(title ~ ., data = train_data) %&amp;gt;%
  step_filter(text != &amp;quot;&amp;quot;) %&amp;gt;%
  step_tokenize(text) %&amp;gt;%
  step_tokenfilter(text, min_times = 11) %&amp;gt;%
  step_tf(text) %&amp;gt;%
  prep(training = train_data)
julia_rec
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          1
## 
## Training data contained 14629 data points and no missing data.
## 
## Operations:
## 
## Row filtering [trained]
## Tokenization for text [trained]
## Text filtering for text [trained]
## Term frequency with text [trained]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This recipe will remove empty texts, tokenize to words (default in &lt;code&gt;step_tokenize()&lt;/code&gt;), keeping words that appear 10 times or more in the training set and then count how many times each word appears. The processed data looks like this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;julia_train_data &amp;lt;- juice(julia_rec)
julia_test_data  &amp;lt;- bake(julia_rec, test_data)

str(julia_train_data, list.len = 10)
## tibble [12,138 √ó 101] (S3: tbl_df/tbl/data.frame)
##  $ title            : Factor w/ 2 levels &amp;quot;Pride and Prejudice&amp;quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ tf_text_a        : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_about    : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_after    : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_again    : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_all      : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ...
##  $ tf_text_am       : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_an       : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_and      : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ...
##  $ tf_text_any      : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##   [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason we get 101 features and Julia got 1652 is because she did her filtering on the full dataset where we only did the filtering on the training set and that Julia didn‚Äôt explicitly remove empty oberservations.&lt;/p&gt;
&lt;p&gt;Back to stop words!! In this case we need a slightly more complicated recipe&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopword_rec &amp;lt;- recipe(title ~ ., data = train_data) %&amp;gt;%
  step_filter(text != &amp;quot;&amp;quot;) %&amp;gt;%
  step_tokenize(text) %&amp;gt;%
  step_stopwords(text, keep = TRUE) %&amp;gt;%
  step_untokenize(text) %&amp;gt;%
  step_tokenize(text, token = &amp;quot;ngrams&amp;quot;, options = list(n = 3, n_min = 1)) %&amp;gt;%
  step_tokenfilter(text, min_times = 10) %&amp;gt;%
  step_tf(text) %&amp;gt;%
  prep(training = train_data)
stopword_rec
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          1
## 
## Training data contained 14629 data points and no missing data.
## 
## Operations:
## 
## Row filtering [trained]
## Tokenization for text [trained]
## Stop word removal for text [trained]
## Untokenization for text [trained]
## Tokenization for text [trained]
## Text filtering for text [trained]
## Term frequency with text [trained]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we tokenize to words, remove all non-stop words, untokenize (which is basically just &lt;code&gt;paste()&lt;/code&gt; with a fancy name), tokenize to ngrams, remove ngrams that appear less then 10 times and lastly we count how often each ngram appear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Processed data
stopword_train_data &amp;lt;- juice(stopword_rec)
stopword_test_data  &amp;lt;- bake(stopword_rec, test_data)

str(stopword_train_data, list.len = 10)
## tibble [12,138 √ó 101] (S3: tbl_df/tbl/data.frame)
##  $ title             : Factor w/ 2 levels &amp;quot;Pride and Prejudice&amp;quot;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ tf_text_a         : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_a and     : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_a of      : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_about     : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_after     : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_again     : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_all       : num [1:12138] 0 0 0 0 1 0 0 0 0 0 ...
##  $ tf_text_am        : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##  $ tf_text_an        : num [1:12138] 0 0 0 0 0 0 0 0 0 0 ...
##   [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we are left with 101 features.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;For modeling we will be using the &lt;a href=&#34;https://github.com/tidymodels/parsnip&#34;&gt;parsnip&lt;/a&gt; package from tidymodels. First we start by defining a model specification. This defines the intent of our model, what we want to do, not what we want to do it on. Meaning we don‚Äôt include the data yet, just the kind of model, its hyperparameters and the engine (the package that will do the work). We will be be using glmnet package here so we will specify a logistic regression model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glmnet_model &amp;lt;- logistic_reg(mixture = 0, penalty = 0.1) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)
glmnet_model
## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0.1
##   mixture = 0
## 
## Computational engine: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we will fit the models using both our training data, first using the stop words, then using the simple would count approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopword_model &amp;lt;- glmnet_model %&amp;gt;%
  fit(title ~ ., data = stopword_train_data)

julia_model &amp;lt;- glmnet_model %&amp;gt;%
  fit(title ~ ., data = julia_train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the part of the workflow where one should do hyperparameter optimization and explore different models to find the best model for the task. For the interest of the length of this post will this step be excluded, possible to be explored in a future post üòâ.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;Now that we have fitted the data based on the training data we can evaluate based on the testing data set. Here we will use the parsnip functions &lt;code&gt;predict_class()&lt;/code&gt; and &lt;code&gt;predict_classprob()&lt;/code&gt; to give us the predicted class and predicted probabilities for the two models. Neatly collecting the whole thing in one tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_tibble &amp;lt;- stopword_test_data %&amp;gt;%
  select(title) %&amp;gt;%
  mutate(
    class_stopword = parsnip:::predict_class(stopword_model, stopword_test_data),
    class_julia    = parsnip:::predict_class(julia_model, julia_test_data),
    prop_stopword  = parsnip:::predict_classprob(stopword_model, stopword_test_data) %&amp;gt;% pull(`The War of the Worlds`),
    prop_julia     = parsnip:::predict_classprob(julia_model, julia_test_data) %&amp;gt;% pull(`The War of the Worlds`)
  )

eval_tibble
## # A tibble: 4,027 x 5
##    title           class_stopword     class_julia       prop_stopword prop_julia
##    &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;              &amp;lt;fct&amp;gt;                     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ The War of the W‚Ä¶         0.475      0.508
##  2 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ Pride and Prejud‚Ä¶         0.498      0.388
##  3 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ Pride and Prejud‚Ä¶         0.335      0.315
##  4 The War of the‚Ä¶ The War of the Wo‚Ä¶ The War of the W‚Ä¶         0.690      0.710
##  5 The War of the‚Ä¶ The War of the Wo‚Ä¶ The War of the W‚Ä¶         0.650      0.607
##  6 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ Pride and Prejud‚Ä¶         0.241      0.264
##  7 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ Pride and Prejud‚Ä¶         0.369      0.351
##  8 The War of the‚Ä¶ Pride and Prejudi‚Ä¶ The War of the W‚Ä¶         0.403      0.568
##  9 The War of the‚Ä¶ The War of the Wo‚Ä¶ The War of the W‚Ä¶         0.520      0.631
## 10 The War of the‚Ä¶ The War of the Wo‚Ä¶ The War of the W‚Ä¶         0.511      0.545
## # ‚Ä¶ with 4,017 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tidymodels includes the &lt;a href=&#34;https://github.com/tidymodels/yardstick&#34;&gt;yardstick&lt;/a&gt; package which makes evaluation calculations much easier and tidy. It can allow us to calculate the accuracy by calling the &lt;code&gt;accuracy()&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;accuracy(eval_tibble, truth = title, estimate = class_stopword)
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.778
accuracy(eval_tibble, truth = title, estimate = class_julia)
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.801&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we see that the stop words model beats the naive model (one that always picks the majority class), while lacking behind the word count model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_data %&amp;gt;%
  filter(text != &amp;quot;&amp;quot;) %&amp;gt;%
  summarise(mean(title == &amp;quot;Pride and Prejudice&amp;quot;))
## # A tibble: 1 x 1
##   `mean(title == &amp;quot;Pride and Prejudice&amp;quot;)`
##                                    &amp;lt;dbl&amp;gt;
## 1                                  0.662&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are also able to plot the ROC curve using &lt;code&gt;roc_curve()&lt;/code&gt;(notice how we are using the predicted probabilities instead of class) and &lt;code&gt;autoplot()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_tibble %&amp;gt;%
  roc_curve(title, prop_stopword) %&amp;gt;%
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-29-text-classification-with-tidymodels/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To superimpose both ROC curve we are going to tidyr our data a little bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_tibble %&amp;gt;%
  rename(`Word Count` = prop_julia, `Stopwords` = prop_stopword) %&amp;gt;%
  gather(&amp;quot;Stopwords&amp;quot;, &amp;quot;Word Count&amp;quot;, key = &amp;quot;Model&amp;quot;, value = &amp;quot;Prop&amp;quot;) %&amp;gt;%
  group_by(Model) %&amp;gt;%
  roc_curve(title, Prop) %&amp;gt;%
  autoplot() +
  labs(title = &amp;quot;ROC curve for text classification using word count or stopwords&amp;quot;,
       subtitle = &amp;quot;Predicting whether text was written by Jane Austen or H.G. Wells&amp;quot;) +
  paletteer::scale_color_paletteer_d(&amp;quot;ggsci::category10_d3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-29-text-classification-with-tidymodels/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I‚Äôm not going to tell you that you should run a ‚Äúall stop words only‚Äù model every-time you want to do text classification. But I hope this exercise shows you that stop words which are assumed to have no information does indeed have some degree on information. Please always look at your stop word list, check if you even need to remove them, some studies &lt;a href=&#34;http://www.cs.cornell.edu/~xanda/stopwords2017.pdf&#34;&gt;shows that removal of stop words might not provide the benefit you thought&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Furthermore I hope to have showed the power of tidymodels. Tidymodels is still growing, so if you have any feedback/bug reports/suggests please go to the respective repositories, we would highly appreciate it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;p&gt;This plot was suggested in the comments, Thanks Isaiah!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopword_model$fit %&amp;gt;% 
  tidy() %&amp;gt;%
  mutate(term = str_replace(term, &amp;quot;tf_text_&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
  group_by(estimate &amp;gt; 0) %&amp;gt;%
  top_n(10, abs(estimate)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &amp;gt; 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  theme_minimal() +
  labs(x = NULL,
  title = &amp;quot;Coefficients that increase/decrease probability the most&amp;quot;,
  subtitle = &amp;quot;Stopwords only&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-29-text-classification-with-tidymodels/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And Isaiah notes that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Whereas Julia‚Äôs analysis using non stop words showed that Elizabeth is the opposite of a Martian, stop words shows that Pride and Prejudice talks of men and women, and War of the Worlds makes declarations about existence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which I would like to say looks pretty spot on.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>usethis workflow for package development</title>
      <link>/2018/09/02/usethis-workflow-for-package-development/</link>
      <pubDate>Sun, 02 Sep 2018 00:00:00 +0000</pubDate>
      <guid>/2018/09/02/usethis-workflow-for-package-development/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-20.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There have been a lot of progress in the aid of package development in R in recent times. The classic blogpost by Hilary Parker &lt;a href=&#34;https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/&#34;&gt;Writing an R Package From Scratch&lt;/a&gt; and its younger sister &lt;a href=&#34;https://r-mageddon.netlify.com/post/writing-an-r-package-from-scratch/&#34;&gt;Writing an R package from scratch&lt;/a&gt; by Tomas Westlake are both great sources of information to create a package. Fo more general documentation on package development you would be right to look at Hadley Wickhams book &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R packages&lt;/a&gt;. The &lt;strong&gt;devtools&lt;/strong&gt; package have always been instrumental for good package development, but some of these features and additional ones are now to be found in the &lt;a href=&#34;https://github.com/r-lib/usethis&#34;&gt;usethis&lt;/a&gt; package. The &lt;strong&gt;usethis&lt;/strong&gt; promises to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚Ä¶ it automates repetitive tasks that arise during project setup and development, both for R packages and non-package projects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this blogpost I‚Äôll outline the basis workflow you can acquire using the tools in &lt;strong&gt;usethis&lt;/strong&gt;. More specifically I‚Äôll outline a workflow of a R package development. The course of any R package development can be broken down into these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before creation&lt;/li&gt;
&lt;li&gt;Creating minimal functional package&lt;/li&gt;
&lt;li&gt;One time modifications&lt;/li&gt;
&lt;li&gt;Multiple time modifications&lt;/li&gt;
&lt;li&gt;Before every commit&lt;/li&gt;
&lt;li&gt;Before every release&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before we start, I assume that you will be using Rstudio for this tutorial.&lt;/p&gt;
&lt;div id=&#34;before-creation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Before creation&lt;/h2&gt;
&lt;p&gt;Before we get started we need to make sure we have the essential packages installed to create a R package development workflow&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(c(&amp;quot;devtools&amp;quot;, &amp;quot;roxygen2&amp;quot;, &amp;quot;usethis&amp;quot;))
library(devtools)
library(roxygen2)
library(usethis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Side-note, if you are to create a R package, you need a name. It needs to be unique, especially if you plan on getting your package on CRAN. The &lt;strong&gt;available&lt;/strong&gt; package can help you evaluate possible names to make sure they don‚Äôt clash with other names and that they don‚Äôt mean something rude. For this example I‚Äôm going to make a horrible name by shortening the phrases ‚Äú&lt;strong&gt;u&lt;/strong&gt;se&lt;strong&gt;t&lt;/strong&gt;his &lt;strong&gt;w&lt;/strong&gt;ork&lt;strong&gt;f&lt;/strong&gt;low‚Äù&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(available)
available(&amp;quot;utwf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2019.34.04.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;the only acronym it finds is ‚ÄúUmauma Triple Water Falls‚Äù so we are good to go. Next we need to make sure that you have setup &lt;strong&gt;usethis&lt;/strong&gt;, for this section I‚Äôll refer to the original documentation &lt;a href=&#34;http://usethis.r-lib.org/articles/articles/usethis-setup.html&#34;&gt;usethis setup&lt;/a&gt; as it explains these steps better then I could.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-minimal-functional-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating minimal functional package&lt;/h2&gt;
&lt;p&gt;Now that you have followed the setup guide you are ready to create a minimal functional package.&lt;/p&gt;
&lt;p&gt;For creation we will use the &lt;code&gt;create_package()&lt;/code&gt; function to create a R package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_package(&amp;quot;~/Desktop/utwf&amp;quot;)
use_git()
use_github()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2016.39.18.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2016.41.05.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2016.42.53.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we are done! We now have a minimal R package, complete with Github repository. With these files included:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2016.56.20.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right now it doesn‚Äôt have much, in fact it doesn‚Äôt even have a single function in it. We can check that the package works by pressing ‚ÄúInstall and Restart‚Äù in the ‚ÄúBuild‚Äù panel. Alternatively you can use the keyboard shortcut Cmd+Shift+B (Ctrl+Shift+B for Windows).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-time-modifications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One time modifications&lt;/h2&gt;
&lt;p&gt;Now that we are up and running there is a bunch of things we should do before we start writing code. Firstly we will go over all the actions that only have to be done once and get those out of the way.&lt;/p&gt;
&lt;p&gt;Firstly we will go into the &lt;strong&gt;DESCRIPTION&lt;/strong&gt; file and make sure that the &lt;em&gt;&lt;a href=&#34;mailto:Authors@R&#34; class=&#34;email&#34;&gt;Authors@R&lt;/a&gt;&lt;/em&gt; is populated correctly and modify the &lt;em&gt;Title&lt;/em&gt; and &lt;em&gt;Description&lt;/em&gt; fields.&lt;/p&gt;
&lt;p&gt;Next we will license the package. This can be done using one of the following functions (we will use MIT for this example)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_mit_license()
use_gpl3_license()
use_apl2_license()
use_cc0_license()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.17.47.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Choice of which license you neeed is beyond the scope of this post. Please refer to the &lt;a href=&#34;http://r-pkgs.had.co.nz/description.html#license&#34;&gt;R Packages license section&lt;/a&gt; or &lt;a href=&#34;https://choosealicense.com/&#34;&gt;https://choosealicense.com/&lt;/a&gt; for further assistance.&lt;/p&gt;
&lt;p&gt;Now we add the &lt;em&gt;readme&lt;/em&gt; files, this is done using the&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_readme_rmd()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.39.55.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This will create a &lt;em&gt;readme.Rmd&lt;/em&gt; file that you can edit and knit as you normally would.&lt;/p&gt;
&lt;p&gt;Next we will setup some continuous integration. I‚Äôll recommend trying to do all of the 3 following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_travis()
use_appveyor()
use_coverage(type = c(&amp;quot;codecov&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.34.04.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These calls won‚Äôt do all the work for you, so you would have to follow the directions (following red circles) and turn on the services on the Travis and AppVeyor websites respectively, copy badges to the readme (typically placed right under the main title ‚Äú# utwf‚Äù) and copy the code snippet to the &lt;em&gt;.travis.yml&lt;/em&gt; file.&lt;/p&gt;
&lt;p&gt;You will most likely also want to include unit testing, this can be achieved using the &lt;a href=&#34;https://github.com/r-lib/testthat&#34;&gt;testthat&lt;/a&gt; package, to include the testing capasity of &lt;strong&gt;testthat&lt;/strong&gt; in your package simply run the following&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_testthat()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.29.02.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;you will need to add at least one test to avoid failed builds on Travis-ci and Appveyor. More information on how to do testing can be found at the &lt;a href=&#34;http://r-pkgs.had.co.nz/tests.html&#34;&gt;Testing&lt;/a&gt; chapter in the R packages book.&lt;/p&gt;
&lt;p&gt;Next we will add spell checking to our workflow, this is done with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_spell_check()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.22.40.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Make sure that the &lt;strong&gt;spelling&lt;/strong&gt; package is installed before running.&lt;/p&gt;
&lt;p&gt;If you are going to include data in your package, you would want to include a &lt;em&gt;data-raw&lt;/em&gt; folder where the data is created/formatted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_data_raw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.27.12.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly if you plan on doing a little larger project a &lt;em&gt;NEWS&lt;/em&gt; file is very handy to keep track on what is happening in your package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_news_md()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.45.27.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-time-modifications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple time modifications&lt;/h2&gt;
&lt;p&gt;Now that we have setup all the basics, the general development can begin.&lt;/p&gt;
&lt;p&gt;You typical workflow will be repeating the following steps in the order that suits your flow&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write some code&lt;/li&gt;
&lt;li&gt;Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)&lt;/li&gt;
&lt;li&gt;Build and Reload Cmd+Shift+B (Ctrl+Shift+B for Windows)&lt;/li&gt;
&lt;li&gt;Test Package Cmd+Shift+T (Ctrl+Shift+T for Windows)&lt;/li&gt;
&lt;li&gt;Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows)&lt;/li&gt;
&lt;li&gt;Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Writing code most likely includes writing functions, this is helped by the &lt;code&gt;use_r()&lt;/code&gt; function by adding and opening a .R file that you write your function in&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_r(&amp;quot;function_name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2017.57.38.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This function is very important and you will using it a lot, not only will it create the files you save your functions in, but it will also open the files if they are already created, this makes navigating your R files much easier. Once you have created your function it is time to add some tests! This is done using the &lt;code&gt;use_test()&lt;/code&gt; function, and it works much the same way as the &lt;code&gt;use_r()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_test(&amp;quot;function_name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2018.04.35.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the creating of your functions, you might need to depend on another package, to add a function to the &lt;em&gt;imports&lt;/em&gt; field in the &lt;em&gt;DESCRIPTION&lt;/em&gt; file you can use the &lt;code&gt;use_package()&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_package(&amp;quot;dplyr&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2018.03.21.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Special cases function includes &lt;code&gt;use_rcpp()&lt;/code&gt;, &lt;code&gt;use_pipe()&lt;/code&gt; and &lt;code&gt;use_tibble()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An vignette provides a nice piece of documentation once you have added a bunch of capabilities to your package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_vignette(&amp;quot;How to do this cool analysis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2018.08.50.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;before-every-commit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Before every commit&lt;/h2&gt;
&lt;p&gt;Before you commit, run the following commands one more time to make sure you didn‚Äôt break anything.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)&lt;/li&gt;
&lt;li&gt;Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows)&lt;/li&gt;
&lt;li&gt;Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;before-every-release&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Before every release&lt;/h2&gt;
&lt;p&gt;You have worked and have created something wonderful. You want to showcase the work. First go knit the &lt;em&gt;readme.Rmd&lt;/em&gt; file and then run these commands again to check that everything is working.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restart R Session Cmd+Shift+F10 (Ctrl+Shift+F10 for Windows)&lt;/li&gt;
&lt;li&gt;Document Package Cmd+Shift+D (Ctrl+Shift+D for Windows)&lt;/li&gt;
&lt;li&gt;Check Package Cmd+Shift+E (Ctrl+Shift+E for Windows)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;update the version number with the use of&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_version()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Screen%20Shot%202018-09-02%20at%2019.16.46.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And you are good to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is the end of this post, and there are many more functions in &lt;strong&gt;usethis&lt;/strong&gt; that I haven‚Äôt covered here, both for development and otherwise. One set of functions I would like to highlight in particular is the &lt;a href=&#34;http://usethis.r-lib.org/reference/tidyverse.html&#34;&gt;Helpers for tidyverse development&lt;/a&gt; which helps you follow tidyverse conventions which are generally a little stricter than the defaults. If you have any questions or additions you would like to have added please don‚Äôt refrain from contacting me!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What are the reviews telling us?</title>
      <link>/2018/08/17/what-are-the-reviews-telling-us/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/2018/08/17/what-are-the-reviews-telling-us/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-20.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this post we will look at a handful of movies reviews from &lt;a href=&#34;https://www.imdb.com/&#34;&gt;imdb&lt;/a&gt; which I have scraped and placed in this repository &lt;a href=&#34;https://github.com/EmilHvitfeldt/movie-reviews&#34;&gt;movie reviews&lt;/a&gt;. I took a look at the best and worst rated movies with their best and worst reviews respectively. From that we will try to see if we are able to see how positive reviews on good movies are different then positive reviews on bad movies and so on.&lt;/p&gt;
&lt;p&gt;We will use fairly standard packages with the inclusion of &lt;strong&gt;paletteer&lt;/strong&gt; for the sole reason of self promotion. (yay!!!)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(plotly)
library(paletteer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we will read in the data using &lt;strong&gt;readr&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reviews_raw &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/EmilHvitfeldt/movie-reviews/master/reviews_v1.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets take a look at the data I prepared for us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(reviews_raw)
## Rows: 9,764
## Columns: 7
## $ text          &amp;lt;chr&amp;gt; &amp;quot;It is a very boring and weird movie. Watch it only if ‚Ä¶
## $ id            &amp;lt;chr&amp;gt; &amp;quot;tt0012349&amp;quot;, &amp;quot;tt0012349&amp;quot;, &amp;quot;tt0012349&amp;quot;, &amp;quot;tt0012349&amp;quot;, &amp;quot;tt‚Ä¶
## $ review_rating &amp;lt;chr&amp;gt; &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;bad&amp;quot;,‚Ä¶
## $ title         &amp;lt;chr&amp;gt; &amp;quot;The Kid&amp;quot;, &amp;quot;The Kid&amp;quot;, &amp;quot;The Kid&amp;quot;, &amp;quot;The Kid&amp;quot;, &amp;quot;The Kid&amp;quot;, ‚Ä¶
## $ rating        &amp;lt;dbl&amp;gt; 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, 8.3, ‚Ä¶
## $ url           &amp;lt;chr&amp;gt; &amp;quot;https://www.imdb.com/title/tt0012349/&amp;quot;, &amp;quot;https://www.i‚Ä¶
## $ movie_rating  &amp;lt;chr&amp;gt; &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;, &amp;quot;good&amp;quot;,‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It include 7 different variables. There is some redundancy, the &lt;code&gt;url&lt;/code&gt; variable contains the url of the movie, and &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;title&lt;/code&gt; are just the extracts from the &lt;code&gt;url&lt;/code&gt; variable. The &lt;code&gt;rating&lt;/code&gt; variable is the average rating of the movie and will not be used in this analysis. Lastly we have the &lt;code&gt;review_rating&lt;/code&gt; and &lt;code&gt;movie_rating&lt;/code&gt; which will denote if the review is positive or negative and if the movie being reviewed is good or bad respectively.&lt;/p&gt;
&lt;p&gt;Lets start by unnesting the words and get the counts. We also don‚Äôt want to look at all the stopwords and words that contains numbers, this it likely not a great number of words but we will exclude them for now anyways.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counted_words &amp;lt;- unnest_tokens(reviews_raw, word, text) %&amp;gt;%
  count(word, movie_rating, review_rating) %&amp;gt;%
  anti_join(stop_words, by = &amp;quot;word&amp;quot;) %&amp;gt;%
  filter(!str_detect(word, &amp;quot;\\d&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And lets have a quick looks at the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counted_words %&amp;gt;% arrange(desc(n)) %&amp;gt;% head(n = 15)
## # A tibble: 15 x 4
##    word   movie_rating review_rating     n
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;
##  1 movie  bad          good           7504
##  2 movie  bad          bad            7426
##  3 movie  good         bad            5692
##  4 movie  good         good           5507
##  5 film   good         good           4701
##  6 film   good         bad            3926
##  7 film   bad          bad            3243
##  8 film   bad          good           3023
##  9 bad    bad          bad            2080
## 10 time   good         good           1757
## 11 story  good         good           1496
## 12 people bad          good           1409
## 13 time   good         bad            1387
## 14 people good         bad            1292
## 15 time   bad          bad            1263&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we notice that the word &lt;em&gt;movie&lt;/em&gt; have been used quite a lot more in reviews of bad movies then in good movies.&lt;/p&gt;
&lt;div id=&#34;log-odds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Log odds&lt;/h2&gt;
&lt;p&gt;We have a bunch of counts here and we would like to find a worthwhile transformation of them. Since we have the number of reviews for good movies and bad movies we would be able to find the percentage of words appearing in good movies. This would give us a number between 0 and 1, where the interesting words would be when the percentage is close to 0 and 1 as it would show that the word is being used more in one than another.&lt;/p&gt;
&lt;p&gt;By doing this transformation to both the review scores and movie scores will give us the following plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counted_words %&amp;gt;%
  mutate(rating = str_c(movie_rating, &amp;quot;_&amp;quot;, review_rating)) %&amp;gt;%
  select(-movie_rating, -review_rating) %&amp;gt;%
  spread(rating, n) %&amp;gt;%
  drop_na() %&amp;gt;%
  mutate(review_lo = (bad_good + good_good) / (bad_bad + good_bad + bad_good + good_good),
         movie_lo = (good_bad + good_good) / (bad_bad + bad_good + good_bad + good_good)) %&amp;gt;%
  ggplot() +
  aes(movie_lo, review_lo) +
  geom_text(aes(label = word))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-14-what-are-the-reviews-telling-us/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to do this is to take the log of the odds of one event happening over the other event. We will create this little helper function for us.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_odds &amp;lt;- function(x, y) {
  total &amp;lt;- x + y
  p &amp;lt;- x / total
  log(p / (1 - p))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;applying this transformation instead expands the the limit from 0 to 1 to the whole number range where the midpoint is 0, this has some nice properties from a visualization perspective, it will also compact the center points a little more allowing outliers to be more prominent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data &amp;lt;- counted_words %&amp;gt;%
  mutate(rating = str_c(movie_rating, &amp;quot;_&amp;quot;, review_rating)) %&amp;gt;%
  select(-movie_rating, -review_rating) %&amp;gt;%
  spread(rating, n) %&amp;gt;%
  drop_na() %&amp;gt;%
  mutate(review_lo = log_odds(bad_good + good_good, bad_bad + good_bad),
         movie_lo = log_odds(good_bad + good_good, bad_bad + bad_good))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data %&amp;gt;%
  ggplot() +
  aes(movie_lo, review_lo, label = word) +
  geom_text()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-14-what-are-the-reviews-telling-us/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have a good degree of over plotting in this plot, but part of that might be because of the text, a quick look at the scatterplot still reveals a good deal of overplotting. We will try to counter that later on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data %&amp;gt;%
  ggplot() +
  aes(movie_lo, review_lo) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-14-what-are-the-reviews-telling-us/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lets stay in the in the scatterplot. Lets tighten up the theme and include guidelines at y = 0 and x = 0. We will also find the range of the data to make sure we include all the points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data %&amp;gt;% 
  select(movie_lo, review_lo) %&amp;gt;%
  range()
## [1] -4.574711  3.970292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data %&amp;gt;%
  ggplot() +
  aes(movie_lo, review_lo) +
  geom_vline(xintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_hline(yintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  coord_cartesian(ylim = c(-4.6, 4.6),
                  xlim = c(-4.6, 4.6)) +
  labs(x = &amp;quot;‚Üê Bad Movies - Good Movies ‚Üí&amp;quot;, y = &amp;quot;‚Üê Bad Reviews - Good Reviews ‚Üí&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-14-what-are-the-reviews-telling-us/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We still have quite a bit of over plotting, I‚Äôm going to sample the points based on importance. The importance matrix I‚Äôm going to work with is the distance from the middle. In addition we are going to display the number of times a word is used by the size of the points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(13)
plot_data_v2 &amp;lt;- plot_data %&amp;gt;%
  mutate(distance = review_lo ^ 2 + movie_lo ^ 2,
         n = bad_bad + bad_good + good_bad + good_good) %&amp;gt;%
  sample_frac(0.1, weight = distance)

plot_data_v2 %&amp;gt;%  
  ggplot() +
  aes(movie_lo, review_lo, size = n) +
  geom_vline(xintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_hline(yintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  coord_cartesian(ylim = c(-4.6, 4.6),
                  xlim = c(-4.6, 4.6)) +
  labs(x = &amp;quot;‚Üê Bad Movies - Good Movies ‚Üí&amp;quot;, y = &amp;quot;‚Üê Bad Reviews - Good Reviews ‚Üí&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-14-what-are-the-reviews-telling-us/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly we will make the whole thing interactive with &lt;strong&gt;plotly&lt;/strong&gt; to allow hover text. We include some color to indicate distance to the center.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- plot_data_v2 %&amp;gt;%  
  ggplot() +
  aes(movie_lo, review_lo, size = n, color = distance, text = word) +
  geom_vline(xintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_hline(yintercept = 0, color = &amp;quot;grey&amp;quot;) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  coord_cartesian(ylim = c(-4.6, 4.6),
                  xlim = c(-4.6, 4.6)) +
  labs(x = &amp;quot;‚Üê Bad Movies - Good Movies ‚Üí&amp;quot;, 
       y = &amp;quot;‚Üê Bad Reviews - Good Reviews ‚Üí&amp;quot;,
       title = &amp;quot;What are people saying about the best and worst movies on IMDB?&amp;quot;) +
  scale_color_paletteer_c(&amp;quot;viridis::viridis&amp;quot;) +
  guides(color = &amp;quot;none&amp;quot;, size = &amp;quot;none&amp;quot;)

ggplotly(p, width = 700, height = 700, displayModeBar = FALSE,
         tooltip = &amp;quot;text&amp;quot;) %&amp;gt;% 
  config(displayModeBar = F)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:700px;height:700px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;x&#34;:[0,0],&#34;y&#34;:[-5.06,5.06],&#34;text&#34;:&#34;&#34;,&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(190,190,190,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[-5.06,5.06],&#34;y&#34;:[0,0],&#34;text&#34;:&#34;&#34;,&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(190,190,190,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[0.0540672212702756,2.01490302054226,1.20397280432594,2.42353770341171,0.405465108108164,3.70306408910589,-0.78845736036427,0.857450231851222,0.628608659422374,3.22457384598284,-0.154150679827258,-0.405465108108164,0.336472236621213,0.328504066972036,-1.3054422644908,1.65822807660353,1.70474809223843,1.17865499634165,-0.0606246218164347,0.348306694268216,-2.484906649788,-0.200670695462151,-0.847297860387204,1.25276296849537,2.44234703536921,0.773189888233482,3.17805383034794,-1.09861228866811,-0.680243775724037,-0.500775287912489,1.38629436111989,1.6094379124341,0.0741079721537218,0.916290731874155,-1.04982212449868,-0.474457979595116,1.20397280432594,-3.09104245335832,-1.02450431651439,-2.81839825827108,0.916290731874155,-2.11021320034659,0.510825623765991,-0.916290731874155,-1.24078677744865,-0.0392207131532813,-2.33537491581704,1.57553636075842,1.09861228866811,0.897941593205959,1.3415584672785,-0.693147180559945,-2.90872089656436,-0.182321556793955,2.69848075008606,-0.713214743610755,-2.38122822031316,1.79175946922805,1.76358859226136,1.67397643357167,1.17865499634165,-1.32175583998232,0.916290731874155,0.22314355131421,-1.70474809223843,1.36948724280351,-1.6094379124341,-2.75153531304195,1.32175583998232,-1.6094379124341,1.38629436111989,1.90954250488444,-1.70474809223843,1.50407739677627,0.336472236621213,0.650587566141149,2.01490302054226,0.117783035656383,-2.20727491318972,-1.19392246847243,-1.38629436111989,-1.55814461804655,0.613104472886409,0.657587878523458,1.07535542650384,-1.29928298413026,0.394654192003949,0.993251773010283,1.79175946922805,0.133531392624523,1.20397280432594,-0.63907995928967,1.25276296849537,-0.479573080261886,3.3787245258101,1.38629436111989,1.20397280432594,0.287682072451781,-0.693147180559945,-0.792238083204176,0.470003629245736,-0.916290731874155,-0.957839734787027,-0.22314355131421,-0.0786431273191131,-4.57471097850338,1.36919992776059,2.484906649788,0.693147180559945,-1.09861228866811,-1.68639895357023,-1.17007125265025,-2.07944154167984,1.50737229267313,-0.0689928714869513,2.14006616349627,-0.470003629245736,1.09861228866811,-0.965080896043587,0.693147180559945,-1.87180217690159,1.6094379124341,-1.25276296849537,-0.287682072451781,-1.53623450841081,3.44468249360189,1.99809590222588,-2.91568956588045,0.8754687373539,0.405465108108164,-2.28577797467766,-0.826678573184468,1.87180217690159,2.2512917986065,-0.125163142954006,-0.523248143764548,0.693147180559945,-0.451985123743057,-0.653926467406664,2.10006082888257,1.74296930505862,-2.54553127160444,-1.49065437644413,0.78845736036427,-1.6094379124341,-0.367724780125317,-2.4423470353692,-0.331357135954442,0,0.575364144903562,-2.4423470353692,-1.54044504094715,-0.405465108108164,0.916290731874155,1.67397643357167,1.5114575040739,0.117783035656383,0,0.955511445027436,0.725937003382936,1.46633706879343,0.757685701697516,-1.25276296849537,0.893817876022097,2.56494935746154,0.693147180559945,3.3499040872746,0.78845736036427,0,0.113328685307003,1.50407739677627,-0.916290731874155,1.09861228866811,1.82253112789481,-1.46633706879343,-4.28358656186063,-0.0444517625708338,0.510825623765991,0.8754687373539,-0.451985123743057,-0.510825623765991,-3.25809653802148,-0.268263986594679,0.405465108108164,-2.484906649788,0.916290731874155,2.72457950305342,-1.25276296849537,2.75153531304195,-0.305381649551182,1.6094379124341,1.09861228866811,-1.36524095192206,-3.23867845216438,0.693147180559945,-0.510825623765991,0.798507696217772,0.613104472886409,-0.287682072451781,1.68639895357023,0.60077386042893,0.405465108108164,0.318453731118535,0.916290731874155,-2.9704144655697,0.980829253011726,-1.22377543162212,0,-1.62225860086316,0.0923733201310153,-1.40477917579399,0.798507696217772,-0.710846757659346,3.25809653802148,1.1314021114911,-2.59026716544583,-1.38629436111989,1.33850369728354,0.405465108108164,1.38629436111989,-1.94591014905531,-0.525266307920785,0.517943091534855,-0.693147180559945,2.32727770558442,1.09861228866811,1.94591014905531,-0.606135803570315,0,1.38629436111989,1.07451473708905,1.75785791755237,0.916290731874155,0.217723483844871,-0.693147180559945,-0.352821374622742,1.09861228866811,1.79175946922805,-0.955511445027436,-1.69773051957978,0.712949807856125,1.87180217690159,1.05314991459135,3.10608033072286,1.46633706879343,1.48807705542983,-1.04145387482816,0.318453731118535,2.16905370036952,1.22377543162212,-1.42138568093116,2.69462718077007,1.6094379124341,-0.798507696217772,0.72054615474806,0.857450231851222,-1.7730673362159,3.58351893845611,-2.12823170584927,0.82098055206983,0.916290731874155,1.11861295537478,1.25276296849537,-0.336472236621213,1.16315080980568,2.13162729485041,-0.802346472524937,0.336472236621213,-0.502091943797236,2.96183072187831,-1.09861228866811,1.1239300966524,1.50407739677627,-1.46633706879343,-0.543615446588982,0.903970247486115,0,-0.99212880826566,-2.60268968544438,-0.693147180559945,-1.04982212449868,3.25809653802148,-2.35137525716348,-0.559615787935423,2.34180580614733,1.70474809223843,2.75684036527164,1.75401914124521,1.09861228866811,2.52572864430826,-0.405465108108164,0.980829253011726,-2.89037175789616,1.09861228866811,-1.67397643357167,3.97029191355212,2.80336038090653,0.105360515657826,2.16905370036952,0,3.73766961828337,2.81626378574244,0.405465108108164,0.980829253011726,2.44234703536921,-1.29928298413026,0.412532275331257,2.11021320034659,0.810930216216329,-3.09516617054218,1.87180217690159,1.25276296849537,0.22314355131421,-2.56494935746154,3.66356164612965,1.25276296849537,-1.42222637034595,0.133531392624523,0.362335747211132,-0.693147180559945,1.33500106673234,-0.22314355131421,-0.339867825622351,-0.287682072451781,1.7227665977411,2.65675690671466,0.133531392624523,1.15267950993839,0.8754687373539,1.46633706879343,1.46343009000212,1.04145387482816,0.182321556793954,0.955511445027436,-1.38629436111989,1.19770319131234,-0.916290731874155,1.50407739677627,2.39789527279837,-0.405465108108164,-0.252152651297294,0.510825623765991,1.54044504094715,-1.94591014905531,-0.16799173123913,-1.87180217690159,-0.182321556793955,2.60268968544438,0.287682072451781,1.68639895357023,0,1.29928298413026,-0.510825623765991,0.693147180559945,-2.04769284336526,-0.182321556793955,-0.287682072451781,0.281851152140987,-1.70474809223843,-0.385662480811985,1.50407739677627,2.83321334405622,0.635988766719997,1.50407739677627,3.2188758248682,-2.58399755243223,1.38629436111989,1.20397280432594,0.470003629245736,-0.251314428280906,-1.29928298413026,0,1.27686052007443,3.40119738166216,1.6094379124341,0.271933715483642,2.19722457733622,1.54044504094715,0.287682072451781,0.653926467406664,0.825074723602493,3.1267605359604,1.09861228866811,1.32175583998232,0.980829253011726,0.980829253011726,1.74919985480926,0,1.01160091167848,-1.20397280432594,1.09861228866811,0.133531392624523,0.693147180559945,1.18199389760716,0.22314355131421,-1.70474809223843,1.09861228866811,-0.810930216216329,-0.405465108108164,2.09714111877924,-0.587786664902119,-2.11453286149111,0.955511445027436,0.916290731874155,1.33500106673234,0,0.559615787935423,1.09861228866811,0.693147180559945,-0.631271776841858,1.65292302437384,-1.29532258291416,1.22377543162212,1.84582669049833,0.405465108108164,1.29928298413026,2.39789527279837,3.6603513704994,-2.07944154167984,0,-2.92673940206704,2.35137525716348,1.28519824424852,0.606135803570316,-1.43508452528932,-0.908855753386637,0.980829253011726,2.42774823594805,2.8233610476132,1.22866541691631,-0.510825623765991,0.281639757995818,2.14006616349627,-0.267957567104002,0.520534437892952,-0.38193461069797,1.44691898293633,-0.154150679827258,1.50407739677627,0.63907995928967,-1.8294997972109,2.00148000021012,-0.916290731874155,-1.92896059074154,0.82098055206983,0.955511445027436,0.693147180559945,0,-1.09861228866811,1.29392104098888,-1.04145387482816,-1.07755887947028,1.6094379124341,1.48538526376412,-0.916290731874155,-0.693147180559945,0.559615787935423,1.74919985480926,0,0,-1.38629436111989,1.79175946922805,2.49869997192034,1.58412010444981,1.32175583998232,0.780158557549575,0.693147180559945,0.847297860387203,-2.51230562397611,0.356674943938732,1.46633706879343,0.510825623765991,-1.64865862558738,-0.251314428280906,-2.35137525716348,1.22377543162212,1.6094379124341,0.847297860387203,-0.59783700075562,0.606135803570316,1.25276296849537,0.0800427076735366,0.773189888233482,-0.8754687373539,3.34403896782221,-1.64412347042199,-2.2512917986065,-0.810930216216329,-1.72474875894509,2.67414864942653,1.09861228866811,0.559615787935423,1.67397643357167,1.40534255609058,-1.22377543162212,-0.287682072451781,3.10608033072286,1.50407739677627,1.38629436111989,-2.39789527279837,-0.133531392624523,-1.49165487677772,0.916290731874155,0.405465108108164,1.20397280432594,2.07944154167984,0.133531392624523,-2.30258509299405,-0.22314355131421,-1.44691898293633,0.676886659688165,-0.693147180559945,1.29928298413026,-0.905708622543618,-0.356674943938732,-0.352821374622742,3.00403107636869,2.01490302054226,2.23804657185647,-0.880358722648092,0.296020417368935,0.59783700075562,2.42036812865043,0.693147180559945,3.58351893845611,1.34117392583942,-0.22314355131421,1.29928298413026,0.641853886172395,-0.693147180559945,0.78845736036427,0.798507696217772,0.587786664902119,2.15176220325946,-0.980829253011726,-0.916290731874155,-0.528969875889394,1.6094379124341,0.693147180559945,0.896088024556636,-1.84582669049833,0.44558510189759,2.30258509299405,0.975131231897089,1.29928298413026,1.09861228866811,1.6094379124341,0.980829253011726,-2.79320800944252,1.6094379124341,2.54206506362795,-1.41369333530801,0.430782916092454,1.42996942462255,0.916290731874155,0.356674943938732,-1.99243016469021,0.933288308242726,0.498991166118988,-0.998528830111127,-1.44036158239017,0.916290731874155,0.1633250561033,-0.287682072451781,-0.22314355131421,-0.916290731874155,1.30992138233532,-0.372809145134112,-1.38629436111989,1.46633706879343,-0.287682072451781,2.32060359849672,2.01490302054226,0.336472236621213,0.510825623765991,0.823200308808143,0.980829253011726,-1.019831410815,-0.916290731874155,1.02961941718116,0,1.28093384546206,3.33220451017521,1.43508452528932,0.234281295724666,-1.50407739677627,-1.20397280432594,2.00372971994414,1.23214368129263,-0.510825623765991,2.15948424935337,0.831297519040763,-0.78845736036427,-0.893817876022096,-2.67797274586493,1.09861228866811,2.33537491581704,0.693147180559945,-1.25276296849537,1.06087196068526,2.2512917986065,0.538996500732687,-0.570544858467613,0.693147180559945,-0.575364144903562,0.55206858230004,0.1633250561033,-2.19722457733622,-2.11021320034659,-1.42138568093116,1.17865499634165,-2.484906649788,-0.959775843813894,2.39789527279837,0.176930708159078,-2.30258509299405,-1.73460105538811,1.09861228866811,2.33939906611676,1.67397643357167,0.78845736036427,0.635988766719997,0.635988766719997,-0.916290731874155,1.20397280432594,0.405465108108164,0.693147180559945,1.6094379124341,3.16406758837321,2.41591377830105,-0.405465108108164,-0.405465108108164,1.99243016469021,-1.09861228866811,-1.09861228866811,-3.15700042115011,-0.405465108108164,1.09861228866811,1.16315080980568,2.86931834869833,1.019831410815,0.693147180559945,0.693147180559945,0.700473220652018,-0.926762031741451,1.29037423924115,1.34992671694902,-0.628608659422374,1.84582669049833,-2.96183072187831,-0.693147180559945,1.76358859226136,0.382992252256106,1.38629436111989,1.70474809223843,1.38629436111989,0.54654370636807,1.01160091167848,0.105360515657826,1.32175583998232,0.693147180559945,-1.64865862558738,0.847297860387203,1.17865499634165,-0.428995605518359,-0.117783035656384,0.693147180559945,-0.133531392624523,1.09861228866811,-2.52572864430826,0.182321556793954,1.145132304303,2.86220088092947,0.405465108108164,-0.129211731480006,-1.09861228866811,-0.451985123743057,2.70805020110221,-0.916290731874155,-1.38629436111989,3.29583686600433,-2.14006616349627,1.56861591791385,-1.0055218656021,1.65292302437384,-0.510825623765991,1.98100146886658,2.085998942226,-0.470003629245736,0.510825623765991,1.25276296849537,2.35137525716348,0.587786664902119,0.664976303593249,2.44234703536921,0,0.593774706746742,0.405465108108164,-0.0540672212702757,-3.41224721784874,-0.287682072451781,1.6094379124341,-0.405465108108164,0.356674943938732,1.09861228866811,-1.50407739677627,-0.251314428280906,0.00836824967051658,1.38629436111989,0.916290731874155,2.07944154167984,2.65926003693278,0.133531392624523,-0.336472236621213,0.510825623765991,2.87919845729804,0.587786664902119,-1.6094379124341,-1.17865499634165,0.8754687373539,0.887303195000903,0.693147180559945,0.200670695462151,0.929535958624175,0.451985123743057,0.606135803570316,-0.470003629245736,-1.09861228866811,0.606135803570316,-0.693147180559945,2.83321334405622,0.332133835022615,1.25276296849537,2.91777073208428,-2.00976162104185,-0.980829253011726,0.980829253011726,0.356674943938732,0.887303195000903,0,1.00458333901983,-0.356674943938732,-1.79175946922805,-1.01160091167848,-0.810930216216329,1.21639532432449,-0.405465108108164,-1.36330484289519,0,-0.117783035656384,0.405465108108164,-0.451985123743057,1.25276296849537,-1.38629436111989,0.693147180559945,1.09861228866811,1.70474809223843,-1.09861228866811,1.25276296849537,0.22314355131421,0.139761942375158,0.693147180559945,0.628608659422374,-1.25276296849537,-1.38629436111989,1.50407739677627,1.55973824388183,2.41358186896607,1.21924027645672,-1.09861228866811,1.6094379124341,0.772636331618184,1.09861228866811,3.03495298670727,0.0165293019512105,1.20896034583697,-1.20397280432594,-1.32954154452744,-1.24782468685479,0.916290731874155,-1.04982212449868,0.587786664902119,0.538996500732687,0,-0.847297860387204,-0.614366302706831,1.13497993283898,1.29928298413026,0.8754687373539,-0.510825623765991,0.405465108108164,0.510825623765991,1.84582669049833,-0.22314355131421,-0.693147180559945,0.847297860387203,-1.02165124753198,-0.693147180559945,-1.79175946922805,1.76958561173373,0.510825623765991,0.78845736036427,-0.356674943938732,-0.508322493547872,-0.693147180559945,0.847297860387203,1.38629436111989,-1.85389125033506,1.49009115480153],&#34;y&#34;:[1.13497993283898,0.479573080261886,-1.20397280432594,1.40089316054104,-0.405465108108164,0.49801666547341,1.09861228866811,0.567984037605939,0.534082485930258,0.319633672258383,-1.20397280432594,0.693147180559945,0.693147180559945,-1.32913594727994,0.153509859555409,0.575364144903562,0.470003629245736,-0.606135803570315,0.832909122935104,-1.145132304303,-0.470003629245736,-1.38629436111989,0.847297860387203,-0.693147180559945,1.38629436111989,1.02961941718116,0.944461608840851,1.09861228866811,-1.69459572077441,-1.15820438587036,0.847297860387203,0.405465108108164,0.693147180559945,0.287682072451781,-0.955511445027436,2.61006979274201,0.998528830111127,0.262364264467491,-0.189241999638528,0.169418151958047,-0.916290731874155,-0.0540672212702757,-0.200670695462151,-0.916290731874155,-0.0748012130826984,-0.693147180559945,-0.737598943130779,0.916290731874155,0,-0.773189888233482,0.324049716622656,-1.51982575374441,-0.27763173659828,-0.182321556793955,-2.20727491318972,-1.38629436111989,-0.602175402354219,0,1.1314021114911,2.14006616349627,0.8754687373539,-0.246860077931526,0.287682072451781,-1.25276296849537,0.470003629245736,0.7339691750802,-1.01160091167848,0.241162056816888,1.02961941718116,0,-1.38629436111989,0.0645385211375712,-0.310154928303839,1.84582669049833,1.6094379124341,-0.0571584138399485,1.02165124753198,-1.54044504094715,-0.266691796559945,-0.233614851181505,-0.171850256926659,-0.0870113769896297,-0.0540672212702757,1.4743052384426,0.318453731118535,0,-0.448950220047903,1.28785428830664,0.435318071257845,-0.133531392624523,-1.70474809223843,0.0727593542824283,-2.07944154167984,0.479573080261886,0.516690743218389,1.18958406687384,-0.470003629245736,-0.916290731874155,0,0.394654192003949,1.20397280432594,-0.916290731874155,0.253448900809539,1.25276296849537,-1.74663903394759,-0.329479201130242,1.32720544474988,-0.470003629245736,-1.09861228866811,-0.302280871872934,0,-2.4567357728213,-0.693147180559945,0.176398538490832,-0.965080896043587,0.773189888233482,-0.470003629245736,1.09861228866811,-0.348306694268216,0,1.87180217690159,1.90423745265474,-0.22314355131421,1.29928298413026,0.336472236621213,0.616774201775371,1.07880966137193,-0.102857385439708,-0.117783035656384,0.405465108108164,0.0923733201310153,-0.0870113769896297,0.405465108108164,0.485507815781701,-0.78845736036427,-0.328504066972036,0.405465108108164,-1.25276296849537,-1.48807705542983,0.481838086892738,1.74296930505862,0.255933374137201,-0.117783035656384,-0.78845736036427,0.955511445027436,0.679541528504167,2.19722457733622,-1.15923691048454,-1.38629436111989,1.01160091167848,-0.75377180237638,-0.8754687373539,-0.405465108108164,0.916290731874155,0.538996500732687,0.569094531889966,1.17865499634165,1.09861228866811,-0.693147180559945,-0.262364264467491,0.379489621704904,-0.387765531008763,0.22314355131421,-0.325422400434628,0.143100843640673,-0.693147180559945,0.630233355149376,-0.510825623765991,-0.693147180559945,0.664976303593249,0.182321556793954,-0.916290731874155,-0.510825623765991,0.706219262127298,-1.94591014905531,-0.259957524436926,0.594707107746693,-1.38629436111989,-0.117783035656384,1.6094379124341,1.09861228866811,0,-1.87180217690159,-0.405465108108164,-0.635988766719997,-0.287682072451781,0.0718257345712558,-0.22314355131421,0,-0.0606246218164347,0.22314355131421,1.09861228866811,-2.17853244432407,-0.189241999638528,1.09861228866811,0.510825623765991,1.145132304303,-1.13497993283898,-0.916290731874155,-0.510825623765991,-1.6094379124341,-0.241162056816888,-1.02961941718116,0.916290731874155,-0.245122458032985,-0.559615787935423,-0.559615787935423,-0.916290731874155,-0.170625517030763,-0.341749293722057,-1.1623789070271,-1.56861591791385,-0.260283098263666,1.6094379124341,-0.34484048629173,-0.728238500371215,0.405465108108164,0.43324467221524,0.405465108108164,0.405465108108164,-0.887303195000903,-2.86220088092947,0.268263986594679,0.693147180559945,-0.22314355131421,0.510825623765991,0.78845736036427,1.54044504094715,-1.46633706879343,1.38629436111989,-1.07451473708905,0.356674943938732,0.287682072451781,0.958850346292951,-0.693147180559945,-0.765467842139572,-0.510825623765991,1.16315080980568,-1.25276296849537,0.385662480811985,-0.105360515657827,0.769133087537867,0.492476485097794,-0.714653385780909,0.251314428280906,0.897941593205959,1.04145387482816,-0.693147180559945,1.51982575374441,-0.559615787935423,-0.693147180559945,0.228841572428847,0,0.641853886172395,-1.27629346590556,1.39518330853714,-1.53393035992596,0.162518929497775,0.127833371509885,1.25276296849537,0.287682072451781,0.393042588109607,0.498991166118988,0.693147180559945,-0.287682072451781,-0.121360857004267,-0.485507815781701,1.09861228866811,0.230523658611832,0.870828357797398,-1.09861228866811,0.750305594399894,0.832909122935104,0,-0.818310323513951,0.710846757659346,0.916290731874155,-0.432864082296279,-0.0689928714869513,0,-0.610909082322973,-0.0741079721537221,-0.628608659422374,-0.980829253011726,0.390866308687012,0.154150679827258,1.7404661748405,1.03407376753054,-0.405465108108164,0.610909082322973,-0.405465108108164,0.367724780125318,-0.158224005214894,-1.09861228866811,-0.105360515657827,0.864997437486605,-0.916290731874155,-0.693147180559945,1.06471073699243,-0.693147180559945,0.891998039305111,-0.562785362696702,-0.405465108108164,-0.182321556793955,0.405465108108164,0,-1.58923520511658,0.0540672212702756,1.20397280432594,0.498246841559131,0.847297860387203,0.693147180559945,1.25276296849537,0.287682072451781,0.63740519755171,-0.693147180559945,-0.0157981168765913,-0.693147180559945,0.0932574934865829,-2.2512917986065,0.336472236621213,1.25276296849537,-0.723000143709626,-0.916290731874155,0.0606246218164348,1.40691364832263,1.01160091167848,0.75377180237638,0.606135803570316,0.251314428280906,1.66139765136481,0,1.22377543162212,0.22314355131421,-0.133531392624523,2.18122423598978,0.287682072451781,0.559615787935423,0.78845736036427,0.405465108108164,-0.0924750857648483,0,0.606135803570316,1.46633706879343,-0.839329690738027,-0.405465108108164,-0.182321556793955,0.492476485097794,0.916290731874155,0.125163142954006,0.251314428280906,0.287682072451781,-1.09861228866811,-1.09861228866811,-0.526093095896779,-0.980829253011726,-0.916290731874155,0.447312218043665,-0.470003629245736,-1.25276296849537,0.980829253011726,0.955511445027436,0.810930216216329,2.30258509299405,0.154150679827258,-0.576887374444083,0.693147180559945,0.998528830111127,1.20397280432594,0.78845736036427,-2.12026353620009,-1.46633706879343,0.10648348040245,0.816761136527122,0.111225635110224,-0.271933715483642,0.475423696715075,0.356674943938732,-0.916290731874155,0.897941593205959,-0.219628609206765,0.322083499169114,-0.336472236621213,0.318453731118535,-0.559615787935423,-0.980829253011726,1.48160454092422,-0.619039208406224,0.268263986594679,-1.20397280432594,0.847297860387203,1.38629436111989,0.693147180559945,-0.0408219945202552,0.693147180559945,0.154150679827258,-1.6094379124341,-0.693147180559945,-1.38629436111989,1.46633706879343,-0.587786664902119,-0.739667196194838,0,-0.287682072451781,1.09861228866811,-0.78845736036427,-0.559615787935423,0,0.22314355131421,-0.22314355131421,0.83034830207343,-0.168622712435793,2.30258509299405,-1.22377543162212,-0.405465108108164,0,0.693147180559945,0.479182684004732,-0.0444517625708338,1.38629436111989,-0.307484699747961,-0.628608659422374,0.0666913744986724,0.117783035656383,-1.43508452528932,0.432133355190326,0.559615787935423,0.613104472886409,0.397682967666109,0.189241999638528,-0.510825623765991,-0.84483176789201,0.773189888233482,-0.194744076792512,1.12300374179227,-0.460815203191329,0.287682072451781,-0.810930216216329,2.30258509299405,0.330241686870577,-0.0851578083403066,-0.485507815781701,1.16315080980568,-0.164549387048157,1.42138568093116,-0.451985123743057,-0.693147180559945,-0.955511445027436,-1.29928298413026,0.572069249006709,-0.262364264467491,-0.553385238184787,-0.693147180559945,-0.216223108469636,-0.916290731874155,-0.336472236621213,-0.182321556793955,-0.0741079721537221,1.09861228866811,0.510825623765991,0.0571584138399484,0.0659579677917976,0.952008814476234,-0.0851578083403066,0.773189888233482,-0.526093095896779,-0.693147180559945,1.38629436111989,-1.55059741241117,0.8754687373539,0.510825623765991,-0.887303195000903,-0.59783700075562,0.78845736036427,0.333491608483075,-0.414943852062708,1.25276296849537,-1.73460105538811,-1.64865862558738,-1.17865499634165,-0.22314355131421,0.944461608840851,1.02961941718116,-1.17865499634165,0.710241613919245,0,-0.313657558855041,-0.470003629245736,-0.569533224592769,0.741937344729377,0,-1.22377543162212,0.105360515657826,0,-1.05860695405441,-0.916290731874155,0.405465108108164,1.50407739677627,-0.575364144903562,0,-1.01160091167848,-1.79175946922805,0.693147180559945,-1.09861228866811,-1.20397280432594,0.955511445027436,-1.87180217690159,-0.27443684570176,-1.25276296849537,-0.095310179804325,-0.0434851119397389,-1.6094379124341,0.287682072451781,0.0303053494953288,-0.117783035656384,-0.693147180559945,0.966440515559627,0.606135803570316,-0.466089729924599,-2.25606507735915,-0.966843011036987,1.05605267424931,1.01856958099457,-0.693147180559945,0.126293725324292,0.344840486291729,0.22314355131421,-1.52605630349505,-0.965080896043587,0,1.09861228866811,0.591364486250003,1.79175946922805,1.21302263984585,-0.182321556793955,-0.916290731874155,-0.528969875889394,0.451985123743057,1.42138568093116,0.50310357767208,-0.367724780125317,0.592051063688577,0.559615787935423,0.643754425230369,0,0.693147180559945,0.451985123743057,0.182321556793954,-0.310154928303839,0.268263986594679,-0.163453072489572,-0.262364264467491,-0.559615787935423,1.26165191591261,0.287682072451781,-1.17865499634165,-0.405465108108164,0.129211731480006,-0.405465108108164,-0.998528830111127,-0.961411167154625,0.287682072451781,0.328857986635967,-0.916290731874155,0.955511445027436,-0.287682072451781,0.619039208406224,-0.931821673905032,-1.01160091167848,1.09861228866811,1.29928298413026,0.962275845115979,-1.54044504094715,1.42138568093116,-0.510825623765991,-0.238411023444998,-0.559615787935423,-1.5114575040739,-0.916290731874155,1.02961941718116,-1.29928298413026,0.896088024556636,0.27763173659828,0.154150679827258,0.0636256958802116,-0.980829253011726,-0.154150679827258,1.67006253425054,-0.325422400434628,0.510825623765991,-0.965080896043587,-1.59601489210196,-1.46633706879343,-0.45953232937844,-0.143100843640673,0.75377180237638,-0.117783035656384,-0.693147180559945,0.693147180559945,0.405465108108164,0.693147180559945,-0.105360515657827,-2.07944154167984,1.46040233327361,-1.15267950993839,0.998528830111127,-1.16899308542991,0.847297860387203,-0.0180185055026783,0,0.356674943938732,0.470003629245736,-0.602175402354219,0.887303195000903,-0.606135803570315,-0.559615787935423,-1.38629436111989,-1.09861228866811,0.611801541105993,1.02961941718116,0,0.154150679827258,-0.470003629245736,-0.287682072451781,-0.810930216216329,0.405465108108164,-0.336472236621213,1.09861228866811,0.4678082386823,0.644357016390513,0.405465108108164,-0.650587566141149,0.75377180237638,-1.09861228866811,0,-0.287682072451781,-1.09861228866811,1.09861228866811,0.485507815781701,-0.564892845036267,0.569094531889966,1.25276296849537,-0.693147180559945,0.44628710262842,-1.7404661748405,1.71410853499799,-0.23638877806423,0.826678573184468,0.274436845701761,0.342095494175575,1.25276296849537,-2.53897387105828,-0.860201265223111,-1.38629436111989,-0.998528830111127,1.18958406687384,0.405465108108164,0.693147180559945,1.02961941718116,0.773189888233482,0,-0.194156014440957,0,-1.17865499634165,-0.800777844752311,1.17865499634165,0.693147180559945,-1.38629436111989,-0.167054084663166,0.374693449441411,0.559615787935423,-0.641853886172395,1.13497993283898,0.405465108108164,0.893817876022097,-1.09861228866811,-0.451985123743057,0.444685821261446,0.916290731874155,0,0.324239668185579,0.595983432106298,0.492476485097794,-0.287682072451781,1.19625075823203,-0.887303195000903,0.0606246218164348,0.624154309072994,0.470003629245736,-0.336472236621213,-0.22314355131421,-0.351397886837888,0.916290731874155,0.581921545449721,0.405465108108164,0.693147180559945,0.0384662808277959,-0.847297860387204,-1.28785428830664,-0.227932068046007,-0.287682072451781,1.01160091167848,-0.405465108108164,-1.17865499634165,0.693147180559945,-0.559615787935423,-1.46633706879343,0.451007128555081,0.405465108108164,-0.916290731874155,0.53062825106217,0.637577329405134,0,-1.09861228866811,-0.510825623765991,0.432133355190326,-0.587786664902119,0.42744401482694,-0.356674943938732,0.117783035656383,0.887303195000903,-0.693147180559945,-1.09861228866811,0.0377403279828471,0,-2.01490302054226,-0.470003629245736,-0.78845736036427,1.17865499634165,-0.336472236621213,0,0.02531780798429,0.22314355131421,0.2578291093021,0.145711811181394,-0.182321556793955,-0.559615787935423,1.17865499634165,-0.336472236621213,-0.847297860387204,0.527354925717201,-0.8754687373539,-0.485507815781701,0.133531392624523,-1.20397280432594,0.916290731874155,-0.405465108108164,-1.14862270924277,0.619039208406224,0.8754687373539,-0.650587566141149,-0.693147180559945,0.22314355131421,-1.87180217690159,-0.693147180559945,1.66500776358891,-0.154150679827258,-0.693147180559945,-0.22314355131421,0.864997437486605,-0.836248024200619,0.693147180559945,1.04145387482816,0.22314355131421,-0.268263986594679,1.50407739677627,0.677398823591806,0.287682072451781,0.615185639090233,1.09861228866811,1.6094379124341,-1.49645699273658,-0.336472236621213,0.62509371731493,0.559615787935423,0.641853886172395,-0.810930216216329,-0.0125061755052297,0.123232640423948,0.693147180559945,-2.52572864430826,0,-1.02961941718116,-0.916290731874155,0,0,-0.993251773010283,-0.916290731874155,0.782759339249632,-1.09861228866811,0.405465108108164,-0.510825623765991,0.367724780125318,1.25276296849537,-0.22314355131421,-0.619039208406224,0.8754687373539,-0.336472236621213,-0.287682072451781,0.316799471022508,-1.09861228866811,-0.510825623765991,-1.17865499634165,-1.28935241592766,0.693147180559945,0,1.38629436111989,-0.251314428280906,0.852777326151829],&#34;text&#34;:[&#34;patrick&#34;,&#34;debbie&#34;,&#34;ploy&#34;,&#34;travis&#34;,&#34;elevating&#34;,&#34;hitchcock&#34;,&#34;pumping&#34;,&#34;strength&#34;,&#34;lessons&#34;,&#34;luke&#34;,&#34;associate&#34;,&#34;uncut&#34;,&#34;sheets&#34;,&#34;flashback&#34;,&#34;low&#34;,&#34;remarkably&#34;,&#34;gimli&#34;,&#34;committing&#34;,&#34;recognition&#34;,&#34;challenged&#34;,&#34;underwater&#34;,&#34;monotone&#34;,&#34;colourful&#34;,&#34;overdose&#34;,&#34;pursuit&#34;,&#34;adored&#34;,&#34;marion&#34;,&#34;tasty&#34;,&#34;avoid&#34;,&#34;propaganda&#34;,&#34;seedy&#34;,&#34;historically&#34;,&#34;agrees&#34;,&#34;restrictions&#34;,&#34;spending&#34;,&#34;fabulous&#34;,&#34;prequels&#34;,&#34;mortal&#34;,&#34;bus&#34;,&#34;daniel&#34;,&#34;jokers&#34;,&#34;beer&#34;,&#34;spends&#34;,&#34;taped&#34;,&#34;cartoon&#34;,&#34;pass&#34;,&#34;jon&#34;,&#34;polish&#34;,&#34;endings&#34;,&#34;justify&#34;,&#34;modern&#34;,&#34;ugh&#34;,&#34;cuba&#34;,&#34;geeks&#34;,&#34;overrated&#34;,&#34;poorly&#34;,&#34;turkey&#34;,&#34;wits&#34;,&#34;astonishing&#34;,&#34;annette&#34;,&#34;stirring&#34;,&#34;rental&#34;,&#34;it.if&#34;,&#34;nah&#34;,&#34;ethan&#34;,&#34;touching&#34;,&#34;stale&#34;,&#34;julia&#34;,&#34;dean&#34;,&#34;carey&#39;s&#34;,&#34;offset&#34;,&#34;platoon&#34;,&#34;hilarity&#34;,&#34;gandalf&#34;,&#34;collected&#34;,&#34;bone&#34;,&#34;overlook&#34;,&#34;unlikable&#34;,&#34;jokes&#34;,&#34;steel&#34;,&#34;balls&#34;,&#34;imitation&#34;,&#34;humans&#34;,&#34;excellent&#34;,&#34;terminator&#34;,&#34;lil&#34;,&#34;display&#34;,&#34;edition&#34;,&#34;perkins&#34;,&#34;tool&#34;,&#34;alleged&#34;,&#34;entertained&#34;,&#34;illogical&#34;,&#34;absurdity&#34;,&#34;pitt&#34;,&#34;doubts&#34;,&#34;filling&#34;,&#34;clarify&#34;,&#34;knights&#34;,&#34;humorous&#34;,&#34;flowing&#34;,&#34;ann&#34;,&#34;dance&#34;,&#34;biblical&#34;,&#34;pointless&#34;,&#34;shark&#34;,&#34;powerful&#34;,&#34;murdering&#34;,&#34;ai&#34;,&#34;cable&#34;,&#34;dolls&#34;,&#34;stinker&#34;,&#34;chuckle&#34;,&#34;war&#34;,&#34;insults&#34;,&#34;samuel&#34;,&#34;swinging&#34;,&#34;catalyst&#34;,&#34;omg&#34;,&#34;observer&#34;,&#34;funky&#34;,&#34;flawless&#34;,&#34;roars&#34;,&#34;recreate&#34;,&#34;jamie&#34;,&#34;coppola&#34;,&#34;machines&#34;,&#34;jaws&#34;,&#34;invented&#34;,&#34;relish&#34;,&#34;wire&#34;,&#34;trap&#34;,&#34;assembly&#34;,&#34;jackson&#39;s&#34;,&#34;passes&#34;,&#34;hint&#34;,&#34;symbols&#34;,&#34;fools&#34;,&#34;unbearable&#34;,&#34;neo&#34;,&#34;superbly&#34;,&#34;jessica&#34;,&#34;jason&#34;,&#34;shout&#34;,&#34;nonstop&#34;,&#34;surprisingly&#34;,&#34;haters&#34;,&#34;bothered&#34;,&#34;rescuing&#34;,&#34;experienced&#34;,&#34;dungeons&#34;,&#34;breasts&#34;,&#34;objectification&#34;,&#34;rounded&#34;,&#34;scar&#34;,&#34;photography&#34;,&#34;realizes&#34;,&#34;damien&#34;,&#34;pig&#34;,&#34;surface&#34;,&#34;uplifting&#34;,&#34;fought&#34;,&#34;thirsty&#34;,&#34;fed&#34;,&#34;camps&#34;,&#34;photographed&#34;,&#34;ford&#34;,&#34;portrayals&#34;,&#34;booze&#34;,&#34;highlight&#34;,&#34;anticipating&#34;,&#34;violated&#34;,&#34;stumbling&#34;,&#34;shining&#34;,&#34;carpenter&#34;,&#34;hilton&#34;,&#34;absence&#34;,&#34;senseless&#34;,&#34;worship&#34;,&#34;exceeded&#34;,&#34;expanded&#34;,&#34;barney&#34;,&#34;repeating&#34;,&#34;festivals&#34;,&#34;shitty&#34;,&#34;herring&#34;,&#34;toys&#34;,&#34;fashionable&#34;,&#34;suspects&#34;,&#34;parent&#34;,&#34;defense&#34;,&#34;tasks&#34;,&#34;abomination&#34;,&#34;golf&#34;,&#34;feared&#34;,&#34;heaven&#39;s&#34;,&#34;heartwarming&#34;,&#34;rave&#34;,&#34;mercy&#34;,&#34;depressed&#34;,&#34;shallow&#34;,&#34;accurately&#34;,&#34;debate&#34;,&#34;honour&#34;,&#34;ron&#34;,&#34;trigger&#34;,&#34;awkward&#34;,&#34;inform&#34;,&#34;spy&#34;,&#34;appears&#34;,&#34;joke&#34;,&#34;soap&#34;,&#34;corny&#34;,&#34;welles&#34;,&#34;globe&#34;,&#34;nicholas&#34;,&#34;pillow&#34;,&#34;fiction&#34;,&#34;supremely&#34;,&#34;loathing&#34;,&#34;lava&#34;,&#34;redeeming&#34;,&#34;leading&#34;,&#34;intends&#34;,&#34;roman&#34;,&#34;gratification&#34;,&#34;artwork&#34;,&#34;possess&#34;,&#34;injury&#34;,&#34;luc&#34;,&#34;drags&#34;,&#34;explore&#34;,&#34;characters.the&#34;,&#34;cried&#34;,&#34;misfortune&#34;,&#34;unbelievably&#34;,&#34;lurid&#34;,&#34;miguel&#34;,&#34;vomit&#34;,&#34;jr&#34;,&#34;examples&#34;,&#34;odyssey&#34;,&#34;united&#34;,&#34;sentimental&#34;,&#34;interactions&#34;,&#34;insight&#34;,&#34;wicked&#34;,&#34;bore&#34;,&#34;impeccable&#34;,&#34;subjects&#34;,&#34;cox&#34;,&#34;jedi&#34;,&#34;denial&#34;,&#34;communication&#34;,&#34;overdone&#34;,&#34;spectacular&#34;,&#34;crappy&#34;,&#34;scorsese&#34;,&#34;vegas&#34;,&#34;darkness&#34;,&#34;certainty&#34;,&#34;pace&#34;,&#34;slave&#34;,&#34;wounds&#34;,&#34;puppet&#34;,&#34;murderer&#34;,&#34;dick&#34;,&#34;hur&#34;,&#34;infamous&#34;,&#34;lang&#34;,&#34;depraved&#34;,&#34;survival&#34;,&#34;hardy&#34;,&#34;harold&#34;,&#34;swear&#34;,&#34;twists&#34;,&#34;elegant&#34;,&#34;ass&#34;,&#34;june&#34;,&#34;cornball&#34;,&#34;channel&#34;,&#34;bogart&#34;,&#34;shields&#34;,&#34;coma&#34;,&#34;lucas&#34;,&#34;redux&#34;,&#34;masterful&#34;,&#34;innocence&#34;,&#34;electric&#34;,&#34;robinson&#34;,&#34;proposal&#34;,&#34;convincingly&#34;,&#34;vampires&#34;,&#34;novelty&#34;,&#34;demonic&#34;,&#34;hitchcock&#39;s&#34;,&#34;communist&#34;,&#34;accidentally&#34;,&#34;influential&#34;,&#34;nudge&#34;,&#34;anthony&#34;,&#34;nazis&#34;,&#34;one.this&#34;,&#34;carol&#34;,&#34;straightforward&#34;,&#34;mischief&#34;,&#34;mediocre&#34;,&#34;conditions&#34;,&#34;illness&#34;,&#34;superman&#34;,&#34;circle&#34;,&#34;thrilled&#34;,&#34;triggered&#34;,&#34;collins&#34;,&#34;nolan&#34;,&#34;falsely&#34;,&#34;scary&#34;,&#34;sinking&#34;,&#34;shot&#34;,&#34;placement&#34;,&#34;compassion&#34;,&#34;plunge&#34;,&#34;media&#34;,&#34;peril&#34;,&#34;spectacle&#34;,&#34;leonard&#34;,&#34;surrealism&#34;,&#34;reed&#34;,&#34;handled&#34;,&#34;glover&#34;,&#34;superb&#34;,&#34;philosophical&#34;,&#34;nailed&#34;,&#34;dawn&#34;,&#34;cheat&#34;,&#34;breathtaking&#34;,&#34;gospel&#34;,&#34;restored&#34;,&#34;leonardo&#34;,&#34;hears&#34;,&#34;rating&#34;,&#34;mexicans&#34;,&#34;assigned&#34;,&#34;facility&#34;,&#34;fake&#34;,&#34;gadgets&#34;,&#34;swedish&#34;,&#34;inglourious&#34;,&#34;disturbs&#34;,&#34;complexity&#34;,&#34;goal&#34;,&#34;vicious&#34;,&#34;nausea&#34;,&#34;struggled&#34;,&#34;boobs&#34;,&#34;neon&#34;,&#34;claw&#34;,&#34;scale&#34;,&#34;tooth&#34;,&#34;utter&#34;,&#34;apt&#34;,&#34;strangelove&#34;,&#34;arrival&#34;,&#34;excellence&#34;,&#34;poverty&#34;,&#34;troll&#34;,&#34;strongest&#34;,&#34;layers&#34;,&#34;mixing&#34;,&#34;grief&#34;,&#34;monstrosity&#34;,&#34;nutshell&#34;,&#34;psycho&#34;,&#34;reservoir&#34;,&#34;directorial&#34;,&#34;trek&#34;,&#34;charles&#34;,&#34;rooted&#34;,&#34;ticked&#34;,&#34;treasure&#34;,&#34;bruce&#34;,&#34;quentin&#34;,&#34;praises&#34;,&#34;unit&#34;,&#34;address&#34;,&#34;sections&#34;,&#34;engrossing&#34;,&#34;laden&#34;,&#34;novels&#34;,&#34;tropical&#34;,&#34;paranoid&#34;,&#34;smoothly&#34;,&#34;tremendously&#34;,&#34;lived&#34;,&#34;solves&#34;,&#34;hostage&#34;,&#34;glossy&#34;,&#34;crack&#34;,&#34;moderately&#34;,&#34;andrew&#34;,&#34;would&#39;ve&#34;,&#34;fart&#34;,&#34;unstable&#34;,&#34;tens&#34;,&#34;unfolds&#34;,&#34;duck&#34;,&#34;instincts&#34;,&#34;women&#39;s&#34;,&#34;robbing&#34;,&#34;throwing&#34;,&#34;joseph&#34;,&#34;tommy&#34;,&#34;captures&#34;,&#34;november&#34;,&#34;outdo&#34;,&#34;chandler&#34;,&#34;raymond&#34;,&#34;tarantino&#34;,&#34;zombie&#34;,&#34;well.the&#34;,&#34;austin&#34;,&#34;morally&#34;,&#34;window&#34;,&#34;spiral&#34;,&#34;pitiful&#34;,&#34;moon&#34;,&#34;switches&#34;,&#34;gregory&#34;,&#34;wilder&#34;,&#34;japan&#34;,&#34;coarse&#34;,&#34;weak&#34;,&#34;profoundly&#34;,&#34;acting&#34;,&#34;relate&#34;,&#34;giant&#34;,&#34;progression&#34;,&#34;tin&#34;,&#34;haunting&#34;,&#34;extended&#34;,&#34;island&#34;,&#34;lawyer&#34;,&#34;bible&#34;,&#34;parody&#34;,&#34;phenomenal&#34;,&#34;cliched&#34;,&#34;raving&#34;,&#34;adolescent&#34;,&#34;resemble&#34;,&#34;humanity&#34;,&#34;corpse&#34;,&#34;continuity&#34;,&#34;limitations&#34;,&#34;factory&#34;,&#34;dracula&#34;,&#34;advertised&#34;,&#34;sophistication&#34;,&#34;arc&#34;,&#34;premier&#34;,&#34;classroom&#34;,&#34;activity&#34;,&#34;boot&#34;,&#34;extraordinary&#34;,&#34;period&#34;,&#34;nominations&#34;,&#34;similarly&#34;,&#34;cowardly&#34;,&#34;decoration&#34;,&#34;unfunny&#34;,&#34;distinction&#34;,&#34;deemed&#34;,&#34;suspend&#34;,&#34;moore&#34;,&#34;germany&#39;s&#34;,&#34;fighter&#34;,&#34;trick&#34;,&#34;del&#34;,&#34;unpleasant&#34;,&#34;incoherent&#34;,&#34;exaggeration&#34;,&#34;crafting&#34;,&#34;capturing&#34;,&#34;exploration&#34;,&#34;basement&#34;,&#34;brad&#34;,&#34;girls&#34;,&#34;games&#34;,&#34;stereo&#34;,&#34;pg&#34;,&#34;eli&#34;,&#34;boxing&#34;,&#34;despicable&#34;,&#34;daycare&#34;,&#34;depiction&#34;,&#34;sh&#34;,&#34;horrifically&#34;,&#34;norton&#34;,&#34;brilliantly&#34;,&#34;institution&#34;,&#34;bros&#34;,&#34;submit&#34;,&#34;embarrassed&#34;,&#34;faced&#34;,&#34;monkeys&#34;,&#34;timeline&#34;,&#34;transported&#34;,&#34;catastrophe&#34;,&#34;shyamalan&#34;,&#34;grate&#34;,&#34;juno&#34;,&#34;pacing&#34;,&#34;tossed&#34;,&#34;exposure&#34;,&#34;bear&#34;,&#34;upcoming&#34;,&#34;load&#34;,&#34;samurai&#34;,&#34;authorities&#34;,&#34;technical&#34;,&#34;pile&#34;,&#34;hour&#34;,&#34;differences&#34;,&#34;bates&#34;,&#34;versa&#34;,&#34;jews&#34;,&#34;filmmaking&#34;,&#34;slipped&#34;,&#34;blacks&#34;,&#34;shouting&#34;,&#34;triple&#34;,&#34;bounty&#34;,&#34;steven&#34;,&#34;vasquez&#34;,&#34;nomination&#34;,&#34;ordeal&#34;,&#34;categorize&#34;,&#34;looked&#34;,&#34;primarily&#34;,&#34;scariest&#34;,&#34;expression&#34;,&#34;youtube&#34;,&#34;created&#34;,&#34;jew&#34;,&#34;george&#34;,&#34;enduring&#34;,&#34;savvy&#34;,&#34;definitive&#34;,&#34;conveying&#34;,&#34;dennis&#34;,&#34;crop&#34;,&#34;silent&#34;,&#34;previews&#34;,&#34;lastly&#34;,&#34;journey&#34;,&#34;indictment&#34;,&#34;circles&#34;,&#34;independence&#34;,&#34;classics&#34;,&#34;trapped&#34;,&#34;signed&#34;,&#34;bucks&#34;,&#34;abandons&#34;,&#34;fi&#34;,&#34;confirm&#34;,&#34;hysterical&#34;,&#34;fruit&#34;,&#34;tragic&#34;,&#34;ridiculous&#34;,&#34;hmm&#34;,&#34;transcends&#34;,&#34;messiah&#34;,&#34;henry&#34;,&#34;calculated&#34;,&#34;unexpected&#34;,&#34;agenda&#34;,&#34;behavior&#34;,&#34;traumatized&#34;,&#34;atrocious&#34;,&#34;pacifist&#34;,&#34;imaginative&#34;,&#34;spared&#34;,&#34;crafted&#34;,&#34;noir&#34;,&#34;crucial&#34;,&#34;follow&#34;,&#34;um&#34;,&#34;transformers&#34;,&#34;unforgettable&#34;,&#34;pages&#34;,&#34;portal&#34;,&#34;accolades&#34;,&#34;contrived&#34;,&#34;semblance&#34;,&#34;replace&#34;,&#34;mike&#34;,&#34;struggle&#34;,&#34;france&#34;,&#34;formal&#34;,&#34;day&#39;s&#34;,&#34;mary&#34;,&#34;raises&#34;,&#34;resistance&#34;,&#34;disappear&#34;,&#34;wonderfully&#34;,&#34;subjected&#34;,&#34;roller&#34;,&#34;clich√©&#34;,&#34;pirates&#34;,&#34;grey&#34;,&#34;cannon&#34;,&#34;alec&#34;,&#34;wives&#34;,&#34;teeth&#34;,&#34;gangs&#34;,&#34;false&#34;,&#34;wicker&#34;,&#34;rubber&#34;,&#34;attitudes&#34;,&#34;freeman&#34;,&#34;achieves&#34;,&#34;expressing&#34;,&#34;broken&#34;,&#34;sounding&#34;,&#34;barrels&#34;,&#34;obsolete&#34;,&#34;deviates&#34;,&#34;patch&#34;,&#34;caan&#34;,&#34;joker&#34;,&#34;towers&#34;,&#34;it.this&#34;,&#34;mentioning&#34;,&#34;doc&#34;,&#34;piper&#34;,&#34;vengeful&#34;,&#34;leatherface&#34;,&#34;crystal&#34;,&#34;resorting&#34;,&#34;environmental&#34;,&#34;holocaust&#34;,&#34;engaging&#34;,&#34;restrained&#34;,&#34;jail&#34;,&#34;relationship&#34;,&#34;costs&#34;,&#34;outstanding&#34;,&#34;surrounded&#34;,&#34;freaky&#34;,&#34;inception&#34;,&#34;wayans&#34;,&#34;wizards&#34;,&#34;affairs&#34;,&#34;endure&#34;,&#34;manhattan&#34;,&#34;scottish&#34;,&#34;provoking&#34;,&#34;translation&#34;,&#34;keen&#34;,&#34;retired&#34;,&#34;spawned&#34;,&#34;brink&#34;,&#34;mall&#34;,&#34;inch&#34;,&#34;historic&#34;,&#34;sign&#34;,&#34;stray&#34;,&#34;plummer&#34;,&#34;juvenile&#34;,&#34;heroic&#34;,&#34;stack&#34;,&#34;tables&#34;,&#34;england&#34;,&#34;eve&#34;,&#34;improbable&#34;,&#34;lovely&#34;,&#34;jake&#34;,&#34;spit&#34;,&#34;basterds&#34;,&#34;makeover&#34;,&#34;closet&#34;,&#34;hanks&#34;,&#34;exorcist&#34;,&#34;compelling&#34;,&#34;flying&#34;,&#34;touched&#34;,&#34;redeem&#34;,&#34;requiem&#34;,&#34;lawrence&#34;,&#34;formed&#34;,&#34;demanding&#34;,&#34;listens&#34;,&#34;submarine&#34;,&#34;duel&#34;,&#34;learns&#34;,&#34;norman&#34;,&#34;transforming&#34;,&#34;deliver&#34;,&#34;simpson&#34;,&#34;severe&#34;,&#34;spoof&#34;,&#34;requirement&#34;,&#34;blend&#34;,&#34;travesties&#34;,&#34;stabbed&#34;,&#34;improving&#34;,&#34;overweight&#34;,&#34;fairness&#34;,&#34;books&#34;,&#34;raging&#34;,&#34;sweating&#34;,&#34;akira&#34;,&#34;vader&#34;,&#34;shortly&#34;,&#34;bend&#34;,&#34;omnipresent&#34;,&#34;forrest&#34;,&#34;uttered&#34;,&#34;laughed&#34;,&#34;nerds&#34;,&#34;planes&#34;,&#34;blown&#34;,&#34;elicited&#34;,&#34;incapable&#34;,&#34;sadistic&#34;,&#34;mother&#39;s&#34;,&#34;unexplained&#34;,&#34;buys&#34;,&#34;gosh&#34;,&#34;visions&#34;,&#34;superheroes&#34;,&#34;eastwood&#34;,&#34;killer&#34;,&#34;heightened&#34;,&#34;nest&#34;,&#34;ben&#34;,&#34;shelby&#34;,&#34;allied&#34;,&#34;joining&#34;,&#34;predicted&#34;,&#34;allusions&#34;,&#34;officer&#34;,&#34;flashes&#34;,&#34;hopper&#34;,&#34;directions&#34;,&#34;stalking&#34;,&#34;influenced&#34;,&#34;elm&#34;,&#34;videos&#34;,&#34;attract&#34;,&#34;bent&#34;,&#34;positively&#34;,&#34;bearing&#34;,&#34;relentlessly&#34;,&#34;stinkers&#34;,&#34;unknowingly&#34;,&#34;calm&#34;,&#34;oppressed&#34;,&#34;modified&#34;,&#34;adequately&#34;,&#34;duncan&#34;,&#34;paint&#34;,&#34;commercially&#34;,&#34;arthur&#34;,&#34;wile&#34;,&#34;rap&#34;,&#34;tender&#34;,&#34;knight&#34;,&#34;toy&#34;,&#34;obsession&#34;,&#34;preconceived&#34;,&#34;harrowing&#34;,&#34;boring&#34;,&#34;interact&#34;,&#34;woody&#34;,&#34;ride&#34;,&#34;apartment&#34;,&#34;beard&#34;,&#34;funny&#34;,&#34;wood&#34;,&#34;widow&#34;,&#34;moronic&#34;,&#34;delve&#34;,&#34;reasoning&#34;,&#34;tests&#34;,&#34;snobbish&#34;,&#34;weekend&#34;,&#34;atrocities&#34;,&#34;emotionless&#34;,&#34;technique&#34;,&#34;dives&#34;,&#34;sacrilegious&#34;,&#34;styled&#34;,&#34;ambiguous&#34;,&#34;psychosis&#34;,&#34;editors&#34;,&#34;healthy&#34;,&#34;hilariously&#34;,&#34;stumble&#34;,&#34;clark&#34;,&#34;godfather&#34;,&#34;assumption&#34;,&#34;psychic&#34;,&#34;stooges&#34;,&#34;badly&#34;,&#34;scroll&#34;,&#34;implied&#34;,&#34;studying&#34;,&#34;geniuses&#34;,&#34;kane&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;markers&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:[&#34;rgba(72,23,105,1)&#34;,&#34;rgba(64,69,136,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(44,113,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(52,182,121,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(33,144,141,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,34,116,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(44,113,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(31,149,139,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(36,134,142,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(44,115,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(31,162,135,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(47,108,142,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(56,87,140,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(68,1,84,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,18,101,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(253,231,37,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(46,109,142,1)&#34;,&#34;rgba(62,76,138,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(32,163,134,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(68,60,132,1)&#34;,&#34;rgba(59,82,139,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(32,147,140,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(30,157,137,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(171,220,50,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(46,109,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(33,144,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(40,174,128,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(47,107,142,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,101,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(34,140,141,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(47,108,142,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(37,171,130,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(37,132,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(42,119,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(112,207,87,1)&#34;,&#34;rgba(41,123,142,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(68,191,112,1)&#34;,&#34;rgba(42,118,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(35,136,142,1)&#34;,&#34;rgba(65,68,135,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(50,100,142,1)&#34;,&#34;rgba(50,182,122,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(39,127,142,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,21,104,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(61,77,138,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(68,60,132,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(33,143,141,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(32,163,134,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(69,3,87,1)&#34;,&#34;rgba(60,79,138,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(35,137,142,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(59,82,139,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(72,21,104,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(60,79,138,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(61,77,138,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(46,179,124,1)&#34;,&#34;rgba(64,69,136,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(41,123,142,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(43,116,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(65,68,135,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(67,61,132,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(72,21,103,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(48,106,142,1)&#34;,&#34;rgba(71,43,122,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(40,124,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(65,67,135,1)&#34;,&#34;rgba(67,61,132,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(45,112,142,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,34,116,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(35,136,142,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(56,89,140,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(58,83,139,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,16,98,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(35,138,141,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(49,102,142,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(37,171,130,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(44,114,142,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(52,97,141,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(31,151,139,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(68,1,85,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(48,106,142,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,8,91,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(34,141,141,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(34,139,141,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(41,122,142,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(37,133,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(37,132,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(70,8,91,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(31,149,139,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(65,67,135,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(62,75,137,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(57,87,140,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(69,4,87,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(68,1,84,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(44,115,142,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(41,122,142,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(66,64,134,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(72,32,114,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(37,133,142,1)&#34;,&#34;rgba(70,6,90,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,39,119,1)&#34;,&#34;rgba(71,43,122,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(69,54,129,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(70,49,126,1)&#34;],&#34;opacity&#34;:0.5,&#34;size&#34;:[5.85498525186645,5.75530441896398,4.81725640546078,7.08156052852364,3.77952755905512,9.9842436220335,4.99637249295625,6.1572630538017,6.1287861523447,8.67448473764607,4.81725640546078,4.93974368044287,4.75023400996066,6.04120514519478,11.800976881346,5.42032093320559,4.81725640546078,5.05048064158557,5.7209404608662,5.57692664550957,5.46084045125838,5.20049630343061,4.59992424613035,4.51331266338146,5.42032093320559,5.15231378700878,6.24071762028082,4.41500410032034,9.30732123076958,8.33254439454472,4.59992424613035,5.61399031987096,5.50040616788309,4.29839198225795,6.3477754241973,6.80500105069749,5.46084045125838,7.20167346483616,6.32143372411601,8.07389478160306,4.29839198225795,5.85498525186645,5.95009316947746,4.29839198225795,9.08363165588957,7.39300013384823,5.75530441896398,5.7890808290592,5.5390824263879,5.88716480950467,10.9409844765614,5.91886038129558,6.45054565636157,4.67822710228234,9.18419234317619,9.22142353961344,7.88151099443129,4.88020521554463,5.98088287203413,5.15231378700878,5.05048064158557,6.42522737787542,4.29839198225795,4.51331266338146,4.81725640546078,8.16692436252423,5.61399031987096,6.24071762028082,5.15231378700878,4.75023400996066,4.59992424613035,5.65031984192698,5.46084045125838,5.2922643048763,4.75023400996066,5.7890808290592,5.75530441896398,5.05048064158557,13.9735960212014,6.04120514519478,5.7890808290592,5.33612082866362,8.16692436252423,13.2479147743441,7.60999825947506,4.88020521554463,6.89271409827211,5.85498525186645,5.5390824263879,5.61399031987096,4.81725640546078,7.53905747746838,5.10237746846527,5.75530441896398,7.18194911665511,5.61399031987096,4.81725640546078,4.29839198225795,4.14642011121829,6.89271409827211,4.81725640546078,4.29839198225795,10.663041994367,5.10237746846527,9.27067247086978,10.016700945829,9.9842436220335,5.46084045125838,4.75023400996066,5.95009316947746,5.68595718285079,5.88716480950467,5.50040616788309,15.9976555097511,5.57692664550957,5.88716480950467,4.81725640546078,4.41500410032034,5.57692664550957,4.14642011121829,4.93974368044287,7.50308028699906,4.51331266338146,4.88020521554463,7.27945944121871,7.29863729372068,6.6684424037042,9.55735724835328,5.75530441896398,3.77952755905512,6.6214650478061,5.33612082866362,4.93974368044287,5.2470977677078,5.68595718285079,6.04120514519478,4.93974368044287,5.10237746846527,5.88716480950467,6.37384967506928,6.1572630538017,6.37384967506928,7.97880475556136,4.99637249295625,5.10237746846527,7.53905747746838,6.24071762028082,6.6684424037042,4.59992424613035,6.84917088333504,5.42032093320559,5.75530441896398,3.77952755905512,4.88020521554463,5.88716480950467,7.01983484339694,6.26791391617673,4.99637249295625,5.10237746846527,6.1287861523447,5.68595718285079,6.1572630538017,4.51331266338146,5.65031984192698,5.5390824263879,4.51331266338146,7.67964888615596,4.99637249295625,4.14642011121829,6.32143372411601,4.67822710228234,4.88020521554463,4.41500410032034,7.62752910941972,4.99637249295625,8.15155690093495,6.09995980183062,5.95009316947746,5.05048064158557,5.10237746846527,4.41500410032034,6.3477754241973,5.61399031987096,3.77952755905512,5.46084045125838,4.29839198225795,8.83679238485105,4.51331266338146,6.24071762028082,5.7209404608662,5.82229883597697,4.75023400996066,6.47562618873679,6.32143372411601,5.3787751170721,4.41500410032034,5.57692664550957,5.85498525186645,5.2470977677078,5.68595718285079,7.27945944121871,5.42032093320559,5.15231378700878,4.29839198225795,5.98088287203413,4.67822710228234,7.12207778622041,4.88020521554463,8.74275800951811,8.83679238485105,10.4947638556414,5.57692664550957,7.06111430735605,6.3477754241973,5.98088287203413,6.04120514519478,4.59992424613035,10.702042388254,4.59992424613035,4.59992424613035,5.3787751170721,8.16692436252423,8.19749892618241,5.10237746846527,6.09995980183062,4.41500410032034,4.99637249295625,5.05048064158557,4.99637249295625,4.59992424613035,6.37384967506928,5.75530441896398,4.29839198225795,7.01983484339694,5.50040616788309,6.57369799056115,4.41500410032034,5.2470977677078,5.82229883597697,7.04053989294647,6.8710191542449,6.50047554933428,6.45054565636157,6.73750988052566,4.99637249295625,5.88716480950467,5.33612082866362,6.42522737787542,5.91886038129558,5.2922643048763,5.82229883597697,6.9356566308181,4.75023400996066,5.57692664550957,6.37384967506928,8.05819320353604,7.7818527876071,7.55691757760989,6.1572630538017,6.78267252788327,4.29839198225795,7.94662591203789,6.09995980183062,4.75023400996066,5.2470977677078,6.64504999568007,6.01124782799873,4.75023400996066,6.52510001496245,6.52510001496245,4.75023400996066,6.32143372411601,5.7209404608662,4.99637249295625,6.21321742685738,8.49234772297105,4.88020521554463,7.74807728728558,6.45054565636157,4.14642011121829,6.3477754241973,6.3477754241973,5.33612082866362,4.67822710228234,7.60999825947506,5.46084045125838,6.6684424037042,6.52510001496245,5.20049630343061,6.3477754241973,3.77952755905512,5.2922643048763,6.8710191542449,4.75023400996066,5.15231378700878,7.50308028699906,5.7890808290592,6.42522737787542,5.91886038129558,4.14642011121829,7.08156052852364,7.7818527876071,3.77952755905512,4.67822710228234,5.42032093320559,4.88020521554463,7.67964888615596,5.85498525186645,4.81725640546078,9.56899437935629,5.61399031987096,5.10237746846527,4.51331266338146,5.5390824263879,9.40388388095622,4.51331266338146,12.9738285632099,4.93974368044287,12.407355216846,5.2470977677078,5.3787751170721,4.51331266338146,7.37432573196402,4.88020521554463,5.7209404608662,6.52510001496245,4.93974368044287,5.42032093320559,6.26791391617673,4.99637249295625,9.88583660124146,6.1287861523447,5.2922643048763,5.10237746846527,4.93974368044287,6.71466797636047,4.29839198225795,5.2922643048763,6.18540291488937,4.93974368044287,12.7740090363008,4.41500410032034,5.05048064158557,4.99637249295625,8.61917271143022,4.93974368044287,4.67822710228234,5.57692664550957,4.29839198225795,5.68595718285079,6.59768272602791,5.5390824263879,4.41500410032034,4.75023400996066,5.7890808290592,4.67822710228234,4.29839198225795,7.35555381163162,4.81725640546078,7.81534563284998,4.67822710228234,6.78267252788327,5.46084045125838,5.7209404608662,6.29481617214543,7.60999825947506,5.61399031987096,5.46084045125838,4.81725640546078,4.99637249295625,5.5390824263879,4.99637249295625,8.74275800951811,6.54950558342098,5.82229883597697,5.85498525186645,7.71401185255184,5.05048064158557,4.29839198225795,5.88716480950467,7.84856280275047,7.69686804858876,4.75023400996066,5.15231378700878,5.2922643048763,4.67822710228234,5.50040616788309,5.20049630343061,5.61399031987096,4.81725640546078,5.20049630343061,4.93974368044287,4.93974368044287,7.31771119892611,4.51331266338146,4.81725640546078,4.75023400996066,5.91886038129558,4.59992424613035,6.59768272602791,7.31771119892611,6.6214650478061,5.10237746846527,4.29839198225795,5.3787751170721,4.99637249295625,4.67822710228234,4.41500410032034,4.51331266338146,6.78267252788327,6.39966446129382,7.48496070147531,6.07077081294139,5.2922643048763,3.77952755905512,4.88020521554463,5.3787751170721,10.2808801473632,6.09995980183062,4.59992424613035,6.47562618873679,5.33612082866362,8.63305981358281,5.05048064158557,5.46084045125838,7.24078497365455,5.2922643048763,5.85498525186645,7.48496070147531,6.32143372411601,4.41500410032034,8.81010478523574,5.15231378700878,22.6771653543307,7.53905747746838,7.46675207461058,5.2470977677078,4.81725640546078,6.64504999568007,6.37384967506928,7.24078497365455,6.01124782799873,5.2470977677078,9.72952487762825,6.78267252788327,5.10237746846527,4.93974368044287,5.10237746846527,5.5390824263879,8.31773779651867,5.33612082866362,6.57369799056115,5.10237746846527,6.6214650478061,5.2470977677078,4.75023400996066,4.67822710228234,5.50040616788309,4.75023400996066,4.41500410032034,5.7890808290592,7.18194911665511,6.9356566308181,8.74275800951811,5.88716480950467,5.7890808290592,4.75023400996066,4.59992424613035,8.34730299673642,5.05048064158557,4.99637249295625,5.3787751170721,5.65031984192698,4.99637249295625,7.62752910941972,7.12207778622041,5.10237746846527,5.95009316947746,5.65031984192698,5.05048064158557,4.51331266338146,5.42032093320559,5.15231378700878,5.05048064158557,7.12207778622041,11.2536970852041,10.2393375328987,4.81725640546078,7.69686804858876,5.65031984192698,5.3787751170721,5.2922643048763,5.15231378700878,6.64504999568007,6.64504999568007,4.29839198225795,6.73750988052566,7.53905747746838,5.42032093320559,6.50047554933428,4.93974368044287,6.21321742685738,6.01124782799873,5.20049630343061,4.81725640546078,5.10237746846527,4.93974368044287,6.07077081294139,4.51331266338146,5.2470977677078,7.20167346483616,4.75023400996066,4.88020521554463,7.91419663816726,5.05048064158557,6.57369799056115,7.83198825221453,5.05048064158557,7.01983484339694,7.64498045314673,11.8594978711237,5.65031984192698,6.21321742685738,4.51331266338146,7.55691757760989,6.99899663828633,4.51331266338146,5.5390824263879,5.57692664550957,4.75023400996066,5.68595718285079,7.10188092320679,4.88020521554463,6.18540291488937,4.67822710228234,4.29839198225795,10.9127340432949,5.82229883597697,5.82229883597697,6.71466797636047,6.64504999568007,9.99508156228079,5.7209404608662,9.4158378930007,4.88020521554463,4.75023400996066,5.10237746846527,4.67822710228234,6.29481617214543,5.61399031987096,9.31948360778685,6.1287861523447,5.7209404608662,8.12065877989981,4.29839198225795,5.05048064158557,5.42032093320559,7.7818527876071,6.09995980183062,5.46084045125838,6.1572630538017,4.29839198225795,9.7521055937645,4.29839198225795,5.82229883597697,4.29839198225795,6.95691026538124,11.1357003562906,4.93974368044287,4.99637249295625,4.88020521554463,7.76500081705985,5.05048064158557,6.78267252788327,4.99637249295625,6.47562618873679,4.67822710228234,7.01983484339694,4.29839198225795,5.88716480950467,4.88020521554463,6.71466797636047,7.64498045314673,5.46084045125838,9.89684888979454,5.2922643048763,5.46084045125838,7.37432573196402,5.65031984192698,4.41500410032034,5.57692664550957,7.14215334346163,4.99637249295625,5.65031984192698,8.04243379218159,7.35555381163162,5.75530441896398,4.14642011121829,4.51331266338146,5.7890808290592,5.2470977677078,5.15231378700878,5.82229883597697,6.71466797636047,5.42032093320559,6.29481617214543,7.96274627533861,5.20049630343061,7.55691757760989,5.82229883597697,5.75530441896398,5.46084045125838,6.6214650478061,5.3787751170721,6.69164691177173,5.2922643048763,5.20049630343061,4.75023400996066,7.18194911665511,5.15231378700878,4.99637249295625,6.91425889886634,5.46084045125838,4.29839198225795,4.81725640546078,3.77952755905512,4.75023400996066,4.75023400996066,8.16692436252423,6.52510001496245,5.20049630343061,5.7890808290592,5.42032093320559,4.41500410032034,4.41500410032034,6.21321742685738,5.20049630343061,4.41500410032034,5.2470977677078,8.18223818501315,7.01983484339694,4.51331266338146,6.26791391617673,8.96817179108344,6.6684424037042,8.76980578717714,5.75530441896398,5.33612082866362,7.12207778622041,8.67448473764607,4.51331266338146,5.98088287203413,5.85498525186645,4.59992424613035,5.46084045125838,6.50047554933428,5.61399031987096,4.93974368044287,5.15231378700878,5.15231378700878,4.14642011121829,5.65031984192698,4.59992424613035,5.05048064158557,6.76017674320361,5.05048064158557,4.51331266338146,6.09995980183062,6.18540291488937,5.50040616788309,4.67822710228234,5.57692664550957,6.82716598798061,4.59992424613035,6.54950558342098,4.99637249295625,5.10237746846527,6.59768272602791,4.29839198225795,4.59992424613035,7.57469362882203,6.8710191542449,7.10188092320679,7.57469362882203,6.39966446129382,5.3787751170721,5.7209404608662,8.52082423687209,4.81725640546078,5.3787751170721,4.51331266338146,6.1287861523447,4.88020521554463,6.32143372411601,6.24071762028082,4.14642011121829,7.4300623607585,4.59992424613035,5.85498525186645,9.88583660124146,4.29839198225795,5.61399031987096,3.77952755905512,5.05048064158557,4.75023400996066,4.67822710228234,4.99637249295625,9.39190440767069,5.42032093320559,4.29839198225795,5.50040616788309,7.48496070147531,5.61399031987096,4.75023400996066,4.41500410032034,7.24078497365455,4.88020521554463,9.2584017406471,5.05048064158557,5.05048064158557,7.27945944121871,4.14642011121829,5.20049630343061,6.32143372411601,5.10237746846527,5.05048064158557,4.81725640546078,4.99637249295625,5.05048064158557,4.75023400996066,6.3477754241973,10.2497481923104,4.51331266338146,5.91886038129558,7.53905747746838,4.67822710228234,4.67822710228234,5.05048064158557,5.3787751170721,4.59992424613035,7.29863729372068,5.05048064158557,5.2470977677078,4.93974368044287,4.81725640546078,5.7890808290592,4.59992424613035,6.3477754241973,5.20049630343061,5.05048064158557,5.7890808290592,5.10237746846527,4.51331266338146,4.93974368044287,4.14642011121829,6.07077081294139,4.81725640546078,4.75023400996066,4.51331266338146,5.50040616788309,6.04120514519478,4.51331266338146,5.33612082866362,4.51331266338146,5.61399031987096,4.67822710228234,8.76980578717714,9.86375222856076,6.42522737787542,4.41500410032034,5.10237746846527,14.6323556111669,4.75023400996066,7.52111212479883,7.73108127887284,7.10188092320679,4.81725640546078,20.4882510406151,7.88151099443129,5.2470977677078,5.50040616788309,4.88020521554463,5.15231378700878,4.88020521554463,4.59992424613035,7.24078497365455,5.85498525186645,4.88020521554463,6.26791391617673,4.41500410032034,3.77952755905512,4.41500410032034,5.2922643048763,4.51331266338146,4.51331266338146,5.20049630343061,5.75530441896398,4.75023400996066,5.7890808290592,9.64980839366583,4.41500410032034,4.99637249295625,5.05048064158557,9.07092719669573,4.14642011121829,5.20049630343061,4.93974368044287,7.27945944121871,8.54913073717632],&#34;symbol&#34;:&#34;circle&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:[&#34;rgba(72,23,105,1)&#34;,&#34;rgba(64,69,136,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(44,113,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(52,182,121,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(33,144,141,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,34,116,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(44,113,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(31,149,139,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(36,134,142,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(44,115,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(31,162,135,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(47,108,142,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(56,87,140,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(68,1,84,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,18,101,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(253,231,37,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(46,109,142,1)&#34;,&#34;rgba(62,76,138,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(32,163,134,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(68,60,132,1)&#34;,&#34;rgba(59,82,139,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(54,92,141,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(32,147,140,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(30,157,137,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(72,39,120,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(171,220,50,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(46,109,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(51,99,141,1)&#34;,&#34;rgba(33,144,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(40,174,128,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(47,107,142,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,101,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(34,140,141,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(47,108,142,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(37,171,130,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(37,132,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(32,146,140,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(42,119,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(112,207,87,1)&#34;,&#34;rgba(41,123,142,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(68,191,112,1)&#34;,&#34;rgba(42,118,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(35,136,142,1)&#34;,&#34;rgba(65,68,135,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(50,100,142,1)&#34;,&#34;rgba(50,182,122,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,12,95,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(39,127,142,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,21,104,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(61,77,138,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(68,60,132,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(68,2,85,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(33,143,141,1)&#34;,&#34;rgba(49,104,142,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(32,163,134,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(69,3,87,1)&#34;,&#34;rgba(60,79,138,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(35,137,142,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(59,82,139,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(72,21,104,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(60,79,138,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(61,77,138,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(53,94,141,1)&#34;,&#34;rgba(46,179,124,1)&#34;,&#34;rgba(64,69,136,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(41,123,142,1)&#34;,&#34;rgba(55,91,141,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(43,116,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(46,111,142,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(65,68,135,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(67,61,132,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(72,21,103,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(69,5,89,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(48,106,142,1)&#34;,&#34;rgba(71,43,122,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(40,124,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(65,67,135,1)&#34;,&#34;rgba(67,61,132,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(60,80,139,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(45,112,142,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,34,116,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(35,136,142,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(56,89,140,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(58,83,139,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(70,51,127,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(71,16,98,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(35,138,141,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(49,102,142,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(37,171,130,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,42,122,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(44,114,142,1)&#34;,&#34;rgba(71,45,123,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(65,66,135,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(70,50,126,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(68,3,86,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(69,56,130,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(52,97,141,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,37,118,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(31,151,139,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(68,1,85,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(50,101,142,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(48,106,142,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(69,6,89,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(57,85,140,1)&#34;,&#34;rgba(63,71,136,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(52,96,141,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,8,91,1)&#34;,&#34;rgba(57,86,140,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(70,12,95,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(34,141,141,1)&#34;,&#34;rgba(53,95,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(34,139,141,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(41,122,142,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(39,126,142,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(37,133,142,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(69,55,129,1)&#34;,&#34;rgba(70,9,92,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,26,108,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,34,115,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(51,98,141,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,30,112,1)&#34;,&#34;rgba(37,132,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(70,8,91,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(31,149,139,1)&#34;,&#34;rgba(61,78,138,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(65,67,135,1)&#34;,&#34;rgba(72,20,102,1)&#34;,&#34;rgba(66,63,133,1)&#34;,&#34;rgba(62,75,137,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(57,87,140,1)&#34;,&#34;rgba(72,22,104,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(54,93,141,1)&#34;,&#34;rgba(70,9,93,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(31,158,137,1)&#34;,&#34;rgba(69,4,87,1)&#34;,&#34;rgba(68,59,132,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(71,44,122,1)&#34;,&#34;rgba(72,38,119,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,29,111,1)&#34;,&#34;rgba(62,73,137,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(68,1,84,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(41,121,142,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(71,46,124,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(69,4,88,1)&#34;,&#34;rgba(64,70,136,1)&#34;,&#34;rgba(70,8,92,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(44,115,142,1)&#34;,&#34;rgba(68,2,86,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(41,122,142,1)&#34;,&#34;rgba(66,65,134,1)&#34;,&#34;rgba(71,19,101,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,23,105,1)&#34;,&#34;rgba(71,17,100,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(71,19,102,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,40,120,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,52,128,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(58,84,140,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(66,64,134,1)&#34;,&#34;rgba(70,49,126,1)&#34;,&#34;rgba(72,30,111,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(71,15,98,1)&#34;,&#34;rgba(71,14,97,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,35,116,1)&#34;,&#34;rgba(63,72,137,1)&#34;,&#34;rgba(70,48,126,1)&#34;,&#34;rgba(55,90,140,1)&#34;,&#34;rgba(72,32,114,1)&#34;,&#34;rgba(72,41,121,1)&#34;,&#34;rgba(59,81,139,1)&#34;,&#34;rgba(71,47,125,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(37,133,142,1)&#34;,&#34;rgba(70,6,90,1)&#34;,&#34;rgba(72,33,114,1)&#34;,&#34;rgba(72,36,117,1)&#34;,&#34;rgba(72,31,112,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(46,110,142,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(72,24,106,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(70,7,91,1)&#34;,&#34;rgba(72,39,119,1)&#34;,&#34;rgba(71,43,122,1)&#34;,&#34;rgba(72,25,107,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(70,7,90,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(68,58,131,1)&#34;,&#34;rgba(72,28,110,1)&#34;,&#34;rgba(70,10,93,1)&#34;,&#34;rgba(72,20,103,1)&#34;,&#34;rgba(72,32,113,1)&#34;,&#34;rgba(70,11,94,1)&#34;,&#34;rgba(69,54,129,1)&#34;,&#34;rgba(69,53,129,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(71,16,99,1)&#34;,&#34;rgba(72,27,109,1)&#34;,&#34;rgba(72,33,115,1)&#34;,&#34;rgba(71,18,100,1)&#34;,&#34;rgba(71,13,96,1)&#34;,&#34;rgba(67,62,133,1)&#34;,&#34;rgba(68,57,131,1)&#34;,&#34;rgba(70,49,126,1)&#34;]}},&#34;hoveron&#34;:&#34;points&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:40.8401826484018,&#34;r&#34;:7.30593607305936,&#34;b&#34;:37.2602739726027,&#34;l&#34;:48.9497716894977},&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;title&#34;:{&#34;text&#34;:&#34;What are people saying about the best and worst movies on IMDB?&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:17.5342465753425},&#34;x&#34;:0,&#34;xref&#34;:&#34;paper&#34;},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-5.06,5.06],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;-5.0&#34;,&#34;-2.5&#34;,&#34;0.0&#34;,&#34;2.5&#34;,&#34;5.0&#34;],&#34;tickvals&#34;:[-5,-2.5,0,2.5,5],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;-5.0&#34;,&#34;-2.5&#34;,&#34;0.0&#34;,&#34;2.5&#34;,&#34;5.0&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;&#34;,&#34;tickcolor&#34;:null,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;‚Üê Bad Movies - Good Movies ‚Üí&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-5.06,5.06],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;-5.0&#34;,&#34;-2.5&#34;,&#34;0.0&#34;,&#34;2.5&#34;,&#34;5.0&#34;],&#34;tickvals&#34;:[-5,-2.5,0,2.5,5],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;-5.0&#34;,&#34;-2.5&#34;,&#34;0.0&#34;,&#34;2.5&#34;,&#34;5.0&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;&#34;,&#34;tickcolor&#34;:null,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;‚Üê Bad Reviews - Good Reviews ‚Üí&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:null,&#34;line&#34;:{&#34;color&#34;:null,&#34;width&#34;:0,&#34;linetype&#34;:[]},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;bgcolor&#34;:null,&#34;bordercolor&#34;:null,&#34;borderwidth&#34;:0,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;width&#34;:700,&#34;height&#34;:700,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;showSendToCloud&#34;:false,&#34;displayModeBar&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;103ac44aa7ca8&#34;:{&#34;xintercept&#34;:{},&#34;type&#34;:&#34;scatter&#34;},&#34;103ac14de1cd0&#34;:{&#34;yintercept&#34;:{}},&#34;103ac235a664&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;size&#34;:{},&#34;colour&#34;:{},&#34;text&#34;:{}}},&#34;cur_data&#34;:&#34;103ac44aa7ca8&#34;,&#34;visdat&#34;:{&#34;103ac44aa7ca8&#34;:[&#34;function (y) &#34;,&#34;x&#34;],&#34;103ac14de1cd0&#34;:[&#34;function (y) &#34;,&#34;x&#34;],&#34;103ac235a664&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;And we are done and it looks amazing! With this dataviz we are able to see that the word &lt;em&gt;overrated&lt;/em&gt; is mainly used in negative reviews about good movies. Likewise &lt;em&gt;unfunny&lt;/em&gt; is used in bad reviews about bad movies. There is many more examples that I‚Äôll let you explore by yourself.&lt;/p&gt;
&lt;p&gt;Thanks for tagging along!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nytimes.com/interactive/2017/11/07/upshot/modern-love-what-we-write-when-we-write-about-love.html&#34;&gt;The Words Men and Women Use When They Write About Love&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2 trial and error - US trade data</title>
      <link>/2018/06/12/ggplot2-trial-and-error-us-trade-data/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/2018/06/12/ggplot2-trial-and-error-us-trade-data/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blogpost will showcase an example of a workflow and its associated thought process when iterating though visualization styles working with &lt;code&gt;ggplot2&lt;/code&gt;. For this reason will this post include a lot of sub par charts as you are seeing the steps not just the final product.&lt;/p&gt;
&lt;p&gt;We will use census data concerning US trade with other nations which we scrape as well.&lt;/p&gt;
&lt;div id=&#34;setting-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up&lt;/h2&gt;
&lt;p&gt;We will using a standard set of packages, &lt;code&gt;tidyverse&lt;/code&gt; for general data manipulation, &lt;code&gt;rvest&lt;/code&gt; and &lt;code&gt;httr&lt;/code&gt; for scraping and manipulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(rvest)
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2
library(httr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the data&lt;/h2&gt;
&lt;p&gt;This project started when I found the following link &lt;a href=&#34;https://www.census.gov/foreign-trade/balance/c4099.html&#34;&gt;https://www.census.gov/foreign-trade/balance/c4099.html&lt;/a&gt;. It include a month by month breakdown of U.S. trade in goods with Denmark from 1985 till the present. Unfortunately the data is given in yearly tables, so we have a little bit of munching to do. First we notice that the last part of the url include &lt;code&gt;c4099&lt;/code&gt;, which after some googling reveals that 4099 is the country code for Denmark. The fill list of country trade codes are given on the following page &lt;a href=&#34;https://www.census.gov/foreign-trade/schedules/c/countrycode.html&#34;&gt;https://www.census.gov/foreign-trade/schedules/c/countrycode.html&lt;/a&gt; which also include a &lt;code&gt;.txt&lt;/code&gt; file so we don‚Äôt have to scrape. We will remove the first entry and last 6 since US doesn‚Äôt trade with itself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;continent_df &amp;lt;- tibble(number = as.character(1:7),
                       continent = c(&amp;quot;North America&amp;quot;, &amp;quot;Central America&amp;quot;, 
                                     &amp;quot;South America&amp;quot;, &amp;quot;Europe&amp;quot;, &amp;quot;Asia&amp;quot;, 
                                     &amp;quot;Australia and Oceania&amp;quot;, &amp;quot;Africa&amp;quot;))

code_df &amp;lt;- read_lines(&amp;quot;https://www.census.gov/foreign-trade/schedules/c/country.txt&amp;quot;,
                      skip = 5) %&amp;gt;%
  tibble(code = .) %&amp;gt;%
  separate(code, c(&amp;quot;code&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;iso&amp;quot;), sep = &amp;quot;\\|&amp;quot;) %&amp;gt;%
  mutate_all(trimws) %&amp;gt;%
  mutate(con_code = str_sub(code, 1, 1)) %&amp;gt;%
  filter(!is.na(iso), 
         name != &amp;quot;United States of America&amp;quot;, 
         con_code != 9) %&amp;gt;%
  left_join(continent_df, by = c(&amp;quot;con_code&amp;quot; = &amp;quot;number&amp;quot;)) %&amp;gt;%
  select(-con_code)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these code we create the targeted urls we will be scraping&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;target_urls &amp;lt;- str_glue(&amp;quot;https://www.census.gov/foreign-trade/balance/c{code_df$code}.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be replication hrbrmstr‚Äôs scraping code &lt;a href=&#34;https://rud.is/b/2017/09/19/pirating-web-content-responsibly-with-r/&#34;&gt;found here&lt;/a&gt; since it works wonderfully.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_GET &amp;lt;- safely(GET)

pb &amp;lt;- progress_estimated(length(target_urls))
map(target_urls, ~{
  pb$tick()$print()
  Sys.sleep(5)
  s_GET(.x)
}) -&amp;gt; httr_raw_responses

write_rds(httr_raw_responses, &amp;quot;data/2018-us-trade-raw-httr-responses.rds&amp;quot;)

good_responses &amp;lt;- keep(httr_raw_responses, ~!is.null(.x$result))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then we wrangle all the html files by extracting all the tables, parse the numeric variables and combining them to one table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wrangle &amp;lt;- function(x, name) {
  # Read html and extract tables
  read_html(x[[1]]) %&amp;gt;%
  html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;%
  html_table() %&amp;gt;%
  # parse numeric columns
  map(~ mutate_at(.x, vars(Exports:Balance), funs(parse_number))) %&amp;gt;%
  bind_rows() %&amp;gt;%
  mutate(Country = name)
}

full_data &amp;lt;- map2_df(good_responses, code_df$code, wrangle)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly we do some polishing up with the date variables and join in the country information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data &amp;lt;- full_data %&amp;gt;%
  filter(!str_detect(Month, &amp;quot;TOTAL&amp;quot;)) %&amp;gt;%
  mutate(Date = parse_date(Month, format = &amp;quot;%B %Y&amp;quot;), 
         Month = lubridate::month(Date),
         Year = lubridate::year(Date)) %&amp;gt;%
  left_join(code_df, by = c(&amp;quot;Country&amp;quot; = &amp;quot;code&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Giving us this data to work with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(trade_data)
## Rows: 75,379
## Columns: 10
## $ Month     &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,‚Ä¶
## $ Exports   &amp;lt;dbl&amp;gt; 0.1, 0.4, 1.8, 0.2, 0.1, 1.3, 3.2, 0.6, 0.3, 0.6, 0.7, 1.4,‚Ä¶
## $ Imports   &amp;lt;dbl&amp;gt; 1.0, 2.4, 2.2, 0.8, 0.2, 0.5, 0.8, 0.9, 0.4, 2.4, 0.5, 1.0,‚Ä¶
## $ Balance   &amp;lt;dbl&amp;gt; -0.8, -2.0, -0.4, -0.6, -0.1, 0.7, 2.4, -0.3, -0.1, -1.8, 0‚Ä¶
## $ Country   &amp;lt;dbl&amp;gt; 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010,‚Ä¶
## $ Date      &amp;lt;date&amp;gt; 2018-01-01, 2018-02-01, 2018-03-01, 2018-04-01, 2017-01-01‚Ä¶
## $ Year      &amp;lt;dbl&amp;gt; 2018, 2018, 2018, 2018, 2017, 2017, 2017, 2017, 2017, 2017,‚Ä¶
## $ name      &amp;lt;chr&amp;gt; &amp;quot;Greenland&amp;quot;, &amp;quot;Greenland&amp;quot;, &amp;quot;Greenland&amp;quot;, &amp;quot;Greenland&amp;quot;, &amp;quot;Greenl‚Ä¶
## $ iso       &amp;lt;chr&amp;gt; &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;, &amp;quot;GL&amp;quot;,‚Ä¶
## $ continent &amp;lt;chr&amp;gt; &amp;quot;North America&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;North A‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-get-vizualising&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lets get vizualising!&lt;/h2&gt;
&lt;p&gt;Lets set a different theme for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets start out nice and simple by plotting a simple scatter plot for just a single country to get a feel for the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  filter(name == &amp;quot;Greenland&amp;quot;) %&amp;gt;%
  ggplot(aes(Date, Balance)) +
  geom_point() +
  labs(title = &amp;quot;United States Trade Balance in Goods with Greenland&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which looks good already! Lets see how it would look with as a line chart instead&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  filter(name == &amp;quot;Greenland&amp;quot;) %&amp;gt;%
  ggplot(aes(Date, Balance)) +
  geom_line() +
  labs(title = &amp;quot;United States Trade Balance in Goods with Greenland&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;it sure it quite jagged! Lets take a loot at the 4 biggest spiked to see if it is a indication of a trend&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  filter(name == &amp;quot;Greenland&amp;quot;, Balance &amp;gt; 5)
## # A tibble: 4 x 10
##   Month Exports Imports Balance Country Date        Year name   iso   continent 
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;     
## 1     3     7.9     0.5     7.4    1010 2014-03-01  2014 Green‚Ä¶ GL    North Ame‚Ä¶
## 2     3    10.4     1       9.4    1010 2013-03-01  2013 Green‚Ä¶ GL    North Ame‚Ä¶
## 3     3    10.5     0.6     9.9    1010 2012-03-01  2012 Green‚Ä¶ GL    North Ame‚Ä¶
## 4     9    20       1.3    18.8    1010 2008-09-01  2008 Green‚Ä¶ GL    North Ame‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which didn‚Äôt give us much, 3 of the spikes happened in March and the last one a random September. It was worth a try, back to plotting! Lets see how a smooth curve would look overlaid the line chart&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  filter(name == &amp;quot;Greenland&amp;quot;) %&amp;gt;%
  ggplot(aes(Date, Balance)) +
  geom_line() +
  geom_smooth(se = FALSE) +
  labs(title = &amp;quot;United States Trade Balance in Goods with Greenland&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which looks nice in and off itself, however since this chart looks at the trade balance between two countries is the value 0 quite important and should be highlighted better. I will add a line behind the data points such that it highlights rather then hides&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  filter(name == &amp;quot;Greenland&amp;quot;) %&amp;gt;%
  ggplot(aes(Date, Balance)) +
  geom_abline(slope = 0, intercept = 0, color = &amp;quot;orange&amp;quot;) +
  geom_line() +
  geom_smooth(se = FALSE) +
  labs(title = &amp;quot;United States Trade Balance in Goods with Greenland&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us better indication for when the trade is positive or negative with respect to the United States. Lets take it up a notch and include a couple more countries. We remove the filter and add &lt;code&gt;Country&lt;/code&gt; as the &lt;code&gt;group&lt;/code&gt; aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line() +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we have 3 different problems I would like to fix right now. The scale between these different countries is massively different! The very negative balance of one country is making it hard to see what happens to the other countries. Secondly it is hard to distinguish the different countries since they are all the same color. And lastly there is some serious over plotting, this point is tired to the other problems so lets see if we can fix them one at a time.&lt;/p&gt;
&lt;p&gt;First lets transform the scales on the y axis such that we better can identify individual lines. We do this with the square root transformation which gives weights to values close to 0 and shrinks values far away form 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line() + 
  scale_y_sqrt() +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With square root transformation&amp;quot;)
## Warning in self$trans$transform(x): NaNs produced
## Warning: Transformation introduced infinite values in continuous y-axis
## Warning: Removed 11918 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh no! We lost all the negative values. This happened because the normal square root operation only works with positive numbers. We fix this by using the &lt;em&gt;signed square root&lt;/em&gt; which apply the square root to both the positive and negative as if they were positive and then signs them accordingly. for this we create a new transformation with the &lt;code&gt;scales&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;S_sqrt &amp;lt;- function(x) sign(x) * sqrt(abs(x))
IS_sqrt &amp;lt;- function(x) x ^ 2 * sign(x) 
S_sqrt_trans &amp;lt;- function() scales::trans_new(&amp;quot;S_sqrt&amp;quot;, S_sqrt, IS_sqrt)

trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line() + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much better! We will fix the the breaks a little bit too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line() + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now lets solve the problem with over plotting, a standard trick is to introduce transparency, this is done using the &lt;code&gt;alpha&lt;/code&gt; aesthetic. Lets start with &lt;code&gt;0.5&lt;/code&gt; alpha.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line(alpha = 0.5) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;slightly better but not good enough, lets try &lt;code&gt;0.2&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;much better! Another thing we could do if coloring depending on the the continent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country, color = continent)) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is quite messy, however we notice that the data for the African countries don‚Äôt cover the same range as the other countries. Lets see if there are some overall trends within each continent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  geom_line(alpha = 0.2) + 
  geom_smooth(aes(Date, Balance, group = continent, color = continent), se = FALSE) +
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives some more tangible information. There is a upwards trend within North America for the last 10 years, where Asia have had a slow decline since the beginning of the data collection.&lt;/p&gt;
&lt;p&gt;Next lets see what happens when you facet depending on continent&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country)) +
  facet_wrap(~ continent) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation faceted depending on continent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These look really nice, lets free up the scale on the y axis within each facet such that we can differentiate the lines better, on top of that lets re introduce the colors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country, color = continent)) +
  facet_wrap(~ continent, scales = &amp;quot;free_y&amp;quot;) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation faceted depending on continent&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;lets remove the color legend as the information is already present in the facet labels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country, color = continent)) +
  facet_wrap(~ continent, scales = &amp;quot;free_y&amp;quot;) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation faceted depending on continent&amp;quot;) +
  guides(color = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lastly lets overlay the smooth continent average&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country, color = continent)) +
  facet_wrap(~ continent, scales = &amp;quot;free_y&amp;quot;) +
  geom_line(alpha = 0.2) + 
  geom_smooth(aes(group = continent), color = &amp;quot;grey40&amp;quot;, se = FALSE) +
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation faceted depending on continent&amp;quot;) +
  guides(color = &amp;quot;none&amp;quot;)
## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately doesn‚Äôt add too much information so lets remove it again. Lastly lets update the the labels to reflect the the unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trade_data %&amp;gt;% 
  ggplot(aes(Date, Balance, group = Country, color = continent)) +
  facet_wrap(~ continent, scales = &amp;quot;free_y&amp;quot;) +
  geom_line(alpha = 0.2) + 
  coord_trans(y = &amp;quot;S_sqrt&amp;quot;) +
  scale_y_continuous(breaks = c(0, -1750, -7500, -17000, -30000, 0, 1750, 7500),
                     minor_breaks = NULL) +
  labs(title = &amp;quot;United States Trade Balance in Goods with all countries&amp;quot;,
       subtitle = &amp;quot;With signed square root transformation faceted depending on continent&amp;quot;,
       y = &amp;quot;Balance (in millions of U.S. dollars on a nominal basis)&amp;quot;) +
  guides(color = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Emoji use on Twitter</title>
      <link>/2018/06/04/emoji-use-on-twitter/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/2018/06/04/emoji-use-on-twitter/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post will be a short demonstration on how the the occurrence of emojis on twitter can be analysed using tidytools. We start of loading the necessary packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(rtweet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have decided that for this example that I would focus on tweets that include the hash tags #happy and #sad in the hope that both would include a similar number of emojis but hopefully of different groups. We will use the &lt;code&gt;rtweet&lt;/code&gt; package which already conforms to the tidy principles. Notice the &lt;code&gt;retryonratelimit = TRUE&lt;/code&gt; argument as the combined number of tweets (10000 + 10000 = 20000) is larger the the 15 min limit of 18000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets_happy &amp;lt;- search_tweets(&amp;quot;#happy&amp;quot;, n = 10000, include_rts = FALSE)
tweets_sad &amp;lt;- search_tweets(&amp;quot;#sad&amp;quot;, n = 10000, include_rts = FALSE, 
                            retryonratelimit = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a safety will we save these tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_as_csv(tweets_happy, &amp;quot;tweets_happy.csv&amp;quot;)
write_as_csv(tweets_sad, &amp;quot;tweets_sad.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we load this data.frame that contains information regarding the various emojis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emoji &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/EmilHvitfeldt/Emoji-table/master/emoji.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we add the hash tag label as the &lt;code&gt;emotion&lt;/code&gt; variable, next we tokenize all the tweets according to characters (this is done since a lot of the tweets didn‚Äôt use spaces emojis rendering them hard to detect.) and left join with the &lt;code&gt;emoji&lt;/code&gt; data.frame such that we get the descriptions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tweets_all &amp;lt;- bind_rows(
  tweets_happy %&amp;gt;% mutate(emotion = &amp;quot;#happy&amp;quot;),
  tweets_sad %&amp;gt;% mutate(emotion = &amp;quot;#sad&amp;quot;)
)

emoji_all &amp;lt;- unnest_tokens(tweets_all, word, text, 
                           token = &amp;quot;characters&amp;quot;) %&amp;gt;%
  select(word, emotion) %&amp;gt;%
  left_join(emoji, by = c(&amp;quot;word&amp;quot; = &amp;quot;utf&amp;quot;)) %&amp;gt;%
  filter(!is.na(shortname))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly we create a simple faceted bar chart of the number of emojis used within each hash tag.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emoji_all %&amp;gt;%
  count(word, emotion, shortname) %&amp;gt;%
  group_by(emotion) %&amp;gt;%
  arrange(desc(n)) %&amp;gt;%
  top_n(10, n) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(emoji = reorder(shortname, n)) %&amp;gt;%
  ggplot(aes(emoji, n)) +
  geom_col() +
  facet_grid(emotion ~ ., scales = &amp;quot;free_y&amp;quot;) +
  coord_flip() +
  theme_minimal() +
  labs(title = &amp;quot;Emojis used in #happy and #sad tweets&amp;quot;,
       y = &amp;quot;Count&amp;quot;, x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;emoji&lt;/code&gt; data.frame allows us to gain quick insight with the descriptive short names.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using PCA for word embedding in R</title>
      <link>/2018/05/22/using-pca-for-word-embedding-in-r/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      <guid>/2018/05/22/using-pca-for-word-embedding-in-r/</guid>
      <description>


&lt;p&gt;In my earlier post on &lt;a href=&#34;https://www.hvitfeldt.me/2018/03/binary-text-classification-with-tidytext-and-caret/&#34;&gt;binary text classification&lt;/a&gt; was one of the problems that occurred was the sheer size of the data when trying to fit a model. The bag of words method of having each column describe the occurrence of a specific word in each document (row) is appealing from a mathematical perspective, but gives rise for large sparse matrices which aren‚Äôt handled well by some models in R. This leads to slow running code at best and crashing at worst.&lt;/p&gt;
&lt;p&gt;We will try to combat this problem by using something called &lt;a href=&#34;https://en.wikipedia.org/wiki/Word_embedding&#34;&gt;word embedding&lt;/a&gt; which is a general term for the process of mapping textural information to a lower dimensional space. This is a special case of dimensionality reduction, and we will use the simple well known method &lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal component analysis&lt;/a&gt; for our word embedding today. We are essentially trying to squeeze as much information into as little space as possible such that our models can run in a reasonable time.&lt;/p&gt;
&lt;p&gt;We will use the same data as in the earlier post, and the PCA procedure is very inspired by Julia Silge recent post &lt;a href=&#34;https://juliasilge.com/blog/stack-overflow-pca/&#34;&gt;Understanding PCA using Stack Overflow data&lt;/a&gt; which you should read if you haven‚Äôt already!!&lt;/p&gt;
&lt;div id=&#34;data-prepossessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data prepossessing&lt;/h2&gt;
&lt;p&gt;We will use standard &lt;code&gt;tidyverse&lt;/code&gt; tool set for this post. We will use &lt;code&gt;randomForest&lt;/code&gt; model as this approach should be much faster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(broom)
library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data we will be using for this demonstration will be some &lt;a href=&#34;https://data.world/crowdflower/disasters-on-social-media&#34;&gt;social media disaster tweets&lt;/a&gt; discussed in &lt;a href=&#34;https://arxiv.org/pdf/1705.02009.pdf&#34;&gt;this article&lt;/a&gt;.
It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren‚Äôt classified, marked &lt;code&gt;&#34;Can&#39;t Decide&#34;&lt;/code&gt; so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don‚Äôt actually have that much information in them as they are limited in characters and some only contain a couple of words.&lt;/p&gt;
&lt;p&gt;We will at this stage remove what appears to be urls using some regex and &lt;code&gt;str_replace_all&lt;/code&gt;, and we will select the columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;disaster&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_clean &amp;lt;- data %&amp;gt;%
  filter(choose_one != &amp;quot;Can&amp;#39;t Decide&amp;quot;) %&amp;gt;%
  mutate(id = `_unit_id`,
         disaster = choose_one == &amp;quot;Relevant&amp;quot;,
         text = str_replace_all(text, &amp;quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
  select(id, disaster, text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then extract all unigrams, bigram and remove stopwords.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_counts &amp;lt;- map_df(1:2,
                      ~ unnest_tokens(data_clean, word, text, 
                                      token = &amp;quot;ngrams&amp;quot;, n = .x)) %&amp;gt;%
  anti_join(stop_words, by = &amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will only focus on the top 10000 most used words for the remainder of the analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top10000 &amp;lt;- data_counts %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  slice(1:10000) %&amp;gt;%
  select(word)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we will then count the words again, but this time we will count the word occurrence within each document and remove the underused words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unnested_words &amp;lt;- data_counts %&amp;gt;%
  count(id, word, sort = TRUE) %&amp;gt;%
  inner_join(top10000, by = &amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then cast the data.frame to a sparse matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sparse_word_matrix &amp;lt;- unnested_words %&amp;gt;%
  cast_sparse(id, word, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the last post we used this matrix for the modeling, but the size was quite obstacle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(sparse_word_matrix)
## [1] 10829 10000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a row for each document and a row for each of the top 10000 words, but most of the elements are empty so each of the variables don‚Äôt contain much information. We will do word embedding by applying PCA to the sparse word count matrix. Like Julia Silge we will use the wonderful &lt;a href=&#34;https://bwlewis.github.io/irlba/&#34;&gt;irlba&lt;/a&gt; package that facilities PCA on sparse matrices. First we scale the matrix and then we apply PCA where we request 64 columns.&lt;/p&gt;
&lt;p&gt;This stage will take some time, but that is the trade-off we will be making when using word embedding. We take some computation time up front in exchange for quick computation later down the line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_scaled &amp;lt;- scale(sparse_word_matrix)
word_pca &amp;lt;- irlba::prcomp_irlba(word_scaled, n = 64)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we will create a meta data.frame to take care of tweets that disappeared when we cleaned them earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- tibble(id = as.numeric(dimnames(sparse_word_matrix)[[1]])) %&amp;gt;%
  left_join(data_clean[!duplicated(data_clean$id), ], by = &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we combine the PCA matrix with proper response variable (disaster/no-disaster) with the addition ot a training/testing split variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class_df &amp;lt;- data.frame(word_pca$x) %&amp;gt;%
  mutate(response = factor(meta$disaster),
         split = sample(0:1, NROW(meta), replace = TRUE, prob = c(0.2, 0.8)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data frame with 64 explanatory variables instead of the 10000 we started with. This a huge reduction which hopefully should pay off. For this demonstration will we try using two kinds of models. Standard logistic regression and a random forest model. Logistic regression is a good baseline which should be blazing fast now since the reduction have taken place and the random forest model which generally was quite slow should be more manageable this time around.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- glm(response ~ ., 
             data = filter(class_df, split == 1), 
             family = binomial)
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

y_pred &amp;lt;- predict(model, 
                  type = &amp;quot;response&amp;quot;,
                  newdata = filter(class_df, split == 0) %&amp;gt;% select(-response))
## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading

y_pred_logical &amp;lt;- if_else(y_pred &amp;gt; 0.5, 1, 0)
(con &amp;lt;- table(y_pred_logical, filter(class_df, split == 0) %&amp;gt;% pull(response)))
##               
## y_pred_logical FALSE TRUE
##              0  1180  631
##              1    82  330
sum(diag(con)) / sum(con)
## [1] 0.6792623&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it work fairly quickly and we get a decent accuracy of 68%. Remember this method isn‚Äôt meant to improve the accuracy but rather to improve the computational time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- randomForest(response ~ ., 
                      data = filter(class_df, split == 1))

y_pred &amp;lt;- predict(model, 
                  type = &amp;quot;class&amp;quot;,
                  newdata = filter(class_df, split == 0) %&amp;gt;% select(-response))

(con &amp;lt;- table(y_pred, filter(class_df, split == 0) %&amp;gt;% pull(response)))
##        
## y_pred  FALSE TRUE
##   FALSE  1106  379
##   TRUE    156  582
sum(diag(con)) / sum(con)
## [1] 0.7593342&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This one takes slightly longer to run due to the number of trees, but it does give us the nifty 76% accuracy which is pretty good considering we only look at tweets.&lt;/p&gt;
&lt;p&gt;And this is all that there is to it! The dimensionality reduction method was able to reduce the number of variables while retaining most of the information within those variables such that we can run our procedures at a faster phase without much loss. There is still a lot of individual improvements to be done if this was to be used further, both in terms of hyper-parameter selection in the modeling choices but also the number of PCA variables that should be used in the final modelling. Remember that this is just one of the more simpler methods, with more advanced word representation methods being &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;glove&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/tutorials/word2vec&#34;&gt;word2vec&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-viz&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data viz&lt;/h2&gt;
&lt;p&gt;Since Julia did most of the legwork for the visualizations so we will take a look at how each of the words contribute to the first four components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidied_pca &amp;lt;- bind_cols(Tag = colnames(word_scaled),
                        tidy(word_pca$rotation)) %&amp;gt;%
    gather(PC, Contribution, PC1:PC4)
## Warning: &amp;#39;tidy.matrix&amp;#39; is deprecated.
## See help(&amp;quot;Deprecated&amp;quot;)

tidied_pca %&amp;gt;% 
    filter(PC %in% paste0(&amp;quot;PC&amp;quot;, 1:4)) %&amp;gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE) +
    theme(axis.text.x = element_blank(), 
          axis.ticks.x = element_blank()) + 
    labs(x = &amp;quot;Words&amp;quot;,
         y = &amp;quot;Relative importance in each principal component&amp;quot;) +
    facet_wrap(~ PC, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we see is quite different then what Julia found in her study. We have just a few words doing most of the contributions in each of component. Lets zoom in to take a look at the words with the most influence on the different components:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_df(c(-1, 1) * 20,
    ~ tidied_pca %&amp;gt;%
        filter(PC == &amp;quot;PC1&amp;quot;) %&amp;gt;% 
        top_n(.x, Contribution)) %&amp;gt;%
    mutate(Tag = reorder(Tag, Contribution)) %&amp;gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
          axis.ticks.x = element_blank()) +
    labs(x = &amp;quot;Words&amp;quot;,
         y = &amp;quot;Relative importance in principle component&amp;quot;,
         title = &amp;quot;PC1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We would like to see some sensible separation between the positive words and the negative words (with regard to contribution). However I haven‚Äôt been able to come up with a meaning full grouping for the first 3 components. The fourth on the other hand have all the positive influencing words containing numbers in one way or another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_df(c(-1, 1) * 20,
    ~ tidied_pca %&amp;gt;%
        filter(PC == &amp;quot;PC4&amp;quot;) %&amp;gt;% 
        top_n(.x, Contribution)) %&amp;gt;%
    mutate(Tag = reorder(Tag, Contribution)) %&amp;gt;%
    ggplot(aes(Tag, Contribution, fill = Tag)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
          axis.ticks.x = element_blank()) +
    labs(x = &amp;quot;Words&amp;quot;,
         y = &amp;quot;Relative importance in principle component&amp;quot;,
         title = &amp;quot;PC4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-22-using-pca-for-word-embedding-in-r/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is all I have for this time. Hope you enjoyed it!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing ethnic diversity in Californian school</title>
      <link>/2018/05/01/analysing-ethnic-diversity-in-californian-school/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/2018/05/01/analysing-ethnic-diversity-in-californian-school/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I will In this post explore the ethnic diversity of the student population in schools in California. We will utilize the data provided by &lt;a href=&#34;https://www.cde.ca.gov/&#34;&gt;California Department of Education&lt;/a&gt; that has &lt;a href=&#34;https://www.cde.ca.gov/ds/sd/sd/filesenr.asp&#34;&gt;Enrollment by School&lt;/a&gt; which includes ‚Äúschool-level enrollment by racial/ethnic designation, gender, and grade‚Äù.&lt;/p&gt;
&lt;p&gt;We will combine this data with two other datasets that contain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Longitude and latitude of the schools&lt;/li&gt;
&lt;li&gt;Income information of the cities the schools are located in&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully we will be able to draw some cool inference using these datasets while working though the complications you get when you combine datasets from the wild.&lt;/p&gt;
&lt;div id=&#34;loading-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading packages&lt;/h2&gt;
&lt;p&gt;This will be a fairly standard data science exercise so we will stick to the &lt;code&gt;tidyverse&lt;/code&gt;, &lt;code&gt;rvest&lt;/code&gt; for scraping, &lt;code&gt;patchwork&lt;/code&gt; for plot stitching and add a little fancyness with &lt;code&gt;ggmap&lt;/code&gt; for local maps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)
library(ggmap)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;enrollment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enrollment data&lt;/h2&gt;
&lt;p&gt;We start by downloading the dataset to our local disk and read it from there this is mainly to be nice, and for the fact the the download speed on the files we work with are a little slow. This can be done using &lt;code&gt;readr&lt;/code&gt;‚Äôs &lt;code&gt;read_tsv()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- readr::read_tsv(&amp;quot;filesenr.asp.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get an idea of the data lets have a quick &lt;code&gt;glimpse()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 129,813
## Columns: 23
## $ CDS_CODE  &amp;lt;chr&amp;gt; &amp;quot;33672490000001&amp;quot;, &amp;quot;33672490000001&amp;quot;, &amp;quot;33672490000001&amp;quot;, &amp;quot;3367‚Ä¶
## $ COUNTY    &amp;lt;chr&amp;gt; &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Rivers‚Ä¶
## $ DISTRICT  &amp;lt;chr&amp;gt; &amp;quot;San Jacinto Unified&amp;quot;, &amp;quot;San Jacinto Unified&amp;quot;, &amp;quot;San Jacinto ‚Ä¶
## $ SCHOOL    &amp;lt;chr&amp;gt; &amp;quot;Nonpublic, Nonsectarian Schools&amp;quot;, &amp;quot;Nonpublic, Nonsectarian‚Ä¶
## $ ETHNIC    &amp;lt;dbl&amp;gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5,‚Ä¶
## $ GENDER    &amp;lt;chr&amp;gt; &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;,‚Ä¶
## $ KDGN      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4‚Ä¶
## $ GR_1      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4‚Ä¶
## $ GR_2      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3‚Ä¶
## $ GR_3      &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6‚Ä¶
## $ GR_4      &amp;lt;dbl&amp;gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4‚Ä¶
## $ GR_5      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6‚Ä¶
## $ GR_6      &amp;lt;dbl&amp;gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_7      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_8      &amp;lt;dbl&amp;gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ UNGR_ELM  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_9      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_10     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_11     &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_12     &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ UNGR_SEC  &amp;lt;dbl&amp;gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ ENR_TOTAL &amp;lt;dbl&amp;gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1‚Ä¶
## $ ADULT     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we notice that &lt;code&gt;CDS_CODE&lt;/code&gt; is being read as a chr which is favorable in this case since we don‚Äôt actually want it as a integer but rather as an ID variable. If this column had been converted to integers we would have lost leading zeros which could lead to trouble.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;longitude-and-latitude-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Longitude and latitude data&lt;/h2&gt;
&lt;p&gt;The longitude and latitude of the schools are also available on California Department of Education‚Äôs website &lt;a href=&#34;https://www.cde.ca.gov/ds/si/ds/fspubschls.asp&#34;&gt;here&lt;/a&gt;. It includes quite a lot of information, a lot of it uninteresting for this project so will take a subset of it for further analysis. &lt;code&gt;read_tsv&lt;/code&gt; complaints a little bit when reading it in, but after some crosschecking with the .xls file also provided, does it seem to be correct so we will ignore the error (don‚Äôt just ignore error all the time! most of the time they are telling you something important! in this case the problems stems from missing values and tab separated values don‚Äôt mix that nicely).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;longlat_raw &amp;lt;- read_tsv(&amp;quot;pubschls.txt&amp;quot;)
longlat &amp;lt;- longlat_raw %&amp;gt;%
  select(CDSCode, Latitude, Longitude, School, City)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets have a peak at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(longlat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 10,014
## Columns: 5
## $ CDSCode   &amp;lt;chr&amp;gt; &amp;quot;01100170000000&amp;quot;, &amp;quot;01100170109835&amp;quot;, &amp;quot;01100170112607&amp;quot;, &amp;quot;0110‚Ä¶
## $ Latitude  &amp;lt;dbl&amp;gt; 37.65821, 37.52144, 37.80452, 37.86899, 37.78465, 37.84737,‚Ä¶
## $ Longitude &amp;lt;dbl&amp;gt; -122.0971, -121.9939, -122.2682, -122.2784, -122.2386, -122‚Ä¶
## $ School    &amp;lt;chr&amp;gt; NA, &amp;quot;FAME Public Charter&amp;quot;, &amp;quot;Envision Academy for Arts &amp;amp; Tec‚Ä¶
## $ City      &amp;lt;chr&amp;gt; &amp;quot;Hayward&amp;quot;, &amp;quot;Newark&amp;quot;, &amp;quot;Oakland&amp;quot;, &amp;quot;Berkeley&amp;quot;, &amp;quot;Oakland&amp;quot;, &amp;quot;Oak‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we see that the the columns have been read in in appropriate ways. We recognize the &lt;code&gt;CDS_CODE&lt;/code&gt; from before as &lt;code&gt;CDSCode&lt;/code&gt; in this dataset which we will use to combine the datasets later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;income-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Income data&lt;/h2&gt;
&lt;p&gt;Lastly we will get some income data, I found some fairly good data on &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_California_locations_by_income&#34;&gt;Wikipedia&lt;/a&gt;. We use simple &lt;code&gt;rvest&lt;/code&gt; tools to extract the table from the website and give it column names.&lt;/p&gt;
&lt;p&gt;While the income data is quite lovely, would it be outside the scope of this post to identify which census-designated place (CDP) each of the schools belong it. The second best option is to use the income data on a county by county basis. This will ofcause mean that we trade a bit of granularity for time. But hopefully it will still lead to some meaningful findings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;https://en.wikipedia.org/wiki/List_of_California_locations_by_income&amp;quot;
income_data &amp;lt;- read_html(url) %&amp;gt;%
  html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;%
  .[2] %&amp;gt;%
  html_table() %&amp;gt;%
  .[[1]] %&amp;gt;%
  set_names(c(&amp;quot;county&amp;quot;, &amp;quot;population&amp;quot;, &amp;quot;population_density&amp;quot;, 
              &amp;quot;per_capita_income&amp;quot;, &amp;quot;median_household_income&amp;quot;,
              &amp;quot;median_family_income&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lets take a look at the table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(income_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 58
## Columns: 6
## $ county                  &amp;lt;chr&amp;gt; &amp;quot;Alameda&amp;quot;, &amp;quot;Alpine&amp;quot;, &amp;quot;Amador&amp;quot;, &amp;quot;Butte&amp;quot;, &amp;quot;Cala‚Ä¶
## $ population              &amp;lt;chr&amp;gt; &amp;quot;1,559,308&amp;quot;, &amp;quot;1,202&amp;quot;, &amp;quot;37,159&amp;quot;, &amp;quot;221,578&amp;quot;, &amp;quot;4‚Ä¶
## $ population_density      &amp;lt;chr&amp;gt; &amp;quot;2,109.8&amp;quot;, &amp;quot;1.6&amp;quot;, &amp;quot;62.5&amp;quot;, &amp;quot;135.4&amp;quot;, &amp;quot;44.0&amp;quot;, &amp;quot;1‚Ä¶
## $ per_capita_income       &amp;lt;chr&amp;gt; &amp;quot;$36,439&amp;quot;, &amp;quot;$24,375&amp;quot;, &amp;quot;$27,373&amp;quot;, &amp;quot;$24,430&amp;quot;, &amp;quot;‚Ä¶
## $ median_household_income &amp;lt;chr&amp;gt; &amp;quot;$73,775&amp;quot;, &amp;quot;$61,343&amp;quot;, &amp;quot;$52,964&amp;quot;, &amp;quot;$43,165&amp;quot;, &amp;quot;‚Ä¶
## $ median_family_income    &amp;lt;chr&amp;gt; &amp;quot;$90,822&amp;quot;, &amp;quot;$71,932&amp;quot;, &amp;quot;$68,765&amp;quot;, &amp;quot;$56,934&amp;quot;, &amp;quot;‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;here we see a couple of things that look weirds. Every column characters valued, which it shouldn‚Äôt be since 5 out of the 6 variables should be numerical.&lt;/p&gt;
&lt;p&gt;We will use the wonderful &lt;code&gt;parse_number&lt;/code&gt; function from the &lt;code&gt;readr&lt;/code&gt; package to convert the character values to numeric.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;income_data_clean &amp;lt;- income_data %&amp;gt;%
  mutate_at(vars(population:median_family_income), parse_number)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;a quick glimpse to see everything went well&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(income_data_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 58
## Columns: 6
## $ county                  &amp;lt;chr&amp;gt; &amp;quot;Alameda&amp;quot;, &amp;quot;Alpine&amp;quot;, &amp;quot;Amador&amp;quot;, &amp;quot;Butte&amp;quot;, &amp;quot;Cala‚Ä¶
## $ population              &amp;lt;dbl&amp;gt; 1559308, 1202, 37159, 221578, 44921, 21424, 1‚Ä¶
## $ population_density      &amp;lt;dbl&amp;gt; 2109.8, 1.6, 62.5, 135.4, 44.0, 18.6, 1496.0,‚Ä¶
## $ per_capita_income       &amp;lt;dbl&amp;gt; 36439, 24375, 27373, 24430, 29296, 22211, 387‚Ä¶
## $ median_household_income &amp;lt;dbl&amp;gt; 73775, 61343, 52964, 43165, 54936, 50503, 797‚Ä¶
## $ median_family_income    &amp;lt;dbl&amp;gt; 90822, 71932, 68765, 56934, 67100, 56472, 950‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we are good to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ethnic-diversity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ethnic diversity&lt;/h2&gt;
&lt;p&gt;To be able to compare the ethnic diversity between schools we need a measure that describes diversity to begin with. I‚Äôll use the diversity index &lt;a href=&#34;https://www.unc.edu/~pmeyer/carstat/tools.html&#34;&gt;developed in 1991&lt;/a&gt; by a researcher at the University of North Carolina at Chapel Hill. The index simply asks: ‚ÄúWhat is the probability that two people in a population picked at random will be from different ethnicity‚Äù. At first this can be seen as a daunting task, but if we look at it geometrically we notice that the calculations are quite straight forward.&lt;/p&gt;
&lt;p&gt;If we imagine a population of 10 people split into two ethnicities, with 5 in each. Then we can draw the following outcome space:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(makeup = rep(5, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(the code for &lt;code&gt;diversity_plot&lt;/code&gt; is displayed in the end of the article for those interested.)&lt;/p&gt;
&lt;p&gt;Where the colored squares represent random picks with the same ethnicity, light grey squares picks with different ethnicities. Dark grey indicate impossible picks (same person picked twice) and should be ignored. Now the diversity index can be calculated by dividing the number of light grey squares with the sum of light grey and colored squares.&lt;/p&gt;
&lt;p&gt;Before we go on, lets look at how different populations have different diversity indexes. If we only have two groups, will the diversity score converge to 0.5 for large populations, however with small populations will the index be quite large since each person contribute such a big percentage of the group, making it harder to pick another one in the same group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(rep(1, 2)) + 
  diversity_plot(rep(2, 2)) +
  diversity_plot(rep(4, 2)) +
  diversity_plot(rep(8, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Effectively giving that adding a single person from a new group will maximize the contribution to the index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(c(rep(1, 2), 1, 1, 1)) + 
  diversity_plot(c(rep(2, 2), 1, 1, 1)) +
  diversity_plot(c(rep(4, 2), 1, 1, 1)) +
  diversity_plot(c(rep(8, 2), 1, 1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(c(2, 3, 4, 5, 6) * 1) + 
  diversity_plot(c(2, 3, 4, 5, 6) * 2) +
  diversity_plot(c(2, 3, 4, 5, 6) * 3) +
  diversity_plot(c(2, 3, 4, 5, 6) * 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have seen the diversity index in use lets apply it to the data we have collected.&lt;/p&gt;
&lt;p&gt;We would like to have the data in a different kind of tidy format, namely we want each row to represent each school. Taking another look at the data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 129,813
## Columns: 23
## $ CDS_CODE  &amp;lt;chr&amp;gt; &amp;quot;33672490000001&amp;quot;, &amp;quot;33672490000001&amp;quot;, &amp;quot;33672490000001&amp;quot;, &amp;quot;3367‚Ä¶
## $ COUNTY    &amp;lt;chr&amp;gt; &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Riverside&amp;quot;, &amp;quot;Rivers‚Ä¶
## $ DISTRICT  &amp;lt;chr&amp;gt; &amp;quot;San Jacinto Unified&amp;quot;, &amp;quot;San Jacinto Unified&amp;quot;, &amp;quot;San Jacinto ‚Ä¶
## $ SCHOOL    &amp;lt;chr&amp;gt; &amp;quot;Nonpublic, Nonsectarian Schools&amp;quot;, &amp;quot;Nonpublic, Nonsectarian‚Ä¶
## $ ETHNIC    &amp;lt;dbl&amp;gt; 9, 5, 1, 9, 7, 6, 6, 7, 5, 5, 9, 9, 7, 6, 4, 3, 6, 1, 4, 5,‚Ä¶
## $ GENDER    &amp;lt;chr&amp;gt; &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;, &amp;quot;F&amp;quot;,‚Ä¶
## $ KDGN      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 17, 1, 1, 1, 1, 0, 1, 4‚Ä¶
## $ GR_1      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 18, 2, 1, 0, 0, 0, 1, 4‚Ä¶
## $ GR_2      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 2, 5, 28, 1, 3, 0, 0, 0, 1, 3‚Ä¶
## $ GR_3      &amp;lt;dbl&amp;gt; 1, 0, 1, 0, 0, 0, 0, 2, 0, 4, 5, 3, 24, 1, 3, 0, 0, 0, 3, 6‚Ä¶
## $ GR_4      &amp;lt;dbl&amp;gt; 0, 2, 0, 1, 1, 0, 0, 0, 0, 6, 5, 1, 26, 0, 0, 0, 0, 0, 1, 4‚Ä¶
## $ GR_5      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 3, 0, 1, 0, 6, 7, 7, 30, 2, 2, 0, 1, 1, 1, 6‚Ä¶
## $ GR_6      &amp;lt;dbl&amp;gt; 0, 4, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_7      &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_8      &amp;lt;dbl&amp;gt; 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ UNGR_ELM  &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_9      &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_10     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_11     &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ GR_12     &amp;lt;dbl&amp;gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ UNGR_SEC  &amp;lt;dbl&amp;gt; 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ ENR_TOTAL &amp;lt;dbl&amp;gt; 2, 16, 1, 1, 1, 6, 2, 7, 2, 29, 25, 18, 143, 7, 10, 1, 2, 1‚Ä¶
## $ ADULT     &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we would like to across each &lt;code&gt;ETHNIC&lt;/code&gt; category within each school, ignoring &lt;code&gt;GENDER&lt;/code&gt; (including gender could be a interesting question for another time). Luckily the total enrollment is already calculated for us &lt;code&gt;ENR_TOTAL&lt;/code&gt; so we don‚Äôt have to do that manually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_data &amp;lt;- data %&amp;gt;%
  select(CDS_CODE, ETHNIC, ENR_TOTAL) %&amp;gt;%
  group_by(CDS_CODE, ETHNIC) %&amp;gt;%
  summarize(ENR_TOTAL = sum(ENR_TOTAL)) %&amp;gt;%
  spread(ETHNIC, ENR_TOTAL, fill = 0) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(small_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 10,483
## Columns: 10
## $ CDS_CODE &amp;lt;chr&amp;gt; &amp;quot;01100170112607&amp;quot;, &amp;quot;01100170123968&amp;quot;, &amp;quot;01100170124172&amp;quot;, &amp;quot;01100‚Ä¶
## $ `0`      &amp;lt;dbl&amp;gt; 0, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 23, 0, 4, 2, 7, 0,‚Ä¶
## $ `1`      &amp;lt;dbl&amp;gt; 5, 6, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 3, 6, 0, 0, 4, 1, 1, ‚Ä¶
## $ `2`      &amp;lt;dbl&amp;gt; 8, 24, 161, 22, 0, 11, 0, 15, 0, 3, 30, 3, 98, 109, 49, 91, ‚Ä¶
## $ `3`      &amp;lt;dbl&amp;gt; 5, 2, 1, 0, 0, 2, 0, 23, 0, 2, 2, 0, 4, 16, 0, 1, 1, 9, 0, 0‚Ä¶
## $ `4`      &amp;lt;dbl&amp;gt; 1, 2, 7, 5, 1, 1, 2, 2, 1, 0, 18, 0, 21, 35, 16, 34, 21, 104‚Ä¶
## $ `5`      &amp;lt;dbl&amp;gt; 212, 125, 18, 96, 21, 109, 123, 452, 425, 21, 190, 8, 17, 97‚Ä¶
## $ `6`      &amp;lt;dbl&amp;gt; 150, 23, 14, 75, 58, 95, 21, 84, 23, 1, 33, 6, 7, 81, 98, 63‚Ä¶
## $ `7`      &amp;lt;dbl&amp;gt; 18, 16, 35, 115, 2, 13, 1, 1, 8, 10, 85, 12, 30, 98, 177, 16‚Ä¶
## $ `9`      &amp;lt;dbl&amp;gt; 4, 7, 121, 57, 5, 11, 0, 10, 4, 2, 21, 2, 6, 41, 64, 62, 34,‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we move on, lets reference the data documentation to see what each of the &lt;code&gt;ETHNIC&lt;/code&gt; numbers mean. &lt;a href=&#34;https://www.cde.ca.gov/ds/sd/sd/fsenr.asp&#34;&gt;File Structure&lt;/a&gt; which states the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code 0 = Not reported&lt;/li&gt;
&lt;li&gt;Code 1 = American Indian or Alaska Native, Not Hispanic&lt;/li&gt;
&lt;li&gt;Code 2 = Asian, Not Hispanic&lt;/li&gt;
&lt;li&gt;Code 3 = Pacific Islander, Not Hispanic&lt;/li&gt;
&lt;li&gt;Code 4 = Filipino, Not Hispanic&lt;/li&gt;
&lt;li&gt;Code 5 = Hispanic or Latino&lt;/li&gt;
&lt;li&gt;Code 6 = African American, not Hispanic&lt;/li&gt;
&lt;li&gt;Code 7 = White, not Hispanic&lt;/li&gt;
&lt;li&gt;Code 9 = Two or More Races, Not Hispanic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So now we have two decisions we need to deal with before moving on. How to take care of the ‚ÄúNot reported‚Äù cases, and ‚ÄúTwo or More Races, Not Hispanic‚Äù. The second point have been discussed before &lt;a href=&#34;https://www.unc.edu/~pmeyer/carstat/tools.html&#34;&gt;Updating the USA TODAY Diversity Index&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lets start with the case of code 0. We notice that the category generally is quite small compared to the other groups, so we need to reason about what would happen if we drop them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(c(10, 10, 2)) +
  diversity_plot(c(10, 10)) +
  diversity_plot(c(11, 11)) +
  diversity_plot(c(10, 10, 10, 3)) +
  diversity_plot(c(10, 10, 10)) +
  diversity_plot(c(11, 11, 11)) +
  plot_layout(nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;as we see that having them be a separate group (first column), gives a higher diversity score then ignoring them (second column) or adding them evenly to the remaining groups (third column). However ignoring them and spreading them gives mostly the same diversity index (when the group is small compared the the whole). Thus we will drop the ‚ÄúNot reported‚Äù column as it would otherwise inflate the diversity index.&lt;/p&gt;
&lt;p&gt;Coping with multi ethnicity is quite hard, and we will pick between four options.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assume everyone in ‚ÄúTwo or More Races, Not Hispanic‚Äù is the same ethnicity&lt;/li&gt;
&lt;li&gt;Assume everyone in ‚ÄúTwo or More Races, Not Hispanic‚Äù is all different ethnicities&lt;/li&gt;
&lt;li&gt;Ignore the group ‚ÄúTwo or More Races, Not Hispanic‚Äù&lt;/li&gt;
&lt;li&gt;Distribute the group ‚ÄúTwo or More Races, Not Hispanic‚Äù evenly into the other groups&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets evaluate the different choices with some visualizations.&lt;/p&gt;
&lt;p&gt;Assume we have 3 main groups. with 5 in each, and an additional 3 people who have picked ‚ÄúTwo or More Races, Not Hispanic‚Äù.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot(c(5, 5, 5, 3)) +
  diversity_plot(c(5, 5, 5, rep(1, 3))) +
  diversity_plot(c(5, 5, 5)) +
  diversity_plot(c(6, 6, 6))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Option 3 and 4 both yields low diversity index considering that the choice of ‚ÄúTwo or More Races, Not Hispanic‚Äù must indicate that they are different enough not to be put into one of the more precisely defined groups. The first option while appalling from a computational standpoint would treat a black-white mixture and an Asian-American Indian as members of the same group even though they had no racial identity in common. Thus We will work with the second option which in any case might overestimate the diversity of any given population as they oath to be some people ‚ÄúTwo or More Races, Not Hispanic‚Äù with identical mixes (siblings would be a prime example).&lt;/p&gt;
&lt;p&gt;After deciding we can create a &lt;code&gt;dplyr&lt;/code&gt; friendly function to calculate the diversity index based on a collection of columns. Here we denote &lt;code&gt;y&lt;/code&gt; as the column denoting ‚ÄúTwo or More Races, Not Hispanic‚Äù.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity &amp;lt;- function(..., y) {
 x &amp;lt;- sapply(list(...), cbind)
 total &amp;lt;- cbind(x, y)
 1 - (rowSums(x ^ 2) - rowSums(x)) / (rowSums(total) ^ 2 - rowSums(total))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bring-back-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bring back the data!&lt;/h2&gt;
&lt;p&gt;We are finally able to calculate some diversity indexes!!! Using our recently made function within &lt;code&gt;mutate&lt;/code&gt; gives us what we need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_diversity &amp;lt;- small_data %&amp;gt;%
  mutate(diversity = diversity(`1`, `2`, `3`, `4`, `5`, `6`, `7`, y = `9`))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets run a &lt;code&gt;summary&lt;/code&gt; to see what ranges we get.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(data_diversity$diversity)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&amp;#39;s 
##  0.0000  0.3113  0.5216  0.4686  0.6351  1.0000      88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interesting. All the number are between 0 and 1 which is good! that means that the &lt;code&gt;diversity&lt;/code&gt; worked as intended. However there are 88 schools have &lt;code&gt;NA&lt;/code&gt; valued diversity, lets look at those schools:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(data_diversity, is.na(diversity)) %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##   CDS_CODE         `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 01612590121210     0     0     0     0     0     0     1     0     0       NaN
## 2 04755070000001     2     0     0     0     0     0     0     1     0       NaN
## 3 06615980000001     0     0     0     0     0     0     0     1     0       NaN
## 4 07618040000000     0     0     0     0     0     0     0     1     0       NaN
## 5 09619600000001     0     0     0     0     0     0     0     1     0       NaN
## 6 10622400114587     0     0     0     0     0     1     0     0     0       NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it seems like some of the schools have such a low number of kids that the calculations break down. We will exclude these schools for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_diversity &amp;lt;- data_diversity %&amp;gt;%
  filter(!is.na(diversity))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next lets look at the distribution of diversity indexes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data_diversity, aes(diversity)) +
  geom_histogram(binwidth = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We notice that most of the schools follow a nice shape, except a spike at 0 and 1. Lets look at the 0‚Äôs first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_diversity %&amp;gt;%
  filter(diversity == 0) %&amp;gt;%
  head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 11
##    CDS_CODE        `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 016116800000‚Ä¶     0     0     0     0     0     0     2     0     0         0
##  2 021002502300‚Ä¶     0     2     0     0     0     0     0     0     0         0
##  3 076171300000‚Ä¶     0     0     0     0     0     0     0     6     0         0
##  4 096195200000‚Ä¶     0     0     0     0     0     0     0     3     0         0
##  5 096197800000‚Ä¶     0     0     0     0     0     0     0     4     0         0
##  6 106212510304‚Ä¶     0     0     0     0     0    26     0     0     0         0
##  7 106212510308‚Ä¶     0     0     0     0     0     2     0     0     0         0
##  8 106215801089‚Ä¶     1     0     0     0     0    13     0     0     0         0
##  9 106236401268‚Ä¶     0     0     0     0     0    47     0     0     0         0
## 10 107380910301‚Ä¶     0     0     0     0     0    12     0     0     0         0
## 11 107399900000‚Ä¶     0     0     0     0     0     3     0     0     0         0
## 12 107512710302‚Ä¶     0     0     0     0     0    18     0     0     0         0
## 13 107512710307‚Ä¶     0     0     0     0     0     5     0     0     0         0
## 14 107523410303‚Ä¶     0     0     0     0     0     8     0     0     0         0
## 15 107523460058‚Ä¶     0     0     0     0     0   174     0     0     0         0
## 16 117656211300‚Ä¶     2     0     0     0     0     9     0     0     0         0
## 17 126281000000‚Ä¶     0     0     0     0     0     0     0     2     0         0
## 18 127538212300‚Ä¶     0     0     0     0     0     0     0    11     0         0
## 19 127538261078‚Ä¶     0     0     0     0     0     0     0     7     0         0
## 20 131013213301‚Ä¶     0     0     0     0     0     8     0     0     0         0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These all appear to be schools with just one race in them, most appear to be of category 5 which is ‚ÄúHispanic or Latino‚Äù. Next lets take a look at the 1‚Äôs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_diversity %&amp;gt;%
  filter(diversity == 1) %&amp;gt;%
  head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 11
##    CDS_CODE        `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `9` diversity
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 016112700000‚Ä¶     0     0     1     0     0     1     0     1     0         1
##  2 046152300000‚Ä¶     0     0     0     0     0     0     0     1     1         1
##  3 056155600000‚Ä¶     0     0     0     0     0     1     0     1     0         1
##  4 076176200000‚Ä¶     0     0     0     0     0     0     1     1     0         1
##  5 076177000000‚Ä¶     1     0     1     0     0     1     0     1     0         1
##  6 097378300000‚Ä¶     1     1     0     0     0     1     0     1     0         1
##  7 106215801202‚Ä¶     0     0     0     0     0     0     1     1     0         1
##  8 117548111301‚Ä¶     0     0     0     0     0     1     0     1     0         1
##  9 141014014300‚Ä¶     0     1     0     0     0     1     0     1     0         1
## 10 156336200000‚Ä¶     0     0     0     0     0     1     1     0     0         1
## 11 176402200000‚Ä¶     0     0     0     0     0     1     0     1     0         1
## 12 196456819961‚Ä¶     0     0     0     0     0     1     0     1     0         1
## 13 197343761144‚Ä¶     0     0     0     0     0     1     1     0     1         1
## 14 207641420300‚Ä¶     0     0     0     0     0     1     0     1     0         1
## 15 246578900000‚Ä¶     0     0     0     0     0     0     1     1     0         1
## 16 257358525301‚Ä¶     0     0     0     0     0     0     0     1     1         1
## 17 261026401286‚Ä¶     0     0     0     0     0     1     0     1     1         1
## 18 286624100000‚Ä¶     0     0     0     0     0     1     0     1     0         1
## 19 316684500000‚Ä¶     0     0     0     0     0     1     0     1     0         1
## 20 336697701341‚Ä¶     0     0     0     0     0     1     1     1     0         1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we have seen before, a diversity index of 1 can only happen if the maximal number of student in each category is 1. These schools seem to be rather small. Taking a look at the first school here&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% filter(CDS_CODE == &amp;quot;17640220000001&amp;quot;) %&amp;gt;% pull(SCHOOL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Nonpublic, Nonsectarian Schools&amp;quot; &amp;quot;Nonpublic, Nonsectarian Schools&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get that it is a ‚ÄúNonpublic, Nonsectarian Schools‚Äù which by further investigation shows that there are quite a few of those. Lets take a look at the total enrollment in each of the schools, we have seen a couple of instances with low enrollment and we wouldn‚Äôt want those to distort our view.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_students &amp;lt;- data %&amp;gt;% 
  select(CDS_CODE:SCHOOL, ENR_TOTAL) %&amp;gt;% 
  group_by(CDS_CODE) %&amp;gt;%
  summarise(ENR_TOTAL = sum(ENR_TOTAL)) %&amp;gt;% 
  ungroup() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also create a &lt;code&gt;meta&lt;/code&gt; data.frame which stores the &lt;code&gt;CDS_CODE&lt;/code&gt; along with county, district and school name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- data %&amp;gt;%
  select(CDS_CODE:SCHOOL) %&amp;gt;%
  distinct() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the distribution of total enrollment&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_students %&amp;gt;% 
  ggplot(aes(ENR_TOTAL)) +
  geom_histogram(bins = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see a big hump around the 600-700 mark, but also a big spike towards 0, lets take a closer look at the small values&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_students %&amp;gt;% 
  filter(ENR_TOTAL &amp;lt; 250) %&amp;gt;%
  ggplot(aes(ENR_TOTAL)) +
  geom_histogram(bins = 250)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we make another choice that will ultimately change the outcome of the analysis. But I will restrict the investigation to school with a total enrollment of 50 or more.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_big &amp;lt;- total_students %&amp;gt;%
  left_join(meta, by = &amp;quot;CDS_CODE&amp;quot;) %&amp;gt;%
  filter(ENR_TOTAL &amp;gt;= 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we join that back to our enrollment data such that we have meta and diversity information in the same data.frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_enrollment &amp;lt;- data_diversity %&amp;gt;%
  right_join(data_big, by = &amp;quot;CDS_CODE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We start by looking to see if there is some correlation between total enrollment (&lt;code&gt;ENR_TOTAL&lt;/code&gt;) and the diversity index (&lt;code&gt;diversity&lt;/code&gt;). We fit with a simple linear model to begin with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_enrollment %&amp;gt;%
  ggplot(aes(ENR_TOTAL, diversity)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  theme_minimal() +
  ylim(0, 1) +
  labs(x = &amp;quot;Enrollment&amp;quot;, 
       y = &amp;quot;Diversity Index&amp;quot;,
       title = &amp;quot;Larger Californian schools tend to have a higher diversity index&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Its quite a small slope but still about 5% increase in diversity per 2000 students. Lets verify the correlation by checking the null hypothesis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_enrollment %&amp;gt;%
  mutate(ENR_TOTAL = ENR_TOTAL / 1000) %&amp;gt;%
  lm(diversity ~ ENR_TOTAL, data = .) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = diversity ~ ENR_TOTAL, data = .)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5053 -0.1584  0.0533  0.1658  0.3926 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.447968   0.003489 128.399  &amp;lt; 2e-16 ***
## ENR_TOTAL   0.023239   0.004132   5.624 1.92e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2111 on 9407 degrees of freedom
## Multiple R-squared:  0.003351,   Adjusted R-squared:  0.003245 
## F-statistic: 31.63 on 1 and 9407 DF,  p-value: 1.923e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can‚Äôt reject the null hypothesis of no correlation and we get a better estimate of the slope to 2.3%. Of cause we can‚Äôt extrapolate the results outside the range of the existing data since the response variable is bounded.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-are-we&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where are we?&lt;/h2&gt;
&lt;p&gt;Lets try to join the enrollment data to the geographical data so we can bring out some maps! The &lt;code&gt;longlat&lt;/code&gt; dataset has quite a unfortunate number of missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;longlat$Latitude %&amp;gt;% is.na() %&amp;gt;% mean()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2784102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we will see if we can work with it anyways.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;miss_lat_div &amp;lt;- data_enrollment %&amp;gt;%
  left_join(longlat, by = c(&amp;quot;CDS_CODE&amp;quot; = &amp;quot;CDSCode&amp;quot;)) %&amp;gt;% 
  filter(is.na(Latitude)) %&amp;gt;%
  pull(diversity)

have_lat_div &amp;lt;- data_enrollment %&amp;gt;%
  left_join(longlat, by = c(&amp;quot;CDS_CODE&amp;quot; = &amp;quot;CDSCode&amp;quot;)) %&amp;gt;% 
  filter(!is.na(Latitude)) %&amp;gt;%
  pull(diversity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sadly it turns out that the longitude and latitude is NOT missing at random as the distribution of the diversity index in not the same for missing and non-missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;density(miss_lat_div) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;density(have_lat_div) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-41-2.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will still venture on with the remaining data, but with a higher caution then before. Hopefully we will still be able to see some clustering in the remaining data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map &amp;lt;- map_data(&amp;quot;state&amp;quot;) %&amp;gt;%
  filter(region == &amp;quot;california&amp;quot;)

map_point &amp;lt;- data_enrollment %&amp;gt;%
  left_join(longlat, by = c(&amp;quot;CDS_CODE&amp;quot; = &amp;quot;CDSCode&amp;quot;)) %&amp;gt;%
  filter(!is.na(Latitude)) 

map %&amp;gt;%
  ggplot(aes(long, lat, group = group)) +
  geom_polygon(fill = &amp;quot;grey80&amp;quot;) +
  coord_map() +
  theme_minimal() +
  geom_point(aes(Longitude, Latitude, color = diversity),
             data = map_point,
             inherit.aes = FALSE, alpha = 0.1) +
  scale_color_viridis_c(option = &amp;quot;A&amp;quot;) +
  labs(title = &amp;quot;Diversity of schools across California&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We don‚Äôt notice any mayor geographical trends. However there are still some gold nuggets here. Dark grey points are actually overlapping low-diversity index points since the alpha is set to &lt;code&gt;0.1&lt;/code&gt; therefore we see a couple of instances in mid California where there are a couple of low-diversity index schools, with each point landing on a certain city.&lt;/p&gt;
&lt;p&gt;The two mayor cities (Los Angeles and San Francisco) both have quite a few schools with high diversity, however Los Angeles have some quite dark spots. Lets zoom in to city level. Since we are getting more local, we will be using the &lt;code&gt;ggmap&lt;/code&gt; package to quarry the Google Maps so we get a nice underlying map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LA_map &amp;lt;- get_map(location = &amp;#39;Los Angeles&amp;#39;, zoom = 9)
ggmap(LA_map) +
  geom_point(data = map_point %&amp;gt;%
                      filter(Longitude &amp;gt; -119, Longitude &amp;lt; -117,
                             Latitude &amp;lt; 34.5, Latitude &amp;gt; 33.5), 
             aes(Longitude, Latitude, color = diversity),
             alpha = 0.2) +
  scale_color_viridis_c(option = &amp;quot;A&amp;quot;) +
  labs(title = &amp;quot;Low diversity areas tends towards city centers \nin Greater Los Angeles Area&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;map.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This maps shows much more interesting trends!! It appears that low diversity schools cluster together near city centers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-is-the-money&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where is the money?&lt;/h2&gt;
&lt;p&gt;We will end this post by taking a look at if the median household income in the county in which the school is located correlates with the diversity index we have calculated for each school.&lt;/p&gt;
&lt;p&gt;This is simply done by joining the two data.frames together and piping them into &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_enrollment %&amp;gt;% 
  left_join(income_data_clean, by = c(&amp;quot;COUNTY&amp;quot; = &amp;quot;county&amp;quot;)) %&amp;gt;%
  ggplot(aes(median_household_income, diversity)) +
  geom_jitter(alpha = 0.1, width = 500) +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  ylim(0, 1) +
  theme_minimal() +
  labs(x = &amp;quot;Median household income&amp;quot;, y = &amp;quot;Diversity Index&amp;quot;,
       title = &amp;quot;Higher diversity index in CA counties with high Median household income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-01-analysing-ethnic-diversity-in-californian-school/index_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and we do indeed see a positive correlation between median household income and ethnic diversity in schools in California.&lt;/p&gt;
&lt;p&gt;This is the end of this analysis, I hope you enjoyed it! See you again next time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diversity-plotting-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diversity plotting function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diversity_plot &amp;lt;- function(makeup, fix = 0.45) {
  df &amp;lt;- tibble(id = seq_len(sum(makeup)),
               race = imap(makeup, ~ rep(.y, .x)) %&amp;gt;% unlist())
  
  cols &amp;lt;- structure(rainbow(length(makeup)), 
            names = as.character(seq_len(length(makeup)))) %&amp;gt;%
    c(&amp;quot;diag&amp;quot; = &amp;quot;grey30&amp;quot;,
      &amp;quot;other&amp;quot; = &amp;quot;grey70&amp;quot;)
  
  df1 &amp;lt;- crossing(df, df) %&amp;gt;%
    mutate(fill = case_when(id == id1 ~ &amp;quot;diag&amp;quot;,
                            race == race1 ~ as.character(race),
                            TRUE ~ &amp;quot;other&amp;quot;)) 
  
  df1 %&amp;gt;%
    ggplot(aes(xmin = id - fix, xmax = id + fix,
               ymin = id1 - fix, ymax = id1 + fix)) +
    geom_rect(aes(fill = fill)) +
    theme(panel.background = element_blank(), 
          panel.border = element_blank()) +
    scale_y_continuous(breaks = df[[&amp;quot;id&amp;quot;]], labels = df[[&amp;quot;race&amp;quot;]]) +
    scale_x_continuous(breaks = df[[&amp;quot;id&amp;quot;]], labels = df[[&amp;quot;race&amp;quot;]]) +
    coord_fixed() +
    scale_fill_manual(values = cols) +
    guides(fill = &amp;quot;none&amp;quot;) +
    labs(title = paste0(&amp;quot;The diversity score is &amp;quot;, 
                        signif(mean(df1$fill %in% c(&amp;quot;diag&amp;quot;, &amp;quot;other&amp;quot;)), digits = 3)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggpage version 0.2.0 showcase</title>
      <link>/2018/04/07/ggpage-version-0.2.0-showcase/</link>
      <pubDate>Sat, 07 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/2018/04/07/ggpage-version-0.2.0-showcase/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;ggpage-version-0.2.0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ggpage version 0.2.0&lt;/h2&gt;
&lt;p&gt;In this post I will highlight a couple of the new features in the new update of my package &lt;a href=&#34;https://github.com/EmilHvitfeldt/ggpage&#34;&gt;ggpage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;first we load the packages we need, which is &lt;code&gt;tidyverse&lt;/code&gt; for general tidy tools, &lt;code&gt;ggpage&lt;/code&gt; for visualization and finally &lt;code&gt;rtweet&lt;/code&gt; and &lt;code&gt;rvest&lt;/code&gt; for data collection.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(ggpage)
library(rtweet)
library(rvest)
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The basics&lt;/h2&gt;
&lt;p&gt;The packages includes 2 main functions, &lt;code&gt;ggpage_build&lt;/code&gt; and &lt;code&gt;ggpage_plot&lt;/code&gt; that will transform the data in the right way and plot it respectively. The reason for the split of the functions is to allow additional transformations to be done on the tokenized data to be used in the plotting.&lt;/p&gt;
&lt;p&gt;The package includes a example data set of the text Tinderbox by H.C. Andersen&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tinderbox %&amp;gt;%
  head()
## # A tibble: 6 x 2
##   text                                                              book        
##   &amp;lt;chr&amp;gt;                                                             &amp;lt;chr&amp;gt;       
## 1 &amp;quot;A soldier came marching along the high road: \&amp;quot;Left, right - le‚Ä¶ The tinder-‚Ä¶
## 2 &amp;quot;had his knapsack on his back, and a sword at his side; he had b‚Ä¶ The tinder-‚Ä¶
## 3 &amp;quot;and was now returning home. As he walked on, he met a very frig‚Ä¶ The tinder-‚Ä¶
## 4 &amp;quot;witch in the road. Her under-lip hung quite down on her breast,‚Ä¶ The tinder-‚Ä¶
## 5 &amp;quot;and said, \&amp;quot;Good evening, soldier; you have a very fine sword, ‚Ä¶ The tinder-‚Ä¶
## 6 &amp;quot;knapsack, and you are a real soldier; so you shall have as much‚Ä¶ The tinder-‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data set can be used directly with &lt;code&gt;ggpage_build&lt;/code&gt; and &lt;code&gt;ggpage_plot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpage_build(tinderbox) %&amp;gt;%
  ggpage_plot()
## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead.
## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead.
## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead.
## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-06-ggpage-version-0-2-0-showcase/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ggpage_build&lt;/code&gt; expects the column containing the text to be named ‚Äútext‚Äù which it is in the tinderbox object. This visualization gets exiting when you start combining it with the other tools. We can show where the word ‚Äútinderbox‚Äù appears along with adding some page numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpage_build(tinderbox) %&amp;gt;%
  mutate(tinderbox = word == &amp;quot;tinderbox&amp;quot;) %&amp;gt;%
  ggpage_plot(mapping = aes(fill = tinderbox), page.number = &amp;quot;top-left&amp;quot;)
## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead.
## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead.
## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead.
## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead.
## Warning: Use of `paper_number_data$page` is discouraged. Use `page` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-06-ggpage-version-0-2-0-showcase/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we see that the word tinderbox only appear 3 times in the middle of the story.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vizualizing-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vizualizing tweets&lt;/h2&gt;
&lt;p&gt;We can also use this to showcase a number of tweets. For this we will use the &lt;code&gt;rtweet&lt;/code&gt; package. We will load in 100 tweets that contain the hash tag #rstats.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## whatever name you assigned to your created app
appname &amp;lt;- &amp;quot;********&amp;quot;

## api key (example below is not a real key)
key &amp;lt;- &amp;quot;**********&amp;quot;

## api secret (example below is not a real key)
secret &amp;lt;- &amp;quot;********&amp;quot;

## create token named &amp;quot;twitter_token&amp;quot;
twitter_token &amp;lt;- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstats_tweets &amp;lt;- rtweet::search_tweets(&amp;quot;#rstats&amp;quot;) %&amp;gt;%
  mutate(status_id = as.numeric(as.factor(status_id)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since each tweet is too long by itself will we use the &lt;code&gt;nest_paragraphs&lt;/code&gt; function to wrap the texts within each tweet. By passing the tweet id to &lt;code&gt;page.col&lt;/code&gt; we will make it such that we have a tweet per page. Additionally we can indicate both whether the tweet is a retweet by coloring the paper blue if it is and green if it isn‚Äôt. Lastly we highlight where ‚Äúrstats‚Äù is used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rstats_tweets %&amp;gt;%
  select(status_id, text) %&amp;gt;%
  nest_paragraphs(text) %&amp;gt;%
  ggpage_build(page.col = &amp;quot;status_id&amp;quot;, lpp = 4, ncol = 6) %&amp;gt;%
  mutate(rstats = word == &amp;quot;rstats&amp;quot;) %&amp;gt;%
  ggpage_plot(mapping = aes(fill = rstats), paper.show = TRUE, 
              paper.color = ifelse(rstats_tweets$is_retweet, &amp;quot;lightblue&amp;quot;, &amp;quot;lightgreen&amp;quot;)) +
  scale_fill_manual(values = c(&amp;quot;grey60&amp;quot;, &amp;quot;black&amp;quot;)) +
  labs(title = &amp;quot;100 #rstats tweets&amp;quot;,
       subtitle = &amp;quot;blue = retweet, green = original&amp;quot;)
## Warning: Use of `paper_data$xmin` is discouraged. Use `xmin` instead.
## Warning: Use of `paper_data$xmax` is discouraged. Use `xmax` instead.
## Warning: Use of `paper_data$ymin` is discouraged. Use `ymin` instead.
## Warning: Use of `paper_data$ymax` is discouraged. Use `ymax` instead.
## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead.
## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead.
## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead.
## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-06-ggpage-version-0-2-0-showcase/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vizualizing-documents&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vizualizing documents&lt;/h2&gt;
&lt;p&gt;Next we will look at the &lt;a href=&#34;http://www.ohchr.org/EN/ProfessionalInterest/Pages/CRC.aspx&#34;&gt;Convention on the Rights of the Child&lt;/a&gt; which we will scrape with &lt;code&gt;rvest&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;http://www.ohchr.org/EN/ProfessionalInterest/Pages/CRC.aspx&amp;quot;

rights_text &amp;lt;- read_html(url) %&amp;gt;%
  html_nodes(&amp;#39;div[class=&amp;quot;boxtext&amp;quot;]&amp;#39;) %&amp;gt;%
  html_text() %&amp;gt;%
  str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  str_wrap() %&amp;gt;%
  str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  data.frame(text = ., stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case will we remove the vertical space between the pages have it appear as a long paper like the website.&lt;/p&gt;
&lt;p&gt;The wonderful &lt;code&gt;case_when&lt;/code&gt; comes in vary handy here when we want to highlight multiple different words.&lt;/p&gt;
&lt;p&gt;for the purpose of the ‚ÄúUnited Nations‚Äù was it necessary to check that the words ‚Äúunited‚Äù and ‚Äúnations‚Äù only appeared in pairs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rights_text %&amp;gt;%
  ggpage_build(wtl = FALSE, y_space_pages = 0, ncol = 7) %&amp;gt;%
  mutate(highlight = case_when(word %in% c(&amp;quot;child&amp;quot;, &amp;quot;children&amp;quot;) ~ &amp;quot;child&amp;quot;,
                               word %in% c(&amp;quot;right&amp;quot;, &amp;quot;rights&amp;quot;) ~ &amp;quot;rights&amp;quot;,
                               word %in% c(&amp;quot;united&amp;quot;, &amp;quot;nations&amp;quot;) ~ &amp;quot;United Nations&amp;quot;,
                               TRUE ~ &amp;quot;other&amp;quot;)) %&amp;gt;%
  ggpage_plot(mapping = aes(fill = highlight)) +
  scale_fill_manual(values = c(&amp;quot;darkgreen&amp;quot;, &amp;quot;grey&amp;quot;, &amp;quot;darkblue&amp;quot;, &amp;quot;darkred&amp;quot;)) +
  labs(title = &amp;quot;Word highlights in the &amp;#39;Convention on the Rights of the Child&amp;#39;&amp;quot;,
       fill = NULL)
## Warning: Use of `data$xmin` is discouraged. Use `xmin` instead.
## Warning: Use of `data$xmax` is discouraged. Use `xmax` instead.
## Warning: Use of `data$ymin` is discouraged. Use `ymin` instead.
## Warning: Use of `data$ymax` is discouraged. Use `ymax` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-06-ggpage-version-0-2-0-showcase/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is just a couple of different ways to use this package. I look forward to see what you guys can come up with.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Binary text classification with Tidytext and caret</title>
      <link>/2018/03/31/binary-text-classification-with-tidytext-and-caret/</link>
      <pubDate>Sat, 31 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/2018/03/31/binary-text-classification-with-tidytext-and-caret/</guid>
      <description>


&lt;p&gt;the scope of this blog post is to show how to do binary text classification using standard tools such as &lt;code&gt;tidytext&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt; packages. One of if not the most common binary text classification task is the spam detection (spam vs non-spam) that happens in most email services but has many other application such as language identification (English vs non-English).&lt;/p&gt;
&lt;p&gt;In this post I‚Äôll showcase 5 different classification methods to see how they compare with this data. The methods all land on the less complex side of the spectrum and thus does not include creating complex deep neural networks.&lt;/p&gt;
&lt;p&gt;An expansion of this subject is multiclass text classification which I might write about in the future.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;We load the packages we need for this project. &lt;code&gt;tidyverse&lt;/code&gt; for general data science work, &lt;code&gt;tidytext&lt;/code&gt; for text manipulation and &lt;code&gt;caret&lt;/code&gt; for modeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data we will be using for this demonstration will be some &lt;a href=&#34;https://data.world/crowdflower/disasters-on-social-media&#34;&gt;social media disaster tweets&lt;/a&gt; discussed in &lt;a href=&#34;https://arxiv.org/pdf/1705.02009.pdf&#34;&gt;this article&lt;/a&gt;.
It consist of a number of tweets regarding accidents mixed in with a selection control tweets (not about accidents). We start by loading in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/EmilHvitfeldt/blog/750dc28aa8d514e2c0b8b418ade584df8f4a8c92/data/socialmedia-disaster-tweets-DFE.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for this exercise we will only look at the body of the text. Furthermore a handful of the tweets weren‚Äôt classified, marked &lt;code&gt;&#34;Can&#39;t Decide&#34;&lt;/code&gt; so we are removing those as well. Since we are working with tweet data we have the constraint that most of tweets don‚Äôt actually have that much information in them as they are limited in characters and some only contain a couple of words.&lt;/p&gt;
&lt;p&gt;We will at this stage remove what appears to be urls using some regex and &lt;code&gt;str_replace_all&lt;/code&gt;, and we will select the columns &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;disaster&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_clean &amp;lt;- data %&amp;gt;%
  filter(choose_one != &amp;quot;Can&amp;#39;t Decide&amp;quot;) %&amp;gt;%
  mutate(id = `_unit_id`,
         disaster = choose_one == &amp;quot;Relevant&amp;quot;,
         text = str_replace_all(text, &amp;quot; ?(f|ht)tp(s?)://(.*)[.][a-z]+&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
  select(id, disaster, text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we take a quick look at the distribution of classes and we see if the classes are balanced&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_clean %&amp;gt;%
  ggplot(aes(disaster)) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we see that is fairly balanced so we don‚Äôt have to worry about sampling this time.&lt;/p&gt;
&lt;p&gt;The representation we will be using in this post will be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bag-of-words_model&#34;&gt;bag-of-words&lt;/a&gt; representation in which we just count how many times each word appears in each tweet disregarding grammar and even word order (mostly).&lt;/p&gt;
&lt;p&gt;We will construct a tf-idf vector model in which each unique word is represented as a column and each document (tweet in our case) is a row of the tf-idf values. This will create a very large matrix/data.frame (a column of each unique word in the total data set) which will overload a lot of the different models we can implement, furthermore will a lot of the words (or features in ML slang) not add considerably information. We have a trade off between information and computational speed.&lt;/p&gt;
&lt;p&gt;First we will remove all the stop words, this will insure that common words that usually don‚Äôt carry meaning doesn‚Äôt take up space (and time) in our model. Next will we only look at words that appear in 10 different tweets. Lastly we will be looking at both &lt;a href=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;unigrams and bigrams&lt;/a&gt; to hopefully get a better information extraction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_counts &amp;lt;- map_df(1:2,
                      ~ unnest_tokens(data_clean, word, text, 
                                      token = &amp;quot;ngrams&amp;quot;, n = .x)) %&amp;gt;%
  anti_join(stop_words, by = &amp;quot;word&amp;quot;) %&amp;gt;%
  count(id, word, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will only look at words at appear in at least 10 different tweets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_10 &amp;lt;- data_counts %&amp;gt;%
  group_by(word) %&amp;gt;%
  summarise(n = n()) %&amp;gt;% 
  filter(n &amp;gt;= 10) %&amp;gt;%
  select(word)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_dtm &amp;lt;- data_counts %&amp;gt;%
  right_join(words_10, by = &amp;quot;word&amp;quot;) %&amp;gt;%
  bind_tf_idf(word, id, n) %&amp;gt;%
  cast_dtm(id, word, tf_idf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This leaves us with 2993 features. We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- tibble(id = as.numeric(dimnames(data_dtm)[[1]])) %&amp;gt;%
  left_join(data_clean[!duplicated(data_clean$id), ], by = &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also create the index (based on the &lt;code&gt;meta&lt;/code&gt; data.frame) to separate the data into a training and test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
trainIndex &amp;lt;- createDataPartition(meta$disaster, p = 0.8, list = FALSE, times = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since a lot of the methods take data.frames as inputs we will take the time and create these here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_df_train &amp;lt;- data_dtm[trainIndex, ] %&amp;gt;% as.matrix() %&amp;gt;% as.data.frame()
data_df_test &amp;lt;- data_dtm[-trainIndex, ] %&amp;gt;% as.matrix() %&amp;gt;% as.data.frame()

response_train &amp;lt;- meta$disaster[trainIndex]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now each row in the data.frame is a document/tweet (yay tidy principles!!).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-tweets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing tweets&lt;/h2&gt;
&lt;p&gt;In the feature selection earlier we decided to turn our focus towards certain words and word-pairs, with that we also turned our focus AWAY from certain words. Since the tweets are fairly short in length it wouldn‚Äôt be surprising if a handful of the tweets completely skipped out focus as we noted earlier. Lets take a look at those tweets here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_clean %&amp;gt;%
  anti_join(meta, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  head(25) %&amp;gt;%
  pull(text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that a lot of them appears to be part of urls that our regex didn‚Äôt detect, furthermore it appears that in those tweet the sole text was the url which wouldn‚Äôt have helped us in this case anyways.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;Now that we have the data all clean and tidy we will turn our heads towards modeling. We will be using the wonderful &lt;code&gt;caret&lt;/code&gt; package which we will use to employ the following models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Support_vector_machine&#34;&gt;Support vector machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&#34;&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/LogitBoost&#34;&gt;LogitBoost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;feed-forward neural networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These where chosen because of their frequent use ( &lt;a href=&#34;http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf&#34;&gt;why SVM are good at text classification&lt;/a&gt; ) or because they are common in the classification field. They were also chosen because they where able to work with data with this number of variables in a reasonable time.&lt;/p&gt;
&lt;p&gt;First time around we will not use a resampling method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trctrl &amp;lt;- trainControl(method = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;svm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SVM&lt;/h2&gt;
&lt;p&gt;The first model will be the &lt;code&gt;svmLinearWeights2&lt;/code&gt; model from the &lt;a href=&#34;https://cran.r-project.org/web/packages/LiblineaR/index.html&#34;&gt;LiblineaR&lt;/a&gt; package. Where we specify default parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svm_mod &amp;lt;- train(x = data_df_train,
                 y = as.factor(response_train),
                 method = &amp;quot;svmLinearWeights2&amp;quot;,
                 trControl = trctrl,
                 tuneGrid = data.frame(cost = 1, 
                                       Loss = 0, 
                                       weight = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We predict on the test data set based on the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svm_pred &amp;lt;- predict(svm_mod,
                    newdata = data_df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lastly we calculate the confusion matrix using the &lt;code&gt;confusionMatrix&lt;/code&gt; function in the &lt;code&gt;caret&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svm_cm &amp;lt;- confusionMatrix(svm_pred, meta[-trainIndex, ]$disaster)
svm_cm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get an accuracy of 0.7461646.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;naive-bayes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive-Bayes&lt;/h2&gt;
&lt;p&gt;The second model will be the &lt;code&gt;naive_bayes&lt;/code&gt; model from the &lt;a href=&#34;https://cran.r-project.org/web/packages/naivebayes/index.html&#34;&gt;naivebayes&lt;/a&gt; package. Where we specify default parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_mod &amp;lt;- train(x = data_df_train,
                y = as.factor(response_train),
                method = &amp;quot;naive_bayes&amp;quot;,
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                      usekernel = FALSE,
                                      adjust = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We predict on the test data set based on the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_pred &amp;lt;- predict(nb_mod,
                   newdata = data_df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;calculate the confusion matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_cm &amp;lt;- confusionMatrix(nb_pred, meta[-trainIndex, ]$disaster)
nb_cm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get an accuracy of 0.5564854.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logitboost&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;LogitBoost&lt;/h2&gt;
&lt;p&gt;The third model will be the &lt;code&gt;LogitBoost&lt;/code&gt; model from the &lt;a href=&#34;https://cran.r-project.org/web/packages/caTools/index.html&#34;&gt;caTools&lt;/a&gt; package. We don‚Äôt have to specify any parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logitboost_mod &amp;lt;- train(x = data_df_train,
                        y = as.factor(response_train),
                        method = &amp;quot;LogitBoost&amp;quot;,
                        trControl = trctrl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We predict on the test data set based on the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logitboost_pred &amp;lt;- predict(logitboost_mod,
                           newdata = data_df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;calculate the confusion matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logitboost_cm &amp;lt;- confusionMatrix(logitboost_pred, meta[-trainIndex, ]$disaster)
logitboost_cm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get an accuracy of 0.632729.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest&lt;/h2&gt;
&lt;p&gt;The fourth model will be the &lt;code&gt;ranger&lt;/code&gt; model from the &lt;a href=&#34;https://cran.r-project.org/web/packages/ranger/index.html&#34;&gt;caTools&lt;/a&gt; package. Where we specify default parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_mod &amp;lt;- train(x = data_df_train, 
                y = as.factor(response_train), 
                method = &amp;quot;ranger&amp;quot;,
                trControl = trctrl,
                tuneGrid = data.frame(mtry = floor(sqrt(dim(data_df_train)[2])),
                                      splitrule = &amp;quot;gini&amp;quot;,
                                      min.node.size = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We predict on the test data set based on the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_pred &amp;lt;- predict(rf_mod,
                   newdata = data_df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;calculate the confusion matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_cm &amp;lt;- confusionMatrix(rf_pred, meta[-trainIndex, ]$disaster)
rf_cm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get an accuracy of 0.7777778.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nnet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;nnet&lt;/h2&gt;
&lt;p&gt;The fifth and final model will be the &lt;code&gt;nnet&lt;/code&gt; model from the &lt;a href=&#34;https://cran.r-project.org/web/packages/nnet/index.html&#34;&gt;caTools&lt;/a&gt; package. Where we specify default parameters. We will also specify &lt;code&gt;MaxNWts = 5000&lt;/code&gt; such that it will work. It will need to be more then the number of columns multiplied the size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet_mod &amp;lt;- train(x = data_df_train,
                    y = as.factor(response_train),
                    method = &amp;quot;nnet&amp;quot;,
                    trControl = trctrl,
                    tuneGrid = data.frame(size = 1,
                                          decay = 5e-4),
                    MaxNWts = 5000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We predict on the test data set based on the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet_pred &amp;lt;- predict(nnet_mod,
                     newdata = data_df_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;calculate the confusion matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nnet_cm &amp;lt;- confusionMatrix(nnet_pred, meta[-trainIndex, ]$disaster)
nnet_cm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we get an accuracy of 0.7173408.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing models&lt;/h2&gt;
&lt;p&gt;To see how the different models stack out we combine the metrics together in a &lt;code&gt;data.frame&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_results &amp;lt;- rbind(
  svm_cm$overall, 
  nb_cm$overall,
  logitboost_cm$overall,
  rf_cm$overall,
  nnet_cm$overall
  ) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(model = c(&amp;quot;SVM&amp;quot;, &amp;quot;Naive-Bayes&amp;quot;, &amp;quot;LogitBoost&amp;quot;, &amp;quot;Random forest&amp;quot;, &amp;quot;Neural network&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;visualizing the accuracy for the different models with the red line being the ‚ÄúNo Information Rate‚Äù that is, having a model that just picks the model common class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_results %&amp;gt;%
  ggplot(aes(model, Accuracy)) +
  geom_point() +
  ylim(0, 1) +
  geom_hline(yintercept = mod_results$AccuracyNull[1],
             color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see all but one approach does better then the ‚ÄúNo Information Rate‚Äù on its first try before tuning the hyperparameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tuning-hyperparameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tuning hyperparameters&lt;/h2&gt;
&lt;p&gt;After trying out the different models we saw quite a spread in performance. But it important to remember that the results might be because of good/bad default hyperparameters. There are a few different ways to handle this problem. I‚Äôll show on of them here, grid search, on the SVM model so you get the idea.&lt;/p&gt;
&lt;p&gt;We will be using 10-fold &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;cross-validation&lt;/a&gt; and 3 repeats, which will slow down the procedure, but will try to limit and reduce overfitting. We will be using &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search&#34;&gt;grid search&lt;/a&gt; approach to find optimal hyperparameters. For the sake of time have to fixed 2 of the hyperparameters and only let one vary. Remember that the time it takes to search though all combinations take a long time when then number of hyperparameters increase.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitControl &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;,
                           number = 3,
                           repeats = 3,
                           search = &amp;quot;grid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have decided to limit the search around the &lt;code&gt;weight&lt;/code&gt; parameter‚Äôs default value 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;svm_mod &amp;lt;- train(x = data_df_train,
                 y = as.factor(response_train),
                 method = &amp;quot;svmLinearWeights2&amp;quot;,
                 trControl = fitControl,
                 tuneGrid = data.frame(cost = 0.01, 
                                       Loss = 0, 
                                       weight = seq(0.5, 1.5, 0.1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and once it have finished running we can plot the train object to see which value is highest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(svm_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we see that it appear to be just around 1. It is important to search multiple parameters at the SAME TIME as it can not be assumed that the parameters are independent of each others. Only reason I didn‚Äôt do that here was to same the time.&lt;/p&gt;
&lt;p&gt;I will leave to you the reader to find out which of the models have the highest accuracy after doing parameter tuning.&lt;/p&gt;
&lt;p&gt;I hope you have enjoyed this overview of binary text classification.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recreate - Sankey flow chart</title>
      <link>/2018/03/21/recreate-sankey-flow-chart/</link>
      <pubDate>Wed, 21 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/2018/03/21/recreate-sankey-flow-chart/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;This blogpost uses the old api for gganimate and will not work with current version. No update of this blogpost is planned for this moment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data, or using data of a similar style.&lt;/p&gt;
&lt;div id=&#34;the-goal---a-flowing-sankey-chart-from-nytimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The goal - A flowing sankey chart from nytimes&lt;/h2&gt;
&lt;p&gt;In this excellent article &lt;a href=&#34;https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html&#34;&gt;Extensive Data Shows Punishing
Reach of Racism for Black Boys&lt;/a&gt; by NYTimes includes a lot of very nice charts, both in motion and still. The chart that got biggest reception is the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nytimes.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(see article for moving picture) We see a animated flow chart that follow the style of the classical Sankey chart. This chart will be the goal in this blog post, with 2 changes for brevity. firstly will I use randomly simulated data for my visualization and secondly will I not include the counters on the right-hand side of the chart and only show the creation of the counter on the left-hand as they are created in much the same fashion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R packages&lt;/h2&gt;
&lt;p&gt;First we need some packages, but very few of those. Simply using &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;gganimate&lt;/code&gt; for animation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(gganimate)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;single-point&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Single point&lt;/h2&gt;
&lt;p&gt;We will start with animating a single point first. The path of each point closely resembles a sigmoid curve. I have used those in past visualizations, namely &lt;a href=&#34;https://www.hvitfeldt.me/2018/01/visualizing-trigrams-with-the-tidyverse/&#34;&gt;Visualizing trigrams with the Tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;and we steal the function I created in that post&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmoid &amp;lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) {
  x &amp;lt;- seq(-scale, scale, length = n)
  y &amp;lt;- exp(x) / (exp(x) + 1)
  tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from,
         y = y * (y_to - y_from) + y_from)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to get along with that we will have out data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_points &amp;lt;- 400
data &amp;lt;- tibble(from = rep(4, n_points),
               to = sample(1:4, n_points, TRUE),
               color = sample(c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;), n_points, TRUE)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;here the data is fairly clean and tidy, with numerical values for &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt; but this endpoint should be able to be achieved in most any other appropriate type of data.&lt;/p&gt;
&lt;p&gt;To simulate the path of a single data point we will use the custom &lt;code&gt;sigmoid&lt;/code&gt; on the data for a single row. This gives us this smooth curve of points that resembles the path taken by the points in the original visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]), 
        n = 100, scale = 10) %&amp;gt;%
  ggplot(aes(x, y)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;01-old.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To set this in motion we will employ &lt;code&gt;gganimate&lt;/code&gt;, for this we will add a &lt;code&gt;time&lt;/code&gt; column to act as the frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- sigmoid(0, 1, as.numeric(data[2, 1]), as.numeric(data[2, 2]),
             n = 100, scale = 10) %&amp;gt;%
  mutate(time = row_number()) %&amp;gt;%
  ggplot(aes(x, y, frame = time)) +
  geom_point()

gganimate(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;02-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which looks very nice so far. Next step is to have multiple points flowing towards different locations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;multiple points&lt;/h2&gt;
&lt;p&gt;To account for the multiple points we will wrap everything from last section inside a &lt;code&gt;map_df&lt;/code&gt; to iterate over the rows. To avoid over plotting we introduce some uniform noise to each point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- map_df(seq_len(nrow(data)), 
    ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %&amp;gt;%
      mutate(time = row_number() + .x,
             y = y + runif(1, -0.25, 0.25))) %&amp;gt;%
  ggplot(aes(x, y, frame = time)) +
  geom_point() 

gganimate(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;03-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks good so far, however the points all look the same, so we will do a little bit of beautification now rather then later. In addition to that will we save the data for the different components in different objects.&lt;/p&gt;
&lt;p&gt;the following &lt;code&gt;point_data&lt;/code&gt; have the modification with &lt;code&gt;bind_cols&lt;/code&gt; that binds the information from the &lt;code&gt;data&lt;/code&gt; data.frame to the final object. We include the color and removing all themes and guides.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;point_data &amp;lt;- map_df(seq_len(nrow(data)), 
    ~ sigmoid(0, 1, as.numeric(data[.x, 1]), as.numeric(data[.x, 2])) %&amp;gt;%
      mutate(time = row_number() + .x,
             y = y + runif(1, -0.25, 0.25),
             id = .x) %&amp;gt;%
      bind_cols(bind_rows(replicate(100, data[.x, -(1:2)], simplify = FALSE))))

p &amp;lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +
  geom_point(shape = 15) +
  theme_void() +
  guides(color = &amp;quot;none&amp;quot;)

gganimate(p, title_frame = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;04-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which already looks way better. Next up to include animated counter on the left hand side that indicates how many points that have been introduced in the animation. This is simply done by counting how many have started their paths and afterwards padding to fill the length of the animation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start_data_no_end &amp;lt;- point_data %&amp;gt;%
  group_by(id) %&amp;gt;%
  summarize(time = min(time)) %&amp;gt;%
  count(time) %&amp;gt;%
  arrange(time) %&amp;gt;%
  mutate(n = cumsum(n),
         x = 0.125, 
         y = 2,
         n = str_c(&amp;quot;Follow the lives of &amp;quot;, n, &amp;quot; squares&amp;quot;))
  


# duplicating last number to fill gif
start_data &amp;lt;- start_data_no_end %&amp;gt;%
  bind_rows(
    map_df(unique(point_data$time[point_data$time &amp;gt; max(start_data_no_end$time)]),
          ~ slice(start_data_no_end, nrow(start_data_no_end)) %&amp;gt;%
              mutate(time = .x))
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is added to our plot by the use of &lt;code&gt;geom_text&lt;/code&gt; with a new data argument. We did some &lt;code&gt;stringr&lt;/code&gt; magic to have a little annotation appear instead of the number itself. Important to have the &lt;code&gt;hjust = 0&lt;/code&gt; such that the annotation doesn‚Äôt move around too much.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +
  geom_point(shape = 15) +
  geom_text(data = start_data, hjust = 0,
            aes(label = n, frame = time, x = x, y = y), color = &amp;quot;black&amp;quot;) +
  theme_void() +
  guides(color = &amp;quot;none&amp;quot;)

gganimate(p, title_frame = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;05-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ending-boxes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ending boxes&lt;/h2&gt;
&lt;p&gt;Like the original illustration there are some boxes where the points ‚Äúland‚Äù in. these are very easily replicated. This will be done a little more programmatic such that it adapts to multiple outputs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ending_box &amp;lt;- data %&amp;gt;%
  pull(to) %&amp;gt;%
  unique() %&amp;gt;%
  map_df(~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01),
                      y = c(-0.3, 0.3, 0.3, -0.3, -0.3) + .x,
                      id = .x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will add this in the same way as before, this time we will use &lt;code&gt;geom_path&lt;/code&gt; to draw the box and &lt;code&gt;frame = min(point_data$time)&lt;/code&gt; and &lt;code&gt;cumulative = TRUE&lt;/code&gt; to have the boxes appear at the first frame and stay there forever.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +
  geom_point() +
  geom_text(data = start_data, 
            aes(label = n, frame = time, x = x, y = y), color = &amp;quot;black&amp;quot;) +
  geom_path(data = ending_box,
            aes(x, y, group = id, frame = min(point_data$time),
                cumulative = TRUE), color = &amp;quot;grey70&amp;quot;) +
  theme_void() +
  coord_cartesian(xlim = c(-0.05, 1.15)) +
  guides(color = &amp;quot;none&amp;quot;)

gganimate(p, title_frame = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;06-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filling-the-box&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Filling the box&lt;/h2&gt;
&lt;p&gt;Lastly do we want to fill the boxes as the points approach them. This is done by first figuring out when they appear at the end of their paths, and them drawing boxes at those points, this is done by the &lt;code&gt;end_points&lt;/code&gt; and &lt;code&gt;end_lines&lt;/code&gt; respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;end_points &amp;lt;- point_data %&amp;gt;% 
  group_by(id) %&amp;gt;%
  filter(time == max(time)) %&amp;gt;%
  ungroup()

end_lines &amp;lt;- map_df(end_points$id,
    ~ data.frame(x = c(1.01, 1.01, 1.1, 1.1, 1.01),
                 y = c(-0.01, 0.01, 0.01, -0.01, -0.01) + as.numeric(end_points[.x, 2]),
                 id = .x) %&amp;gt;%
      bind_cols(bind_rows(replicate(5, end_points[.x, -(1:2)], simplify = FALSE)))
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before we add the data in a new &lt;code&gt;geom_&lt;/code&gt;, with &lt;code&gt;cumulative = TRUE&lt;/code&gt; to let the ‚Äúpoints‚Äù stay.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(point_data, aes(x, y, color = color, frame = time)) +
  geom_point() +
  geom_text(data = start_data, 
            aes(label = n, frame = time, x = x, y = y), color = &amp;quot;black&amp;quot;) +
  geom_path(data = ending_box,
            aes(x, y, group = id, frame = min(point_data$time),
                cumulative = TRUE), color = &amp;quot;grey70&amp;quot;) +
  geom_polygon(data = end_lines,
               aes(x, y, fill = color, frame = time, group = id,
                   cumulative = TRUE, color = color)) +
  theme_void() +
  coord_cartesian(xlim = c(-0.05, 1.15)) +
  guides(color = &amp;quot;none&amp;quot;,
         fill = &amp;quot;none&amp;quot;)

gganimate(p, title_frame = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;07-old.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And this is what I have for you for now. Counters on the right hand side could be done in much the same way as we have seen here, but wouldn‚Äôt add much value to showcase that here.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Text Summarization using TextRank</title>
      <link>/2018/03/15/tidy-text-summarization-using-textrank/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/2018/03/15/tidy-text-summarization-using-textrank/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-19.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;text-summarization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text summarization&lt;/h2&gt;
&lt;p&gt;In the realm of text summarization there two main paths:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;extractive summarization&lt;/li&gt;
&lt;li&gt;abstractive summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where extractive scoring word and sentences according to some metric and then using that information to summarize the text. Usually done by copy/pasting (extracting) the most informative parts of the text.&lt;/p&gt;
&lt;p&gt;The abstractive methods aims to build a semantic representation of the text and then use natural language generation techniques to generate text describing the informative parts.&lt;/p&gt;
&lt;p&gt;Extractive summarization is primarily the simpler task, with a handful of algorithms do will do the scoring. While with the advent of deep learning did NLP have a boost in abstractive summarization methods.&lt;/p&gt;
&lt;p&gt;In this post will I focus on an example of a extractive summarization method called &lt;a href=&#34;https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf&#34;&gt;TextRank&lt;/a&gt; which is based on the &lt;a href=&#34;https://en.wikipedia.org/wiki/PageRank&#34;&gt;PageRank&lt;/a&gt; algorithm that is used by Google to rank websites by their importance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;textrank-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TextRank Algorithm&lt;/h2&gt;
&lt;p&gt;The TextRank algorithm is based on graph-based ranking algorithm. Generally used in web searches at Google, but have many other applications. Graph-based ranking algorithms try to decide the importance of a vertex by taking into account information about the entire graph rather then the vertex specific information. A typical piece of information would be information between relationships (edges) between the vertices.&lt;/p&gt;
&lt;p&gt;In the NLP case we need to define the what we want to use as vertices and edges. In our case will we be using sentences as the vertices and words as the connection edges. So sentences with words that appear in many other sentences are seen as more important.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data preparation&lt;/h2&gt;
&lt;p&gt;We start by loading the appropriate packages, which include &lt;code&gt;tidyverse&lt;/code&gt; for general tasks, &lt;code&gt;tidytext&lt;/code&gt; for text manipulations, &lt;code&gt;textrank&lt;/code&gt; for the implementation of the TextRank algorithm and finally &lt;code&gt;rvest&lt;/code&gt; to scrape an article to use as an example. The github for the &lt;code&gt;textrank&lt;/code&gt; package can be found &lt;a href=&#34;https://github.com/bnosac/textrank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(textrank)
library(rvest)
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To showcase this method I have randomly (EXTENSIVELY filtered political and controversial) selected an article as our guinea pig. The main body is selected using the &lt;code&gt;html_nodes&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;http://time.com/5196761/fitbit-ace-kids-fitness-tracker/&amp;quot;
article &amp;lt;- read_html(url) %&amp;gt;%
  html_nodes(&amp;#39;div[class=&amp;quot;padded&amp;quot;]&amp;#39;) %&amp;gt;%
  html_text()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;next we load the article into a &lt;code&gt;tibble&lt;/code&gt; (since tidytext required the input as a data.frame). We start by tokenize according to sentences which is done by setting &lt;code&gt;token = &#34;sentences&#34;&lt;/code&gt; in &lt;code&gt;unnest_tokens&lt;/code&gt;. The tokenization is not always perfect using this tokenizer, but it have a low number of dependencies and is sufficient for this showcase. Lastly we add sentence number column and switch the order of the columns (&lt;code&gt;textrank_sentences&lt;/code&gt; prefers the columns in a certain order).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_sentences &amp;lt;- tibble(text = article) %&amp;gt;%
  unnest_tokens(sentence, text, token = &amp;quot;sentences&amp;quot;) %&amp;gt;%
  mutate(sentence_id = row_number()) %&amp;gt;%
  select(sentence_id, sentence)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;next we will tokenize again but this time to get words. In doing this we will retain the &lt;code&gt;sentence_id&lt;/code&gt; column in our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_words &amp;lt;- article_sentences %&amp;gt;%
  unnest_tokens(word, sentence)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;now we have all the sufficient input for the &lt;code&gt;textrank_sentences&lt;/code&gt; function. However we will go one step further and remove the stop words in &lt;code&gt;article_words&lt;/code&gt; since they would appear in most of the sentences and doesn‚Äôt really carry any information in them self.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_words &amp;lt;- article_words %&amp;gt;%
  anti_join(stop_words, by = &amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;running-textrank&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running TextRank&lt;/h2&gt;
&lt;p&gt;Running the TextRank algorithm is easy, the &lt;code&gt;textrank_sentences&lt;/code&gt; function only required 2 inputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A data.frame with sentences&lt;/li&gt;
&lt;li&gt;A data.frame with tokens (in our case words) which are part of the each sentence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we are ready to run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary &amp;lt;- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output have its own printing method that displays the top 5 sentences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary
## Textrank on sentences, showing top 5 most important sentences found:
##   1. fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.
##   2. fitbit says the tracker is designed for children eight years old and up.
##   3. sign up now                                                                                                                                                check the box if you do not wish to receive promotional offers via email from time.
##   4. the fitbit ace looks a lot like the company‚Äôs alta tracker, but with a few child-friendly tweaks.
##   5. like many of fitbit‚Äôs other products, the fitbit ace can automatically track steps, monitor active minutes, and remind kids to move when they‚Äôve been still for too long.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which in itself is pretty good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;digging-deeper&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Digging deeper&lt;/h2&gt;
&lt;p&gt;While the printing method is good, we can extract the information to good some further analysis. The information about the sentences is stored in &lt;code&gt;sentences&lt;/code&gt;. It includes the information &lt;code&gt;article_sentences&lt;/code&gt; plus the calculated textrank score.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary[[&amp;quot;sentences&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets begging by extracting the top 3 and bottom 3 sentences to see how they differ.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary[[&amp;quot;sentences&amp;quot;]] %&amp;gt;%
  arrange(desc(textrank)) %&amp;gt;% 
  slice(1:3) %&amp;gt;%
  pull(sentence)
## [1] &amp;quot;fitbit is launching a new fitness tracker designed for children called the fitbit ace, which will go on sale for $99.95 in the second quarter of this year.&amp;quot;                                                                                   
## [2] &amp;quot;fitbit says the tracker is designed for children eight years old and up.&amp;quot;                                                                                                                                                                      
## [3] &amp;quot;sign up now                                                                                                                                                check the box if you do not wish to receive promotional offers via email from time.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected these are the same sentences as we saw earlier. However the button sentences, doesn‚Äôt include the word fitbit (properly rather important word) and focuses more ‚Äúother‚Äù things, like the reference to another product in the second sentence.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary[[&amp;quot;sentences&amp;quot;]] %&amp;gt;%
  arrange(textrank) %&amp;gt;% 
  slice(1:3) %&amp;gt;%
  pull(sentence)
## [1] &amp;quot;contact us at editors@time.com.&amp;quot;                                                                                                                                                                                                                                                                                                     
## [2] &amp;quot;by signing up you are agreeing to our terms of use and privacy policy                                                                                                                                                                                                                                                     thank you!&amp;quot;
## [3] &amp;quot;the $39.99 nabi compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we look at the article over time, it would be interesting to see where the important sentences appear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;article_summary[[&amp;quot;sentences&amp;quot;]] %&amp;gt;%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = &amp;quot;none&amp;quot;) +
  labs(x = &amp;quot;Sentence&amp;quot;,
       y = &amp;quot;TextRank score&amp;quot;,
       title = &amp;quot;4 Most informative sentences appear within first half of sentences&amp;quot;,
       subtitle = &amp;#39;In article &amp;quot;Fitbits Newest Fitness Tracker Is Just for Kids&amp;quot;&amp;#39;,
       caption = &amp;quot;Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-15-tidy-text-summarization/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-books&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with books???&lt;/h2&gt;
&lt;p&gt;Summaries help cut down the reading when used on articles. Would the same approach work on books? Lets see what happens when you exchange ‚Äúsentence‚Äù in ‚Äúarticle‚Äù with ‚Äúchapter‚Äù in ‚Äúbook‚Äù. I‚Äôll go to my old friend &lt;code&gt;emma&lt;/code&gt; form the &lt;code&gt;janeaustenr&lt;/code&gt; package. We will borrow some code from the &lt;a href=&#34;https://www.tidytextmining.com/tidytext.html&#34;&gt;Text Mining with R&lt;/a&gt; book to create the chapters. Remember that we want 1 chapter per row.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emma_chapters &amp;lt;- janeaustenr::emma %&amp;gt;%
  tibble(text = .) %&amp;gt;%
  mutate(chapter_id = cumsum(str_detect(text, regex(&amp;quot;^chapter [\\divxlc]&amp;quot;,
                                                 ignore_case = TRUE)))) %&amp;gt;%
  filter(chapter_id &amp;gt; 0) %&amp;gt;%
  group_by(chapter_id) %&amp;gt;%
  summarise(text = paste(text, collapse = &amp;#39; &amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and proceed as before to find the words and remove the stop words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emma_words &amp;lt;- emma_chapters %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words, by = &amp;quot;word&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We run the &lt;code&gt;textrank_sentences&lt;/code&gt; function again. It should still be very quick, as the bottleneck of the algorithm is more with the number of vertices rather then their individual size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emma_summary &amp;lt;- textrank_sentences(data = emma_chapters, 
                                   terminology = emma_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be careful not to use the standard printing method as it would print 5 whole chapter!!&lt;/p&gt;
&lt;p&gt;Instead we will look at the bar chart again to see if the important chapters appear in any particular order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emma_summary[[&amp;quot;sentences&amp;quot;]] %&amp;gt;%
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c(option = &amp;quot;inferno&amp;quot;) +
  guides(fill = &amp;quot;none&amp;quot;) +
  labs(x = &amp;quot;Chapter&amp;quot;,
       y = &amp;quot;TextRank score&amp;quot;,
       title = &amp;quot;Chapter importance in the novel Emma by Jane Austen&amp;quot;) +
  scale_x_continuous(breaks = seq(from = 0, to = 55, by = 5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-15-tidy-text-summarization/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which doesn‚Äôt appear to be the case in this particular text (which is properly good since skipping a chapter would be discouraged in a book like Emma). however it might prove helpful in non-chronological texts.&lt;/p&gt;
&lt;p&gt;There is plenty more to look at but I‚Äôll stop for now. If you have any feedback or suggestions please leave a comment, send a email &lt;a href=&#34;mailto:emilhhvitfeldt@gmail.com&#34;&gt;emilhhvitfeldt@gmail.com&lt;/a&gt; or hit me up on twitter &lt;a href=&#34;https://twitter.com/Emil_Hvitfeldt&#34;&gt;Emil_Hvitfeldt&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Co Occurrence of Characters in Les Miserable</title>
      <link>/2018/02/23/co-occurrence-of-characters-in-les-miserable/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/2018/02/23/co-occurrence-of-characters-in-les-miserable/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2020-04-21.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;what-are-we-doing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are we doing?&lt;/h2&gt;
&lt;p&gt;The inspiration for this post is &lt;a href=&#34;https://bost.ocks.org/mike/miserables/&#34;&gt;this beutiful vizualization&lt;/a&gt; from &lt;a href=&#34;https://bost.ocks.org/mike/&#34;&gt;Mike Bostock&lt;/a&gt;. It nicely visualizes the co-occurrence of characters (when two characters appear in the same chapter) in the novel &lt;a href=&#34;https://en.wikipedia.org/wiki/Les_Mis%C3%A9rables&#34;&gt;Les Mis√©rables&lt;/a&gt; by &lt;a href=&#34;https://en.wikipedia.org/wiki/Victor_Hugo&#34;&gt;Victor Hugo&lt;/a&gt; using the data collected by &lt;a href=&#34;https://en.wikipedia.org/wiki/Jacques_Bertin&#34;&gt;Jacques Bertin&lt;/a&gt; (and his assistants).&lt;/p&gt;
&lt;p&gt;The way this post will differentiate itself from this is that we are going to collect the data ourselves using &lt;a href=&#34;https://en.wikipedia.org/wiki/Named-entity_recognition&#34;&gt;named entity recognition&lt;/a&gt;. Named entity recognition is the discipline of location and classifying named entities in text. Furthermore will we also try to cluster the characters according to their appearance in the novel.&lt;/p&gt;
&lt;p&gt;disclaimer! I have of the time of writing this analysis not read of familiarized myself with Les Mis√©rables in a attempt to show how a blind text analysis would run.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-package-and-backend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading package and backend&lt;/h2&gt;
&lt;p&gt;for this we will need &lt;code&gt;tidyverse&lt;/code&gt; for general data science tasks, &lt;code&gt;spacyr&lt;/code&gt; for the named entity recognition and &lt;code&gt;igraph&lt;/code&gt; for some graph related transformation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(spacyr)
library(igraph)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be using the &lt;a href=&#34;https://spacy.io/&#34;&gt;spacy&lt;/a&gt; NLP back-end as the parser for this analysis since it provides named entity recognition as one of its functionalities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;Les Miserable is quite a long novel, in the terms of words and pages, however due to its age is it in the public domain and is easily available on &lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lesmis_raw &amp;lt;- gutenbergr::gutenberg_download(135)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking thought the beginning of the text we notice how a large part of the beginning of the document is table of content and other information that isn‚Äôt of interest in this analysis. Manually checking leads to be discard the first 650 lines of the data. We will also add a &lt;code&gt;chapter&lt;/code&gt; column using a regex.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lesmis_line &amp;lt;- lesmis_raw %&amp;gt;%
  slice(-(1:650)) %&amp;gt;%
  mutate(chapter = cumsum(str_detect(text, &amp;quot;CHAPTER &amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the use in &lt;code&gt;cnlp_annotate()&lt;/code&gt; we need a data.frame where each row is a full chapter, with the 2 necessary columns &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;. This is accomplished using a simple &lt;code&gt;map&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lesmis &amp;lt;- map_df(seq_len(max(lesmis_line$chapter)),
                 ~ tibble(id = .x,
                          text = lesmis_line %&amp;gt;% 
                                   filter(chapter == .x) %&amp;gt;% 
                                   pull(text) %&amp;gt;% 
                                   paste(collapse = &amp;quot; &amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are all ready to run the spacy parser which will only take a couple of minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lesmis_obj &amp;lt;- spacy_parse(lesmis$text)
## Finding a python executable with spaCy installed...
## spaCy (language model: en_core_web_sm) is installed in /usr/bin/python
## successfully initialized (spaCy Version: 2.0.18, language model: en_core_web_sm)
## (python options: type = &amp;quot;python_executable&amp;quot;, value = &amp;quot;/usr/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the output we are given nothing more then a simple tibble&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lesmis_obj&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   doc_id sentence_id token_id   token   lemma   pos entity
## 1  text1           1        1                 SPACE       
## 2  text1           1        2 CHAPTER chapter  NOUN       
## 3  text1           2        1       I  -PRON-  PRON       
## 4  text1           3        1       ‚Äî       ‚Äî PUNCT       
## 5  text1           4        1       A       a   DET       
## 6  text1           4        2   WOUND   wound PROPN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the entity information can be extracted using &lt;code&gt;entity_extract()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entity_extract(lesmis_obj)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   doc_id sentence_id       entity entity_type
## 1  text2           1 EXPLAINING_A         ORG
## 2  text2           1                     NORP
## 3  text2           1   PHENOMENON         ORG
## 4  text5           1    TOUSSAINT       EVENT
## 5  text6           1  _CHAPTER_IV         ORG
## 6  text7           1       LETTER         ORG&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see quite a few different &lt;code&gt;entity_type&lt;/code&gt;s, in fact lets take a quick look at the different types that are in this text&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entity_extract(lesmis_obj) %&amp;gt;%
  pull(entity_type) %&amp;gt;%
  unique()
##  [1] &amp;quot;ORG&amp;quot;      &amp;quot;NORP&amp;quot;     &amp;quot;EVENT&amp;quot;    &amp;quot;LANGUAGE&amp;quot; &amp;quot;GPE&amp;quot;      &amp;quot;WORK&amp;quot;    
##  [7] &amp;quot;PERSON&amp;quot;   &amp;quot;FAC&amp;quot;      &amp;quot;PRODUCT&amp;quot;  &amp;quot;LOC&amp;quot;      &amp;quot;LAW&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This labeling is explained &lt;a href=&#34;https://spacy.io/api/annotation#named-entities&#34;&gt;here&lt;/a&gt;. After a bit of investigating I have decided that we only will look at ‚ÄúPERSON‚Äù and ‚ÄúORG‚Äù (which is due in part to Napoleon being classified as a organisation.) Furthermore I will limit further analysis to about the 50 most mentioned characters. The rational behind this is that it hopefully would capture most of the important characters, with the weight that characters that are mentioned sparingly but consistently is more important then characters with high density in a few chapter. We will include a few more characters in case we have to exclude some of them after looking.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_person_df &amp;lt;- entity_extract(lesmis_obj) %&amp;gt;%
  filter(entity_type %in% c(&amp;quot;ORG&amp;quot;, &amp;quot;PERSON&amp;quot;)) %&amp;gt;%
  count(entity, sort = TRUE) %&amp;gt;%
  slice(seq_len(60))

top_person_vec &amp;lt;- top_person_df %&amp;gt;% pull(entity)
top_person_vec
##  [1] &amp;quot;Marius&amp;quot;                    &amp;quot;Jean_Valjean&amp;quot;             
##  [3] &amp;quot;Cosette&amp;quot;                   &amp;quot;Javert&amp;quot;                   
##  [5] &amp;quot;Th√©nardier&amp;quot;                &amp;quot;Gavroche&amp;quot;                 
##  [7] &amp;quot;Bishop&amp;quot;                    &amp;quot;Fantine&amp;quot;                  
##  [9] &amp;quot;M._Madeleine&amp;quot;              &amp;quot;Jondrette&amp;quot;                
## [11] &amp;quot; &amp;quot;                         &amp;quot;M._Gillenormand&amp;quot;          
## [13] &amp;quot;_&amp;quot;                         &amp;quot;Napoleon&amp;quot;                 
## [15] &amp;quot;Waterloo&amp;quot;                  &amp;quot;M._Leblanc&amp;quot;               
## [17] &amp;quot;  &amp;quot;                        &amp;quot;Madeleine&amp;quot;                
## [19] &amp;quot;Grantaire&amp;quot;                 &amp;quot;Montparnasse&amp;quot;             
## [21] &amp;quot;Fauchelevent&amp;quot;              &amp;quot;Jean_Valjean_‚Äôs&amp;quot;          
## [23] &amp;quot;√âponine&amp;quot;                   &amp;quot;Bossuet&amp;quot;                  
## [25] &amp;quot;Brujon&amp;quot;                    &amp;quot;Combeferre&amp;quot;               
## [27] &amp;quot;Monseigneur&amp;quot;               &amp;quot;M._Fauchelevent&amp;quot;          
## [29] &amp;quot;Austerlitz&amp;quot;                &amp;quot;Pontmercy&amp;quot;                
## [31] &amp;quot;M._Mabeuf&amp;quot;                 &amp;quot;Champmathieu&amp;quot;             
## [33] &amp;quot;Joly&amp;quot;                      &amp;quot;him:‚Äî&amp;quot;                    
## [35] &amp;quot;Louis_Philippe&amp;quot;            &amp;quot;Nicolette&amp;quot;                
## [37] &amp;quot;Voltaire&amp;quot;                  &amp;quot;C√¶sar&amp;quot;                    
## [39] &amp;quot;Gillenormand&amp;quot;              &amp;quot;Mayor&amp;quot;                    
## [41] &amp;quot;Monsieur&amp;quot;                  &amp;quot;Mademoiselle_Gillenormand&amp;quot;
## [43] &amp;quot;Lark&amp;quot;                      &amp;quot;Magnon&amp;quot;                   
## [45] &amp;quot;Th√©odule&amp;quot;                  &amp;quot;Bahorel&amp;quot;                  
## [47] &amp;quot;Louis_XVIII&amp;quot;               &amp;quot;Mademoiselle_Baptistine&amp;quot;  
## [49] &amp;quot;Blachevelle&amp;quot;               &amp;quot;Bl√ºcher&amp;quot;                  
## [51] &amp;quot;Bonaparte&amp;quot;                 &amp;quot;Gorbeau&amp;quot;                  
## [53] &amp;quot;Jean_Prouvaire&amp;quot;            &amp;quot;Laigle&amp;quot;                   
## [55] &amp;quot;Restoration&amp;quot;               &amp;quot;Courfeyrac&amp;quot;               
## [57] &amp;quot;Favourite&amp;quot;                 &amp;quot;Guelemer&amp;quot;                 
## [59] &amp;quot;Mabeuf&amp;quot;                    &amp;quot;Madame_Th√©nardier&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After looking we see a few things we would like to fix before moving on. Firstly is ‚ÄúCHAPTER IV‚Äù and ‚ÄúCHAPTER VI‚Äù wrongly both classified as ‚ÄúORG‚Äùs. &#34; ‚Äú,‚Äù-&#34; and ‚Äúexclaimed:‚Äì‚Äù and ‚ÄúMonsieur‚Äù have also been misclassified. ‚ÄúJean Valjean‚Äôs‚Äù have been classified differently then ‚ÄúJean Valjean‚Äù which is also the case with ‚ÄúFauchelevent‚Äù and ‚ÄúM. Fauchelevent‚Äù, ‚ÄúM. Madeleine‚Äù and ‚ÄúMadeleine‚Äù, ‚ÄúM. Gillenormand‚Äù, ‚ÄúGillenormand‚Äù and ‚ÄúMademoiselle Gillenormand‚Äù. We will remove the miss-classifications here, and create a list of all the characters with all of their names. The list is named with the character‚Äôs main name for later subsetting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_person_vec_clean &amp;lt;- top_person_vec[-c(9, 13, 29, 34, 42, 56)] 

complications &amp;lt;- list(c(&amp;quot;Jean Valjean&amp;quot;, &amp;quot;Jean Valjean&amp;#39;s&amp;quot;),
                      c(&amp;quot;Fauchelevent&amp;quot;, &amp;quot;M. Fauchelevent&amp;quot;),
                      c(&amp;quot;Madeleine&amp;quot;, &amp;quot;M. Madeleine&amp;quot;),
                      c(&amp;quot;Gillenormand&amp;quot;, &amp;quot;M. Gillenormand&amp;quot;, &amp;quot;Mademoiselle Gillenormand&amp;quot;))

characters &amp;lt;- setdiff(top_person_vec_clean, unlist(complications)) %&amp;gt;%
  as.list() %&amp;gt;%
  c(complications)

names(characters) &amp;lt;- map_chr(characters, ~ .x[1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We expand the grid of all possible co occurrences and count how many times they both occur within a chapter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence &amp;lt;- expand.grid(map_chr(characters, ~ .x[1]), 
                             map_chr(characters, ~ .x[1])) %&amp;gt;%
  set_names(c(&amp;quot;person1&amp;quot;, &amp;quot;person2&amp;quot;)) %&amp;gt;%
  mutate(cooc = map2_dbl(person1, person2,
                         ~ sum(str_detect(lesmis$text, str_c(.x, collapse = &amp;quot;|&amp;quot;)) &amp;amp; 
                               str_detect(lesmis$text, str_c(.y, collapse = &amp;quot;|&amp;quot;)))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize&lt;/h2&gt;
&lt;p&gt;now that we have the co occurrence data we can make some visualizations!! (I will take care of labels etc in the end. Hang on!)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence %&amp;gt;%
  ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So at a first glance is it hard to see anything due to the default color-scale and the fact that a couple of people, Jean Valjean and Marius, appear in a much higher number of chapters (perhaps they are main characters?). To get a more manageable scale we disregard co occurrence if they have been in less then 5 chapters together(remember that there are a total of 365 chapters in novel).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence_1 &amp;lt;- co_occurrence %&amp;gt;%
  mutate(cooc = ifelse(cooc &amp;gt; 5, log(cooc), NA))

co_occurrence_1 %&amp;gt;%
    ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we finally see some of the fruit of our earlier work. It is definitely clear that there are groups of people that might form communities but it is unclear which and how many from this heat-map by itself. We would like to reorder the axis‚Äôs in the hope that it would create more clarity.&lt;/p&gt;
&lt;p&gt;This data here can be seen as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Adjacency_matrix&#34;&gt;Adjacency matrix&lt;/a&gt; here the row numbers are vertices and the tiles-values are the edges connecting them. So in a sense we would like to do some cluster analysis on this graph. This can be done by doing some Spectral Graph Partitioning in which we calculate the eigenvectors and sort the vertices by the second smallest eigenvector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eigen &amp;lt;- co_occurrence_1 %&amp;gt;%
#  mutate(cooc = !is.na(cooc)) %&amp;gt;%
  igraph::graph_from_data_frame() %&amp;gt;%
  igraph::as_adj() %&amp;gt;%
  eigen()

eigenvec2_sort &amp;lt;- data.frame(eigen = eigen$vectors[, length(eigen$values) - 1]) %&amp;gt;%
  mutate(row = row_number(),
         names = names(characters)) %&amp;gt;%
  arrange(eigen)

eigen_names &amp;lt;- eigenvec2_sort %&amp;gt;% pull(names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use sorted names to re-level the factors in the co occurrence data and see if it reveals more structure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence_1 %&amp;gt;%
  mutate(person1 = factor(person1, levels = eigen_names),
         person2 = factor(person2, levels = eigen_names)) %&amp;gt;%
    ggplot(aes(person1, person2, fill = cooc)) +
  geom_tile()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;it isn‚Äôt much but it appears to have moved the data slight closer to the diagonal. We will still need to locate some communities in this data. this can be done using the plotted eigenvector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eigenvec2_sort %&amp;gt;% pull(eigen) %&amp;gt;% plot(type = &amp;quot;o&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And what we are looking at is not their position but at the jumps. There can more easily be seen when we look at the diffs&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eigenvec2_sort %&amp;gt;% pull(eigen) %&amp;gt;% diff() %&amp;gt;% plot()
abline(h = 0.02)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And after playing around a little it seems that &lt;code&gt;0.02&lt;/code&gt; is a appropriate cutoff.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cummunity_df &amp;lt;- eigenvec2_sort %&amp;gt;%
  mutate(community = c(0, diff(eigen) &amp;gt; 0.02) %&amp;gt;% cumsum()) %&amp;gt;%
  select(names, community)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will color-code the final visualization according to this clustering. So with a couple of joins&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence_comm &amp;lt;- co_occurrence_1 %&amp;gt;%
  filter(!is.na(cooc)) %&amp;gt;%
  mutate(person1_chr = as.character(person1),
         person2_chr = as.character(person2),
         person1 = factor(person1, levels = eigen_names),
         person2 = factor(person2, levels = eigen_names)) %&amp;gt;%
  left_join(cummunity_df, by = c(&amp;quot;person1_chr&amp;quot; = &amp;quot;names&amp;quot;)) %&amp;gt;%
  left_join(cummunity_df, by = c(&amp;quot;person2_chr&amp;quot; = &amp;quot;names&amp;quot;)) %&amp;gt;%
  mutate(community = ifelse(community.x == community.y, community.x, NA),
         community = ifelse(!is.na(cooc), community, NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a couple of final touch-ups and we arrive at the final result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co_occurrence_comm %&amp;gt;%
  ggplot(aes(person1, person2, alpha = cooc, fill = factor(community))) +
  geom_tile(color = &amp;quot;grey50&amp;quot;) +
  scale_alpha(range = c(0.5, 1)) +
  scale_fill_brewer(palette = &amp;quot;Set1&amp;quot;, na.value = &amp;quot;grey50&amp;quot;) +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(),
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill = &amp;quot;none&amp;quot;, alpha = &amp;quot;none&amp;quot;) +
  coord_fixed() +
  labs(x = NULL, y = NULL, 
       title = &amp;quot;Les Mis√©rables Co-occurrence&amp;quot;, 
       subtitle = &amp;quot;with color-coded communities&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-23-co-occurrence-of-characters-in-les-miserable/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While I wasn‚Äôt able to find as full clusters as Jacques Bertin I still managed to get quite a lot of information out of the text regardless. I had fun in the progress and there are many more things I see myself doing with this new data set and &lt;code&gt;spacyr&lt;/code&gt;.&lt;br /&gt;
And while I couldn‚Äôt find a good way to include it in the main body of text. I almost finished the main analysis before realizing that &lt;a href=&#34;https://en.wikipedia.org/wiki/Monsieur&#34;&gt;Monsieur&lt;/a&gt; means. Mention your mistakes in your posts so others can learn from them!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rvision: A first look</title>
      <link>/2018/02/15/rvision-a-first-look/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/2018/02/15/rvision-a-first-look/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-19.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Recently I stumbled across the &lt;code&gt;Rvision&lt;/code&gt; package, which frankly looks amazing so far (still in development as this time of writing). So I decided to take it for a spin and show you girls/guys what I found.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;So for this you will need a computer with a webcam and the &lt;code&gt;Rvision&lt;/code&gt; package with its dependencies. It will use &lt;code&gt;ROpenCVLite&lt;/code&gt; to access &lt;code&gt;OpenCV&lt;/code&gt;‚Äôs functionalities. If not already installed, ROpenCVLite will be installed first by the command line below. Furthermore while not necessary for &lt;code&gt;Rvision&lt;/code&gt; I have imported &lt;code&gt;dplyr&lt;/code&gt; for general data manipulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#devtools::install_github(&amp;quot;swarm-lab/Rvision&amp;quot;)
library(Rvision)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;minimal-setup---working-with-a-photo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimal Setup - working with a photo&lt;/h2&gt;
&lt;p&gt;We will start by simply loading a picture of a parrot. This is done using the function &lt;code&gt;image&lt;/code&gt;, which creates an object of class &lt;code&gt;Image&lt;/code&gt;. &lt;code&gt;Image&lt;/code&gt; objects are pointers toward C++ objects stored in memory and will therefore not work with some functions in base R such &lt;code&gt;sum&lt;/code&gt;, &lt;code&gt;%%&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;img &amp;lt;- image(&amp;quot;parrot.jpg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to see the image we loaded we simply plot it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For more information about the &lt;code&gt;Image&lt;/code&gt; object we can turn to the property functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(img)
## [1] 1595 1919    3
nrow(img)
## [1] 1595
ncol(img)
## [1] 1919
nchan(img)
## [1] 3
bitdepth(img)
## [1] &amp;quot;8U&amp;quot;
colorspace(img)
## [1] &amp;quot;BGR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;blurs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Blurs&lt;/h2&gt;
&lt;p&gt;Now that we have an &lt;code&gt;Image&lt;/code&gt; object we can use some of tools at our disposal, which includes standard things like blurs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxFilter(img, k_height = 25, k_width = 25) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gaussianBlur(img, k_height = 25, k_width = 25, sigma_x = 5, sigma_y = 5) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;medianBlur(img, k_size = 25) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-6-3.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrBoxFilter(img, k_height = 25, k_width = 25) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-6-4.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;operators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Operators&lt;/h2&gt;
&lt;p&gt;Other kinds of operations can be done, such as changing the color space:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;changeColorSpace(img, &amp;quot;GRAY&amp;quot;) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And apply edge detection algorithms such as &lt;code&gt;sobel&lt;/code&gt; and &lt;code&gt;laplacian&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sobel(img) %&amp;gt;% plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;draws&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Draws&lt;/h2&gt;
&lt;p&gt;The package also includes a number of drawing functions starting with the prefix &lt;code&gt;draw&lt;/code&gt;, ending with &lt;code&gt;Arrow&lt;/code&gt;, &lt;code&gt;Circle&lt;/code&gt;, &lt;code&gt;Ellipse&lt;/code&gt;, &lt;code&gt;Line&lt;/code&gt;, &lt;code&gt;Rectangle&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;. These functions, unlike the others, modifies the &lt;code&gt;Image&lt;/code&gt; object that is taken in, instead of returning another &lt;code&gt;Image&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;img1 &amp;lt;- cloneImage(img)
drawCircle(img1, x = 750, y = 750, radius = 200, color = &amp;quot;blue&amp;quot;, 
           thickness = 10)
plot(img1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;blob-detection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Blob detection&lt;/h2&gt;
&lt;p&gt;By now we looked at a bunch of different functions but all of them have been used separately. Now lets combine them to detect something inside the picture.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;img &amp;lt;- image(&amp;quot;balls.jpg&amp;quot;)
plot(img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For our further calculations we need to know what color space this image is in&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colorspace(img)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;BGR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is different then the correctly commonly used RGB. In the following code I tried to find all the blue balls. For that I used the &lt;code&gt;split&lt;/code&gt; function to split the &lt;code&gt;Image&lt;/code&gt; object into 3, one for each color channel. Then I used a &lt;code&gt;do.call&lt;/code&gt; to return a object where the blue channel is more the 200, and the red and green are less then 200, in the hope that it would be enough to identify the blue color without also finding bright areas. This being a logical expression gives us a image file that is white when true and black when it isn‚Äôt. Lastly we used the &lt;code&gt;medianBlur&lt;/code&gt; to remove any rough edges and flicker. (you can try comment out the &lt;code&gt;medianBlur&lt;/code&gt; and see what changes)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;img %&amp;gt;%
  split() %&amp;gt;%
  do.call(function(B, G, R) B &amp;gt; 200 &amp;amp; G &amp;lt; 200 &amp;amp; R &amp;lt; 200, .) %&amp;gt;%
  medianBlur() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we would like to highlight these balls on the original image we have to detect where these white blobs are and use the draw functions to draw on our original image. We use &lt;code&gt;simpleBlobDetector&lt;/code&gt; and play around with the settings till be get something reasonable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blue_balls &amp;lt;- img %&amp;gt;%
  split() %&amp;gt;%
  do.call(function(B, G, R) B &amp;gt; 200 &amp;amp; G &amp;lt; 200 &amp;amp; R &amp;lt; 200, .) %&amp;gt;%
  medianBlur() %&amp;gt;%
  simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255,
                     filter_by_convexity = FALSE, 
                     filter_by_inertia = FALSE, min_threshold = 0)
blue_balls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the &lt;code&gt;cloneImage&lt;/code&gt; as it creates a new &lt;code&gt;Image&lt;/code&gt; object such that the drawing doesn‚Äôt change the original &lt;code&gt;Image&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;img1 &amp;lt;- cloneImage(img)

for (i in seq_len(nrow(blue_balls))) {
  drawRectangle(image = img1,
                pt1_x = blue_balls$x[i] - 1 + blue_balls$size[i] / 2, 
                pt1_y = blue_balls$y[i] - 1 + blue_balls$size[i] / 2, 
                pt2_x = blue_balls$x[i] - 1 - blue_balls$size[i] / 2, 
                pt2_y = blue_balls$y[i] - 1 - blue_balls$size[i] / 2, 
                thickness = 3, color = &amp;quot;blue&amp;quot;)
}

plot(img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that it worked fairly well, it didn‚Äôt go all the way till the edges of the balls and it appeared to catch the blue artifact on the lower left side, but more careful ranges could take care of that problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;streams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Streams&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Rvision&lt;/code&gt; also have a &lt;code&gt;Stream&lt;/code&gt; object that we can utilize. the &lt;code&gt;stream&lt;/code&gt; function creates a &lt;code&gt;Stream&lt;/code&gt; object from the the camera connected to your computer. In my case number 0 is the webcam in my Macbook. Its corresponding function is &lt;code&gt;release&lt;/code&gt; which closes the stream object. To capture something we use the handy &lt;code&gt;readNext&lt;/code&gt; function that reads the next frame and returns a &lt;code&gt;Image&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_stream &amp;lt;- stream(0)   # 0 will start your default webcam in general. 
my_img &amp;lt;- readNext(my_stream)
release(my_stream)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets take a look at the image that was captured on my webcam.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(my_img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and what a coincidence!! Its a handful of distinctly colored m&amp;amp;m‚Äôs against a dark background. Lets try against to locate the different colors, but before we do that let us reuse what we did earlier and make it into some custom functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blob_fun &amp;lt;- function(img, fun, color = character()) {
  img %&amp;gt;%
    split() %&amp;gt;%
    do.call(fun, .) %&amp;gt;%
    medianBlur(15) %&amp;gt;%
    simpleBlobDetector(max_area = Inf, min_area = 10, blob_color = 255,
                       filter_by_convexity = FALSE, 
                       filter_by_inertia = FALSE, min_threshold = 0) %&amp;gt;%
    mutate(color = color)
} 

multi_draw &amp;lt;- function(img, blobs) {
  if (nrow(blobs) &amp;gt; 0) {
    for (i in 1:nrow(blobs)) {
      drawRectangle(img, 
                    blobs$x[i] - 1 + blobs$size[i], 
                    blobs$y[i] - 1 + blobs$size[i],
                    blobs$x[i] - 1 - blobs$size[i], 
                    blobs$y[i] - 1 - blobs$size[i], 
                    thickness = 5, color = blobs$color[1])
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like before we found the blue balls by identifying the region in the BGR color space where its blue, we expand the same idea to the other colors. (I have not attempted brown as it is fairly similar in color to the table)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blue &amp;lt;-   function(B, G, R) B &amp;gt; 150 &amp;amp; R &amp;lt; 200 &amp;amp; G &amp;lt; 200
red &amp;lt;-    function(B, G, R) R &amp;gt; 150 &amp;amp; B &amp;lt; 200 &amp;amp; G &amp;lt; 150
green &amp;lt;-  function(B, G, R) G &amp;gt; 150 &amp;amp; B &amp;lt; 200 &amp;amp; R &amp;lt; 200
yellow &amp;lt;- function(B, G, R) G &amp;gt; 150 &amp;amp; B &amp;lt; 200 &amp;amp; B &amp;gt; 150 &amp;amp; R &amp;gt; 150
orange &amp;lt;- function(B, G, R) G &amp;gt; 150 &amp;amp; B &amp;lt; 150 &amp;amp; R &amp;gt; 150&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we just have to run our custom blob detection function and custom drawing function for each color and see the final result&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blue_mms &amp;lt;-   blob_fun(my_img, blue, &amp;quot;blue&amp;quot;)
red_mms &amp;lt;-    blob_fun(my_img, red, &amp;quot;red&amp;quot;)
green_mms &amp;lt;-  blob_fun(my_img, green, &amp;quot;green&amp;quot;)
yellow_mms &amp;lt;- blob_fun(my_img, yellow, &amp;quot;yellow&amp;quot;)
orange_mms &amp;lt;- blob_fun(my_img, orange, &amp;quot;orange&amp;quot;)

multi_draw(my_img, blue_mms)
multi_draw(my_img, red_mms)
multi_draw(my_img, green_mms)
multi_draw(my_img, yellow_mms)
multi_draw(my_img, orange_mms)

plot(my_img)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-14-rvision-a-first-look/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And it is wonderful!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;displays&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Displays&lt;/h2&gt;
&lt;p&gt;Last trip of the tour is a look at the Displays that &lt;code&gt;Rvision&lt;/code&gt; facilitate. And in its simplest form, it creates a window where &lt;code&gt;Image&lt;/code&gt; objects can be displayed. Which mean that we are able to do live m&amp;amp;m‚Äôs detection!!&lt;/p&gt;
&lt;p&gt;In a minimal setup you would have this following chuck of code, which sets up a stream, a display and then populates that display with new images taken from the camera till you stop it. And then termination functions for the stream and display. However this is no different then a video feed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_stream &amp;lt;- stream(0)
newDisplay(&amp;quot;Live test&amp;quot;, 360, 640)
while(TRUE) {
  img &amp;lt;- readNext(my_stream)
  display(img, &amp;quot;Live test&amp;quot;, 25, 360, 640)
}
destroyDisplay(&amp;quot;Live test&amp;quot;)
release(my_stream)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So instead we will use the functions from earlier to detect and highlight the colored m&amp;amp;m‚Äôs!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_stream &amp;lt;- stream(0)
newDisplay(&amp;quot;Live test&amp;quot;, 360, 640)

while(TRUE) {
  img &amp;lt;- readNext(my_stream)
  
  blue_mms &amp;lt;- blob_fun(img, blue, &amp;quot;blue&amp;quot;)
  red_mms &amp;lt;- blob_fun(img, red, &amp;quot;red&amp;quot;)
  green_mms &amp;lt;- blob_fun(img, green, &amp;quot;green&amp;quot;)
  yellow_mms &amp;lt;- blob_fun(img, yellow, &amp;quot;yellow&amp;quot;)
  orange_mms &amp;lt;- blob_fun(img, orange, &amp;quot;orange&amp;quot;)
  
  multi_draw(img, blue_mms)
  multi_draw(img, red_mms)
  multi_draw(img, green_mms)
  multi_draw(img, yellow_mms)
  multi_draw(img, orange_mms)
  
  display(img, &amp;quot;Live test&amp;quot;, 25, 360, 640)
}
destroyDisplay(&amp;quot;Live test&amp;quot;)
release(my_stream)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;Feb-15-2018%2011-12-04.gif&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Its a little choppy but that might be because of my now quite old Macbook.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I had a blast working with &lt;code&gt;Rvision&lt;/code&gt; and I look forward to use it is future projects! I would also recommend against using eatable data points as they tend to disappear over time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predicting authorship in The Federalist Papers with tidytext</title>
      <link>/2018/01/30/predicting-authorship-in-the-federalist-papers-with-tidytext/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/30/predicting-authorship-in-the-federalist-papers-with-tidytext/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post we will&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;talk about The Federalist Papers&lt;/li&gt;
&lt;li&gt;access and tidy the text using the tidytext package&lt;/li&gt;
&lt;li&gt;apply our model to the data to predict the author of the disputed papers&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-federalist-papers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Federalist Papers&lt;/h2&gt;
&lt;p&gt;In the early days of The United States of America around the time when the Constitution was being signed did a series of articles published in various newspapers. These papers where writing under the false name &lt;em&gt;Publius&lt;/em&gt;. It was later revealed to have been the collected works of Alexander Hamilton, James Madison and John Jay.&lt;/p&gt;
&lt;p&gt;The Interesting thing in this was that the authorship of these papers were not consistent. In This is where we come in, in this blog post will we try to see if we are able to classify the troublesome papers.&lt;/p&gt;
&lt;p&gt;If you would like to read more about this story including past attempts to solve this problem please read &lt;a href=&#34;https://priceonomics.com/how-statistics-solved-a-175-year-old-mystery-about/&#34;&gt;How Statistics Solved a 175-Year-Old Mystery About Alexander Hamilton&lt;/a&gt; by Ben Christopher.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Libraries&lt;/h2&gt;
&lt;p&gt;We will start by loading the libraries which includes &lt;code&gt;glmnet&lt;/code&gt; that will be used to construct the predictive model later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(gutenbergr)
library(glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;We are lucky today because all of The Federalist Papers happens to be on gutenberg&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers &amp;lt;- gutenberg_download(1404)
head(papers, n = 10)
## # A tibble: 10 x 2
##    gutenberg_id text                                                
##           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                               
##  1         1404 &amp;quot;THE FEDERALIST PAPERS&amp;quot;                             
##  2         1404 &amp;quot;&amp;quot;                                                  
##  3         1404 &amp;quot;By Alexander Hamilton, John Jay, and James Madison&amp;quot;
##  4         1404 &amp;quot;&amp;quot;                                                  
##  5         1404 &amp;quot;&amp;quot;                                                  
##  6         1404 &amp;quot;&amp;quot;                                                  
##  7         1404 &amp;quot;&amp;quot;                                                  
##  8         1404 &amp;quot;FEDERALIST No. 1&amp;quot;                                  
##  9         1404 &amp;quot;&amp;quot;                                                  
## 10         1404 &amp;quot;General Introduction&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the predictive modeling we are going to do later, I would like to divide each paper up into sentences. This is a rather complicated affair, but I will take a rather ad hoc approach that will be good enough for the purpose of this post. We will do this by collapsing all the lines together and splitting them by ., ! and ?‚Äôs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_sentences &amp;lt;- pull(papers, text) %&amp;gt;% 
  str_c(collapse = &amp;quot; &amp;quot;) %&amp;gt;%
  str_split(pattern = &amp;quot;\\.|\\?|\\!&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  tibble(text = .) %&amp;gt;%
  mutate(sentence = row_number())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would like to assign each of these sentences to the corresponding article number and author. Thus we will first assign each of the 85 papers to the 3 authors and a group for the papers of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hamilton &amp;lt;- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
madison &amp;lt;- c(10, 14, 18:20, 37:48)
jay &amp;lt;- c(2:5, 64)
unknown &amp;lt;- c(49:58, 62:63)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we will simple look for lines that include ‚ÄúFEDERALIST No‚Äù as they would indicate the start of a paper and then label them accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_words &amp;lt;- papers_sentences %&amp;gt;%
  mutate(no = cumsum(str_detect(text, regex(&amp;quot;FEDERALIST No&amp;quot;,
                                            ignore_case = TRUE)))) %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  mutate(author = case_when(no %in% hamilton ~ &amp;quot;hamilton&amp;quot;,
                            no %in% madison ~ &amp;quot;madison&amp;quot;,
                            no %in% jay ~ &amp;quot;jay&amp;quot;,
                            no %in% unknown ~ &amp;quot;unknown&amp;quot;),
         id = paste(no, sentence, sep = &amp;quot;-&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lets take a quick count before we move on&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_words %&amp;gt;%
  count(author)
## # A tibble: 4 x 2
##   author        n
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 hamilton 114688
## 2 jay        8539
## 3 madison   45073
## 4 unknown   24471&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that Jay didn‚Äôt post as many articles as the other two gentlemen so we will exclude him from further analysis&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_words &amp;lt;- papers_words %&amp;gt;%
  filter(author != &amp;quot;jay&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictive modeling&lt;/h2&gt;
&lt;p&gt;To make this predictive model we will use the term-frequency matrix as our input and as the response will be an indicator that Hamilton wrote the paper. For this modeling we will use the &lt;code&gt;glmnet&lt;/code&gt; package which fits a generalized linear model via penalized maximum likelihood. It is quite fast and works great with sparse matrix input, hence the term-frequency matrix.&lt;/p&gt;
&lt;p&gt;The response is set to the binomial family because of the binary nature of the response (did Hamilton write the sentence).&lt;/p&gt;
&lt;p&gt;First we get the term-frequency matrix with the &lt;code&gt;cast_&lt;/code&gt; family in tidytext.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_dtm &amp;lt;- papers_words %&amp;gt;%
  count(id, word, sort = TRUE) %&amp;gt;%
  cast_sparse(id, word, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will need to define a response variable, which we will do with a simple &lt;code&gt;mutate&lt;/code&gt;, along with an indicator for our training set which will be the articles with known authors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %&amp;gt;%
  left_join(papers_words[!duplicated(papers_words$id), ], by = &amp;quot;id&amp;quot;) %&amp;gt;%
  mutate(y = as.numeric(author == &amp;quot;hamilton&amp;quot;),
         train = author != &amp;quot;unknown&amp;quot;)
## Warning: Column `id` joining factor and character vector, coercing into
## character vector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use cross-validation to obtain the best value of the models tuning parameter. This part takes a couple of minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictor &amp;lt;- papers_dtm[meta$train, ]
response &amp;lt;- meta$y[meta$train]

model &amp;lt;- cv.glmnet(predictor, response, family = &amp;quot;binomial&amp;quot;, alpha = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running the model, we will add the predicted values to our &lt;code&gt;meta&lt;/code&gt; data.frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- meta %&amp;gt;%
  mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = &amp;quot;response&amp;quot;,
                        s = model$lambda.1se) %&amp;gt;% as.numeric())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is now time to visualize the results. First we will look at how the training set have been separated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta %&amp;gt;%
  filter(train) %&amp;gt;%
  ggplot(aes(factor(no), pred)) + 
  geom_boxplot(aes(fill = author)) +
  theme_minimal() +
  labs(y = &amp;quot;predicted probability&amp;quot;,
       x = &amp;quot;Article number&amp;quot;) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;#304890&amp;quot;, &amp;quot;#6A7E50&amp;quot;)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index_files/figure-html/old-plot1-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The box plot of predicted probabilities, one value for each sentence, for the 68 papers by
Alexander Hamilton and James Madison. The probability represents the extent to which the
model believe the sentence was written by Alexander Hamilton.&lt;/p&gt;
&lt;p&gt;Lets see if this model can settle the dispute of the 12 papers. We will plot the predicted probabilities of the unknown papers alongside the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta %&amp;gt;%
  ggplot(aes(factor(no), pred)) + 
  geom_boxplot(aes(fill = author)) +
  theme_minimal() +
  labs(y = &amp;quot;predicted probability&amp;quot;,
       x = &amp;quot;Article number&amp;quot;) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;#304890&amp;quot;, &amp;quot;#6A7E50&amp;quot;, &amp;quot;#D6BBD0&amp;quot;)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index_files/figure-html/old-plot1-pink-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;we notice that the predicted probabilities don‚Äôt quite makes up able to determine who the original author is. This can be due to a variety of different reasons. One of them could be that Madison wrote them and Hamilton edited them.&lt;/p&gt;
&lt;p&gt;Despite the unsuccessful attempt to predict the secret author we still managed to showcase the method which while being unsuccessful in this case could provide useful in other cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-showcase&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working showcase&lt;/h2&gt;
&lt;p&gt;Since the method proved unsuccessful in determining the secret author did I decide to add an example where the authorship is know. We will use the same data from earlier, only look at known Hamilton and Madison papers, train on some of them and show that the algorithm is able to detect the authorship of the other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;papers_dtm &amp;lt;- papers_words %&amp;gt;%
  filter(author != &amp;quot;unknown&amp;quot;) %&amp;gt;%
  count(id, word, sort = TRUE) %&amp;gt;% 
  cast_dtm(id, word, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;here we let the first 16 papers that they wrote be the test set and the rest be training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- data.frame(id = dimnames(papers_dtm)[[1]]) %&amp;gt;%
  left_join(papers_words[!duplicated(papers_words$id), ], by = &amp;quot;id&amp;quot;) %&amp;gt;%
  mutate(y = as.numeric(author == &amp;quot;hamilton&amp;quot;),
         train = no &amp;gt; 20)
## Warning: Column `id` joining factor and character vector, coercing into
## character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictor &amp;lt;- papers_dtm[meta$train, ] %&amp;gt;% as.matrix()
response &amp;lt;- meta$y[meta$train]

model &amp;lt;- cv.glmnet(predictor, response, family = &amp;quot;binomial&amp;quot;, alpha = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta &amp;lt;- meta %&amp;gt;%
  mutate(pred = predict(model, newx = as.matrix(papers_dtm), type = &amp;quot;response&amp;quot;,
                        s = model$lambda.1se) %&amp;gt;% as.numeric())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta %&amp;gt;%
  ggplot(aes(factor(no), pred)) + 
  geom_boxplot(aes(fill = author)) +
  theme_minimal() +
  labs(y = &amp;quot;predicted probability&amp;quot;,
       x = &amp;quot;Article number&amp;quot;) +
  theme(legend.position = &amp;quot;top&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;#304890&amp;quot;, &amp;quot;#6A7E50&amp;quot;)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_vline(aes(xintercept = 16.5), color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-30-predicting-authorship-in-the-federalist-papers-tidytext/index_files/figure-html/old-plot2-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we see that while it isn‚Äôt as crystal clear what what the test set predictions are giving us, they still give a pretty good indication.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing trigrams with the Tidyverse</title>
      <link>/2018/01/23/visualizing-trigrams-with-the-tidyverse/</link>
      <pubDate>Tue, 23 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/23/visualizing-trigrams-with-the-tidyverse/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this post I‚Äôll go though how I created the data visualization I posted yesterday on twitter:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;da&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Trying something new! Visualizing top trigrams in Jane Austen&#39;s Emma using &lt;a href=&#34;https://twitter.com/hashtag/tidytext?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytext&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt;! Blogpost coming soon! ü§ó &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; &lt;a href=&#34;https://t.co/Sy1fQJB5Ih&#34;&gt;pic.twitter.com/Sy1fQJB5Ih&lt;/a&gt;
&lt;/p&gt;
‚Äî Emil Hvitfeldt (&lt;span class=&#34;citation&#34;&gt;@Emil_Hvitfeldt&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Emil_Hvitfeldt/status/955675169422327808?ref_src=twsrc%5Etfw&#34;&gt;23. januar 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;what-am-i-looking-at&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What am I looking at?&lt;/h2&gt;
&lt;p&gt;So for this particular data-viz I took novel Emma by Jane Austen, extracted all the trigrams (sentences of length 3), took the 150 most frequent ones and visualized those.&lt;/p&gt;
&lt;p&gt;This visualization is layered horizontal tree graph where the 3 levels (vertical columns of words) correspond words that appear at the nth place in the trigrams, e.g.¬†first column have the first words of the trigram, second column have middle words of trigrams etc. Up to 20 words in each column are kept and they are ordered and sized according to occurrence in the data.&lt;/p&gt;
&lt;p&gt;The curves represent how often two words co-occur, with the color representing starting word and transparency related to frequency.&lt;/p&gt;
&lt;p&gt;All code is presented in the following &lt;a href=&#34;https://gist.github.com/EmilHvitfeldt/f69a65436915ff8b0406cdd27a194e28&#34;&gt;gist&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;packages-and-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages and parameters&lt;/h2&gt;
&lt;p&gt;We will be using the following packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(purrrlyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the overall parameters outlined in description are defined here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_word &amp;lt;- 20
n_top &amp;lt;- 150
n_gramming &amp;lt;- 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trigrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trigrams&lt;/h2&gt;
&lt;p&gt;If you have read &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R&lt;/a&gt; I‚Äôm sure you have encountered the &lt;code&gt;janeaustenr&lt;/code&gt; package. We will use the Emma novel, and &lt;code&gt;tidytext&lt;/code&gt;‚Äôs &lt;code&gt;unnest_tokens&lt;/code&gt; to calculate the trigrams we need. We also specify the starting words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trigrams &amp;lt;- tibble(text = janeaustenr::emma) %&amp;gt;%
  unnest_tokens(trigram, text, token = &amp;quot;ngrams&amp;quot;, n = n_gramming)

start_words &amp;lt;- c(&amp;quot;he&amp;quot;, &amp;quot;she&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;next we find the top 150 trigrams using &lt;code&gt;count&lt;/code&gt; and some regex magic. And we use those top words to filter such that we only will be looking at the top 150.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- str_c(&amp;quot;^&amp;quot;, start_words, &amp;quot; &amp;quot;, collapse = &amp;quot;|&amp;quot;)
top_words &amp;lt;- trigrams %&amp;gt;%
  filter(str_detect(trigram, pattern)) %&amp;gt;%
  count(trigram, sort = TRUE) %&amp;gt;%
  slice(seq_len(n_top)) %&amp;gt;%
  pull(trigram)

trigrams &amp;lt;- trigrams %&amp;gt;%
  filter(trigram %in% top_words)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nodes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nodes&lt;/h2&gt;
&lt;p&gt;Since we know that each trigram have a sample format, we can create a simple function to extract the nth word in a string.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_nth_word &amp;lt;- function(x, n, sep = &amp;quot; &amp;quot;) {
  str_split(x, pattern = &amp;quot; &amp;quot;) %&amp;gt;%
  map_chr(~ .x[n])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following &lt;code&gt;purrr::map_df&lt;/code&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Extracts the nth word in the trigram&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Counts and sorts the occurrences&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Grabs the top 20 words&lt;/li&gt;
&lt;li&gt;Equally space them along the y-axis&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nodes &amp;lt;- map_df(seq_len(n_gramming),
       ~ trigrams %&amp;gt;%
           mutate(word = str_nth_word(trigram, .x)) %&amp;gt;%
           count(word, sort = TRUE) %&amp;gt;%
           slice(seq_len(n_word)) %&amp;gt;% 
           mutate(y = seq(from = n_word + 1, to = 0, 
                          length.out = n() + 2)[seq_len(n()) + 1],
                  x = .x))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;plot-of-node-positions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;plot of node positions&lt;/h3&gt;
&lt;p&gt;Lets see the words so far:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nodes %&amp;gt;% 
  ggplot(aes(x, y, label = word)) +
  geom_text()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;edges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Edges&lt;/h2&gt;
&lt;p&gt;When we look at the final visualization we see that the words are connected by curved lines. I achieved that by using a sigmoid curve and then transform it to match the starting and end points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigmoid &amp;lt;- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) {
  x &amp;lt;- seq(-scale, scale, length = n)
  y &amp;lt;- exp(x) / (exp(x) + 1)
  tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from,
         y = y * (y_to - y_from) + y_from)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following function takes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a list of trigrams&lt;/li&gt;
&lt;li&gt;a data.frame of ‚Äúfrom‚Äù nodes&lt;/li&gt;
&lt;li&gt;a data.frame of ‚Äúto‚Äù nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and returns a data.frame containing the data points for the curves wee need to draw with correct starting and ending points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;egde_lines &amp;lt;- function(trigram, from_word, to_word, scale = 5, n = 50, 
                       x_space = 0) {

  from_word &amp;lt;- from_word %&amp;gt;%
    select(-n) %&amp;gt;%
    set_names(c(&amp;quot;from&amp;quot;, &amp;quot;y_from&amp;quot;, &amp;quot;x_from&amp;quot;))
  
  to_word &amp;lt;- to_word %&amp;gt;%
    select(-n) %&amp;gt;%
    set_names(c(&amp;quot;to&amp;quot;, &amp;quot;y_to&amp;quot;, &amp;quot;x_to&amp;quot;))
  
  links &amp;lt;- crossing(from = from_word$from, 
                    to = to_word$to) %&amp;gt;%
    mutate(word_pair = paste(from, to),
           number = map_dbl(word_pair, 
                            ~ sum(str_detect(trigram$trigram, .x)))) %&amp;gt;%
    left_join(from_word, by = &amp;quot;from&amp;quot;) %&amp;gt;%
    left_join(to_word, by = &amp;quot;to&amp;quot;)
  
  links %&amp;gt;%
    by_row(~ sigmoid(x_from = .x$x_from + 0.2 + x_space,
                     x_to = .x$x_to - 0.05, 
                     y_from = .x$y_from, y_to = .x$y_to, 
                     scale = scale, n = n) %&amp;gt;%
    mutate(word_pair = .x$word_pair,
           number = .x$number,
           from = .x$from)) %&amp;gt;%
    pull(.out) %&amp;gt;%
    bind_rows()
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;plot-of-first-set-of-egdes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;plot of first set of egdes&lt;/h3&gt;
&lt;p&gt;Lets take a look at the first set of edges to see if it is working.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;egde_lines(trigram = trigrams, 
           from_word = filter(nodes, x == 1), 
           to_word = filter(nodes, x == 2)) %&amp;gt;%
  filter(number &amp;gt; 0) %&amp;gt;%
  ggplot(aes(x, y, group = word_pair, alpha = number, color = from)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-all-egdes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculating all egdes&lt;/h3&gt;
&lt;p&gt;For ease (and laziness) I have desired to calculate the edges in sections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edges between first and second column&lt;/li&gt;
&lt;li&gt;edges between second and third column for words that start with ‚Äúhe‚Äù&lt;/li&gt;
&lt;li&gt;edges between second and third column for words that start with ‚Äúshe‚Äù&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and combine by the end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# egdes between first and second column
egde1 &amp;lt;- egde_lines(trigram = trigrams, 
           from_word = filter(nodes, x == 1), 
           to_word = filter(nodes, x == 2), 
           n = 50) %&amp;gt;%
           filter(number &amp;gt; 0) %&amp;gt;%
  mutate(id = word_pair)

# Words in second colunm
## That start with he
second_word_he &amp;lt;- nodes %&amp;gt;%
  filter(x == 2) %&amp;gt;%
  select(-n) %&amp;gt;%
  left_join(
    trigrams %&amp;gt;% 
      filter(str_nth_word(trigram, 1) == start_words[1]) %&amp;gt;%
      mutate(word = str_nth_word(trigram, 2)) %&amp;gt;%
      count(word), 
    by = &amp;quot;word&amp;quot;
  ) %&amp;gt;%
  replace_na(list(n = 0))

## That start with she
second_word_she &amp;lt;- nodes %&amp;gt;%
  filter(x == 2) %&amp;gt;%
  select(-n) %&amp;gt;%
  left_join(
    trigrams %&amp;gt;% 
      filter(str_nth_word(trigram, 1) == start_words[2]) %&amp;gt;%
      mutate(word = str_nth_word(trigram, 2)) %&amp;gt;%
      count(word), 
    by = &amp;quot;word&amp;quot;
  ) %&amp;gt;%
  replace_na(list(n = 0))

# Words in third colunm
## That start with he
third_word_he &amp;lt;- nodes %&amp;gt;%
  filter(x == 3) %&amp;gt;%
  select(-n) %&amp;gt;%
  left_join(
    trigrams %&amp;gt;% 
      filter(str_nth_word(trigram, 1) == start_words[1]) %&amp;gt;%
      mutate(word = str_nth_word(trigram, 3)) %&amp;gt;%
      count(word), 
    by = &amp;quot;word&amp;quot;
  ) %&amp;gt;%
  replace_na(list(n = 0))

## That start with she
third_word_she &amp;lt;- nodes %&amp;gt;%
  filter(x == 3) %&amp;gt;%
  select(-n) %&amp;gt;%
  left_join(
    trigrams %&amp;gt;% 
      filter(str_nth_word(trigram, 1) == start_words[2]) %&amp;gt;%
      mutate(word = str_nth_word(trigram, 3)) %&amp;gt;%
      count(word), 
    by = &amp;quot;word&amp;quot;
  ) %&amp;gt;%
  replace_na(list(n = 0))

# egdes between second and third column that starts with he
egde2_he &amp;lt;- egde_lines(filter(trigrams, 
                              str_detect(trigram, paste0(&amp;quot;^&amp;quot;, start_words[1], &amp;quot; &amp;quot;))), 
             second_word_he, third_word_he, n = 50) %&amp;gt;%
  mutate(y = y + 0.05,
         from = start_words[1],
         id = str_c(from, word_pair, sep = &amp;quot; &amp;quot;)) %&amp;gt;%
  filter(number &amp;gt; 0)

# egdes between second and third column that starts with she
egde2_she &amp;lt;- egde_lines(filter(trigrams, 
                              str_detect(trigram, paste0(&amp;quot;^&amp;quot;, start_words[2], &amp;quot; &amp;quot;))), 
             second_word_she, third_word_she, n = 50) %&amp;gt;%
  mutate(y = y - 0.05,
         from = start_words[2],
         id = str_c(from, word_pair, sep = &amp;quot; &amp;quot;)) %&amp;gt;%
  filter(number &amp;gt; 0)

# All edges
edges &amp;lt;- bind_rows(egde1, egde2_he, egde2_she)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vizualisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;vizualisation&lt;/h2&gt;
&lt;p&gt;Now we just add it all together. All labels, change colors, adjust &lt;code&gt;xlim&lt;/code&gt; to fit words on the page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- nodes %&amp;gt;% 
  ggplot(aes(x, y, label = word, size = n)) +
  geom_text(hjust = 0, color = &amp;quot;#DDDDDD&amp;quot;) +
  theme_void() +
  geom_line(data = edges,
            aes(x, y, group = id, color = from, alpha = sqrt(number)),
            inherit.aes = FALSE) +
  theme(plot.background = element_rect(fill = &amp;quot;#666666&amp;quot;, colour = &amp;#39;black&amp;#39;),
        text = element_text(color = &amp;quot;#EEEEEE&amp;quot;, size = 15)) +
  guides(alpha = &amp;quot;none&amp;quot;, color = &amp;quot;none&amp;quot;, size = &amp;quot;none&amp;quot;) +
  xlim(c(0.9, 3.2)) +
  scale_color_manual(values = c(&amp;quot;#5EF1F1&amp;quot;, &amp;quot;#FA62D0&amp;quot;)) +
  labs(title = &amp;quot; Vizualizing trigrams in Jane Austen&amp;#39;s, Emma&amp;quot;) + 
  scale_size(range = c(3, 8))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;There are a couple of differences between the Viz I posted online yesterday and the result here in this post due to a couple of mistakes found in the code during cleanup.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extra-vizualisations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extra vizualisations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_word &amp;lt;- 20
n_top &amp;lt;- 150
n_gramming &amp;lt;- 3

trigrams &amp;lt;- tibble(text = janeaustenr::emma) %&amp;gt;%
  unnest_tokens(trigram, text, token = &amp;quot;ngrams&amp;quot;, n = n_gramming)

start_words &amp;lt;- c(&amp;quot;i&amp;quot;, &amp;quot;you&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_word &amp;lt;- 20
n_top &amp;lt;- 150
n_gramming &amp;lt;- 3

library(rvest)
sherlock_holmes &amp;lt;- read_html(&amp;quot;https://sherlock-holm.es/stories/plain-text/cnus.txt&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;% 
  str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
  unlist()

trigrams &amp;lt;- tibble(text = sherlock_holmes) %&amp;gt;%
  unnest_tokens(trigram, text, token = &amp;quot;ngrams&amp;quot;, n = n_gramming)

start_words &amp;lt;- c(&amp;quot;holmes&amp;quot;, &amp;quot;watson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: xml2
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2
## 
## Attaching package: &amp;#39;rvest&amp;#39;
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     pluck
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     guess_encoding&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-23-visualizing-trigrams-with-the-tidyverse/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Purrr - tips and tricks</title>
      <link>/2018/01/08/purrr-tips-and-tricks/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/08/purrr-tips-and-tricks/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-18.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;purrr-tips-and-tricks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Purrr tips and tricks&lt;/h2&gt;
&lt;p&gt;If you like me started by only using &lt;code&gt;map()&lt;/code&gt; and its cousins (&lt;code&gt;map_df&lt;/code&gt;, &lt;code&gt;map_dbl&lt;/code&gt;, etc) you are missing out a lot of what &lt;code&gt;purrr&lt;/code&gt; have to offer! With the advent of #purrrresolution on twitter I‚Äôll throw my 2 cents in in form of my bag of tips and tricks (which I‚Äôll update in the future).&lt;/p&gt;
&lt;p&gt;First we load the packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(repurrrsive) # datasets used in some of the examples.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;loading-files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;loading files&lt;/h3&gt;
&lt;p&gt;Multiple files can be read and combined at once using &lt;code&gt;map_df&lt;/code&gt; and &lt;code&gt;read_cvs&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;files &amp;lt;- c(&amp;quot;2015.cvs&amp;quot;, &amp;quot;2016.cvs&amp;quot;, &amp;quot;2017.cvs&amp;quot;)
map_df(files, read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Combine with &lt;code&gt;list.files&lt;/code&gt; to create magic&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;files &amp;lt;- list.files(&amp;quot;../open-data/&amp;quot;, pattern = &amp;quot;^2017&amp;quot;, full.names = TRUE)
full &amp;lt;- map_df(files, read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;combine-if-you-forget-_df-the-first-time-around.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;combine if you forget *_df the first time around.&lt;/h3&gt;
&lt;p&gt;If you like me sometimes forget to end my &lt;code&gt;map()&lt;/code&gt; with my desired out put. A last resort is to manually combine it in a second line if you don‚Äôt want to replace &lt;code&gt;map()&lt;/code&gt; with &lt;code&gt;map_df()&lt;/code&gt; (which is properly the better advice, but can be handy in a pinch).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- map(1:10000, ~ data.frame(x = .x))
X &amp;lt;- bind_rows(X)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;name-shortcut-in-map&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;name shortcut in map&lt;/h3&gt;
&lt;p&gt;provide ‚ÄúTEXT‚Äù to extract the element named ‚ÄúTEXT‚Äù. Follow 3 lines are equivalent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(got_chars, function(x) x[[&amp;quot;name&amp;quot;]]) 
map(got_chars, ~ .x[[&amp;quot;name&amp;quot;]])
map(got_chars, &amp;quot;name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;works the same with indexes.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(got_chars, function(x) x[[1]]) 
map(got_chars, ~ .x[[1]])
map(got_chars, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-inside-map&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;use {} inside map&lt;/h3&gt;
&lt;p&gt;If you don‚Äôt know how to write the proper anonymous function or you want some counter in your &lt;code&gt;map()&lt;/code&gt;, you can use &lt;code&gt;{}&lt;/code&gt; to construct your anonymous function.&lt;/p&gt;
&lt;p&gt;Here is a simple toy example that shows that you can write multiple lines inside &lt;code&gt;map&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(1:3, ~ {
  h &amp;lt;- .x + 2
  g &amp;lt;- .x - 2
  h + g
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(1:3, ~ {
  Sys.sleep(10)
  cat(.x)
  .x
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be very handy if you want to be a responsible (websraping) pirate&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
s_GET &amp;lt;- safely(GET)

pb &amp;lt;- progress_estimated(length(target_urls))
map(target_urls, ~{
  pb$tick()$print()
  Sys.sleep(5)
  s_GET(.x)
}) -&amp;gt; httr_raw_responses&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;discard-keep-and-compact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;discard, keep and compact&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;discard()&lt;/code&gt; and &lt;code&gt;keep()&lt;/code&gt; will provide very valuable since they help you filter your list/vector based on certain predictors.&lt;/p&gt;
&lt;p&gt;They can be useful in cases of webcraping where certain lines are to be ignored.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
url &amp;lt;- &amp;quot;http://www.imdb.com/chart/boxoffice&amp;quot;

read_html(url) %&amp;gt;%
  html_nodes(&amp;#39;tr&amp;#39;) %&amp;gt;%
  html_text() %&amp;gt;%
  str_replace_all(&amp;quot;\n +&amp;quot;, &amp;quot; &amp;quot;) %&amp;gt;%
  trimws() %&amp;gt;%
  keep(~ str_extract(.x, &amp;quot;.$&amp;quot;) %in% 0:9) %&amp;gt;%
  discard(~ as.numeric(str_extract(.x, &amp;quot;.$&amp;quot;)) &amp;gt; 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where we here scrape Top Box Office (US) from IMDb.com and we use &lt;code&gt;keep()&lt;/code&gt; to keeps all lines that end in a integer and &lt;code&gt;discards()&lt;/code&gt; to discards all lines where the integer is more then 5.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;compact()&lt;/code&gt; is a handy wrapper that removed all elements that are NULL.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;safely-compact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;safely + compact&lt;/h3&gt;
&lt;p&gt;If you have a function that sometimes throws an error, warning or for whatever reason isn‚Äôt entirely stable, you can use the wonder of &lt;code&gt;safely()&lt;/code&gt; and &lt;code&gt;compact()&lt;/code&gt;. &lt;code&gt;safely()&lt;/code&gt; is a function that takes a function &lt;code&gt;f()&lt;/code&gt; and returns a function &lt;code&gt;safe_f()&lt;/code&gt; that returns a list with the elements &lt;code&gt;result&lt;/code&gt; and &lt;code&gt;error&lt;/code&gt; where &lt;code&gt;result&lt;/code&gt; is the output of &lt;code&gt;f()&lt;/code&gt; if it is able to run, and &lt;code&gt;NULL&lt;/code&gt; otherwise. This means that we can create a function that will always work!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unstable_function &amp;lt;- function() {
  ...
}

safe_function &amp;lt;- safely(unstable_function)

map(data, ~ safe_function(.x)) %&amp;gt;%
  map(&amp;quot;result&amp;quot;) %&amp;gt;%
  compact()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;combining this with &lt;code&gt;compact&lt;/code&gt; which removes all &lt;code&gt;NULL&lt;/code&gt; values thus returning only the successful calls.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reduce&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reduce&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;purrr&lt;/code&gt; includes an little group of functions called &lt;code&gt;reduce()&lt;/code&gt; (with its cousins &lt;code&gt;reduce_right()&lt;/code&gt;, &lt;code&gt;reduce2()&lt;/code&gt; and &lt;code&gt;reduce2_right()&lt;/code&gt;) which iteratively combines from the left (right for &lt;code&gt;reduce_right()&lt;/code&gt;) making&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduce(list(x1, x2, x3), f)
f(f(x1, x2), x3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;equivalent.&lt;/p&gt;
&lt;p&gt;This example&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; comes from Colin Fay shows how to use &lt;code&gt;reduce()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regex_build &amp;lt;- function(list){
    reduce(list, ~ paste(.x, .y, sep = &amp;quot;|&amp;quot;))
}

regex_build(letters[1:5])
## [1] &amp;quot;a|b|c|d|e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example by Jason Becker&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; shows how to easier label data using &lt;code&gt;reduce_right&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load a directory of .csv files that has each of the lookup tables
lookups &amp;lt;- map(dir(&amp;#39;data/lookups&amp;#39;), read.csv, stringsAsFactors = FALSE)
# Alternatively if you have a single lookup table with code_type as your
# data attribute you&amp;#39;re looking up
# lookups &amp;lt;- split(lookups, code_type)
lookups$real_data &amp;lt;- read.csv(&amp;#39;data/real_data.csv&amp;#39;, 
                              stringsAsFactors = FALSE)
real_data &amp;lt;- reduce_right(lookups, left_join)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pluck&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;pluck&lt;/h3&gt;
&lt;p&gt;I find that subsetting list can be a hassle more often then not. But &lt;code&gt;pluck()&lt;/code&gt; have really helped to alleviate those problems quite a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list(A = list(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;), 
     B = list(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;),
     C = list(&amp;quot;c1&amp;quot;, &amp;quot;c2&amp;quot;),
     D = list(&amp;quot;d1&amp;quot;, &amp;quot;d2&amp;quot;, &amp;quot;d3&amp;quot;)) %&amp;gt;% 
  pluck(1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;head_while-tail_while&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;head_while, tail_while&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;purrr&lt;/code&gt; includes the twins &lt;code&gt;head_while&lt;/code&gt; and &lt;code&gt;tail_while&lt;/code&gt; which will gives you all the elements that satisfy the condition intill the first time it doesn‚Äôt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- sample(1:100)

# This
p &amp;lt;- function(X) !(X &amp;gt;= 10)
X[seq(Position(p, X) - 1)]

# is the same as this
head_while(X, ~ .x &amp;gt;= 10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rerun&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;rerun&lt;/h3&gt;
&lt;p&gt;if you need to do some simulation studies &lt;code&gt;rerun&lt;/code&gt; could prove very useful. It takes 2 arguments. &lt;code&gt;.n&lt;/code&gt; is the number of times to run, and &lt;code&gt;...&lt;/code&gt; is the expression that have to be rerun.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rerun(.n = 10, rnorm(10)) %&amp;gt;%
  map_df(~ tibble(mean = mean(.x),
                  sd = sd(.x),
                  median = median(.x)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compose&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;compose&lt;/h3&gt;
&lt;p&gt;This little wonder of a function composes multiple functions to be applied in order from right to left.&lt;/p&gt;
&lt;p&gt;This toy examples show how it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(x = 1:6, size =  50, replace = TRUE) %&amp;gt;%
  table %&amp;gt;% 
  sort %&amp;gt;%
  names

dice1 &amp;lt;- function(n) sample(size = n, x = 1:6, replace = TRUE)
dice_rank &amp;lt;- compose(names, sort, table, dice1)
dice_rank(50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more informative is found here&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
tidy_lm &amp;lt;- compose(tidy, lm)
tidy_lm(Sepal.Length ~ Species, data = iris)
## # A tibble: 3 x 5
##   term              estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)           5.01    0.0728     68.8  1.13e-113
## 2 Speciesversicolor     0.93    0.103       9.03 8.77e- 16
## 3 Speciesvirginica      1.58    0.103      15.4  2.21e- 32&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;imap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;imap&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;imap()&lt;/code&gt; is a handy little wrapper that acts as the indexed &lt;code&gt;map()&lt;/code&gt;. Thus making it shorthand for &lt;code&gt;map2(x, names(x), ...)&lt;/code&gt; when x have named and &lt;code&gt;map2(x, seq_along(x), ...)&lt;/code&gt; when it doesn‚Äôt have names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imap_dbl(sample(10), ~ {
  cat(&amp;quot;draw nr&amp;quot;, .y, &amp;quot;is&amp;quot;, .x, &amp;quot;\n&amp;quot;)
  .x
  })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or it could be used in conjunction with &lt;code&gt;rerun()&lt;/code&gt; to easily add id to each sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rerun(.n = 10, rnorm(10)) %&amp;gt;%
  imap_dfr(~ tibble(run = .y, 
                    mean = mean(.x),
                    sd = sd(.x),
                    median = median(.x)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://ghement.ca/purrr.html&#34; class=&#34;uri&#34;&gt;http://ghement.ca/purrr.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://statwonk.com/purrr.html&#34; class=&#34;uri&#34;&gt;http://statwonk.com/purrr.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://maraaverick.rbind.io/2017/09/purrr-ty-posts/&#34; class=&#34;uri&#34;&gt;https://maraaverick.rbind.io/2017/09/purrr-ty-posts/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://serialmentor.com/blog/2016/6/13/reading-and-combining-many-tidy-data-files-in-R&#34; class=&#34;uri&#34;&gt;http://serialmentor.com/blog/2016/6/13/reading-and-combining-many-tidy-data-files-in-R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-web-mining/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-web-mining/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-text-wrangling/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-text-wrangling/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-set-na/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-set-na/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-mappers/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-mappers/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-code-optim/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-code-optim/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-statistics/&#34; class=&#34;uri&#34;&gt;http://colinfay.me/purrr-statistics/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://gist.github.com/ColinFay/d74d331825868b181860212cd1577b69&#34;&gt;ColinFay/df a list&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html&#34;&gt;jennybc.github.io - Introduction to map(): extract elements&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rud.is/b/2017/09/19/pirating-web-content-responsibly-with-r/&#34;&gt;Pirating Web Content Responsibly With R&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-text-wrangling/&#34;&gt;A Crazy Little Thing Called {purrr} - Part 2 : Text Wrangling&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.json.blog/2017/03/labeling-data-with-purrr/&#34;&gt;Labeling Data with purrr&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://colinfay.me/purrr-code-optim/&#34;&gt;A Crazy Little Thing Called {purrr} - Part 5: code optimization&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recreate - Sunshine Report</title>
      <link>/2018/01/01/recreate-sunshine-report/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/01/recreate-sunshine-report/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hello again! I this mini-series (of in-determined length) will I try as best as I can to recreate great visualizations in tidyverse. The recreation may be exact in terms of data, or using data of a similar style.&lt;/p&gt;
&lt;div id=&#34;the-goal---an-annual-sunshine-record-report&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The goal - An annual sunshine record report&lt;/h2&gt;
&lt;p&gt;I have recently read &lt;a href=&#34;https://www.edwardtufte.com/tufte/books_vdqi&#34;&gt;The Visual Display of Quantitative Information&lt;/a&gt; by Edward R Tufte, which I highly recommend. In the book the following chart was displayed which showed the sunshine record for each day day of the year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2018-01-01-13.53scan.jpg&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;F.J. Monkhouse and H.R. Wilkinson, &lt;em&gt;Maps and Diagrams&lt;/em&gt; (London, third edition 1971), 242-243.&lt;/p&gt;
&lt;p&gt;The goal for the rest of this post is to create something similar. Since we don‚Äôt have direct access to the data, we will scrape some data for ourselves. All code will be shown together in the end of the post and in this &lt;a href=&#34;https://gist.github.com/EmilHvitfeldt/53e5e33a0ebc5e084dcbcdefacb8ed9a&#34;&gt;gist&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R packages&lt;/h2&gt;
&lt;p&gt;First we need some packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2
library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(lubridate)
library(glue)
library(ehlib) # devtools::install_github(&amp;quot;EmilHvitfeldt/ehlib&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last package is my &lt;a href=&#34;https://hilaryparker.com/2013/04/03/personal-r-packages/&#34;&gt;personal R package&lt;/a&gt; &lt;a href=&#34;https://github.com/EmilHvitfeldt/ehlib&#34;&gt;ehlib&lt;/a&gt; where I store some frequently used functions. If you do not wish to install/load this package just run the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_between &amp;lt;- function(string, start, end) {
  stringr::str_extract(string,
                       stringr::str_c(start, &amp;#39;(.*?)&amp;#39;, end, collapse = &amp;#39;&amp;#39;)) %&amp;gt;%
    stringr::str_replace(start, &amp;quot;&amp;quot;) %&amp;gt;%
    stringr::str_replace(end, &amp;quot;&amp;quot;)
}

str_before &amp;lt;- function(string, pattern) {
  stringr::str_extract(string, stringr::str_c(&amp;quot;.+?(?=&amp;quot;, pattern, &amp;quot;)&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data collection&lt;/h2&gt;
&lt;p&gt;So for this production we need, Weather information. But more specifically we need information about if the sun is shining for various times during the day, preferable for all days of the year. In addition sunrise and sunset times is also needed.&lt;/p&gt;
&lt;p&gt;We will be scraping weather history from &lt;a href=&#34;https://www.wunderground.com&#34;&gt;wunderground&lt;/a&gt;. On the button of the page &lt;a href=&#34;https://www.wunderground.com/history/airport/KCQT/2018/1/1/DailyHistory.html&#34; class=&#34;uri&#34;&gt;https://www.wunderground.com/history/airport/KCQT/2018/1/1/DailyHistory.html&lt;/a&gt; we locate a table with ‚ÄúTime‚Äù and ‚ÄúConditions‚Äù. Furthermore both sunrise and sunset times are present on the page.&lt;/p&gt;
&lt;p&gt;For the website we need an airport code, year, month and day. Airport codes will have to be found manually by browsing the website. For a vector of all the days in a given year we use the following function that uses&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_dates_in &amp;lt;- function(year) {
  if(ymd(glue::glue(&amp;quot;{year}0101&amp;quot;)) &amp;gt; as.Date(Sys.time())) {
    stop(&amp;quot;Please select a past or current year.&amp;quot;)
  }
  
  start &amp;lt;- ymd(glue::glue(&amp;quot;{year}0101&amp;quot;))
  
  if(as.Date(Sys.time()) &amp;gt; ymd(glue::glue(&amp;quot;{year}1231&amp;quot;))) {
    end &amp;lt;- ymd(glue::glue(&amp;quot;{year}1231&amp;quot;))
  } else {
    end &amp;lt;- as.Date(Sys.time())
  }
  
  seq(start, end, by = &amp;quot;day&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this function will work even if you pick a year that have not ended yet. As 2017 have just ended I though it would be appropriate to look back on that year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year &amp;lt;- 2017
dates &amp;lt;- all_dates_in(year)
head(dates)
## [1] &amp;quot;2017-01-01&amp;quot; &amp;quot;2017-01-02&amp;quot; &amp;quot;2017-01-03&amp;quot; &amp;quot;2017-01-04&amp;quot; &amp;quot;2017-01-05&amp;quot;
## [6] &amp;quot;2017-01-06&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;next we have a little function that creates a url from the airport code and the date. For safety we will wrap that function in &lt;code&gt;purrr::safely&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_data_html &amp;lt;- function(date, code) {
  url &amp;lt;- str_c(&amp;quot;https://www.wunderground.com/history/airport/&amp;quot;, code, &amp;quot;/&amp;quot;,
               year(date), &amp;quot;/&amp;quot;, month(date), &amp;quot;/&amp;quot;, mday(date), &amp;quot;/DailyHistory.html&amp;quot;)
  
  html_url &amp;lt;- read_html(url)
}

weather_data_html &amp;lt;- purrr::safely(weather_data_html)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this code-though will we be using airport code KCQT, which is placed in Los Angeles Downtown, CA.&lt;/p&gt;
&lt;p&gt;We add some ‚Äòcrawl-delay‚Äô of 5 seconds and let it run. Please remember that this will take over 30 minutes to run with a delay in place but we do it to be nice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airport_code &amp;lt;- &amp;quot;KCQT&amp;quot;

full_data &amp;lt;- map(dates, ~{
  weather_data_html(.x, airport_code)
  Sys.sleep(5)
  cat(month(.x), &amp;quot;/&amp;quot;, mday(.x), &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;)
  })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check whether all of the links went though.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_lgl(full_data, ~ is.null(.x$error))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data wrangling&lt;/h2&gt;
&lt;p&gt;Since we will be working with times quite a lot in the section we will use the &lt;code&gt;lubridate&lt;/code&gt; package quite some time. In addition to that package I have devised this following function to turn something of the form ‚Äú2:51 PM‚Äù into the number of minutes after midnight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ampm_minutes &amp;lt;- function(x) {
  as.numeric(str_between(x, &amp;quot;:&amp;quot;, &amp;quot; &amp;quot;)) +
  as.numeric(str_replace(str_before(x, &amp;quot;:&amp;quot;), &amp;quot;12&amp;quot;, &amp;quot;0&amp;quot;)) * 60 +
  60 * 12 * str_detect(x, &amp;quot;PM&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we have the main wrangling function that takes the input, extracts the sunrise, sunset times and add them to the table that is also extracted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_wrangling &amp;lt;- function(html_url, date) {
  
  # Sun rise time
    sun_rise &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;div[id=&amp;quot;astronomy-mod&amp;quot;] table&amp;#39;) %&amp;gt;%
    html_text() %&amp;gt;%
    .[1] %&amp;gt;%
    str_between(&amp;quot;Time\n\t\t&amp;quot;, &amp;quot;\n\t\t&amp;quot;)
  # Sun set time
  sun_set &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;div[id=&amp;quot;astronomy-mod&amp;quot;] table&amp;#39;) %&amp;gt;%
    html_text() %&amp;gt;%
    .[1] %&amp;gt;%
    str_between(&amp;quot;\n\t\t&amp;quot;, &amp;quot;\n\t\tCivil&amp;quot;)

  # Table
  table &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;table[id=&amp;quot;obsTable&amp;quot;]&amp;#39;) %&amp;gt;%
    html_table() %&amp;gt;% 
    .[[1]]
  
  # Time column standardization 
  is_daylight &amp;lt;- any(&amp;quot;Time (PDT)&amp;quot; == names(table),
                     &amp;quot;Time (MDT)&amp;quot; == names(table),
                     &amp;quot;Time (CDT)&amp;quot; == names(table),
                     &amp;quot;Time (EDT)&amp;quot; == names(table))
  
  time_names &amp;lt;- str_c(&amp;quot;Time&amp;quot;, c(&amp;quot; (PDT)&amp;quot;, &amp;quot; (MDT)&amp;quot;, &amp;quot; (CDT)&amp;quot;, &amp;quot; (EDT)&amp;quot;,
                                &amp;quot; (PST)&amp;quot;, &amp;quot; (MST)&amp;quot;, &amp;quot; (CST)&amp;quot;, &amp;quot; (EST)&amp;quot;))
  
  names(table) &amp;lt;- if_else(names(table) %in% time_names,
                          &amp;quot;Time&amp;quot;,
                          names(table))
  
  table %&amp;gt;%
    mutate(sun_set = sun_set,
           sun_rise = sun_rise,
           date = date,
           yday = yday(date), 
           day_minutes = ampm_minutes(Time) - is_daylight * 60,
           set_minutes = ampm_minutes(sun_set) - is_daylight * 60,
           rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60,
           sun_up = day_minutes &amp;gt; (rise_minutes + 90) &amp;amp; 
                    day_minutes &amp;lt; (set_minutes - 30))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this function we arbitrarily decide that the sun is up, if it is 90 minutes after sun rise and 30 minutes before sun set. This is done because out future visualization is being made with rectangles and the &lt;code&gt;lag&lt;/code&gt; function, and to ensure that all the sunshine hours are within sun set and sun rise we have to put in some restrains.&lt;/p&gt;
&lt;p&gt;It seems that the 30th of October doesn‚Äôt have hourly history data available so we will exclude it in the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_data2 &amp;lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %&amp;gt;%
                      data_wrangling(.y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point it would be wise to save our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;save(full_data2, file = glue(&amp;quot;{airport_code}-{year}.Rdata&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting data&lt;/h2&gt;
&lt;p&gt;Now that we have all the data we need it is time to turn our heads to ggplot2. But before we do that lets create some axis breaks that we will need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_axis &amp;lt;- dates %&amp;gt;% month() %&amp;gt;% table() %&amp;gt;% cumsum()
names(x_axis) &amp;lt;- month.abb[1:12]

y_axis &amp;lt;- 1:24 * 60
names(y_axis) &amp;lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), 
                       rep(c(&amp;quot;AM&amp;quot;, &amp;quot;PM&amp;quot;), each = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we start by creating a new condition for ‚ÄúClear‚Äù, creating a new day_minutes variable to act as the other side for our sunshine rectangles and lastly remove all the observations where the sun isn‚Äôt up. Using &lt;code&gt;geom_rect()&lt;/code&gt; to create all the little rectangles and &lt;code&gt;geom_line()&lt;/code&gt;‚Äôs to show the sun set and sun rise, we lastly fiddle a little with the theme giving us the final result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_data2 %&amp;gt;%
  mutate(con = Conditions == &amp;quot;Clear&amp;quot;,
         day_minutes2 = lag(day_minutes)) %&amp;gt;%
  filter(sun_up) %&amp;gt;%
  ggplot(aes(fill = con)) +
  geom_rect(aes(xmin = yday, xmax = yday + 1,
                ymin = day_minutes, ymax = day_minutes2)) +
  geom_line(aes(yday, set_minutes)) +
  geom_line(aes(yday, rise_minutes)) +
  scale_fill_manual(values = c(&amp;quot;grey40&amp;quot;, NA)) +
  theme_minimal() +
  guides(fill = &amp;quot;none&amp;quot;) +
  theme(
  panel.grid.major.y = element_blank(),
  panel.grid.minor.y = element_blank(),
  panel.grid.minor.x = element_blank(), 
  axis.text.x.bottom = element_text(hjust = 1.7)
  ) +
  scale_x_continuous(breaks = x_axis, position = &amp;quot;right&amp;quot;) +
  scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) +
  labs(x = NULL, y = NULL, title = &amp;quot;Sunshine report of Los Angeles 2017&amp;quot;)
## Warning: Position guide is perpendicular to the intended axis. Did you mean to
## specify a different guide `position`?
## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-01-recreate-sunshine-report/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extra&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extra&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Position guide is perpendicular to the intended axis. Did you mean to
## specify a different guide `position`?
## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?

## Warning: guide_axis(): Discarding guide on merge. Do you have more than one
## guide with the same position?&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-01-recreate-sunshine-report/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)
library(lubridate)
library(glue)
#library(ehlib) # devtools::install_github(&amp;quot;EmilHvitfeldt/ehlib&amp;quot;)

str_between &amp;lt;- function(string, start, end) {
  stringr::str_extract(string,
                       stringr::str_c(start, &amp;#39;(.*?)&amp;#39;, end, collapse = &amp;#39;&amp;#39;)) %&amp;gt;%
    stringr::str_replace(start, &amp;quot;&amp;quot;) %&amp;gt;%
    stringr::str_replace(end, &amp;quot;&amp;quot;)
}

str_before &amp;lt;- function(string, pattern) {
  stringr::str_extract(string, stringr::str_c(&amp;quot;.+?(?=&amp;quot;, pattern, &amp;quot;)&amp;quot;))
}

all_dates_in &amp;lt;- function(year) {
  if(ymd(glue::glue(&amp;quot;{year}0101&amp;quot;)) &amp;gt; as.Date(Sys.time())) {
    stop(&amp;quot;Please select a past or current year.&amp;quot;)
  }
  
  start &amp;lt;- ymd(glue::glue(&amp;quot;{year}0101&amp;quot;))
  
  if(as.Date(Sys.time()) &amp;gt; ymd(glue::glue(&amp;quot;{year}1231&amp;quot;))) {
    end &amp;lt;- ymd(glue::glue(&amp;quot;{year}1231&amp;quot;))
  } else {
    end &amp;lt;- as.Date(Sys.time())
  }
  
  seq(start, end, by = &amp;quot;day&amp;quot;)
}

airport_code &amp;lt;- &amp;quot;KCQT&amp;quot;

full_data &amp;lt;- map(dates, ~{
  weather_data_html(.x, airport_code)
  Sys.sleep(5)
  cat(month(dates), &amp;quot;/&amp;quot;, mday(dates), &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;)
  })

map_lgl(full_data, ~ is.null(.x$error))

ampm_minutes &amp;lt;- function(x) {
  as.numeric(str_between(x, &amp;quot;:&amp;quot;, &amp;quot; &amp;quot;)) +
  as.numeric(str_replace(str_before(x, &amp;quot;:&amp;quot;), &amp;quot;12&amp;quot;, &amp;quot;0&amp;quot;)) * 60 +
  60 * 12 * str_detect(x, &amp;quot;PM&amp;quot;)
}

data_wrangling &amp;lt;- function(html_url, date) {
  
  # Sun rise time
    sun_rise &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;div[id=&amp;quot;astronomy-mod&amp;quot;] table&amp;#39;) %&amp;gt;%
    html_text() %&amp;gt;%
    .[1] %&amp;gt;%
    str_between(&amp;quot;Time\n\t\t&amp;quot;, &amp;quot;\n\t\t&amp;quot;)
  # Sun set time
  sun_set &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;div[id=&amp;quot;astronomy-mod&amp;quot;] table&amp;#39;) %&amp;gt;%
    html_text() %&amp;gt;%
    .[1] %&amp;gt;%
    str_between(&amp;quot;\n\t\t&amp;quot;, &amp;quot;\n\t\tCivil&amp;quot;)

  # Table
  table &amp;lt;- html_url %&amp;gt;%
    html_nodes(&amp;#39;table[id=&amp;quot;obsTable&amp;quot;]&amp;#39;) %&amp;gt;%
    html_table() %&amp;gt;% 
    .[[1]]
  
  # Time column standardization 
  is_daylight &amp;lt;- any(&amp;quot;Time (PDT)&amp;quot; == names(table),
                     &amp;quot;Time (MDT)&amp;quot; == names(table),
                     &amp;quot;Time (CDT)&amp;quot; == names(table),
                     &amp;quot;Time (EDT)&amp;quot; == names(table))
  
  time_names &amp;lt;- str_c(&amp;quot;Time&amp;quot;, c(&amp;quot; (PDT)&amp;quot;, &amp;quot; (MDT)&amp;quot;, &amp;quot; (CDT)&amp;quot;, &amp;quot; (EDT)&amp;quot;,
                                &amp;quot; (PST)&amp;quot;, &amp;quot; (MST)&amp;quot;, &amp;quot; (CST)&amp;quot;, &amp;quot; (EST)&amp;quot;))
  
  names(table) &amp;lt;- if_else(names(table) %in% time_names,
                          &amp;quot;Time&amp;quot;,
                          names(table))
  
  table %&amp;gt;%
    mutate(sun_set = sun_set,
           sun_rise = sun_rise,
           date = date,
           yday = yday(date), 
           day_minutes = ampm_minutes(Time) - is_daylight * 60,
           set_minutes = ampm_minutes(sun_set) - is_daylight * 60,
           rise_minutes = ampm_minutes(sun_rise) - is_daylight * 60,
           sun_up = day_minutes &amp;gt; (rise_minutes + 90) &amp;amp; 
                    day_minutes &amp;lt; (set_minutes - 30))
}

full_data2 &amp;lt;- map2_df(full_data[-303], dates[-303], ~ .x$result %&amp;gt;%
                      data_wrangling(.y))

x_axis &amp;lt;- dates %&amp;gt;% month() %&amp;gt;% table() %&amp;gt;% cumsum()
names(x_axis) &amp;lt;- month.abb[1:12]

y_axis &amp;lt;- 1:24 * 60
names(y_axis) &amp;lt;- str_c(c(12, rep(1:12, 2, length.out = 23)), 
                       rep(c(&amp;quot;AM&amp;quot;, &amp;quot;PM&amp;quot;), each = 12))

full_data2 %&amp;gt;%
  mutate(con = Conditions == &amp;quot;Clear&amp;quot;,
         day_minutes2 = lag(day_minutes)) %&amp;gt;%
  filter(sun_up) %&amp;gt;%
  ggplot(aes(fill = con)) +
  geom_rect(aes(xmin = yday, xmax = yday + 1,
                ymin = day_minutes, ymax = day_minutes2)) +
  geom_line(aes(yday, set_minutes)) +
  geom_line(aes(yday, rise_minutes)) +
  scale_fill_manual(values = c(&amp;quot;grey40&amp;quot;, NA)) +
  theme_minimal() +
  guides(fill = &amp;quot;none&amp;quot;) +
  theme(
  panel.grid.major.y = element_blank(),
  panel.grid.minor.y = element_blank(),
  panel.grid.minor.x = element_blank(), 
  axis.text.x.bottom = element_text(hjust = 1.7)
  ) +
  scale_x_continuous(breaks = x_axis, position = &amp;quot;right&amp;quot;) +
  scale_y_continuous(breaks = y_axis, limits = c(0, 24 * 60)) +
  labs(x = NULL, y = NULL, title = &amp;quot;Sunshine report of Los Angeles 2017&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysing useR!2017 schedule data</title>
      <link>/2017/07/20/analysing-user2017-schedule-data/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/2017/07/20/analysing-user2017-schedule-data/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/vis/vis.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/vis/vis.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/visNetwork-binding/visNetwork.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After attending useR!2017 for the first time, which great pleasure and new connections made. I decided to see if I could extract some of the information available in the public schedule. So as with my last post I‚Äôll do a bit of scraping followed by a few visualizations.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(utils)
library(plotly)
library(ltm)
require(visNetwork)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Web scraping&lt;/h2&gt;
&lt;p&gt;I found this task easiest with the help of &lt;code&gt;purrr:map()&lt;/code&gt;. First we find the full schedules at the following links&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://user2017.sched.com/2017-07-04/overview&#34; class=&#34;uri&#34;&gt;https://user2017.sched.com/2017-07-04/overview&lt;/a&gt; (Tuesday)&lt;br /&gt;
&lt;a href=&#34;https://user2017.sched.com/2017-07-05/overview&#34; class=&#34;uri&#34;&gt;https://user2017.sched.com/2017-07-05/overview&lt;/a&gt; (Wednesday)&lt;br /&gt;
&lt;a href=&#34;https://user2017.sched.com/2017-07-06/overview&#34; class=&#34;uri&#34;&gt;https://user2017.sched.com/2017-07-06/overview&lt;/a&gt; (Thursday)&lt;br /&gt;
&lt;a href=&#34;https://user2017.sched.com/2017-07-07/overview&#34; class=&#34;uri&#34;&gt;https://user2017.sched.com/2017-07-07/overview&lt;/a&gt; (Friday)&lt;/p&gt;
&lt;p&gt;then we read the entire page into a tibble along with a &lt;em&gt;day&lt;/em&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;day &amp;lt;- c(&amp;quot;Tuesday&amp;quot;, &amp;quot;Wednesday&amp;quot;, &amp;quot;Thursday&amp;quot;, &amp;quot;Friday&amp;quot;)
link &amp;lt;- paste0(&amp;quot;https://user2017.sched.com/2017-07-0&amp;quot;, 4:7, &amp;quot;/overview&amp;quot;, sep = &amp;quot;&amp;quot;)

event0 &amp;lt;- map2_df(link, day,
                  ~ tibble(text = readLines(.x),
                           day = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then with the help of &lt;code&gt;stringr&lt;/code&gt; we extract the desired information from the document, following the idiom that ‚Äúmultiple simple regex are better then one complicated one‚Äù. I also filtered out most non-talk events.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;events &amp;lt;- event0 %&amp;gt;% 
  filter(str_detect(text, &amp;quot;&amp;lt;span class=&amp;#39;&amp;quot;) | str_detect(text, &amp;quot;&amp;lt;/h3&amp;gt;&amp;quot;),
         !str_detect(text, &amp;quot;REGISTRATION&amp;quot;),
         !str_detect(text, &amp;quot;COFFEE BREAK&amp;quot;),
         !str_detect(text, &amp;quot;LUNCH&amp;quot;),
         !str_detect(text, &amp;quot;WELCOME&amp;quot;),
         !str_detect(text, &amp;quot;Poster&amp;quot;),
         !str_detect(text, &amp;quot;RIOT SESSION&amp;quot;),
         !str_detect(text, &amp;quot;Buses&amp;quot;),
         !str_detect(text, &amp;quot;Dinner&amp;quot;),
         !str_detect(text, &amp;quot;CLOSING&amp;quot;)) %&amp;gt;%
  mutate(time = str_extract(text, &amp;quot;&amp;lt;h3&amp;gt;.{1,7}&amp;quot;), # time
         time = str_replace(time, &amp;quot;&amp;lt;h3&amp;gt; *&amp;quot;, &amp;quot;&amp;quot;),
         id = str_extract(text, &amp;quot;id=&amp;#39;\\S{32}&amp;quot;), # id
         id = str_replace(id, &amp;quot;id=&amp;#39;&amp;quot;, &amp;quot;&amp;quot;),
         name = str_extract(text, str_c(id, &amp;quot;.*&amp;quot;)), # name
         name = str_replace(name, str_c(id, &amp;quot;&amp;#39;&amp;gt;&amp;quot;), &amp;quot;&amp;quot;),
         name = str_extract(name, &amp;quot;^.*(?=( &amp;lt;span))&amp;quot;),
         room = str_extract(text, &amp;#39;vs&amp;quot;&amp;gt;(.*?)&amp;lt;&amp;#39;),
         room = str_replace(room, &amp;#39;vs&amp;quot;&amp;gt;&amp;#39;, &amp;quot;&amp;quot;),
         room = str_replace(room, &amp;#39;&amp;lt;&amp;#39;,&amp;quot;&amp;quot;)) %&amp;gt;% # room
  fill(time) %&amp;gt;%
  filter(!str_detect(text, &amp;quot;&amp;lt;h3&amp;gt;&amp;quot;)) %&amp;gt;%
  dplyr::select(-text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lets take a look at what we have by now just to see that we have what we want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(events)
## # A tibble: 6 x 5
##   day     time   id                    name                               room  
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;                              &amp;lt;chr&amp;gt; 
## 1 Tuesday 9:30am 893eab219225a0990770‚Ä¶ Data Carpentry: Open and Reproduc‚Ä¶ 2.02  
## 2 Tuesday 9:30am 30c0eebdc887f3ad3aef‚Ä¶ Dose-response analysis using R     4.02  
## 3 Tuesday 9:30am 57ce234e5ce9082da3cc‚Ä¶ Geospatial visualization using R   4.03  
## 4 Tuesday 9:30am 95b110146486b0a5f802‚Ä¶ Introduction to Bayesian inferenc‚Ä¶ 2.01  
## 5 Tuesday 9:30am 7294f7df20ab1a7c37df‚Ä¶ Introduction to parallel computin‚Ä¶ 3.01  
## 6 Tuesday 9:30am f15703fe51e89294f2b5‚Ä¶ Rcpp: From Simple Examples to Mac‚Ä¶ PLENA‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the information about the different events we can scrape every event page to find its attendees. This following chuck of code might seem a little hard at first, it helps to notice that there is a second tibble inside the big tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- map_df(events$id,
       ~ tibble(attendee = tibble(text = readLines(
         str_c(&amp;quot;https://user2017.sched.com/event-goers/&amp;quot;, .x))) %&amp;gt;%
                filter(str_detect(text, &amp;quot; +&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;)) %&amp;gt;% 
                .$text %&amp;gt;%
                str_split(., &amp;quot;li&amp;gt;&amp;lt;li&amp;quot;) %&amp;gt;% 
                unlist(),
       id = .x) %&amp;gt;%
  mutate(attendee = str_replace(attendee, &amp;quot;(.*?)title=\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;),
         attendee = str_replace(attendee, &amp;quot;\&amp;quot;&amp;gt;&amp;lt;(.*)&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
  filter(!str_detect(attendee, &amp;quot;venue&amp;quot;),
         !str_detect(attendee, &amp;quot;Private&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lets again take a look at what we have by now just to see that we have what we want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(people)
## # A tibble: 6 x 2
##   attendee                                                id                    
##   &amp;lt;chr&amp;gt;                                                   &amp;lt;chr&amp;gt;                 
## 1 &amp;quot;              &amp;lt;li&amp;gt;&amp;lt;a href=\&amp;quot;/\&amp;quot;&amp;gt;Schedule&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;quot;     893eab219225a09907704‚Ä¶
## 2 &amp;quot;                                                     ‚Ä¶ 893eab219225a09907704‚Ä¶
## 3 &amp;quot;Marc Trunjer Kusk Nielsen&amp;quot;                             893eab219225a09907704‚Ä¶
## 4 &amp;quot;lvaudor&amp;quot;                                               893eab219225a09907704‚Ä¶
## 5 &amp;quot;Alan Ponce&amp;quot;                                            893eab219225a09907704‚Ä¶
## 6 &amp;quot;bpiccolo&amp;quot;                                              893eab219225a09907704‚Ä¶&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;visualizations&lt;/h2&gt;
&lt;p&gt;With a data set with this many possibilities the options are quite few, so here I‚Äôll just list a few of the ones I found handy. So first we just do a simple bubble plot, this will be done with &lt;code&gt;left_join&lt;/code&gt;‚Äôs and &lt;code&gt;count&lt;/code&gt; and piped straight into &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(events, people, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  count(id) %&amp;gt;%
  left_join(events, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  filter(day == &amp;quot;Friday&amp;quot;) %&amp;gt;%
  ggplot(aes(time, room, size = n)) +
  geom_point() +
  theme_bw() +
  scale_size(range = c(5, 20)) +
  labs(title = &amp;quot;useR!2017 Friday schedule&amp;quot;,
       x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-07-20-analysing-user-2017-schedule-data/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since both our &lt;code&gt;room&lt;/code&gt; and &lt;code&gt;time&lt;/code&gt; were simply character vectors, the ordering is not right. This can be fixed by setting the levels correctly. Here I have the ordered vectored for both &lt;code&gt;room&lt;/code&gt; and &lt;code&gt;time&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_levels &amp;lt;- c(&amp;quot;9:15am&amp;quot;, &amp;quot;9:30am&amp;quot;, &amp;quot;11:00am&amp;quot;, &amp;quot;11:18am&amp;quot;, &amp;quot;11:30am&amp;quot;, &amp;quot;11:36am&amp;quot;,
                 &amp;quot;11:54am&amp;quot;, &amp;quot;12:12pm&amp;quot;, &amp;quot;1:15pm&amp;quot;, &amp;quot;1:30pm&amp;quot;, &amp;quot;1:48pm&amp;quot;, &amp;quot;2:00pm&amp;quot;, 
                 &amp;quot;2:06pm&amp;quot;, &amp;quot;2:24pm&amp;quot;, &amp;quot;2:42pm&amp;quot;, &amp;quot;3:30pm&amp;quot;, &amp;quot;3:45pm&amp;quot;, &amp;quot;4:00pm&amp;quot;, 
                 &amp;quot;4:45pm&amp;quot;, &amp;quot;4:55pm&amp;quot;, &amp;quot;5:00pm&amp;quot;, &amp;quot;5:05pm&amp;quot;, &amp;quot;5:30pm&amp;quot;, &amp;quot;5:35pm&amp;quot;, 
                 &amp;quot;5:40pm&amp;quot;, &amp;quot;5:45pm&amp;quot;, &amp;quot;5:50pm&amp;quot;, &amp;quot;5:55pm&amp;quot;, &amp;quot;6:00pm&amp;quot;, &amp;quot;6:05pm&amp;quot;, 
                 &amp;quot;6:10pm&amp;quot;, &amp;quot;6:15pm&amp;quot;, &amp;quot;6:20pm&amp;quot;, &amp;quot;7:00pm&amp;quot;)
room_levels &amp;lt;- c(&amp;quot;PLENARY&amp;quot;, &amp;quot;2.01&amp;quot;, &amp;quot;2.02&amp;quot;, &amp;quot;3.01&amp;quot;, &amp;quot;3.02&amp;quot;, &amp;quot;4.01&amp;quot;, &amp;quot;4.02&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we deal with it with a single mutate like so&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(events, people, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  count(id) %&amp;gt;%
  left_join(events, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  mutate(time = factor(time, time_levels),
         room = factor(room, room_levels)) %&amp;gt;%
  filter(day == &amp;quot;Friday&amp;quot;) %&amp;gt;%
  ggplot(aes(time, room, size = n)) +
  geom_point() +
  theme_bw() +
  scale_size(range = c(5, 20)) +
  labs(title = &amp;quot;useR!2017 Friday schedule&amp;quot;,
       x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-07-20-analysing-user-2017-schedule-data/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;another way to visualize it would be to use a stacked bar chart like so&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- left_join(events, people, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  count(id) %&amp;gt;%
  left_join(events, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  filter(day == &amp;quot;Thursday&amp;quot;) %&amp;gt;%
  mutate(time = factor(time, time_levels),
         room = factor(room, rev(room_levels))) %&amp;gt;%
  ggplot(aes(time, fill = room, text = name)) +
  geom_bar(aes(weight = n)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = &amp;quot;useR!2017 Thursday schedule&amp;quot;,
       x = &amp;quot;&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-07-20-analysing-user-2017-schedule-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;or with a bit of interactivity &lt;code&gt;plotly::ggplotly&lt;/code&gt; can be used so that is possible to hover over each event to see name and size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplotly(p, tooltip = c(&amp;quot;n&amp;quot;, &amp;quot;name&amp;quot;), width = 900, height = 555)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:900px;height:555px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[741,472,513,611,695,662,675,570,488,449,341,453,300,480,391,335,267,345,341,224],&#34;x&#34;:[3,4,5,6,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],&#34;y&#34;:[62,259,343,200,91,296,76,120,153,90,47,222,100,58,95,133,169,119,129,80],&#34;text&#34;:[&#34;n:  62&lt;br /&gt;moodler: A new R package to easily fetch data from Moodle&#34;,&#34;n: 259&lt;br /&gt;Can you keep a secret?&#34;,&#34;n: 343&lt;br /&gt;Scraping data with rvest and purrr&#34;,&#34;n: 200&lt;br /&gt;jug: Building Web APIs for R&#34;,&#34;n:  91&lt;br /&gt;Interactive graphs for blind and print disabled people&#34;,&#34;n: 296&lt;br /&gt;Package ggiraph: a ggplot2 Extension for Interactive Graphics&#34;,&#34;n:  76&lt;br /&gt;Visual funnel plot inference for meta-analysis&#34;,&#34;n: 120&lt;br /&gt;mapedit - interactive manipulation of spatial objects&#34;,&#34;n: 153&lt;br /&gt;Exploring and presenting maps with **tmap**&#34;,&#34;n:  90&lt;br /&gt;R in a small-sized bank&#39;s risk management&#34;,&#34;n:  47&lt;br /&gt;**eventstudies**: An *R* package for conducting event studies and a platform for methodological research on event studies&#34;,&#34;n: 222&lt;br /&gt;Automatic Machine Learning in R&#34;,&#34;n: 100&lt;br /&gt;R in a Pharmaceutical Company&#34;,&#34;n:  58&lt;br /&gt;Using R to Analyze Healthcare Cost of Older Patients Using Personal Emergency Response Service&#34;,&#34;n:  95&lt;br /&gt;Statistics hitting the business front line&#34;,&#34;n: 133&lt;br /&gt;An example of Shiny tool at Nestl√© R&amp;D, an enabler to guide product developers in designing gluten free biscuits&#34;,&#34;n: 169&lt;br /&gt;Using R for optimal beer recipe selection&#34;,&#34;n: 119&lt;br /&gt;Candy Crush R Saga&#34;,&#34;n: 129&lt;br /&gt;Gamifyr: Transforming Machine Learning Tasks into Games with Shiny&#34;,&#34;n:  80&lt;br /&gt;Ultra-Fast Data Mining With The R-KDB+ Interface&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(248,118,109,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;4.02&#34;,&#34;legendgroup&#34;:&#34;4.02&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[629,417,460,568,400,293,418,272,413,355,308,190,320,300],&#34;x&#34;:[3,4,5,6,19,20,21,22,23,24,25,26,27,28],&#34;y&#34;:[112,55,53,43,49,48,35,28,67,36,27,77,25,41],&#34;text&#34;:[&#34;n: 112&lt;br /&gt;Bayesian social network analysis with Bergm&#34;,&#34;n:  55&lt;br /&gt;difNLR: Detection of potentional gender/minority bias with extensions of logistic regression&#34;,&#34;n:  53&lt;br /&gt;**BradleyTerryScalable**: Ranking items scalably with the Bradley-Terry model&#34;,&#34;n:  43&lt;br /&gt;IRT test equating with the R package equateIRT&#34;,&#34;n:  49&lt;br /&gt;DNA methylation-based classification of human central nervous system tumors&#34;,&#34;n:  48&lt;br /&gt;Multivariate statistics for PAT data analysis: short overview of existing R packages and methods&#34;,&#34;n:  35&lt;br /&gt;R in research on microbial mutation rates&#34;,&#34;n:  28&lt;br /&gt;Plasmid Profiler: Comparative Analysis of Plasmid Content in WGS Data&#34;,&#34;n:  67&lt;br /&gt;Application of R and Shiny in multiomics understanding of blood cancer biology and drug response&#34;,&#34;n:  36&lt;br /&gt;Simulate phenotype(s) with epistatic interactions&#34;,&#34;n:  27&lt;br /&gt;Introducing the DynNom package for the generation of dynamic nomograms&#34;,&#34;n:  77&lt;br /&gt;Graduate from plot to ggplot2: Using R to visualize the story of Ebola survivors in the PREVAIL III Ebola Natural History Study&#34;,&#34;n:  25&lt;br /&gt;**BivRegBLS**, a new *R* package: Tolerance Intervals and Errors-in-Variables Regressions in Method Comparison Studies&#34;,&#34;n:  41&lt;br /&gt;What is missing from the meta-analysis landscape?&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(196,154,0,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;4.01&#34;,&#34;legendgroup&#34;:&#34;4.01&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[511,412,510,342,402,250,177,270,219,322,271,260,146,228,219],&#34;x&#34;:[8,9,10,11,12,19,20,21,22,23,24,25,26,27,28],&#34;y&#34;:[184,250,165,228,86,150,116,148,53,91,84,48,44,92,81],&#34;text&#34;:[&#34;n: 184&lt;br /&gt;ReinforcementLearning: A package for replicating human behavior in R&#34;,&#34;n: 250&lt;br /&gt;Deep Learning for Natural Language Processing in R&#34;,&#34;n: 165&lt;br /&gt;R4ML: A Scalable R for Machine Learning&#34;,&#34;n: 228&lt;br /&gt;Computer Vision and Image Recognition algorithms for R users&#34;,&#34;n:  86&lt;br /&gt;Depth and depth-based classification with R package **ddalpha**&#34;,&#34;n: 150&lt;br /&gt;R Blogging with blogdown and GitHub&#34;,&#34;n: 116&lt;br /&gt;**redmineR** and the story of automating *useR!2017* abstract review process&#34;,&#34;n: 148&lt;br /&gt;The current state of naming conventions in R&#34;,&#34;n:  53&lt;br /&gt;An Introduction to the r2anki-package&#34;,&#34;n:  91&lt;br /&gt;rOpenGov: community project for open government data&#34;,&#34;n:  84&lt;br /&gt;R.gov: making R work for government&#34;,&#34;n:  48&lt;br /&gt;nsoAPI - retrieving data from National Statistical Offices with R&#34;,&#34;n:  44&lt;br /&gt;Jurimetrics: quantitative analysis of judicial decisions using R&#34;,&#34;n:  92&lt;br /&gt;Shiny Apps for Maths and Stats Exercises&#34;,&#34;n:  81&lt;br /&gt;Working with R when internet is not reliable&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(83,180,0,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;3.02&#34;,&#34;legendgroup&#34;:&#34;3.02&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[592,366,407,479,542,439,280,362,222,241,185,107,157,143,196,195,134,97,151,154,88],&#34;x&#34;:[3,4,5,6,7,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],&#34;y&#34;:[37,51,53,89,107,72,132,148,120,161,65,70,113,76,126,76,126,49,77,65,136],&#34;text&#34;:[&#34;n:  37&lt;br /&gt;**rags2ridges**: A One-Stop-Go for Network Modeling of Precision Matrices&#34;,&#34;n:  51&lt;br /&gt;Various Versatile Variances: An Object-Oriented Implementation of Clustered Covariances in *R*&#34;,&#34;n:  53&lt;br /&gt;factorMerger: a set of tools to support results from post hoc testing&#34;,&#34;n:  89&lt;br /&gt;Estimating the Parameters of a Continuous-Time Markov Chain from Discrete-Time Data with ctmcd&#34;,&#34;n: 107&lt;br /&gt;MCMC Output Analysis Using R package mcmcse&#34;,&#34;n:  72&lt;br /&gt;An Efficient Algorithm for Solving Large Fixed Effects OLS Problems with Clustered Standard Error Estimation&#34;,&#34;n: 132&lt;br /&gt;R Package glmm: Likelihood-Based Inference for Generalized Linear Mixed Models&#34;,&#34;n: 148&lt;br /&gt;**countreg**: Tools for count data regression&#34;,&#34;n: 120&lt;br /&gt;How to Use (R)Stan to Estimate Models in External R Packages&#34;,&#34;n: 161&lt;br /&gt;brms: Bayesian Multilevel Models using Stan&#34;,&#34;n:  65&lt;br /&gt;The cutpointr package: Improved and tidy estimation of optimal cutpoints&#34;,&#34;n:  70&lt;br /&gt;Preparing Datetime Data with Padr&#34;,&#34;n: 113&lt;br /&gt;R in Minecraft&#34;,&#34;n:  76&lt;br /&gt;Digital Signal Processing with R&#34;,&#34;n: 126&lt;br /&gt;Data Analysis Using Hierarchical Generalized Linear Models with R&#34;,&#34;n:  76&lt;br /&gt;The R package bigstatsr: Memory- and Computation-Efficient Statistical Tools for Big Matrices&#34;,&#34;n: 126&lt;br /&gt;Advanced R Solutions -- A Bookdown Project&#34;,&#34;n:  49&lt;br /&gt;Functional Input Validation with valaddin&#34;,&#34;n:  77&lt;br /&gt;ROI - R Optimization Infrastructure&#34;,&#34;n:  65&lt;br /&gt;simmer: Discrete-Event Simulation for R&#34;,&#34;n: 136&lt;br /&gt;Data Error! But where?&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(0,192,148,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;3.01&#34;,&#34;legendgroup&#34;:&#34;3.01&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[521,157,233,257,272,354,229,336,193,57,28,22,24,39,46,50,30,24,35],&#34;x&#34;:[3,4,5,6,7,8,9,10,11,19,20,21,22,23,24,25,26,27,28],&#34;y&#34;:[71,209,174,222,270,85,51,26,29,128,79,135,119,157,149,84,67,127,119],&#34;text&#34;:[&#34;n:  71&lt;br /&gt;Hosting Data Packages via `drat`: A Case Study with Hurricane Exposure Data&#34;,&#34;n: 209&lt;br /&gt;Clouds, Containers and R, towards a global hub for reproducible and collaborative data science&#34;,&#34;n: 174&lt;br /&gt;codebookr: Codebooks in *R*&#34;,&#34;n: 222&lt;br /&gt;Show me the errors you didn&#39;t look for&#34;,&#34;n: 270&lt;br /&gt;Automatically archiving reproducible studies with Docker&#34;,&#34;n:  85&lt;br /&gt;The **renjin** package: Painless Just-in-time Compilation for High Performance R&#34;,&#34;n:  51&lt;br /&gt;An LLVM-based Compiler Toolkit for R&#34;,&#34;n:  26&lt;br /&gt;Performance Benchmarking of the R Programming Environment on Knight&#39;s Landing&#34;,&#34;n:  29&lt;br /&gt;*GNU R* on a Programmable Logic Controller (PLC) in an Embedded-Linux Environment&#34;,&#34;n: 128&lt;br /&gt;R and Tableau Integration: A Case approach&#34;,&#34;n:  79&lt;br /&gt;The dataCompareR package&#34;,&#34;n: 135&lt;br /&gt;Use of templates within an R package to create a (semi-)automated analysis workflow and/or report&#34;,&#34;n: 119&lt;br /&gt;graphiT: an interactive, user-friendly tool to produce graphics based on the grammar of graphics&#39; principles&#34;,&#34;n: 157&lt;br /&gt;**heatmaply**: an *R* package for creating interactive cluster heatmaps&#34;,&#34;n: 149&lt;br /&gt;Plot Colour Helper ‚Äì Finally an easy way to pick colours for your R plots!&#34;,&#34;n:  84&lt;br /&gt;bsplus: Using Twitter Bootstrap to extend your Shiny app&#34;,&#34;n:  67&lt;br /&gt;TAGS - Table Assorting Guided System: an HTML widget to create multiple tables from Excel spreadsheets&#34;,&#34;n: 127&lt;br /&gt;Object-oriented markdown in R to facilitate collaboration&#34;,&#34;n: 119&lt;br /&gt;Strategies for Reproducible Research with Packrat&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(0,182,235,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;2.02&#34;,&#34;legendgroup&#34;:&#34;2.02&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[485,74,176,167,196,114,117,133,66,149,0,0,0,0,0,0,0,0,0,0,0],&#34;x&#34;:[3,4,5,6,7,8,9,10,11,12,19,20,21,22,23,24,25,26,27,28,29],&#34;y&#34;:[36,83,57,90,76,240,112,203,127,92,57,28,22,24,39,46,50,30,24,35,88],&#34;text&#34;:[&#34;n:  36&lt;br /&gt;Using the alphabetr package to determine paired T cell receptor sequences&#34;,&#34;n:  83&lt;br /&gt;Differentiation of brain tumor tissue using hierarchical non-negative matrix factorization&#34;,&#34;n:  57&lt;br /&gt;Biosignature-Based Drug Design: from high dimensional data to business impact&#34;,&#34;n:  90&lt;br /&gt;Interactive and Reproducible Research for RNA Sequencing Analysis&#34;,&#34;n:  76&lt;br /&gt;Stochastic Gradient Descent Log-Likelihood Estimation in the Cox Proportional Hazards Model with Applications to The Cancer Genome Atlas Data&#34;,&#34;n: 240&lt;br /&gt;R-based computing with big data on disk&#34;,&#34;n: 112&lt;br /&gt;Daff: diff, patch and merge for data.frames&#34;,&#34;n: 203&lt;br /&gt;odbc - A modern database interface&#34;,&#34;n: 127&lt;br /&gt;Improving DBI&#34;,&#34;n:  92&lt;br /&gt;*implyr**: A **dplyr** Backend for a Apache Impala&#34;,&#34;n:  57&lt;br /&gt;rdwd ‚Äì manage German weather observations&#34;,&#34;n:  28&lt;br /&gt;eseis ‚Äì A toolbox to weld seismologic and geomorphic data analysis&#34;,&#34;n:  22&lt;br /&gt;An R Decision Support Framework for the Identification of BMP in Catchments&#34;,&#34;n:  24&lt;br /&gt;Reproducible research in computational subsurface hydrology - First steps in R with RMODFLOW and RMT3DMS&#34;,&#34;n:  39&lt;br /&gt;Using an R package as platform for harmonized cleaning of data from RTI MicroPEM air quality sensors&#34;,&#34;n:  46&lt;br /&gt;map data from **naturalearth** : aiming for sustainability through specialisation and **rOpenSci**&#34;,&#34;n:  50&lt;br /&gt;OpenSpat, spread the spatial world&#34;,&#34;n:  30&lt;br /&gt;smires -- Calculating Hydrological Metrics for Univariate Time Series&#34;,&#34;n:  24&lt;br /&gt;minimalRSD and FMC: R packages to construct cost efficient experimental designs&#34;,&#34;n:  35&lt;br /&gt;ICtest: Estimating the Number of Interesting Components&#34;,&#34;n:  88&lt;br /&gt;Better Confidence Intervals for Quantiles&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(165,138,255,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;2.01&#34;,&#34;legendgroup&#34;:&#34;2.01&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;orientation&#34;:&#34;v&#34;,&#34;width&#34;:[0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999],&#34;base&#34;:[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],&#34;x&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],&#34;y&#34;:[274,448,485,74,176,167,196,114,117,133,66,149,328,525,37,36,31,33],&#34;text&#34;:[&#34;n: 274&lt;br /&gt;Sponsor Talk Open Analytics&#34;,&#34;n: 448&lt;br /&gt;KEYNOTE: Dose-response analysis: considering dose both as qualitative factor and quantitative covariate- using R*&#34;,&#34;n: 485&lt;br /&gt;Show Me Your Model: tools for visualisation of statistical models&#34;,&#34;n:  74&lt;br /&gt;Quantitative fisheries advice using R and FLR&#34;,&#34;n: 176&lt;br /&gt;*jamovi*: a spreadsheet for R&#34;,&#34;n: 167&lt;br /&gt;The growing popularity of R in data journalism&#34;,&#34;n: 196&lt;br /&gt;FFTrees: An R package to create, visualise and use fast and frugal decision trees&#34;,&#34;n: 114&lt;br /&gt;Community-based learning and knowledge sharing&#34;,&#34;n: 117&lt;br /&gt;Data Carpentry: Teaching Reproducible Data Driven Discovery&#34;,&#34;n: 133&lt;br /&gt;Statistics in Action with R: an educative platform&#34;,&#34;n:  66&lt;br /&gt;A quasi-experiment for the influence of the user interface on the acceptance of R&#34;,&#34;n: 149&lt;br /&gt;The analysis of R learning styles with R&#34;,&#34;n: 328&lt;br /&gt;Sponsor Talk Rstudio&#34;,&#34;n: 525&lt;br /&gt;KEYNOTE: Parallel Computation in R:¬† What We Want, and How We (Might) Get It&#34;,&#34;n:  37&lt;br /&gt;SPONSOR TALK EODA&#34;,&#34;n:  36&lt;br /&gt;SPONSOR TALK ORACLE&#34;,&#34;n:  31&lt;br /&gt;SPONSOR TALK MANGO SOLUTIONS&#34;,&#34;n:  33&lt;br /&gt;SPONSOR TALK ALTERYX&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(251,97,215,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;name&#34;:&#34;PLENARY&#34;,&#34;legendgroup&#34;:&#34;PLENARY&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:45.3817104776009,&#34;r&#34;:7.30593607305936,&#34;b&#34;:37.8531000277654,&#34;l&#34;:48.9497716894977},&#34;plot_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;paper_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;title&#34;:{&#34;text&#34;:&#34;useR!2017 Thursday schedule&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:17.5342465753425},&#34;x&#34;:0,&#34;xref&#34;:&#34;paper&#34;},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[0.4,29.6],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;9:15am&#34;,&#34;9:30am&#34;,&#34;11:00am&#34;,&#34;11:18am&#34;,&#34;11:36am&#34;,&#34;11:54am&#34;,&#34;12:12pm&#34;,&#34;1:30pm&#34;,&#34;1:48pm&#34;,&#34;2:06pm&#34;,&#34;2:24pm&#34;,&#34;2:42pm&#34;,&#34;3:30pm&#34;,&#34;3:45pm&#34;,&#34;4:45pm&#34;,&#34;4:55pm&#34;,&#34;5:00pm&#34;,&#34;5:05pm&#34;,&#34;5:30pm&#34;,&#34;5:35pm&#34;,&#34;5:40pm&#34;,&#34;5:45pm&#34;,&#34;5:50pm&#34;,&#34;5:55pm&#34;,&#34;6:00pm&#34;,&#34;6:05pm&#34;,&#34;6:10pm&#34;,&#34;6:15pm&#34;,&#34;6:20pm&#34;],&#34;tickvals&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;9:15am&#34;,&#34;9:30am&#34;,&#34;11:00am&#34;,&#34;11:18am&#34;,&#34;11:36am&#34;,&#34;11:54am&#34;,&#34;12:12pm&#34;,&#34;1:30pm&#34;,&#34;1:48pm&#34;,&#34;2:06pm&#34;,&#34;2:24pm&#34;,&#34;2:42pm&#34;,&#34;3:30pm&#34;,&#34;3:45pm&#34;,&#34;4:45pm&#34;,&#34;4:55pm&#34;,&#34;5:00pm&#34;,&#34;5:05pm&#34;,&#34;5:30pm&#34;,&#34;5:35pm&#34;,&#34;5:40pm&#34;,&#34;5:45pm&#34;,&#34;5:50pm&#34;,&#34;5:55pm&#34;,&#34;6:00pm&#34;,&#34;6:05pm&#34;,&#34;6:10pm&#34;,&#34;6:15pm&#34;,&#34;6:20pm&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-45,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-47.9,1005.9],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;0&#34;,&#34;250&#34;,&#34;500&#34;,&#34;750&#34;,&#34;1000&#34;],&#34;tickvals&#34;:[0,250,500,750,1000],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;0&#34;,&#34;250&#34;,&#34;500&#34;,&#34;750&#34;,&#34;1000&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;count&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:&#34;transparent&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;width&#34;:0.66417600664176,&#34;linetype&#34;:&#34;solid&#34;},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:true,&#34;legend&#34;:{&#34;bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;bordercolor&#34;:&#34;transparent&#34;,&#34;borderwidth&#34;:1.88976377952756,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;y&#34;:0.943817833581613},&#34;annotations&#34;:[{&#34;text&#34;:&#34;room&#34;,&#34;x&#34;:1.02,&#34;y&#34;:1,&#34;showarrow&#34;:false,&#34;ax&#34;:0,&#34;ay&#34;:0,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;xref&#34;:&#34;paper&#34;,&#34;yref&#34;:&#34;paper&#34;,&#34;textangle&#34;:-0,&#34;xanchor&#34;:&#34;left&#34;,&#34;yanchor&#34;:&#34;bottom&#34;,&#34;legendTitle&#34;:true}],&#34;hovermode&#34;:&#34;closest&#34;,&#34;width&#34;:900,&#34;height&#34;:555,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;showSendToCloud&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;9efd52dcd1ab&#34;:{&#34;weight&#34;:{},&#34;x&#34;:{},&#34;fill&#34;:{},&#34;text&#34;:{},&#34;type&#34;:&#34;bar&#34;}},&#34;cur_data&#34;:&#34;9efd52dcd1ab&#34;,&#34;visdat&#34;:{&#34;9efd52dcd1ab&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;div id=&#34;network-graph&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Network graph&lt;/h3&gt;
&lt;p&gt;To make our-self a simple network graph will I be using the &lt;a href=&#34;https://github.com/datastorm-open/visNetwork&#34;&gt;visNetwork&lt;/a&gt; package, which have a lovely vignette. So here first of all to create a manageable graph I‚Äôll subset all the Wednesday talks in room 4.02, which was the ‚ÄúShiny I‚Äù and ‚ÄúText Mining‚Äù block.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sub_data &amp;lt;- left_join(events, people, by = &amp;quot;id&amp;quot;) %&amp;gt;%
  filter(day == &amp;quot;Wednesday&amp;quot;, room == &amp;quot;4.02&amp;quot;) %&amp;gt;%
  dplyr::select(name, attendee, time)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I this graph I will let each node be a event and let the edges be to which degree they share attendees. So we start&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;node_size &amp;lt;- sub_data %&amp;gt;% 
  group_by(name, time) %&amp;gt;%
  summarize(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to find how many attendees the events share with each other we first find all the different pairs of events with &lt;code&gt;utils::combn&lt;/code&gt; function and with &lt;code&gt;purrr&lt;/code&gt; and &lt;code&gt;inner_join&lt;/code&gt; finds how many they have in common. Since &lt;code&gt;utils::combn&lt;/code&gt; gives its result in a matrix we have to fiddle just a bit to our way back to a tibble.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conn &amp;lt;- combn(node_size$name, 2) %&amp;gt;%
  as.tibble() %&amp;gt;%
  map_int(~ inner_join(sub_data %&amp;gt;% filter(name == .x[1]), 
                       sub_data %&amp;gt;% filter(name == .x[2]), by = &amp;quot;attendee&amp;quot;)
              %&amp;gt;% nrow()) %&amp;gt;%
  rbind(combn(node_size$name, 2)) %&amp;gt;% t() %&amp;gt;% as.tibble()
## Warning: `as.tibble()` is deprecated as of tibble 2.0.0.
## Please use `as_tibble()` instead.
## The signature and semantics have changed, see `?as_tibble`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.
## Warning: The `x` argument of `as_tibble.matrix()` must have column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.
names(conn) &amp;lt;- c(&amp;quot;n&amp;quot;, &amp;quot;from&amp;quot;, &amp;quot;to&amp;quot;)
conn
## # A tibble: 45 x 3
##    n     from                               to                                  
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                              &amp;lt;chr&amp;gt;                               
##  1 21    A Tidy Data Model for Natural Lan‚Ä¶ bradio: Add data music widgets to y‚Ä¶
##  2 57    A Tidy Data Model for Natural Lan‚Ä¶ Developing and deploying large scal‚Ä¶
##  3 82    A Tidy Data Model for Natural Lan‚Ä¶ How we built a Shiny App for 700 us‚Ä¶
##  4 84    A Tidy Data Model for Natural Lan‚Ä¶ Interacting with databases from Shi‚Ä¶
##  5 84    A Tidy Data Model for Natural Lan‚Ä¶ manifestoR - a tool for data journa‚Ä¶
##  6 99    A Tidy Data Model for Natural Lan‚Ä¶ Neural Embeddings and NLP with R an‚Ä¶
##  7 83    A Tidy Data Model for Natural Lan‚Ä¶ ShinyProxy                          
##  8 155   A Tidy Data Model for Natural Lan‚Ä¶ Text Analysis and Text Mining Using‚Ä¶
##  9 168   A Tidy Data Model for Natural Lan‚Ä¶ Text mining, the tidy way           
## 10 46    bradio: Add data music widgets to‚Ä¶ Developing and deploying large scal‚Ä¶
## # ‚Ä¶ with 35 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;for the node tibble we need to supply it with a &lt;em&gt;id&lt;/em&gt; column, but I will also supply it with a label (name of the event), size (number of people in the event) and color (what book is this event in. green = Shiny I, blue = Text Mining).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Shiny_I &amp;lt;- c(&amp;quot;11:00am&amp;quot;, &amp;quot;11:18am&amp;quot;, &amp;quot;11:36am&amp;quot;, &amp;quot;11:54am&amp;quot;, &amp;quot;12:12pm&amp;quot;)
Text_Mining &amp;lt;- c(&amp;quot;1:30pm&amp;quot;, &amp;quot;1:48pm&amp;quot;, &amp;quot;2:06pm&amp;quot;, &amp;quot;2:24pm&amp;quot;, &amp;quot;2:42pm&amp;quot;)
nodes &amp;lt;- node_size %&amp;gt;% 
  mutate(id = name,
         label = str_wrap(name, width = 20),
         size = n / 20,
         color = case_when(
           time %in% Shiny_I ~ &amp;quot;lightgreen&amp;quot;,
           time %in% Text_Mining ~ &amp;quot;lightblue&amp;quot;
         ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;for the edge tibble it needs &lt;em&gt;from&lt;/em&gt; and &lt;em&gt;to&lt;/em&gt; columns that matches with the &lt;em&gt;id&lt;/em&gt; in the node tibble. I will also supply with a constant color column (because if omitted it would borrow from the node coloring) and a width column to indicate how many attendees they share. This is again done with a couple of left_joins and the connectivity is the average percentage of attendees they share. Lastly we remove any edge with less then 0.5 connectivity to clear out the graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges &amp;lt;- conn %&amp;gt;% 
  left_join(node_size %&amp;gt;% 
              dplyr::select(-time) %&amp;gt;% 
              rename(n_from = n), 
                   by = c(&amp;quot;from&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
  left_join(node_size %&amp;gt;% 
              dplyr::select(-time) %&amp;gt;% 
              rename(n_to = n), 
                   by = c(&amp;quot;to&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
  mutate(n = as.numeric(n),
         n_to = as.numeric(n_to),
         n_from = as.numeric(n_from),
         connectivity = (n / n_from + n / n_to) / 2,
         width = connectivity * 10,
         color = &amp;quot;grey&amp;quot;) %&amp;gt;%
  filter(connectivity &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which yields us with the wonderful graph which show a somehow clear divide between the two blocks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;visNetwork(nodes, edges, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:415.296px;&#34; class=&#34;visNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;nodes&#34;:{&#34;name&#34;:[&#34;A Tidy Data Model for Natural Language Processing&#34;,&#34;bradio: Add data music widgets to your business intelligence dashboards&#34;,&#34;Developing and deploying large scale Shiny applications for non-life insurance&#34;,&#34;How we built a Shiny App for 700 users?&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software&#34;,&#34;Neural Embeddings and NLP with R and Spark&#34;,&#34;ShinyProxy&#34;,&#34;Text Analysis and Text Mining Using R&#34;,&#34;Text mining, the tidy way&#34;],&#34;time&#34;:[&#34;2:06pm&#34;,&#34;11:18am&#34;,&#34;12:12pm&#34;,&#34;11:00am&#34;,&#34;11:36am&#34;,&#34;1:48pm&#34;,&#34;2:42pm&#34;,&#34;11:54am&#34;,&#34;2:24pm&#34;,&#34;1:30pm&#34;],&#34;n&#34;:[209,73,191,307,294,138,199,289,251,329],&#34;id&#34;:[&#34;A Tidy Data Model for Natural Language Processing&#34;,&#34;bradio: Add data music widgets to your business intelligence dashboards&#34;,&#34;Developing and deploying large scale Shiny applications for non-life insurance&#34;,&#34;How we built a Shiny App for 700 users?&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software&#34;,&#34;Neural Embeddings and NLP with R and Spark&#34;,&#34;ShinyProxy&#34;,&#34;Text Analysis and Text Mining Using R&#34;,&#34;Text mining, the tidy way&#34;],&#34;label&#34;:[&#34;A Tidy Data Model\nfor Natural Language\nProcessing&#34;,&#34;bradio: Add data\nmusic widgets\nto your business\nintelligence\ndashboards&#34;,&#34;Developing and\ndeploying large\nscale Shiny\napplications for\nnon-life insurance&#34;,&#34;How we built a Shiny\nApp for 700 users?&#34;,&#34;Interacting with\ndatabases from Shiny&#34;,&#34;manifestoR - a\ntool for data\njournalists, a\nsource for text\nminers and a\nprototype for\nreproducibility\nsoftware&#34;,&#34;Neural Embeddings\nand NLP with R and\nSpark&#34;,&#34;ShinyProxy&#34;,&#34;Text Analysis and\nText Mining Using R&#34;,&#34;Text mining, the\ntidy way&#34;],&#34;size&#34;:[10.45,3.65,9.55,15.35,14.7,6.9,9.95,14.45,12.55,16.45],&#34;color&#34;:[&#34;lightblue&#34;,&#34;lightgreen&#34;,&#34;lightgreen&#34;,&#34;lightgreen&#34;,&#34;lightgreen&#34;,&#34;lightblue&#34;,&#34;lightblue&#34;,&#34;lightgreen&#34;,&#34;lightblue&#34;,&#34;lightblue&#34;]},&#34;edges&#34;:{&#34;n&#34;:[84,155,168,61,144,137,146,212,201,203,110,188],&#34;from&#34;:[&#34;A Tidy Data Model for Natural Language Processing&#34;,&#34;A Tidy Data Model for Natural Language Processing&#34;,&#34;A Tidy Data Model for Natural Language Processing&#34;,&#34;bradio: Add data music widgets to your business intelligence dashboards&#34;,&#34;Developing and deploying large scale Shiny applications for non-life insurance&#34;,&#34;Developing and deploying large scale Shiny applications for non-life insurance&#34;,&#34;Developing and deploying large scale Shiny applications for non-life insurance&#34;,&#34;How we built a Shiny App for 700 users?&#34;,&#34;How we built a Shiny App for 700 users?&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software&#34;,&#34;Text Analysis and Text Mining Using R&#34;],&#34;to&#34;:[&#34;manifestoR - a tool for data journalists, a source for text miners and a prototype for reproducibility software&#34;,&#34;Text Analysis and Text Mining Using R&#34;,&#34;Text mining, the tidy way&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;How we built a Shiny App for 700 users?&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;ShinyProxy&#34;,&#34;Interacting with databases from Shiny&#34;,&#34;ShinyProxy&#34;,&#34;ShinyProxy&#34;,&#34;Text mining, the tidy way&#34;,&#34;Text mining, the tidy way&#34;],&#34;n_from&#34;:[209,209,209,73,191,191,191,307,307,294,138,251],&#34;n_to&#34;:[138,251,329,294,307,294,289,294,289,289,329,329],&#34;connectivity&#34;:[0.505304763886,0.67957833736823,0.657233024534256,0.521549715776722,0.611491038081757,0.591631940734409,0.634794108588924,0.705821090651244,0.675112428569818,0.696449167902455,0.565723976917316,0.660216277746158],&#34;width&#34;:[5.05304763886,6.7957833736823,6.57233024534256,5.21549715776722,6.11491038081757,5.91631940734409,6.34794108588924,7.05821090651244,6.75112428569818,6.96449167902455,5.65723976917316,6.60216277746158],&#34;color&#34;:[&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;,&#34;grey&#34;]},&#34;nodesToDataframe&#34;:true,&#34;edgesToDataframe&#34;:true,&#34;options&#34;:{&#34;width&#34;:&#34;100%&#34;,&#34;height&#34;:&#34;100%&#34;,&#34;nodes&#34;:{&#34;shape&#34;:&#34;dot&#34;},&#34;manipulation&#34;:{&#34;enabled&#34;:false}},&#34;groups&#34;:null,&#34;width&#34;:&#34;100%&#34;,&#34;height&#34;:null,&#34;idselection&#34;:{&#34;enabled&#34;:false},&#34;byselection&#34;:{&#34;enabled&#34;:false},&#34;main&#34;:null,&#34;submain&#34;:null,&#34;footer&#34;:null,&#34;background&#34;:&#34;rgba(0, 0, 0, 0)&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;I hope you enjoyed this post and I would love you see any and all visualization or analysis you might have regarding this data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Repetition in musicals with tidytext</title>
      <link>/2017/06/05/repetition-in-musicals-with-tidytext/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      <guid>/2017/06/05/repetition-in-musicals-with-tidytext/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Lately I have been wondering how to quantify how repetitive a text is, specifically how repetitive are the lyrics to songs. I‚Äôm by no means the first one, Colin Morris did a great piece on language compression with his &lt;a href=&#34;https://pudding.cool/2017/05/song-repetition/&#34;&gt;Are Pop Lyrics Getting More Repetitive?&lt;/a&gt; which i highly recommend you go read. Instead of looking at pop lyrics will we instead be focusing some popular musicals to see if general patterns emerge within each show.&lt;/p&gt;
&lt;p&gt;My plan is to use the magic of the &lt;code&gt;tidyverse&lt;/code&gt; with the inclusion of &lt;code&gt;tidytext&lt;/code&gt; to find the percentage of repeated consecutive sequences of words() also called n-grams) of varying length and then compare the results. We will use &lt;code&gt;rvest&lt;/code&gt; to extract the lyrics from the web. However for larger data needs official APIs are recommended.&lt;/p&gt;
&lt;div id=&#34;extracting-song-lyrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extracting song lyrics&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2
library(tidytext)
library(rvest)
## Warning: package &amp;#39;xml2&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will first take a peek at a specific song,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_stars_look_down &amp;lt;- &amp;quot;https://genius.com/Elton-john-the-stars-look-down-lyrics&amp;quot;

the_stars_look_down %&amp;gt;%
  read_lines() %&amp;gt;%
  head(20)
##  [1] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
##  [2] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
##  [3] &amp;quot;&amp;lt;!DOCTYPE html&amp;gt;&amp;quot;                                                                                                                                                                                                                                          
##  [4] &amp;quot;&amp;lt;html class=\&amp;quot;snarly apple_music_player--enabled bagon_song_page--enabled song_stories_public_launch--enabled react_forums--disabled\&amp;quot; xmlns=\&amp;quot;http://www.w3.org/1999/xhtml\&amp;quot; xmlns:fb=\&amp;quot;http://www.facebook.com/2008/fbml\&amp;quot; lang=\&amp;quot;en\&amp;quot; xml:lang=\&amp;quot;en\&amp;quot;&amp;gt;&amp;quot;
##  [5] &amp;quot;  &amp;lt;head&amp;gt;&amp;quot;                                                                                                                                                                                                                                                 
##  [6] &amp;quot;    &amp;lt;base target=&amp;#39;_top&amp;#39; href=\&amp;quot;//genius.com/\&amp;quot;&amp;gt;&amp;quot;                                                                                                                                                                                                          
##  [7] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
##  [8] &amp;quot;    &amp;lt;script type=\&amp;quot;text/javascript\&amp;quot;&amp;gt;&amp;quot;                                                                                                                                                                                                                    
##  [9] &amp;quot;//&amp;lt;![CDATA[&amp;quot;                                                                                                                                                                                                                                              
## [10] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
## [11] &amp;quot;  var _sf_startpt=(new Date()).getTime();&amp;quot;                                                                                                                                                                                                                
## [12] &amp;quot;  if (window.performance &amp;amp;&amp;amp; performance.mark) {&amp;quot;                                                                                                                                                                                                          
## [13] &amp;quot;    window.performance.mark(&amp;#39;parse_start&amp;#39;);&amp;quot;                                                                                                                                                                                                              
## [14] &amp;quot;  }&amp;quot;                                                                                                                                                                                                                                                      
## [15] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
## [16] &amp;quot;//]]&amp;gt;&amp;quot;                                                                                                                                                                                                                                                    
## [17] &amp;quot;&amp;lt;/script&amp;gt;&amp;quot;                                                                                                                                                                                                                                                
## [18] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                         
## [19] &amp;quot;&amp;lt;title&amp;gt;Elton¬†John ‚Äì The Stars Look Down Lyrics | Genius Lyrics&amp;lt;/title&amp;gt;&amp;quot;                                                                                                                                                                                   
## [20] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we find a whole of lines of no interest of us, which is to be expected. After some digging I manage to find that the lyrics are packed between &lt;em&gt;p&lt;/em&gt; tags we can do this with &lt;code&gt;rvest&lt;/code&gt;‚Äôs &lt;code&gt;html_nodes()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_stars_look_down_lyrics &amp;lt;- the_stars_look_down %&amp;gt;%
  read_html() %&amp;gt;%
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  tibble(text = .)

the_stars_look_down_lyrics
## # A tibble: 142 x 1
##    text                                                                         
##    &amp;lt;chr&amp;gt;                                                                        
##  1 [Spoken OVERHEAD VOICE]                                                      
##  2 For over seventy years miners of Durham County have come together once a yea‚Ä¶
##  3 [Spoken HERBERT MORRISON]                                                    
##  4 I want you men of the pits to come through. I want these streets clean of na‚Ä¶
##  5 [ENSEMBLE, MINERS]                                                           
##  6 Through the dark, and                                                        
##  7 Through the hunger                                                           
##  8 Through the night and                                                        
##  9 Through the fear                                                             
## 10 Through the fight and                                                        
## # ‚Ä¶ with 132 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We notice some of the rows are indications of who is talking, these are quickly dealt with by a &lt;code&gt;filter&lt;/code&gt;. Now we employ our &lt;code&gt;tidytext&lt;/code&gt; arsenal with &lt;code&gt;unnest_tokens&lt;/code&gt; and we find all the bi-grams (pairs of consecutive words). (notice how they overlap)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_stars_look_down_lyrics %&amp;gt;%
  filter(!str_detect(text, &amp;quot;\\[&amp;quot;)) %&amp;gt;%
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2)
## # A tibble: 719 x 1
##    bigram       
##    &amp;lt;chr&amp;gt;        
##  1 for over     
##  2 over seventy 
##  3 seventy years
##  4 years miners 
##  5 miners of    
##  6 of durham    
##  7 durham county
##  8 county have  
##  9 have come    
## 10 come together
## # ‚Ä¶ with 709 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;now from this we can summarize to find the number of unique bigrams and by extension the percentage of repeated bigrams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_stars_look_down_lyrics %&amp;gt;%
  filter(!str_detect(text, &amp;quot;\\[&amp;quot;)) %&amp;gt;%
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  summarise(length = length(bigram),
            unique = length(unique(bigram))) %&amp;gt;%
  mutate(repetition = 1 - unique / length)
## # A tibble: 1 x 3
##   length unique repetition
##    &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;
## 1    719    447      0.378&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we see that in this particular song around 38% bigrams are present at least twice. We will expect this percentage to be strictly decreasing for &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; increasing, but what we are interested in the both the rate it is decreasing but also the general level.&lt;br /&gt;
Now we generalizing this procedure using some &lt;code&gt;purrr&lt;/code&gt; to give us a nice data.frame out in the end. The range &lt;code&gt;1:5&lt;/code&gt; was picked after some trial and error, and it seemed to me that most trends died out around the 5-6 mark rendering data points over rather uninteresting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songfun &amp;lt;- function(hyperlink, repnum = 1:5) {
  
  lyrics &amp;lt;- hyperlink %&amp;gt;%
  read_html() %&amp;gt;%
  html_nodes(&amp;quot;p&amp;quot;) %&amp;gt;%
  html_text() %&amp;gt;%
  str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  tibble(text = .)
  
  map_dfr(repnum, 
          ~ lyrics %&amp;gt;% 
            unnest_tokens(ngram, text, token = &amp;quot;ngrams&amp;quot;, n = .x) %&amp;gt;%
            summarise(n = .x,
                      length = length(ngram),
                      unique = length(unique(ngram))) %&amp;gt;%
            mutate(repetition = 1 - unique / length,
                   name = hyperlink))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to try this out, we plug in the link again, and pipe the result into ggplot to give us a nice visualization&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songfun(&amp;quot;https://genius.com/Elton-john-the-stars-look-down-lyrics&amp;quot;) %&amp;gt;%
  ggplot(aes(n, repetition)) +
  geom_line() +
  coord_cartesian(ylim = 0:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-06-05-repetition-in-musicals-with-tidytext/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;from this plot alone we can see that roughly 3/4 of the words used in the song are used more then twice, while on the other end of the scale just shy of 25% of the 5-grams are used more then once. This plot by itself doesn‚Äôt provide too much meaningful information by itself. So next step is to gather information for more songs to compare.&lt;/p&gt;
&lt;p&gt;This function takes a link to an album page, and uses similar techniques used earlier to detect the song in the album, find the lyrics with &lt;code&gt;songfun&lt;/code&gt;, process it and spits out a data.frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;albumfun &amp;lt;- function(hlink, ...) { 
  song_links &amp;lt;- tibble(text = readLines(hlink)) %&amp;gt;%
    filter(str_detect(text, &amp;quot;          &amp;lt;a href=\&amp;quot;https://genius.com/&amp;quot;)) %&amp;gt;%
    mutate(text = str_replace(text, &amp;quot;&amp;lt;a href=\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
    mutate(text = str_replace(text, &amp;quot;\&amp;quot; class=\&amp;quot;u-display_block\&amp;quot;&amp;gt;&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
    mutate(text = str_replace(text, &amp;quot; *&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
    mutate(song = str_replace(text, &amp;quot;https://genius.com/&amp;quot;, &amp;quot;&amp;quot;))

  nsongs &amp;lt;- nrow(song_links)
  out &amp;lt;- tibble()
  for (i in 1:nsongs) {
    ting &amp;lt;- songfun(song_links$text[i], ...)
    out &amp;lt;- rbind(out, ting)
  }
  out %&amp;gt;%
    mutate(album = hlink)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;We use our function to get the data for a number of different musicals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;billyelliot &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Elton-john/Billy-elliot-the-musical-original-london-cast-recording&amp;quot;)
thebookofmormon &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Book-of-mormon/The-book-of-mormon-original-broadway-recording&amp;quot;)
lionking &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/The-lion-king/The-lion-king-original-broadway-cast-recording&amp;quot;)
avenueq &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Robert-lopez-and-jeff-marx/Avenue-q-original-broadway-cast-recording&amp;quot;)
oklahoma &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Richard-rodgers/Oklahoma-original-motion-picture-soundtrack&amp;quot;)
soundofmusic &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Richard-rodgers/The-sound-of-music-original-soundtrack-recording&amp;quot;)
intheheights &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Lin-manuel-miranda/In-the-heights-original-broadway-cast-recording&amp;quot;)
lemiserables &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Les-miserables-original-broadway-cast/Les-miserables-1987-original-broadway-cast&amp;quot;)
phantomoftheopera &amp;lt;- albumfun(hlink = &amp;quot;https://genius.com/albums/Andrew-lloyd-webber/The-phantom-of-the-opera-original-london-cast-recording&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and a quick explorative plot tells us that it is working as intended, we see some variation both slopes and offset, telling us that Billy Elliot have some range in its songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;billyelliot %&amp;gt;%
  ggplot(aes(n, repetition)) +
  geom_line(aes(group = name)) +
  labs(title = &amp;quot;Billy Elliot&amp;quot;) +
  coord_cartesian(ylim = 0:1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-06-05-repetition-in-musicals-with-tidytext/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;to further compare we bind all the data.frames together for ease of handling&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;musical_names &amp;lt;- c(&amp;quot;The Phantom of the Opera&amp;quot;, &amp;quot;The Book of Mormon&amp;quot;, 
                   &amp;quot;Billy Elliot&amp;quot;, &amp;quot;Les Miserables&amp;quot;, &amp;quot;In the Heights&amp;quot;, 
                   &amp;quot;Oklahoma&amp;quot;, &amp;quot;The Sound of music&amp;quot;, &amp;quot;Avenue Q&amp;quot;, &amp;quot;Lion King&amp;quot;)

rbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma,
      soundofmusic, intheheights, lemiserables, phantomoftheopera) %&amp;gt;%
  mutate(album = factor(album, label = musical_names)) %&amp;gt;%
  ggplot(aes(n, repetition)) +
  geom_line(aes(group = name), alpha = 0.5) +
  facet_wrap(~ album)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-06-05-repetition-in-musicals-with-tidytext/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wow, here we clearly see some differences in lyrical styles for the different musical, from the evenness of the soundtrack to ‚ÄúIn the Heights‚Äù to the range of ‚ÄúLion King‚Äù. To try having them all in the same graph would be overwhelming. However we could still plot the trend of each album in the same plot, fading out individual songs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(billyelliot, thebookofmormon, lionking, avenueq, oklahoma,
      soundofmusic, intheheights, lemiserables, phantomoftheopera) %&amp;gt;%
  ggplot(aes(n, repetition)) +
  coord_cartesian(ylim = 0:1) +
  geom_line(aes(group = name), alpha = 0.05) +
  geom_smooth(aes(group = album, color = album), se = FALSE) +
  theme_bw() +
  labs(title = &amp;quot;Repetition in musicals&amp;quot;) +
  scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;,
                      name = &amp;quot;Musical&amp;quot;,
                      labels = c(&amp;quot;The Phantom of the Opera&amp;quot;, &amp;quot;The Book of Mormon&amp;quot;, 
                                 &amp;quot;Billy Elliot&amp;quot;, &amp;quot;Les Miserables&amp;quot;,
                                 &amp;quot;In the Heights&amp;quot;, &amp;quot;Oklahoma&amp;quot;, 
                                 &amp;quot;The Sound of music&amp;quot;, &amp;quot;Avenue Q&amp;quot;, &amp;quot;Lion King&amp;quot;))
## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-06-05-repetition-in-musicals-with-tidytext/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2017 World Press Freedom Index with emojis</title>
      <link>/2017/04/26/2017-world-press-freedom-index-with-emojis/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/2017/04/26/2017-world-press-freedom-index-with-emojis/</guid>
      <description>


&lt;p&gt;&lt;em&gt;This code have been lightly revised to make sure it works as of 2018-12-16.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With &lt;a href=&#34;https://rsf.org/en&#34;&gt;Reporters Without Borders&lt;/a&gt; coming out with its &lt;a href=&#34;https://rsf.org/en/ranking/2017&#34;&gt;2017 World Press Freedom Index&lt;/a&gt; in the same week as Hadley Wickham coming out with the &lt;a href=&#34;https://github.com/hadley/emo&#34;&gt;emo(ji)&lt;/a&gt; package, I had no choice but to explore both of them at the same time.&lt;/p&gt;
&lt;p&gt;Disclaimer! This post is not an exercise in statistical inference but rather a proof of concept of how to use the emo(ji) package with ggplot2.&lt;/p&gt;
&lt;div id=&#34;loading-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(hrbrthemes)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
library(tibble)
# remotes::install_github(&amp;#39;hadley/emo&amp;#39;)
library(emo)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/hrbrmstr/hrbrthemes&#34;&gt;hrbrthemes&lt;/a&gt; is not necessary for this project but it is one of my personal favorite ggplot2 themes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gathering-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gathering data&lt;/h3&gt;
&lt;p&gt;Here we collect the data from Reporters Without Borders, emoji flags and The World Bank (so we have something to plot against).&lt;/p&gt;
&lt;div id=&#34;world-press-freedom-index&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2017 World Press Freedom Index&lt;/h4&gt;
&lt;p&gt;We have the &lt;a href=&#34;https://rsf.org/en/ranking/2017&#34;&gt;2017 World Press Freedom Index&lt;/a&gt; &lt;a href=&#34;https://rsf.org/sites/default/files/index_format_upload_2017-v2_1_0.csv&#34;&gt;(direct download link)&lt;/a&gt; data which we load in as normal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(freedom_index &amp;lt;- read_csv(&amp;quot;https://rsf.org/sites/default/files/index_format_upload_2017-v2_1_0.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 180 x 12
##    ISO    Rank FR_Country EN_country ES_country `Underlying sit‚Ä¶
##    &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;
##  1 NOR       1 Norv√®ge    Norway     Noruega                 760
##  2 SWE       2 Suede      Sweden     Suecia                  759
##  3 FIN       3 Finlande   Finland    Finlandia               892
##  4 DNK       4 Danemark   Denmark    Dinamarca              1036
##  5 NLD       5 Pays-Bas   Netherlan‚Ä¶ Pa√≠ses Ba‚Ä¶              963
##  6 CRI       6 Costa Rica Costa Rica Costa Rica             1193
##  7 CHE       7 Suisse     Switzerla‚Ä¶ Suiza                  1213
##  8 JAM       8 Jama√Øque   Jamaica    Jamaica                1273
##  9 BEL       9 Belgique   Belgium    B√©lgica                1247
## 10 ISL      10 Islande    Iceland    Islandia               1303
## # ‚Ä¶ with 170 more rows, and 6 more variables: `Abuse score 2016` &amp;lt;chr&amp;gt;,
## #   `Overall Score 2016` &amp;lt;dbl&amp;gt;, `Progression RANK` &amp;lt;dbl&amp;gt;, `Rank 2015` &amp;lt;dbl&amp;gt;,
## #   `Score 2015` &amp;lt;dbl&amp;gt;, Zone &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we see that a total of 180 countries have a score (Overall Score 2016).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gdp-per-capita&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;GDP per capita&lt;/h4&gt;
&lt;p&gt;To have something somehow meaningful to compare the freedom index to. I‚Äôve found some data about GDP per capita, mostly because I figured it would have data for quite a lot of the countries covered by the freedom index. So from &lt;a href=&#34;http://data.worldbank.org/indicator/NY.GDP.PCAP.CD&#34;&gt;The World Bank&lt;/a&gt; &lt;a href=&#34;http://api.worldbank.org/v2/en/indicator/NY.GDP.PCAP.CD?downloadformat=csv&#34;&gt;(direct download link)&lt;/a&gt;
which we load in as normal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(gdp_pcap &amp;lt;- read_csv(&amp;quot;API_NY.GDP.PCAP.CD_DS2_en_csv_v2.csv&amp;quot;, skip = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which have quite a few variables but for now we will just focus on the 2015 variable as the 2016 appears empty. Now that we have two data sets which we would like to combine, a general question would be if the gdp_pcap data have information matching our 180 countries. So with the following bit of code we join the two datasets together by the ICO ALPHA-3 Code available in both datasets and select the countries who don‚Äôt have a value for the year 2015.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(freedom_index, gdp_pcap, by = c(&amp;quot;ISO&amp;quot; = &amp;quot;Country Code&amp;quot;)) %&amp;gt;% 
  filter(is.na(`2015`)) %&amp;gt;% 
  select(EN_country)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 1
##    EN_country                           
##    &amp;lt;chr&amp;gt;                                
##  1 Liechtenstein                        
##  2 Andorra                              
##  3 OECS                                 
##  4 Taiwan                               
##  5 Papua New Guinea                     
##  6 Cyprus North                         
##  7 Kosovo                               
##  8 Venezuela                            
##  9 Libya                                
## 10 Syrian Arab Republic                 
## 11 Eritrea                              
## 12 Democratic People&amp;#39;s Republic of Korea&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which leaves us with 166 countries. I could have looked for the data for these countries, but that is outside the reach for this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flag-emoji&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Flag emoji&lt;/h4&gt;
&lt;p&gt;I would like to use the different flag emojis&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## üá¶üá®üá¶üá©üá¶üá™üá¶üá´üá¶üá¨üá¶üáÆüá¶üá±üá¶üá≤üá¶üá¥üá¶üá∂üá¶üá∑üá¶üá∏üá¶üáπüá¶üá∫üá¶üáºüá¶üáΩüá¶üáøüáßüá¶üáßüáßüáßüá©üáßüá™üáßüá´üáßüá¨üáßüá≠üáßüáÆüáßüáØüáßüá±üáßüá≤üáßüá≥üáßüá¥üáßüá∂üáßüá∑üáßüá∏üáßüáπüáßüáªüáßüáºüáßüáæüáßüáøüá®üá¶üá®üá®üá®üá©üá®üá´üá®üá¨üá®üá≠üá®üáÆüá®üá∞üá®üá±üá®üá≤üá®üá≥üá®üá¥üá®üáµüá®üá∑üá®üá∫üá®üáªüá®üáºüá®üáΩüá®üáæüá®üáøüá©üá™üá©üá™üá©üá¨üá©üáØüá©üá∞üá©üá≤üá©üá¥üá©üáøüá™üá¶üá™üá®üá™üá™üá™üá¨üá™üá≠üá™üá∑üá™üá∏üá™üáπüá™üá∫üá´üáÆüá´üáØüá´üá∞üá´üá≤üá´üá¥üá´üá∑üá¨üá¶üá¨üáßüá¨üáßüá¨üá©üá¨üá™üá¨üá´üá¨üá¨üá¨üá≠üá¨üáÆüá¨üá±üá¨üá≤üá¨üá≥üá¨üáµüá¨üá∂üá¨üá∑üá¨üá∏üá¨üáπüá¨üá∫üá¨üáºüá¨üáæüá≠üá∞üá≠üá≤üá≠üá≥üá≠üá∑üá≠üáπüá≠üá∫üáÆüá®üáÆüá©üáÆüá™üáÆüá±üáÆüá≤üáÆüá≥üáÆüá¥üáÆüá∂üáÆüá∑üáÆüá∏üáÆüáπüáØüá™üáØüá≤üáØüá¥üáØüáµüá∞üá™üá∞üá¨üá∞üá≠üá∞üáÆüá∞üá≤üá∞üá≥üá∞üáµüá∞üá∑üá∞üáºüá∞üáæüá∞üáøüá±üá¶üá±üáßüá±üá®üá±üáÆüá±üá∞üá±üá∑üá±üá∏üá±üáπüá±üá∫üá±üáªüá±üáæüá≤üá¶üá≤üá®üá≤üá©üá≤üá™üá≤üá´üá≤üá¨üá≤üá≠üá≤üá∞üá≤üá±üá≤üá≤üá≤üá≥üá≤üá¥üá≤üáµüá≤üá∂üá≤üá∑üá≤üá∏üá≤üáπüá≤üá∫üá≤üáªüá≤üáºüá≤üáΩüá≤üáæüá≤üáøüá≥üá¶üá≥üá®üá≥üá™üá≥üá´üá≥üá¨üá≥üáÆüá≥üá±üá≥üá¥üá≥üáµüá≥üá∑üá≥üá∫üá≥üáøüá¥üá≤üáµüá¶üáµüá™üáµüá´üáµüá¨üáµüá≠üáµüá∞üáµüá±üáµüá≤üáµüá≥üáµüá∑üáµüá∏üáµüáπüáµüáºüáµüáæüá∂üá¶üá∑üá™üá∑üá¥üá∑üá∏üá∑üá∫üá∑üáºüá∏üá¶üá∏üáßüá∏üá®üá∏üá©üá∏üá™üá∏üá¨üá∏üá≠üá∏üáÆüá∏üáØüá∏üá∞üá∏üá±üá∏üá≤üá∏üá≥üá∏üá¥üá∏üá∑üá∏üá∏üá∏üáπüá∏üáªüá∏üáΩüá∏üáæüá∏üáøüáπüá¶üáπüá®üáπüá©üáπüá´üáπüá¨üáπüá≠üáπüáØüáπüá∞üáπüá±üáπüá≤üáπüá≥üáπüá¥üáπüá∑üáπüáπüáπüáªüáπüáºüáπüáøüá∫üá¶üá∫üá¨üá∫üá≤üá∫üá≥üá∫üá∏üá∫üá∏üá∫üáæüá∫üáøüáªüá¶üáªüá®üáªüá™üáªüá¨üáªüáÆüáªüá≥üáªüá∫üáºüá´üáºüá∏üáΩüá∞üáæüá™üáæüáπüáøüá¶üáøüá≤üáøüáºüè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åøüè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åøüè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which all can be found with the new emo(ji) package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emo::ji_find(&amp;quot;flag&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 264 x 2
##    name                 emoji
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;
##  1 Ascension_Island     üá¶üá®   
##  2 andorra              üá¶üá©   
##  3 united_arab_emirates üá¶üá™   
##  4 afghanistan          üá¶üá´   
##  5 antigua_barbuda      üá¶üá¨   
##  6 anguilla             üá¶üáÆ   
##  7 albania              üá¶üá±   
##  8 armenia              üá¶üá≤   
##  9 angola               üá¶üá¥   
## 10 antarctica           üá¶üá∂   
## # ‚Ä¶ with 254 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we first notice that the first two emojis are not country flags, and that the name of the countries are not on same format as what we have from earlier, so we replace the underscores with spaces and translate everything to lowercase before joining. This time by country name. Again we check for missed joints.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(freedom_index, gdp_pcap, by = c(&amp;quot;ISO&amp;quot; = &amp;quot;Country Code&amp;quot;)) %&amp;gt;% 
  mutate(EN_country = tolower(EN_country)) %&amp;gt;% 
  left_join(emo::ji_find(&amp;quot;flag&amp;quot;) %&amp;gt;% 
              mutate(name = str_replace_all(name, &amp;quot;_&amp;quot;, &amp;quot; &amp;quot;)) %&amp;gt;% 
              filter(name != &amp;quot;japan&amp;quot;, name != &amp;quot;crossed flags&amp;quot;), 
            by = c(&amp;quot;EN_country&amp;quot; = &amp;quot;name&amp;quot;))  %&amp;gt;% 
  filter(!is.na(`2015`)) %&amp;gt;% 
  filter(is.na(emoji)) %&amp;gt;% 
  select(EN_country)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 22 x 1
##    EN_country            
##    &amp;lt;chr&amp;gt;                 
##  1 germany               
##  2 spain                 
##  3 trinidad and tobago   
##  4 france                
##  5 united kingdom        
##  6 united states         
##  7 italy                 
##  8 south korea           
##  9 bosnia and herzegovina
## 10 japan                 
## # ‚Ä¶ with 12 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is quite a few. It turns out that the naming convention for the emoji names have not been that consistent, ‚Äúde‚Äù used instead of ‚Äúgermany‚Äù etc. To clear up code later on we make a new emoji tibble with all the changes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newemoji &amp;lt;- emo::ji_find(&amp;quot;flag&amp;quot;) %&amp;gt;% 
              mutate(name = str_replace_all(string = name,
                                            pattern = &amp;quot;_&amp;quot;,
                                            replacement =  &amp;quot; &amp;quot;)) %&amp;gt;% 
  filter(name != &amp;quot;japan&amp;quot;, name != &amp;quot;crossed flags&amp;quot;) %&amp;gt;% 
  mutate(name = str_replace(name, &amp;quot;^de$&amp;quot;, &amp;quot;germany&amp;quot;),
         name = str_replace(name, &amp;quot;^es$&amp;quot;, &amp;quot;spain&amp;quot;),
         name = str_replace(name, &amp;quot;^trinidad tobago$&amp;quot;, &amp;quot;trinidad and tobago&amp;quot;),
         name = str_replace(name, &amp;quot;^fr$&amp;quot;, &amp;quot;france&amp;quot;),
         name = str_replace(name, &amp;quot;^uk$&amp;quot;, &amp;quot;united kingdom&amp;quot;),
         name = str_replace(name, &amp;quot;^us$&amp;quot;, &amp;quot;united states&amp;quot;),
         name = str_replace(name, &amp;quot;^it$&amp;quot;, &amp;quot;italy&amp;quot;),
         name = str_replace(name, &amp;quot;^kr$&amp;quot;, &amp;quot;south korea&amp;quot;),
         name = str_replace(name, &amp;quot;^bosnia herzegovina$&amp;quot;, &amp;quot;bosnia and herzegovina&amp;quot;),
         name = str_replace(name, &amp;quot;^guinea bissau$&amp;quot;, &amp;quot;guinea-bissau&amp;quot;),
         name = str_replace(name, &amp;quot;^cote divoire$&amp;quot;, &amp;quot;ivory coast&amp;quot;),
         name = str_replace(name, &amp;quot;^timor leste$&amp;quot;, &amp;quot;east timor&amp;quot;),
         name = str_replace(name, &amp;quot;^congo brazzaville$&amp;quot;, &amp;quot;congo&amp;quot;),
         name = str_replace(name, &amp;quot;^palestinian territories$&amp;quot;, &amp;quot;palestine&amp;quot;),
         name = str_replace(name, &amp;quot;^ru$&amp;quot;, &amp;quot;russian federation&amp;quot;),
         name = str_replace(name, &amp;quot;^congo kinshasa$&amp;quot;, &amp;quot;the democratic republic of the congo&amp;quot;),
         name = str_replace(name, &amp;quot;^tr$&amp;quot;, &amp;quot;turkey&amp;quot;),
         name = str_replace(name, &amp;quot;^brunei$&amp;quot;, &amp;quot;brunei darussalam&amp;quot;),
         name = str_replace(name, &amp;quot;^laos$&amp;quot;, &amp;quot;lao people&amp;#39;s democratic republic&amp;quot;),
         name = str_replace(name, &amp;quot;^cn$&amp;quot;, &amp;quot;china&amp;quot;),
         name = str_replace(name, &amp;quot;^jp$&amp;quot;, &amp;quot;japan&amp;quot;))
newemoji&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 264 x 2
##    name                 emoji
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;
##  1 Ascension Island     üá¶üá®   
##  2 andorra              üá¶üá©   
##  3 united arab emirates üá¶üá™   
##  4 afghanistan          üá¶üá´   
##  5 antigua barbuda      üá¶üá¨   
##  6 anguilla             üá¶üáÆ   
##  7 albania              üá¶üá±   
##  8 armenia              üá¶üá≤   
##  9 angola               üá¶üá¥   
## 10 antarctica           üá¶üá∂   
## # ‚Ä¶ with 254 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-it-all-with-ggplot2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting it all with ggplot2&lt;/h3&gt;
&lt;p&gt;Now with all the preparation done we do a naive first plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(freedom_index, gdp_pcap, by = c(&amp;quot;ISO&amp;quot; = &amp;quot;Country Code&amp;quot;)) %&amp;gt;% 
  mutate(EN_country = tolower(EN_country)) %&amp;gt;% 
  left_join(newemoji, by = c(&amp;quot;EN_country&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = `2015`, y = `Overall Score 2016`)) +
  geom_text(aes(label = emoji))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 14 rows containing missing values (geom_text).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-04-26-2017-world-press-freedom-index-with-emojis/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But wait, we have a couple of problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The emojis don‚Äôt show up.&lt;/li&gt;
&lt;li&gt;The freedom score is 100 times to much as the actual.&lt;/li&gt;
&lt;li&gt;The gdp_pcap is quite skewed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But these are not problems too great for us. It turns out that R‚Äôs graphical devices don‚Äôt support AppleColorEmoji font. We can alleviate the that problem by saving the plot as a svg file. And we will do a simple log transformation of the gdp_pcap.&lt;/p&gt;
&lt;p&gt;Our final plot is thus the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(freedom_index, gdp_pcap, by = c(&amp;quot;ISO&amp;quot; = &amp;quot;Country Code&amp;quot;)) %&amp;gt;% 
  mutate(EN_country = tolower(EN_country),
         `Overall Score 2016` = `Overall Score 2016` / 100) %&amp;gt;% 
  left_join(newemoji, by = c(&amp;quot;EN_country&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = `2015`, y = `Overall Score 2016`)) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;grey&amp;quot;, se = FALSE) +
  geom_text(aes(label = emoji)) +
  scale_x_log10() +
  annotation_logticks(sides = &amp;quot;b&amp;quot;)  +
  theme_ipsum() +
  labs(x = &amp;quot;GDP per capita (current US$)&amp;quot;, y = &amp;quot;2017 World Press Freedom Index&amp;quot;,
       title = &amp;quot;Countries with high GDP per capita\ntend to have low Freedom Index&amp;quot;,
       subtitle = &amp;quot;Visualized with emojis&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;final.svg&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
